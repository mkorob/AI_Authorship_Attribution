,Author,Abstract,GPT_abstract,GPT_introduction,GPT_conclusion
0,Aman Madaan,"Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%.2","In the landscape of cloud-based Large Language Models (LLMs), the array of sizes and configurations available presents a challenge in effectively balancing computational cost and performance optimization. This paper introduces AutoMix, an innovative approach designed to strategically route queries to larger LLMs based on the approximate correctness of outputs from a smaller LM. At its core, AutoMix incorporates a few-shot self-verification mechanism, allowing it to estimate the reliability of its own outputs without the need for additional training. To address the inherent noise in verifications, AutoMix employs a meta verifier, enhancing the accuracy of these assessments. Experimental results using LLAMA2-13/70B on five context-grounded reasoning datasets illustrate that AutoMix outperforms established baselines, yielding an up to 89% improvement in the incremental benefit per cost.","The availability of diverse Large Language Models (LLMs) through cloud API providers offers a spectrum of choices, but the challenge lies in efficiently utilizing these options to strike a balance between computational cost and performance optimization. This work presents AutoMix, a novel approach addressing this challenge by intelligently directing queries to larger LLMs based on the reliability of outputs from a smaller LM. The key innovation lies in a few-shot self-verification mechanism, eliminating the need for extensive training to estimate output reliability. Recognizing the potential noise in verifications, AutoMix incorporates a meta verifier to refine the accuracy of these assessments. The experimental evaluation, conducted on LLAMA2-13/70B across five context-grounded reasoning datasets, demonstrates the superiority of AutoMix over established baselines, showcasing a remarkable up to 89% enhancement in the incremental benefit per cost.","AutoMix emerges as a promising solution in the realm of cloud-based LLMs, providing a strategic means to balance computational cost and performance. By leveraging a few-shot self-verification mechanism, the system accurately gauges the reliability of its outputs, eliminating the need for extensive training. The incorporation of a meta verifier further enhances the precision of these assessments, addressing the inherent noise in the verification process. Experimental results, conducted on LLAMA2-13/70B across context-grounded reasoning datasets, affirm the effectiveness of AutoMix. Notably, the approach surpasses established baselines, showcasing a substantial up to 89% improvement in the incremental benefit per cost. AutoMix stands as a valuable contribution in the pursuit of optimizing the utilization of cloud-based LLMs, promising a more cost-effective and performance-driven approach in natural language processing tasks."
1,Aman Madaan,"Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ‚àº20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.","Large Language Models (LLMs) often fall short in generating optimal outputs on their initial attempts. Drawing inspiration from the human iterative refinement process, we introduce SELF-REFINE, a novel approach for enhancing LLM outputs through iterative feedback and self-improvement. The core concept involves generating an initial output using an LLM, then utilizing the same model to provide feedback on its output and iteratively refine itself. This method requires no additional supervised training data, extra training, or reinforcement learning, relying solely on a single LLM to serve as the generator, refiner, and feedback provider. We assess SELF-REFINE across a diverse set of 7 tasks, spanning from dialog response generation to mathematical reasoning, employing state-of-the-art LLMs such as GPT-3.5 and GPT-4. The results demonstrate that outputs produced with SELF-REFINE are preferred by both human evaluators and automatic metrics compared to outputs generated using conventional one-step generation. On average, task performance improves by approximately 20% absolute across all evaluated tasks. Our findings underscore the potential for further enhancing even state-of-the-art LLMs, like GPT-4, at test-time through the adoption of our straightforward, standalone approach.","Large Language Models (LLMs) have exhibited remarkable capabilities in natural language processing tasks. However, their performance is not infallible, often producing suboptimal outputs in the first attempt. This motivates the development of SELF-REFINE, an innovative approach aimed at refining LLM outputs through an iterative feedback loop. Inspired by human writing refinement processes, SELF-REFINE leverages a single LLM for the entire feedback and improvement pipeline. Unlike existing methods, our approach circumvents the need for additional supervised training data, specialized training, or reinforcement learning. In this paper, we explore the efficacy of SELF-REFINE across a spectrum of 7 diverse tasks, spanning conversational contexts to intricate mathematical reasoning. Utilizing cutting-edge LLMs, including GPT-3.5 and GPT-4, we present compelling evidence that SELF-REFINE consistently outperforms conventional one-step generation, showcasing an average improvement of around 20% in task performance. Our work underscores the untapped potential for refining even the most advanced LLMs at test-time with a straightforward, standalone strategy.","In this study, we introduced SELF-REFINE, a novel approach to enhance the outputs of Large Language Models (LLMs) through an iterative feedback and refinement process. By mimicking the iterative refinement process observed in human writing, SELF-REFINE utilizes a single LLM as the generator, refiner, and feedback provider, eliminating the need for additional training or specialized data. Across seven diverse tasks, ranging from dialog response generation to mathematical reasoning, we demonstrated the effectiveness of SELF-REFINE using state-of-the-art LLMs like GPT-3.5 and GPT-4. Our results consistently revealed that outputs generated with SELF-REFINE outperformed those from conventional one-step generation, with an average improvement of approximately 20% in task performance according to both human evaluations and automatic metrics. This research highlights the potential for further improving the capabilities of even the most advanced LLMs at test-time through the adoption of a simple and standalone approach like SELF-REFINE."
2,Aman Madaan,"Machine learning systems typically apply the
same model to both easy and tough cases. This is
in stark contrast with humans, who tend to evoke
either fast (instinctive) or slow (analytical) thinking process, depending on the difficulty of the
problem‚Äîa property called the dual-process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual-process
theory of mind. Depending on the difficulty of
graph completion at the current step, the system either calls a FAST (weaker) module or a
SLOW (stronger) module for the task. These modules have identical architectures, but vary in the
number of parameters and consequently differ
in generative power. Experiments on real-world
graphs show that FLOWGEN can successfully generate graphs similar to those generated by a single
large model, while being up to 2x faster.
","Machine learning systems often lack the adaptive cognitive mechanisms observed in humans, employing a uniform model for both easy and challenging tasks. In contrast, humans engage in either fast (instinctive) or slow (analytical) thinking processes based on the problem's difficulty‚Äîa phenomenon known as the dual-process theory of mind. In this paper, we introduce FLOWGEN, a graph generation model inspired by the dual-process theory. FLOWGEN adapts to the complexity of graph completion by employing either a FAST (weaker) or SLOW (stronger) module. While these modules share architectures, they differ in parameter count and generative power. Through experiments on real-world graphs, FLOWGEN demonstrates its ability to generate graphs comparable to those produced by a single large model, all while achieving up to a 2x speed improvement.","Machine learning models commonly lack the nuanced adaptability demonstrated by human cognitive processes. Humans instinctively switch between fast and slow thinking modes based on the difficulty of a task, as articulated in the dual-process theory of mind. Addressing this disparity, we present FLOWGEN, a novel graph generation model inspired by the dual-process theory. FLOWGEN incorporates both FAST (weaker) and SLOW (stronger) modules, sharing architectural similarities but diverging in parameter count and generative capabilities. The model dynamically selects the appropriate module based on the difficulty of the current graph completion step. Through empirical evaluations on real-world graphs, FLOWGEN exhibits the capability to generate graphs comparable to those produced by a single large model, with the added advantage of being up to 2x faster.","In this work, we introduced FLOWGEN, a graph generation model that embraces the dual-process theory of mind, allowing for adaptive responses to varying task complexities. FLOWGEN employs either a FAST (weaker) or SLOW (stronger) module, each characterized by identical architectures but differing in parameter count and generative power. Our experiments on real-world graphs underscore the effectiveness of FLOWGEN, showcasing its ability to generate graphs akin to those produced by a single large model. Notably, FLOWGEN achieves this while demonstrating a speed improvement of up to 2x. This research contributes to bridging the gap between machine learning models and human cognitive processes, offering a promising avenue for developing more adaptive and efficient graph generation systems."
3,Aman Madaan,"We propose a method of curating high-quality comparable training data
for low-resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the
source and target languages by getting captions for such images in both
languages independently. Human evaluations on the English-Hindi comparable corpora created with our method show that 81.1% of the pairs
are acceptable translations, and only 2.47% of the pairs are not translations at all. We further establish the potential of the dataset collected
through our approach by experimenting on two downstream tasks ‚Äì machine translation and dictionary extraction. All code and data are available
at https://github.com/madaan/PML4DC-Comparable-Data-Collection","In this paper, we present an innovative method for curating high-quality comparable training data tailored for low-resource languages, employing monolingual annotators. Our approach centers around the strategic use of a meticulously chosen set of images, serving as a pivot between source and target languages. We independently collect captions for these images in both languages, thereby establishing a robust and comparable corpus. Human evaluations conducted on the English-Hindi comparable corpora generated through our method reveal a noteworthy 81.1% acceptability rate for translation pairs, with only 2.47% identified as non-translations. To underscore the utility of the dataset, we conduct experiments on two downstream tasks‚Äîmachine translation and dictionary extraction. The outcomes highlight the potential of our curated dataset. All code and data are openly accessible at https://github.com/madaan/PML4DC-Comparable-Data-Collection.","Training data scarcity remains a formidable challenge in low-resource language domains. In response, we propose a novel method for curating high-quality comparable training data, leveraging the capabilities of monolingual annotators. Our methodology revolves around the selection of a judicious set of images that act as linguistic pivots between source and target languages. By independently eliciting captions for these images in both languages, we establish a robust and comparable corpus. To assess the efficacy of our approach, we conduct human evaluations on the English-Hindi comparable corpora, revealing an impressive 81.1% acceptability rate for translation pairs, with a mere 2.47% identified as non-translations. Additionally, we explore the potential of the curated dataset through experiments on two downstream tasks‚Äîmachine translation and dictionary extraction.","In this work, we introduced a method for curating high-quality comparable training data specifically designed for low-resource languages, utilizing monolingual annotators. The strategic use of carefully chosen images as linguistic pivots between source and target languages proved effective, as evidenced by human evaluations on the English-Hindi comparable corpora. Our method demonstrated an impressive 81.1% acceptability rate for translation pairs, with a minimal 2.47% identified as non-translations. To underscore the practical utility of our curated dataset, we conducted experiments on two downstream tasks‚Äîmachine translation and dictionary extraction‚Äîrevealing promising outcomes. The availability of all code and data at https://github.com/madaan/PML4DC-Comparable-Data-Collection emphasizes the transparency and reproducibility of our approach, contributing a valuable resource for researchers in low-resource language domains."
4,Aman Madaan,"This paper introduces a new task of politeness
transfer which involves converting non-polite
sentences to polite sentences while preserving
the meaning. We also provide a dataset of
more than 1.39 million instances automatically
labeled for politeness to encourage benchmark
evaluations on this new task. We design a tag
and generate pipeline that identifies stylistic attributes and subsequently generates a sentence
in the target style while preserving most of the
source content. For politeness as well as five
other transfer tasks, our model outperforms the
state-of-the-art methods on automatic metrics
for content preservation, with a comparable
or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer
accuracy across all the six style transfer tasks.
The data and code is located at https://
github.com/tag-and-generate/
","This paper introduces a novel task in the domain of natural language processing ñ the task of politeness transfer. The objective is to convert non-polite sentences into polite equivalents while retaining the original meaning. To facilitate research in this area, we present a comprehensive dataset comprising over 1.39 million instances, meticulously labeled for politeness. This dataset serves as a foundation for benchmark evaluations, encouraging advancements in politeness transfer techniques. Our proposed approach involves a tag-and-generate pipeline, which first identifies stylistic attributes and then generates a sentence in the target style while preserving the majority of the source content. Extensive experiments demonstrate that our model outperforms existing state-of-the-art methods on automatic metrics related to content preservation, achieving comparable or superior performance in style transfer accuracy. Moreover, our model excels in human evaluations, demonstrating superior grammaticality, meaning preservation, and transfer accuracy across not only the politeness transfer task but also five other style transfer tasks. The code and dataset are made publicly available, fostering collaboration and further research in this emerging field.","In the realm of natural language processing, this paper pioneers the task of politeness transfer ñ a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 1.39 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag-and-generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state-of-the-art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open-sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research.","In conclusion, this paper introduces and addresses the novel task of politeness transfer, providing a substantial dataset and a robust tag-and-generate pipeline to facilitate research in this burgeoning field of natural language processing. Our model not only outperforms state-of-the-art methods on automatic metrics related to content preservation but also demonstrates superior performance in human evaluations, showcasing its effectiveness in achieving grammaticality, meaning preservation, and transfer accuracy across a diverse set of style transfer tasks. By making our code and dataset publicly available, we encourage collaboration and invite the community to build upon our work, fostering advancements in politeness transfer and broader style transfer applications. The presented contributions aim to propel the field forward, offering valuable insights and tools for researchers and practitioners alike."
5,Aman Madaan,"Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a ‚Äúwhat-if‚Äù Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning","Understanding processes involves the critical task of reasoning about events and tracking their influences. In this paper, we introduce EIGEN, a novel method harnessing pre-trained language models to generate event influences based on context, the nature of influence, and their position in a reasoning chain. We present a new dataset tailored for researching and evaluating event influence generation methods. EIGEN surpasses robust baselines, demonstrating superior performance in automated evaluation metrics, achieving a remarkable 10 ROUGE points improvement. Human judgments affirm the excellence of EIGEN in terms of both closeness to reference and the relevance of generated content. Additionally, we showcase that the event influences produced by EIGEN significantly enhance performance on a ""what-if"" Question Answering (WIQA) benchmark, exhibiting an impressive over 3% increase in F1 score. This improvement is particularly notable for questions demanding background knowledge and multi-hop reasoning.","Comprehending dynamic processes necessitates the ability to analyze events and comprehend their intricate interconnections. This paper introduces EIGEN, an innovative approach leveraging pre-trained language models for the generation of event influences. By considering context, the nature of influence, and the position in a reasoning chain, EIGEN excels in producing high-quality event influence representations. We address the current gap in research with the introduction of a dedicated dataset, providing a standardized platform for evaluating methods in the realm of event influence generation. Through rigorous evaluation against strong baselines, EIGEN emerges as a superior solution, not only in terms of automated metrics but also in human assessments of generated content, affirming its effectiveness in capturing both reference closeness and relevance.","In conclusion, EIGEN stands as a robust and effective method for event influence generation, showcasing advancements in both automated evaluation metrics and human judgment. The introduced dataset serves as a valuable resource for future research in this domain. The demonstrated enhancement of performance on the WIQA benchmark, especially for questions involving background knowledge and multi-hop reasoning, underlines the practical utility of EIGEN. As the understanding of event influences plays a pivotal role in various applications, ranging from natural language understanding to decision support systems, EIGEN contributes significantly to advancing the state-of-the-art in this field."
6,Hugo Touvron,"Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers
has been little studied so far. In this work, we build and optimize deeper
transformer networks for image classification. In particular, we investigate
the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models
whose performance does not saturate early with more depth, for instance
we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters.
Moreover, our best model establishes the new state of the art on Imagenet
with Reassessed labels and Imagenet-V2 / match frequency, in the setting
with no additional training data. We share our code and models.","Recent advancements in adapting transformers for large-scale image classification have challenged the longstanding dominance of convolutional neural networks (CNNs). This paper focuses on building and optimizing deeper transformer networks specifically tailored for image classification tasks. Through a thorough exploration of the interplay between architecture and optimization strategies, two key modifications are introduced, resulting in a significant improvement in the accuracy of deep transformers. These architectural enhancements not only prevent early saturation of performance with increased depth but also lead to the development of models achieving a remarkable 86.5% top-1 accuracy on ImageNet without external data. This accomplishment establishes a new state-of-the-art performance, surpassing existing models in terms of both computational efficiency (FLOPs) and model parameters. Furthermore, the proposed model attains superior results on ImageNet with Reassessed labels and ImageNet-V2/match frequency, showcasing its robustness in a setting with no additional training data. The code and models developed in this study are shared with the research community.","Convolutional Neural Networks (CNNs) have long reigned supreme in the realm of image classification, but recent breakthroughs have demonstrated the potential of transformers in this domain. This work delves into the unexplored territory of optimizing image transformers for large-scale image classification tasks. The primary focus is on understanding and enhancing the interplay between the architecture and optimization techniques of dedicated transformers. Two crucial architectural modifications are introduced, each contributing significantly to the accuracy of deep transformers. These changes not only address early saturation issues associated with increased model depth but also result in models that outperform existing state-of-the-art solutions on ImageNet. The achieved 86.5% top-1 accuracy without external data showcases the effectiveness of the proposed approach in achieving superior performance with reduced computational complexity and model parameters.","In this study, we have presented a novel approach to building and optimizing deeper transformer networks for image classification, challenging the traditional dominance of convolutional neural networks. Through careful exploration of architectural modifications and optimization strategies, we have achieved substantial improvements in the accuracy of deep transformers. Our proposed model not only prevents early saturation issues but also sets a new standard in image classification, reaching 86.5% top-1 accuracy on ImageNet without external data. This result establishes the model as the current state-of-the-art solution, surpassing existing benchmarks in terms of both computational efficiency (FLOPs) and model parameters. Additionally, our model exhibits robust performance on ImageNet with Reassessed labels and ImageNet-V2/match frequency, even in the absence of additional training data. The code and models developed in this work are made publicly available, contributing to the ongoing progress and collaboration in the field of large-scale image classification with transformers."
7,Hugo Touvron,"We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising,
deblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a
residual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training
schedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number
of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with
the number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance.
Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance
of our model is comparable or better than CycleGAN with significantly fewer parameters.","This paper introduces a straightforward architecture designed to tackle unpaired image-to-image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed-weight image autoencoder architecture. We introduce a task-specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof-of-concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters.","Addressing unpaired image-to-image translation tasks, such as style or class transfer, denoising, and more, requires efficient architectures that balance simplicity and effectiveness. In this work, we present a novel approach starting from a fixed-weight image autoencoder architecture. Our method introduces task-specific residual blocks operating in the latent space, employed iteratively until the target domain is reached. To counteract the exponentiation effect inherent in iterative processes, a specific training schedule is implemented. At the testing phase, our approach stands out due to its limited number of weight parameters and a compositional design allowing the modulation of transformation strength based on the number of iterations. This proves particularly advantageous when the characteristics of the noise to be suppressed are uncertain. We provide experimental evidence supporting the efficacy of our method, demonstrating its utility for various transformations, and showcasing comparable or superior performance to CycleGAN while utilizing significantly fewer parameters.","In conclusion, we propose a straightforward yet powerful architecture for unpaired image-to-image translation tasks, leveraging a fixed-weight image autoencoder as its foundation. The introduction of task-specific residual blocks in the latent space, applied iteratively, yields impressive results across diverse transformations, including style transfer, denoising, and more. A carefully crafted training schedule mitigates the challenges associated with iterative processes. During testing, our approach offers a compelling set of advantages, characterized by a constrained number of weight parameters and a compositional design that facilitates the adjustment of transformation strength based on the number of iterations. This adaptability proves invaluable in scenarios where the nature or magnitude of noise is uncertain. Experimental validations not only illustrate the effectiveness of our method but also showcase comparable or superior performance to CycleGAN, highlighting the potential for achieving high-quality image transformations with significantly reduced model complexity."
8,Hugo Touvron,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","In this paper, we introduce Llama 2, a comprehensive collection of pretrained and fine-tuned large language models (LLMs) with parameter scales ranging from 7 billion to 70 billion. Specifically, we present Llama 2-Chat, a set of fine-tuned models optimized for dialogue applications. Through extensive benchmark testing, we demonstrate that our models consistently outperform existing open-source chat models. Human evaluations on helpfulness and safety suggest that Llama 2-Chat may serve as a viable alternative to closed-source models. We provide a detailed account of our fine-tuning methodology and safety enhancements, aiming to empower the community to build upon our work and contribute to the responsible development of large language models.","In response to the growing demand for sophisticated language models, we present Llama 2, a significant advancement in the landscape of pretrained and fine-tuned large language models (LLMs). Llama 2 encompasses a diverse range of models, spanning from 7 billion to 70 billion parameters. Notably, our focus lies on Llama 2-Chat, a set of fine-tuned models tailored for dialogue-based applications. Through rigorous benchmark evaluations, we establish the superior performance of our models compared to existing open-source chat models. Human evaluations, particularly in terms of helpfulness and safety, suggest the potential suitability of Llama 2-Chat as a substitute for closed-source alternatives. This paper outlines our approach to fine-tuning and safety enhancements, aiming to facilitate knowledge-sharing within the community and foster responsible advancements in LLM development.","The development of Llama 2 marks a significant contribution to the field of large language models, offering a diverse and scalable collection of pretrained and fine-tuned models. In particular, Llama 2-Chat demonstrates remarkable performance in dialogue applications, surpassing existing open-source chat models across various benchmarks. Human evaluations affirm the potential of our models as viable alternatives to closed-source counterparts in terms of both helpfulness and safety. We emphasize the importance of responsible development and knowledge-sharing in the era of large language models and provide a detailed account of our fine-tuning methodology and safety enhancements. Through the release of Llama 2 and the associated documentation, we invite the community to build upon our work, fostering collaborative efforts towards advancing the capabilities and ethical considerations of large language models."
9,Hugo Touvron,"The growing availability of large neuroimaging databases
offers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these
databases may be prone to several sources of variability (e.g.,
age, gender, acquisition parameters,...). These nuisance variables can hamper the performance of a classification method
and can even lead to misinterpret its behavior. We focus in
this paper on how to account for data coming from different
databases. First, we present experiments on simulated data
that illustrate how interactions with other confounds such as
age can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust
data and evaluate them in the real scenario of a ComputerAided Diagnosis (CAD) system that discriminates healthy
from Alzheimer‚Äôs Disease (AD) subjects based on volumetric
characteristics derived from structural MRI.","The surge in availability of large neuroimaging databases presents unprecedented opportunities for training more efficient machine learning algorithms. However, these databases often exhibit variability stemming from diverse sources such as age, gender, and acquisition parameters. These nuisance variables can significantly impact the performance of classification methods, leading to potential misinterpretation of their behavior. This paper focuses on addressing the challenge of integrating data from different databases. Initial experiments on simulated data demonstrate how interactions with confounding variables, such as age, can pose challenges for the adjustment of data from multiple sources. Subsequently, we compare three strategies for data adjustment and evaluate their efficacy in the context of a Computer-Aided Diagnosis (CAD) system. The CAD system discriminates between healthy and Alzheimer's Disease (AD) subjects based on volumetric characteristics derived from structural MRI.","The growing availability of extensive neuroimaging databases has opened up remarkable opportunities for advancing machine learning algorithms. However, the inherent variability in these databases, arising from factors such as age, gender, and acquisition parameters, poses significant challenges to classification methods. These nuisance variables not only affect performance but can also introduce complexities in interpreting the model's behavior. This paper addresses the crucial issue of handling data from different databases. Initial experiments, conducted on simulated data, illustrate the potential pitfalls associated with interactions between data and confounding variables, particularly age. Subsequently, we delve into a comparison of three strategies for data adjustment, with a specific focus on their application in a real-world scenario involving a Computer-Aided Diagnosis (CAD) system. This system aims to distinguish between healthy and Alzheimer's Disease (AD) subjects, utilizing volumetric characteristics derived from structural MRI.","In conclusion, this paper underscores the critical challenge of integrating data from diverse neuroimaging databases, where variability due to factors like age, gender, and acquisition parameters can impact the performance of classification methods. Through experiments on simulated data, we highlight the intricate interactions between data and confounding variables, emphasizing the potential pitfalls in adjusting data from multiple sources. We compare and evaluate three strategies for data adjustment in the context of a Computer-Aided Diagnosis (CAD) system designed to discern healthy subjects from those with Alzheimer's Disease (AD). The findings contribute valuable insights into mitigating variability in large neuroimaging databases, advancing the robustness and interpretability of machine learning models in real-world applications."
10,Hugo Touvron,"Nowadays, machine learning and more particularly deep learning have an increasing impact in our society. This field has become prevalent, for instance in natural language processing where it has led to concrete applications to hate speech detection and document summarization. Sim- ilarly for computer vision, it enables better image interpretation, medical diagnosis, and major steps towards autonomous driving. Deep learning success is often associated with emblematic architectures, like AlexNet [123], ResNet [94] or GPT [155]. These successes were also powered by well-designed optimisation procedures, which are usually not the main focus of discussions. In image classification, the ImageNet [168] challenge was an accelerator for the development of new architectures but also for new optimisation recipes. In this thesis, we discuss the interactions between architectures and training procedures. We study more specifically Transformers [204] architecture applied for visual understanding. Cur- rently, transformers training procedures are less mature than those employed with convolutional networks (convnets). However, training is key to overcoming the limited architectural priors of transformers. For this reason, we focus on training procedures capable of obtaining interesting performance for transformers or even simpler architectures close to the multi-layer perceptron. We start by studying the possibility of learning with coarse labels through a modification of the training procedure. We then study different kinds of architectures for computer vision. We study their features, their advantages, their drawbacks and how to train them. Finally, we study the impact of the interaction between architecture and training process. All our approaches are evalu- ated in image classification on ImageNet and in transfer learning. We also evaluate our methods on additional tasks such as semantic segmentation.","In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever-expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as AlexNet, ResNet, and GPT, coupled with well-crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Transformer architectures to visual understanding.

Presently, training procedures for transformers are less mature compared to those for convolutional networks (convnets). Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi-layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in-depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process.

All proposed approaches are rigorously evaluated through image classification on the ImageNet dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness.","The ubiquity of machine learning, particularly deep learning, has ushered in a transformative era across diverse societal realms. Noteworthy applications in natural language processing and computer vision have demonstrated the prowess of emblematic architectures like AlexNet, ResNet, and GPT. While these architectures have rightfully garnered attention, the complementary role of well-designed optimization procedures remains a critical, albeit often overlooked, component of their success.

This thesis embarks on an exploration of the intricate dynamics between architectures and training procedures, with a specific focus on the application of Transformer architectures to visual understanding. Unlike their convolutional counterparts, transformers' training procedures are in a nascent stage, prompting an imperative need for refinement. The pivotal role of training in mitigating the architectural constraints of transformers serves as the guiding principle for our investigation.

The research journey unfolds by delving into the potential of learning with coarse labels, introducing a modification to the training procedure. Subsequent chapters undertake a comprehensive study of various computer vision architectures, dissecting their attributes, advantages, drawbacks, and the nuances of effective training methodologies. This exploration ultimately leads to an analysis of the symbiotic relationship between architecture and training processes.","In the evolving landscape of deep learning, this thesis has illuminated the symbiotic relationship between architectures and training procedures, with a specific focus on the transformative potential of Transformer architectures in visual understanding. The exploration began with a nuanced investigation into the feasibility of learning with coarse labels, revealing a promising avenue for enhancing training procedures.

The subsequent scrutiny of diverse computer vision architectures provided a panoramic view of their features, strengths, and challenges, alongside insights into optimal training strategies. As we navigated through this intricate landscape, the imperative role of training in unlocking the full potential of architectures became increasingly evident.

Our evaluations, conducted rigorously across tasks such as image classification on ImageNet, transfer learning, and semantic segmentation, attest to the efficacy of the proposed approaches. By addressing the less mature training procedures for transformers, this research not only contributes to advancing the field but also opens avenues for the development of more robust and versatile deep learning models.

In conclusion, the synergy between architecture and training processes emerges as a cornerstone for achieving breakthroughs in deep learning applications. As we continue to unravel the complexities of this dynamic interplay, the quest for refined optimization procedures remains paramount for the continued evolution of machine learning in shaping our technological future."
11,Hugo Touvron,"We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity tradeoffs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.","This paper introduces ResMLP, a novel architecture for image classification based entirely on multi-layer perceptrons (MLPs). ResMLP is designed as a simple residual network, leveraging alternating layers that enable interaction among image patches and channels. This interaction occurs independently and identically, leading to a streamlined yet effective approach. Through modern training strategies incorporating extensive data augmentation and optional distillation, ResMLP demonstrates remarkable accuracy-to-complexity tradeoffs on the ImageNet dataset. The paper also explores self-supervised training of ResMLP models, further diminishing reliance on labeled datasets. Additionally, by adapting the ResMLP architecture to machine translation tasks, the paper showcases unexpectedly strong performance. Pre-trained models and code, implemented using the Timm library, are shared to facilitate broader exploration and application of ResMLP.","Traditional convolutional neural networks (CNNs) have been the cornerstone of image classification architectures, but this paper proposes a paradigm shift with ResMLP. Built solely upon multi-layer perceptrons, ResMLP introduces a straightforward yet highly effective residual network. The architecture alternates between two key components: a linear layer facilitating interaction among image patches across channels, and a two-layer feed-forward network promoting independent channel interaction per patch. This design choice, coupled with contemporary training strategies encompassing robust data augmentation and optional distillation, yields unexpectedly favorable results on the ImageNet dataset.

Moreover, this paper extends the application of ResMLP to self-supervised learning, showcasing its ability to learn meaningful representations without explicit supervision. By reducing reliance on labeled datasets, ResMLP demonstrates versatility and robustness across diverse training scenarios. The paper also explores the adaptability of ResMLP to machine translation, achieving surprisingly competitive results in this domain.","In conclusion, ResMLP presents a compelling alternative to traditional CNN architectures for image classification. The architecture's simplicity, relying exclusively on multi-layer perceptrons, does not compromise its performance. Through systematic exploration of training strategies, including heavy data augmentation and optional distillation, ResMLP achieves a remarkable balance between accuracy and model complexity on the ImageNet dataset.

Furthermore, the extension of ResMLP to self-supervised learning underscores its ability to learn meaningful representations in the absence of labeled data. This aspect enhances its applicability in scenarios where labeled datasets are scarce or costly to obtain. The adaptation of ResMLP to machine translation, with unexpectedly strong results, emphasizes the architecture's potential for broader applications beyond image classification.

To facilitate further exploration and adoption, the paper provides pre-trained ResMLP models and code based on the Timm library. The promising results obtained across various tasks suggest that ResMLP is a valuable addition to the landscape of neural network architectures, offering a simple yet powerful approach to diverse machine learning challenges."
12,Zhiqing Sun,"Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.","Knowledge Graph Completion (KGC) endeavors to autonomously predict missing links within expansive knowledge graphs. Recent years have seen a surge in state-of-the-art KGC techniques presented at top conferences across various research domains, including data mining, machine learning, and natural language processing. However, a notable trend reveals that several recent papers boast exceedingly high performance, surpassing established state-of-the-art methods. In this paper, we identify that this phenomenon can be attributed to the adoption of inappropriate evaluation protocols. We propose a straightforward yet effective evaluation protocol to rectify this issue, designed to handle model bias and mitigate its impact on final results. Extensive experiments are conducted, presenting the performance of numerous existing methods using our proposed protocol. We have made the reproducible code publicly available to facilitate transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques.","The dynamic landscape of Knowledge Graph Completion (KGC) has witnessed a proliferation of advanced techniques, each vying for superiority in predicting missing links within expansive knowledge graphs. Notably, recent publications have showcased seemingly exceptional performance, surpassing established state-of-the-art methods. However, a critical examination reveals that the reported advancements may be linked to the adoption of inappropriate evaluation protocols. In this paper, we address this issue by proposing a simple yet robust evaluation protocol. This protocol aims to rectify the inadequacies of existing evaluation methodologies and, crucially, is designed to handle model bias effectively. We embark on extensive experiments to assess the performance of several existing KGC methods using our proposed protocol. The openness of our reproducible code further enhances the transparency and replicability of our findings.","In conclusion, our work sheds light on a critical concern in the realm of Knowledge Graph Completion (KGC) research, where recent publications tout unprecedented performance levels that surpass established state-of-the-art methods. Our investigation reveals that this trend can be attributed to the use of inappropriate evaluation protocols. To address this, we introduce a simple yet robust evaluation protocol designed to rectify shortcomings in existing methodologies. Crucially, our protocol is adept at handling model bias, a factor that can significantly impact final results. Through extensive experiments and evaluations of numerous existing KGC methods using our proposed protocol, we provide a comprehensive perspective on the true performance of these techniques. The public availability of our reproducible code contributes to fostering transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques, thereby advancing the reliability of research outcomes in this evolving field."
13,Zhiqing Sun,"Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8‚àº14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models.","Autoregressive sequence models, while excelling in performance, suffer from significant latency during inference, prompting the exploration of non-autoregressive alternatives. However, existing non-autoregressive models, assuming conditional independence during token decoding, often produce inconsistent outputs, leading to inferior accuracy compared to autoregressive counterparts. To address this, we propose incorporating a structured inference module into non-autoregressive models. Specifically, we introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models. Additionally, we present a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation showcase that our model, while incurring minimal latency (8‚àº14ms), achieves significantly improved translation performance compared to previous non-autoregressive models across different datasets. Notably, on the WMT14 En-De dataset, our model attains a BLEU score of 26.80, outperforming prior non-autoregressive baselines and trailing purely autoregressive models by only 0.61 in BLEU.","Autoregressive sequence models have demonstrated state-of-the-art performance in diverse domains such as machine translation. Despite their prowess, these models suffer from substantial inference latency due to their autoregressive factorization nature. Non-autoregressive sequence models have been introduced as a solution to reduce inference time, but their assumption of conditional independence during token decoding often leads to output inconsistencies and inferior accuracy compared to autoregressive counterparts. This paper proposes a novel approach to enhance decoding consistency and reduce inference cost simultaneously by integrating a structured inference module into non-autoregressive models. We introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models and incorporate a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation demonstrate that our model achieves significantly improved translation performance compared to previous non-autoregressive models, with minimal additional latency (8‚àº14ms). Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and closely approaches the accuracy of purely autoregressive models, trailing by only 0.61 in BLEU.","In conclusion, this paper presents a novel strategy to address the latency challenges of autoregressive sequence models during inference while preserving translation accuracy. By incorporating a structured inference module into non-autoregressive models, leveraging an efficient approximation for Conditional Random Fields (CRF), and introducing a dynamic transition technique for positional context modeling, our proposed model achieves significantly enhanced decoding consistency and improved translation performance. Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and approaches the accuracy of purely autoregressive models, showcasing its efficacy in balancing latency reduction and translation quality. The minimal additional latency (8‚àº14ms) positions our model as a promising solution for practical applications where both efficiency and accuracy are paramount."
14,Zhiqing Sun,"Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principlefOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.","Supervised Fine-Tuning (SFT) on response demonstrations coupled with Reinforcement Learning from Human Feedback (RLHF) offers a robust paradigm for aligning AI agents based on large language models (LLMs). However, the reliance on high-quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in-distribution preferences is challenging. This paper introduces SALMON (Self-ALignMent with principlefOllowiNg reward models), a novel approach to align base language models with minimal human supervision. SALMON utilizes a principle-following reward model, trained on synthetic preference data, to generate reward scores based on human-defined principles. By adjusting these principles during RL training, we gain precise control over preferences, influencing RL-trained policies and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method produces the AI assistant Dromedary-2, surpassing state-of-the-art AI systems with only 6 exemplars for in-context learning and 31 human-defined principles. We open-source the code and model weights, encouraging further research in aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.","Supervised Fine-Tuning (SFT) in conjunction with Reinforcement Learning from Human Feedback (RLHF) has proven effective for aligning AI agents based on large language models (LLMs). However, the necessity for high-quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in-distribution preferences is non-trivial. This paper presents SALMON (Self-ALignMent with principlefOllowiNg reward models), a pioneering approach to aligning base language models with minimal human supervision. Central to SALMON is a principle-following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human-defined principles. By adjusting these principles during RL training, we achieve precise control over preferences, influencing the behavior of RL-trained policies, and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method results in the creation of Dromedary-2, an AI assistant outperforming state-of-the-art systems with only 6 exemplars for in-context learning and 31 human-defined principles.","In conclusion, SALMON represents a groundbreaking approach to aligning AI agents based on large language models, minimizing the dependency on extensive human supervision. Leveraging a principle-following reward model trained on synthetic preference data, our method achieves superior performance with minimal human-defined principles. By adjusting these principles during RL training, we gain unprecedented control over preferences, influencing RL-trained policies and obviating the need for online human preferences. Applied to the LLaMA-2-70b base language model, Dromedary-2, our AI assistant, significantly outperforms state-of-the-art systems on various benchmark datasets with minimal human intervention. We have open-sourced the code and model weights, encouraging further exploration and advancements in aligning LLM-based AI agents with heightened supervision efficiency, improved controllability, and scalable oversight."
15,Zhiqing Sun,"We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.","This paper addresses the challenge of learning representations for entities and relations in knowledge graphs, with the goal of predicting missing links. The effectiveness of this task hinges on the model's capacity to capture and infer intricate patterns within relations. We introduce RotatE, a novel knowledge graph embedding approach designed to model and infer diverse relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation from the source entity to the target entity in the complex vector space. Furthermore, we propose a novel self-adversarial negative sampling technique to efficiently and effectively train the RotatE model. Experimental results across multiple benchmark knowledge graphs demonstrate that RotatE is not only scalable but also adept at inferring and modeling various relation patterns, surpassing existing state-of-the-art models in link prediction.","Learning effective representations of entities and relations in knowledge graphs is pivotal for predicting missing links. The success of this task crucially depends on the model's ability to capture and infer intricate patterns within relations. In response, we present RotatE, a novel knowledge graph embedding approach specifically designed to model and infer a range of relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation in the complex vector space, providing a versatile framework for capturing diverse relation characteristics. Additionally, we introduce a novel self-adversarial negative sampling technique, enhancing the efficiency and effectiveness of RotatE model training. Through comprehensive experiments on multiple benchmark knowledge graphs, we demonstrate that RotatE is not only scalable but also excels in inferring and modeling various relation patterns, surpassing the performance of existing state-of-the-art models in link prediction.","In conclusion, this paper introduces RotatE, a novel knowledge graph embedding approach tailored for the challenging task of predicting missing links. The RotatE model stands out for its ability to effectively model and infer a spectrum of relation patterns, including symmetry/antisymmetry, inversion, and composition. By representing each relation as a rotation in the complex vector space, RotatE offers a versatile framework for capturing diverse relation characteristics. Furthermore, we propose a novel self-adversarial negative sampling technique, contributing to the efficiency and effectiveness of RotatE model training. Experimental results across multiple benchmark knowledge graphs underscore the scalability and superior performance of RotatE, establishing it as a state-of-the-art model in link prediction. The capabilities of RotatE in capturing intricate relation patterns make it a valuable contribution to the field of knowledge graph embeddings."
16,Zhiqing Sun,"We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs‚Äô own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at.","This paper introduces a novel paradigm, RECITation-augmented gEneration (RECITE), aimed at enhancing the accuracy of factual knowledge generation in Large Language Models (LLMs) without relying on external corpuses. Departing from conventional retrieval-augmented language models, RECITE employs a recite-and-answer approach, where relevant passages are sampled from the LLMs' internal memory before generating final responses. The proposed RECITE paradigm demonstrates significant efficacy in knowledge-intensive Natural Language Processing (NLP) tasks, achieving state-of-the-art performance in various closed-book question answering (CBQA) scenarios. Through comprehensive experiments on four pre-trained models (PaLM, UL2, OPT, and Codex) across three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA), we validate the effectiveness of RECITE. The code for our approach is publicly available for further exploration.","In the realm of Large Language Models (LLMs), the quest for more accurate factual knowledge generation has led to the development of RECITation-augmented gEneration (RECITE). In contrast to traditional retrieval-augmented models, RECITE pioneers a recite-first approach, where pertinent passages are drawn from the LLMs' internal memory through sampling before the final answers are generated. This innovative strategy is tailored for knowledge-intensive Natural Language Processing (NLP) tasks, specifically closed-book question answering (CBQA). Our motivation stems from the need to reduce reliance on external corpuses and enhance the model's autonomy in generating precise and contextually relevant information.

In this paper, we present RECITE as a powerful paradigm by demonstrating its applicability across diverse CBQA tasks, including Natural Questions, TriviaQA, and HotpotQA. We conduct extensive experiments employing four pre-trained modelsóPaLM, UL2, OPT, and Codexóto validate RECITE's effectiveness. The results showcase that the recite-and-answer scheme consistently achieves new state-of-the-art performance, establishing RECITE as a promising avenue for advancing factual knowledge generation within LLMs.","In conclusion, RECITation-augmented gEneration (RECITE) emerges as a groundbreaking paradigm, offering a fresh perspective on enhancing factual knowledge generation in Large Language Models (LLMs). By introducing a recite-first approach, RECITE minimizes reliance on external corpuses, addressing the limitations of conventional retrieval-augmented models. Our experiments across multiple closed-book question answering (CBQA) tasks, leveraging four pre-trained modelsóPaLM, UL2, OPT, and Codexóunderscore the potency of RECITE in achieving state-of-the-art performance.

The recite-and-answer scheme introduced by RECITE not only demonstrates its efficacy in improving accuracy but also positions itself as a versatile solution applicable to various knowledge-intensive Natural Language Processing (NLP) tasks. This paper contributes not only a novel paradigm but also comprehensive empirical evidence supporting its effectiveness. As we make our code publicly available, we encourage further exploration and adoption of RECITE, anticipating its integration into the evolving landscape of large language models and knowledge-intensive NLP applications."
17,Zhiqing Sun,"Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3√ó smaller and 5.5√ó faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE)","Recent advances in Natural Language Processing (NLP) have seen significant success through the utilization of large pre-trained models with millions of parameters. However, the deployment of such models to resource-limited mobile devices is hindered by their substantial size and high latency. This paper introduces MobileBERT, a solution designed to compress and accelerate the widely used BERT model. MobileBERT retains the task-agnostic nature of the original BERT, allowing it to be applied to various NLP tasks through straightforward fine-tuning. Essentially, MobileBERT is a streamlined version of BERTLARGE, featuring bottleneck structures and a meticulously balanced combination of self-attentions and feed-forward networks. The training process involves the creation of a specially designed teacher model, an inverted-bottleneck incorporated BERTLARGE model, from which knowledge is transferred to MobileBERT. Empirical studies demonstrate that MobileBERT achieves a 4.3◊ reduction in size and a 5.5◊ improvement in speed compared to BERTBASE while delivering competitive results on established benchmarks. In particular, on GLUE's natural language inference tasks, MobileBERT attains a GLUE score of 77.7 (0.6 lower than BERTBASE) with a latency of 62 ms on a Pixel 4 phone. Additionally, on the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE).
","Natural Language Processing (NLP) has made remarkable strides, leveraging massive pre-trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces MobileBERT, a solution that addresses these challenges by compressing and accelerating the popular BERT model. Maintaining the versatility of the original BERT, MobileBERT is designed to be task-agnostic, allowing seamless application to diverse NLP tasks with minimal fine-tuning. The architecture of MobileBERT draws inspiration from BERTLARGE but incorporates bottleneck structures and a finely tuned balance between self-attentions and feed-forward networks. The training strategy involves a specially crafted teacher model, an inverted-bottleneck incorporated BERTLARGE, from which knowledge is transferred to MobileBERT. Empirical results demonstrate the efficiency of MobileBERT, achieving a 4.3◊ reduction in size and a 5.5◊ acceleration compared to BERTBASE, while still maintaining competitive performance on established benchmarks.","In this paper, we present MobileBERT, a novel approach to addressing the challenges associated with deploying large pre-trained NLP models on resource-limited mobile devices. By compressing and accelerating the widely used BERT model, MobileBERT achieves a significant reduction in size and improvement in speed without compromising performance. The task-agnostic nature of MobileBERT, inherited from the original BERT, ensures its applicability to various downstream NLP tasks through straightforward fine-tuning. The architectural enhancements, including bottleneck structures and a nuanced balance between self-attentions and feed-forward networks, contribute to the model's efficiency. Empirical studies validate the effectiveness of MobileBERT, showcasing a 4.3◊ size reduction and a 5.5◊ speedup compared to BERTBASE, with competitive results on standard benchmarks. MobileBERT's GLUE score of 77.7 on natural language inference tasks and its high performance on the SQuAD question answering task underscore its practical viability for real-world applications on mobile devices."
18,Timo Schick,"Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today‚Äôs language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER‚Äôs full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.","Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present PEER, a collaborative language model trained to imitate the entire writing process. PEER can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of PEER are trained to infill various parts of the writing process, leveraging self-training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate PEER's strong performance across diverse domains and editing tasks.","Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce PEER, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of PEER to cover various parts of the writing process, leveraging self-training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEER's applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEER's robust performance across a range of domains and editing tasks.","In conclusion, this paper introduces PEER, a collaborative language model designed to emulate the entire writing process, addressing critical limitations in existing language models related to collaborative writing scenarios. PEER exhibits the capability to generate drafts, propose edits, and provide explanations for its actions, unlocking new dimensions of controllability and adaptability. The novel approach of training multiple instances of PEER to cover various stages of the writing process, coupled with self-training techniques, enhances the quality, quantity, and diversity of training data. This empowers PEER to operate effectively in domains without edit histories and improves its ability to follow instructions, generate insightful comments, and explain its actions. Across diverse domains and editing tasks, PEER demonstrates strong and versatile performance, showcasing its potential as a valuable tool for collaborative writing processes."
19,Timo Schick,"Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch¬®utze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.","The success of pretraining deep language models in NLP has been remarkable, but recent findings highlight their struggle to comprehend rare words. Addressing this limitation, we present BERTRAM, an architecture built upon BERT that excels in generating high-quality embeddings for rare words. Inspired by strategies employed in static word embeddings, BERTRAM enables the interaction between the surface form and contexts of a word within a deep architecture. Integrating BERTRAM into BERT yields substantial performance improvements, particularly in the representation of rare and medium-frequency words. This enhancement is evident across a rare word probing task and three downstream tasks, showcasing BERTRAM's effectiveness in improving the treatment of rare words in pretrained language models.","Pretraining deep language models, exemplified by BERT, has yielded significant advancements in natural language processing (NLP). However, recent observations underscore the challenge these models face in comprehending rare words. This work addresses the issue by introducing BERTRAM, an innovative architecture based on BERT designed to generate high-quality embeddings for rare words. Drawing inspiration from strategies applied to static word embeddings, BERTRAM facilitates the interaction between a word's surface form and contexts within a deep architecture. When integrated into BERT, BERTRAM achieves substantial performance boosts, particularly in enhancing representations of rare and medium-frequency words. This improvement is demonstrated across various tasks, including a rare word probing task and three downstream applications, highlighting BERTRAM's efficacy in enhancing the treatment of rare words within pretrained language models.","In conclusion, this paper introduces BERTRAM, a powerful architecture derived from BERT, with a specific focus on addressing the challenges associated with rare words in pretrained language models. Leveraging insights from static word embeddings, BERTRAM facilitates the interaction between the surface form and contexts of words within a deep architecture. The integration of BERTRAM into BERT results in significant performance enhancements, particularly in the representation of rare and medium-frequency words. This improvement is consistently demonstrated across a rare word probing task and three downstream tasks, underscoring BERTRAM's efficacy in ameliorating the treatment of rare words within the context of pretrained language models. The findings contribute to advancing our understanding and strategies for handling rare words in contemporary NLP applications."
20,Timo Schick,"Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with ‚Äútask descriptions‚Äù in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.","While some natural language processing (NLP) tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Pattern-Exploiting Training (PET), a novel semi-supervised training approach that combines unsupervised and supervised learning. PET reformulates input examples as cloze-style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, PET demonstrates superior performance compared to both supervised training and strong semi-supervised approaches, particularly in low-resource settings.","The prospect of solving natural language processing (NLP) tasks in a fully unsupervised manner by providing pretrained language models with task descriptions has been explored but often falls short of supervised counterparts. In this work, we introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that seeks to bridge the gap between unsupervised and supervised learning. PET transforms input examples into cloze-style phrases, serving as guidance for language models in understanding the underlying task. These phrases are then leveraged to assign soft labels to a substantial set of unlabeled examples. Finally, standard supervised training is applied to the resulting training set. Across diverse tasks and languages, PET showcases superior performance compared to both supervised training and robust semi-supervised approaches, particularly demonstrating efficacy in low-resource settings.","In conclusion, this paper introduces Pattern-Exploiting Training (PET), a novel semi-supervised learning approach that effectively combines the strengths of unsupervised and supervised learning for natural language processing (NLP) tasks. By reformulating input examples as cloze-style phrases, PET provides valuable guidance to language models in understanding specific tasks. Soft labels assigned to a large set of unlabeled examples using these phrases enable effective leveraging of unlabeled data. Standard supervised training on the resulting training set leads to superior performance across various tasks and languages, outperforming both traditional supervised training and strong semi-supervised approaches. PET emerges as a promising strategy, especially in low-resource settings, showcasing its potential to enhance the efficiency and efficacy of language models in NLP applications."
21,Timo Schick,"Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for novel words on the fly, i.e., for words that did not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: (i) learning an embedding from the novel word‚Äôs surface-form (e.g., subword n-grams) and (ii) learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information ‚Äì surface-form and context ‚Äì and show that it results in large increases in embedding quality. Our architecture obtains state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words.","Word embeddings are integral to high-performing natural language processing (NLP) systems, yet learning effective representations for novel words remains challenging, especially for words absent in the training data. Existing approaches either focus on the surface-form of novel words or their contextual usage. In this paper, we introduce an architecture that combines information from both surface-form and context, demonstrating substantial improvements in embedding quality. Our model achieves state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, making it easily integrable into any NLP system to enhance its ability to handle novel words.","Effective representation learning for novel words in natural language processing (NLP) systems remains a significant challenge. Existing approaches tackle this issue by either focusing on the surface-form or contextual usage of novel words. In this paper, we present an innovative architecture that leverages both surface-form and context information to enhance the quality of word embeddings. Our model achieves state-of-the-art results on benchmark datasets such as Definitional Nonce and Contextual Rare Words. Crucially, our approach requires only an embedding set and an unlabeled corpus for training, offering a versatile solution that can be seamlessly integrated into any existing NLP system, augmenting its capability to handle novel words.","In conclusion, this paper introduces a novel architecture for learning word embeddings, specifically designed to address the challenge of effectively representing novel words in natural language processing (NLP) systems. By incorporating information from both surface-form and context, our model demonstrates significant improvements in embedding quality, as evidenced by state-of-the-art performance on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, offering a practical and easily integrable solution. This model can be seamlessly incorporated into any existing NLP system, enhancing its ability to handle novel words and thereby contributing to the adaptability and robustness of NLP applications."
22,Timo Schick,"To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pre-training objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.","This paper introduces an innovative approach to obtaining high-quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models (PLMs). Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high-performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique.","The quest for high-quality sentence embeddings has been marked by the dichotomy between augmenting pretrained language models (PLMs) with supplementary pretraining objectives and relying on finetuning with large labeled datasets. While the latter typically outperforms the former, the formidable challenge lies in the human effort required to generate expansive, suitable datasets. This paper presents a groundbreaking alternative, demonstrating how the generative capabilities of large and high-performing PLMs can be harnessed to overcome this challenge. By generating entire datasets of labeled text pairs de novo, our approach eliminates the need for labeled data, finetuning, or modifications to pretraining objectives. This innovative strategy facilitates the finetuning of smaller and more efficient models, achieving superior performance compared to established baselines on diverse semantic textual similarity datasets.","In conclusion, this paper introduces a paradigm shift in the pursuit of high-quality sentence embeddings, circumventing the limitations associated with labeled data, finetuning, and modifications to pretraining objectives. Leveraging the generative abilities of large and high-performing pretrained language models (PLMs), we showcase a fully unsupervised approach that produces impressive results on various semantic textual similarity datasets. The demonstrated efficacy of our methodology not only highlights its potential for practical applications but also invites further exploration into the synergies between generative language models and unsupervised learning techniques. This innovative approach opens new avenues for research and development in the realm of natural language processing and representation learning."
23,Timo Schick,"Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GENPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GENPET gives consistent improvements over strong baselines in few-shot
settings.","This paper explores the synergy between pretrained language models and simple task descriptions to address unsupervised tasks and enhance few-shot learning in text classification. The proposed approach, GENPET, leverages pattern-exploiting training, originally designed for classification tasks, to enable text generation. We demonstrate the efficacy of GENPET across various summarization and headline generation datasets, showcasing its consistent improvements over robust baselines in few-shot scenarios. The method addresses challenges such as finding easily understandable task descriptions for pretrained models and ensuring their effective utilization, while also implementing measures to mitigate overfitting. GENPET emerges as a promising solution to enhance data efficiency in generative settings, offering a novel perspective on combining task descriptions and example-based learning for text generation.","The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text-related tasks and improving few-shot learning outcomes. This paper introduces GENPET, a method rooted in pattern-exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few-shot text generation.","In conclusion, this paper introduces GENPET as a promising method that successfully marries pretrained language models with task descriptions for enhanced text generation capabilities. By extending the concept of pattern-exploiting training to generative tasks, GENPET overcomes challenges related to task description clarity, model utilization, and overfitting prevention. Through empirical validation on summarization and headline generation datasets, GENPET consistently outperforms robust baselines in few-shot scenarios, emphasizing its efficacy and versatility. The presented approach not only contributes to the ongoing discourse on improving data efficiency in generative settings but also opens avenues for further exploration in combining task descriptions and example-based learning for text generation tasks."
