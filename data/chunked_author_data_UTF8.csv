,Author,Pub,Type,Chunk
0,GPT-3.5,"[' In the landscape of cloud-based Large Language Models (LLMs), the array of sizes and configurations available presents a challenge in effectively balancing computational cost and performance optimization. This paper introduces AutoMix, an innovative approach designed to strategically route queries to larger LLMs based on the approximate correctness of outputs from a smaller LM. At its core, AutoMix incorporates a few-shot self-verification mechanism, allowing it to estimate the reliability of its own outputs without the need for additional training. To address the inherent noise in verifications, AutoMix employs a meta verifier, enhancing the accuracy of these assessments. Experimental results using LLAMA2-13/70B on five context-grounded reasoning datasets illustrate that AutoMix outperforms established baselines, yielding an up to 89% improvement in the incremental benefit per cost.']",abstract_chunked," In the landscape of cloud-based Large Language Models (LLMs), the array of sizes and configurations available presents a challenge in effectively balancing computational cost and performance optimization. This paper introduces AutoMix, an innovative approach designed to strategically route queries to larger LLMs based on the approximate correctness of outputs from a smaller LM. At its core, AutoMix incorporates a few-shot self-verification mechanism, allowing it to estimate the reliability of its own outputs without the need for additional training. To address the inherent noise in verifications, AutoMix employs a meta verifier, enhancing the accuracy of these assessments. Experimental results using LLAMA2-13/70B on five context-grounded reasoning datasets illustrate that AutoMix outperforms established baselines, yielding an up to 89% improvement in the incremental benefit per cost."
1,GPT-3.5,"[' Large Language Models (LLMs) often fall short in generating optimal outputs on their initial attempts. Drawing inspiration from the human iterative refinement process, we introduce SELF-REFINE, a novel approach for enhancing LLM outputs through iterative feedback and self-improvement. The core concept involves generating an initial output using an LLM, then utilizing the same model to provide feedback on its output and iteratively refine itself. This method requires no additional supervised training data, extra training, or reinforcement learning, relying solely on a single LLM to serve as the generator, refiner, and feedback provider. We assess SELF-REFINE across a diverse set of 7 tasks, spanning from dialog response generation to mathematical reasoning, employing state-of-the-art LLMs such as GPT-3.5 and GPT-4. The results demonstrate that outputs produced with SELF-REFINE are preferred by both human evaluators and automatic metrics compared to outputs generated using conventional one-step generation. On average, task performance improves by approximately 20% absolute across all evaluated tasks. Our findings underscore the potential for further enhancing even state-of-the-art LLMs, like GPT-4, at test-time through the adoption of our straightforward, standalone approach.']",abstract_chunked," Large Language Models (LLMs) often fall short in generating optimal outputs on their initial attempts. Drawing inspiration from the human iterative refinement process, we introduce SELF-REFINE, a novel approach for enhancing LLM outputs through iterative feedback and self-improvement. The core concept involves generating an initial output using an LLM, then utilizing the same model to provide feedback on its output and iteratively refine itself. This method requires no additional supervised training data, extra training, or reinforcement learning, relying solely on a single LLM to serve as the generator, refiner, and feedback provider. We assess SELF-REFINE across a diverse set of 7 tasks, spanning from dialog response generation to mathematical reasoning, employing state-of-the-art LLMs such as GPT-3.5 and GPT-4. The results demonstrate that outputs produced with SELF-REFINE are preferred by both human evaluators and automatic metrics compared to outputs generated using conventional one-step generation. On average, task performance improves by approximately 20% absolute across all evaluated tasks. Our findings underscore the potential for further enhancing even state-of-the-art LLMs, like GPT-4, at test-time through the adoption of our straightforward, standalone approach."
2,GPT-3.5,"["" Machine learning systems often lack the adaptive cognitive mechanisms observed in humans, employing a uniform model for both easy and challenging tasks. In contrast, humans engage in either fast (instinctive) or slow (analytical) thinking processes based on the problem's difficultyâ€”a phenomenon known as the dual-process theory of mind. In this paper, we introduce FLOWGEN, a graph generation model inspired by the dual-process theory. FLOWGEN adapts to the complexity of graph completion by employing either a FAST (weaker) or SLOW (stronger) module. While these modules share architectures, they differ in parameter count and generative power. Through experiments on real-world graphs, FLOWGEN demonstrates its ability to generate graphs comparable to those produced by a single large model, all while achieving up to a 2x speed improvement.""]",abstract_chunked," Machine learning systems often lack the adaptive cognitive mechanisms observed in humans, employing a uniform model for both easy and challenging tasks. In contrast, humans engage in either fast (instinctive) or slow (analytical) thinking processes based on the problem's difficultyâ€”a phenomenon known as the dual-process theory of mind. In this paper, we introduce FLOWGEN, a graph generation model inspired by the dual-process theory. FLOWGEN adapts to the complexity of graph completion by employing either a FAST (weaker) or SLOW (stronger) module. While these modules share architectures, they differ in parameter count and generative power. Through experiments on real-world graphs, FLOWGEN demonstrates its ability to generate graphs comparable to those produced by a single large model, all while achieving up to a 2x speed improvement."
3,GPT-3.5,"[' In this paper, we present an innovative method for curating high-quality comparable training data tailored for low-resource languages, employing monolingual annotators. Our approach centers around the strategic use of a meticulously chosen set of images, serving as a pivot between source and target languages. We independently collect captions for these images in both languages, thereby establishing a robust and comparable corpus. Human evaluations conducted on the English-Hindi comparable corpora generated through our method reveal a noteworthy 81.1% acceptability rate for translation pairs, with only 2.47% identified as non-translations. To underscore the utility of the dataset, we conduct experiments on two downstream tasksâ€”machine translation and dictionary extraction. The outcomes highlight the potential of our curated dataset. All code and data are openly accessible at https://github.com/madaan/PML4DC-Comparable-Data-Collection.']",abstract_chunked," In this paper, we present an innovative method for curating high-quality comparable training data tailored for low-resource languages, employing monolingual annotators. Our approach centers around the strategic use of a meticulously chosen set of images, serving as a pivot between source and target languages. We independently collect captions for these images in both languages, thereby establishing a robust and comparable corpus. Human evaluations conducted on the English-Hindi comparable corpora generated through our method reveal a noteworthy 81.1% acceptability rate for translation pairs, with only 2.47% identified as non-translations. To underscore the utility of the dataset, we conduct experiments on two downstream tasksâ€”machine translation and dictionary extraction. The outcomes highlight the potential of our curated dataset. All code and data are openly accessible at https://github.com/madaan/PML4DC-Comparable-Data-Collection."
4,GPT-3.5,"[' This paper introduces a novel task in the domain of natural language processing – the task of politeness transfer. The objective is to convert non-polite sentences into polite equivalents while retaining the original meaning. To facilitate research in this area, we present a comprehensive dataset comprising over 1.39 million instances, meticulously labeled for politeness. This dataset serves as a foundation for benchmark evaluations, encouraging advancements in politeness transfer techniques. Our proposed approach involves a tag-and-generate pipeline, which first identifies stylistic attributes and then generates a sentence in the target style while preserving the majority of the source content. Extensive experiments demonstrate that our model outperforms existing state-of-the-art methods on automatic metrics related to content preservation, achieving comparable or superior performance in style transfer accuracy. Moreover, our model excels in human evaluations, demonstrating superior grammaticality, meaning preservation, and transfer accuracy across not only the politeness transfer task but also five other style transfer tasks. The code and dataset are made publicly available, fostering collaboration and further research in this emerging field.']",abstract_chunked," This paper introduces a novel task in the domain of natural language processing – the task of politeness transfer. The objective is to convert non-polite sentences into polite equivalents while retaining the original meaning. To facilitate research in this area, we present a comprehensive dataset comprising over 1.39 million instances, meticulously labeled for politeness. This dataset serves as a foundation for benchmark evaluations, encouraging advancements in politeness transfer techniques. Our proposed approach involves a tag-and-generate pipeline, which first identifies stylistic attributes and then generates a sentence in the target style while preserving the majority of the source content. Extensive experiments demonstrate that our model outperforms existing state-of-the-art methods on automatic metrics related to content preservation, achieving comparable or superior performance in style transfer accuracy. Moreover, our model excels in human evaluations, demonstrating superior grammaticality, meaning preservation, and transfer accuracy across not only the politeness transfer task but also five other style transfer tasks. The code and dataset are made publicly available, fostering collaboration and further research in this emerging field."
5,GPT-3.5,"[' Understanding processes involves the critical task of reasoning about events and tracking their influences. In this paper, we introduce EIGEN, a novel method harnessing pre-trained language models to generate event influences based on context, the nature of influence, and their position in a reasoning chain. We present a new dataset tailored for researching and evaluating event influence generation methods. EIGEN surpasses robust baselines, demonstrating superior performance in automated evaluation metrics, achieving a remarkable 10 ROUGE points improvement. Human judgments affirm the excellence of EIGEN in terms of both closeness to reference and the relevance of generated content. Additionally, we showcase that the event influences produced by EIGEN significantly enhance performance on a ""what-if"" Question Answering (WIQA) benchmark, exhibiting an impressive over 3% increase in F1 score. This improvement is particularly notable for questions demanding background knowledge and multi-hop reasoning.']",abstract_chunked," Understanding processes involves the critical task of reasoning about events and tracking their influences. In this paper, we introduce EIGEN, a novel method harnessing pre-trained language models to generate event influences based on context, the nature of influence, and their position in a reasoning chain. We present a new dataset tailored for researching and evaluating event influence generation methods. EIGEN surpasses robust baselines, demonstrating superior performance in automated evaluation metrics, achieving a remarkable 10 ROUGE points improvement. Human judgments affirm the excellence of EIGEN in terms of both closeness to reference and the relevance of generated content. Additionally, we showcase that the event influences produced by EIGEN significantly enhance performance on a ""what-if"" Question Answering (WIQA) benchmark, exhibiting an impressive over 3% increase in F1 score. This improvement is particularly notable for questions demanding background knowledge and multi-hop reasoning."
6,GPT-3.5,"[' Recent advancements in adapting transformers for large-scale image classification have challenged the longstanding dominance of convolutional neural networks (CNNs). This paper focuses on building and optimizing deeper transformer networks specifically tailored for image classification tasks. Through a thorough exploration of the interplay between architecture and optimization strategies, two key modifications are introduced, resulting in a significant improvement in the accuracy of deep transformers. These architectural enhancements not only prevent early saturation of performance with increased depth but also lead to the development of models achieving a remarkable 86.5% top-1 accuracy on ImageNet without external data. This accomplishment establishes a new state-of-the-art performance, surpassing existing models in terms of both computational efficiency (FLOPs) and model parameters. Furthermore, the proposed model attains superior results on ImageNet with Reassessed labels and ImageNet-V2/match frequency, showcasing its robustness in a setting with no additional training data. The code and models developed in this study are shared with the research community.']",abstract_chunked," Recent advancements in adapting transformers for large-scale image classification have challenged the longstanding dominance of convolutional neural networks (CNNs). This paper focuses on building and optimizing deeper transformer networks specifically tailored for image classification tasks. Through a thorough exploration of the interplay between architecture and optimization strategies, two key modifications are introduced, resulting in a significant improvement in the accuracy of deep transformers. These architectural enhancements not only prevent early saturation of performance with increased depth but also lead to the development of models achieving a remarkable 86.5% top-1 accuracy on ImageNet without external data. This accomplishment establishes a new state-of-the-art performance, surpassing existing models in terms of both computational efficiency (FLOPs) and model parameters. Furthermore, the proposed model attains superior results on ImageNet with Reassessed labels and ImageNet-V2/match frequency, showcasing its robustness in a setting with no additional training data. The code and models developed in this study are shared with the research community."
7,GPT-3.5,"[' This paper introduces a straightforward architecture designed to tackle unpaired image-to-image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed-weight image autoencoder architecture. We introduce a task-specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof-of-concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters.']",abstract_chunked," This paper introduces a straightforward architecture designed to tackle unpaired image-to-image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed-weight image autoencoder architecture. We introduce a task-specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof-of-concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters."
8,GPT-3.5,"[' In this paper, we introduce Llama 2, a comprehensive collection of pretrained and fine-tuned large language models (LLMs) with parameter scales ranging from 7 billion to 70 billion. Specifically, we present Llama 2-Chat, a set of fine-tuned models optimized for dialogue applications. Through extensive benchmark testing, we demonstrate that our models consistently outperform existing open-source chat models. Human evaluations on helpfulness and safety suggest that Llama 2-Chat may serve as a viable alternative to closed-source models. We provide a detailed account of our fine-tuning methodology and safety enhancements, aiming to empower the community to build upon our work and contribute to the responsible development of large language models.']",abstract_chunked," In this paper, we introduce Llama 2, a comprehensive collection of pretrained and fine-tuned large language models (LLMs) with parameter scales ranging from 7 billion to 70 billion. Specifically, we present Llama 2-Chat, a set of fine-tuned models optimized for dialogue applications. Through extensive benchmark testing, we demonstrate that our models consistently outperform existing open-source chat models. Human evaluations on helpfulness and safety suggest that Llama 2-Chat may serve as a viable alternative to closed-source models. We provide a detailed account of our fine-tuning methodology and safety enhancements, aiming to empower the community to build upon our work and contribute to the responsible development of large language models."
9,GPT-3.5,"["" The surge in availability of large neuroimaging databases presents unprecedented opportunities for training more efficient machine learning algorithms. However, these databases often exhibit variability stemming from diverse sources such as age, gender, and acquisition parameters. These nuisance variables can significantly impact the performance of classification methods, leading to potential misinterpretation of their behavior. This paper focuses on addressing the challenge of integrating data from different databases. Initial experiments on simulated data demonstrate how interactions with confounding variables, such as age, can pose challenges for the adjustment of data from multiple sources. Subsequently, we compare three strategies for data adjustment and evaluate their efficacy in the context of a Computer-Aided Diagnosis (CAD) system. The CAD system discriminates between healthy and Alzheimer's Disease (AD) subjects based on volumetric characteristics derived from structural MRI.""]",abstract_chunked," The surge in availability of large neuroimaging databases presents unprecedented opportunities for training more efficient machine learning algorithms. However, these databases often exhibit variability stemming from diverse sources such as age, gender, and acquisition parameters. These nuisance variables can significantly impact the performance of classification methods, leading to potential misinterpretation of their behavior. This paper focuses on addressing the challenge of integrating data from different databases. Initial experiments on simulated data demonstrate how interactions with confounding variables, such as age, can pose challenges for the adjustment of data from multiple sources. Subsequently, we compare three strategies for data adjustment and evaluate their efficacy in the context of a Computer-Aided Diagnosis (CAD) system. The CAD system discriminates between healthy and Alzheimer's Disease (AD) subjects based on volumetric characteristics derived from structural MRI."
10,GPT-3.5,"[' In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever-expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as AlexNet, ResNet, and GPT, coupled with well-crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Transformer architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks (convnets). Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi-layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in-depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the ImageNet dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness.']",abstract_chunked," In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever-expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as AlexNet, ResNet, and GPT, coupled with well-crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Transformer architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks (convnets). Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi-layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in-depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the ImageNet dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness."
11,GPT-3.5,"[' This paper introduces ResMLP, a novel architecture for image classification based entirely on multi-layer perceptrons (MLPs). ResMLP is designed as a simple residual network, leveraging alternating layers that enable interaction among image patches and channels. This interaction occurs independently and identically, leading to a streamlined yet effective approach. Through modern training strategies incorporating extensive data augmentation and optional distillation, ResMLP demonstrates remarkable accuracy-to-complexity tradeoffs on the ImageNet dataset. The paper also explores self-supervised training of ResMLP models, further diminishing reliance on labeled datasets. Additionally, by adapting the ResMLP architecture to machine translation tasks, the paper showcases unexpectedly strong performance. Pre-trained models and code, implemented using the Timm library, are shared to facilitate broader exploration and application of ResMLP.']",abstract_chunked," This paper introduces ResMLP, a novel architecture for image classification based entirely on multi-layer perceptrons (MLPs). ResMLP is designed as a simple residual network, leveraging alternating layers that enable interaction among image patches and channels. This interaction occurs independently and identically, leading to a streamlined yet effective approach. Through modern training strategies incorporating extensive data augmentation and optional distillation, ResMLP demonstrates remarkable accuracy-to-complexity tradeoffs on the ImageNet dataset. The paper also explores self-supervised training of ResMLP models, further diminishing reliance on labeled datasets. Additionally, by adapting the ResMLP architecture to machine translation tasks, the paper showcases unexpectedly strong performance. Pre-trained models and code, implemented using the Timm library, are shared to facilitate broader exploration and application of ResMLP."
12,GPT-3.5,"[' Knowledge Graph Completion (KGC) endeavors to autonomously predict missing links within expansive knowledge graphs. Recent years have seen a surge in state-of-the-art KGC techniques presented at top conferences across various research domains, including data mining, machine learning, and natural language processing. However, a notable trend reveals that several recent papers boast exceedingly high performance, surpassing established state-of-the-art methods. In this paper, we identify that this phenomenon can be attributed to the adoption of inappropriate evaluation protocols. We propose a straightforward yet effective evaluation protocol to rectify this issue, designed to handle model bias and mitigate its impact on final results. Extensive experiments are conducted, presenting the performance of numerous existing methods using our proposed protocol. We have made the reproducible code publicly available to facilitate transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques.']",abstract_chunked," Knowledge Graph Completion (KGC) endeavors to autonomously predict missing links within expansive knowledge graphs. Recent years have seen a surge in state-of-the-art KGC techniques presented at top conferences across various research domains, including data mining, machine learning, and natural language processing. However, a notable trend reveals that several recent papers boast exceedingly high performance, surpassing established state-of-the-art methods. In this paper, we identify that this phenomenon can be attributed to the adoption of inappropriate evaluation protocols. We propose a straightforward yet effective evaluation protocol to rectify this issue, designed to handle model bias and mitigate its impact on final results. Extensive experiments are conducted, presenting the performance of numerous existing methods using our proposed protocol. We have made the reproducible code publicly available to facilitate transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques."
13,GPT-3.5,"[' Autoregressive sequence models, while excelling in performance, suffer from significant latency during inference, prompting the exploration of non-autoregressive alternatives. However, existing non-autoregressive models, assuming conditional independence during token decoding, often produce inconsistent outputs, leading to inferior accuracy compared to autoregressive counterparts. To address this, we propose incorporating a structured inference module into non-autoregressive models. Specifically, we introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models. Additionally, we present a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation showcase that our model, while incurring minimal latency (8âˆ¼14ms), achieves significantly improved translation performance compared to previous non-autoregressive models across different datasets. Notably, on the WMT14 En-De dataset, our model attains a BLEU score of 26.80, outperforming prior non-autoregressive baselines and trailing purely autoregressive models by only 0.61 in BLEU.']",abstract_chunked," Autoregressive sequence models, while excelling in performance, suffer from significant latency during inference, prompting the exploration of non-autoregressive alternatives. However, existing non-autoregressive models, assuming conditional independence during token decoding, often produce inconsistent outputs, leading to inferior accuracy compared to autoregressive counterparts. To address this, we propose incorporating a structured inference module into non-autoregressive models. Specifically, we introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models. Additionally, we present a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation showcase that our model, while incurring minimal latency (8âˆ¼14ms), achieves significantly improved translation performance compared to previous non-autoregressive models across different datasets. Notably, on the WMT14 En-De dataset, our model attains a BLEU score of 26.80, outperforming prior non-autoregressive baselines and trailing purely autoregressive models by only 0.61 in BLEU."
14,GPT-3.5,"[' Supervised Fine-Tuning (SFT) on response demonstrations coupled with Reinforcement Learning from Human Feedback (RLHF) offers a robust paradigm for aligning AI agents based on large language models (LLMs). However, the reliance on high-quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in-distribution preferences is challenging. This paper introduces SALMON (Self-ALignMent with principlefOllowiNg reward models), a novel approach to align base language models with minimal human supervision. SALMON utilizes a principle-following reward model, trained on synthetic preference data, to generate reward scores based on human-defined principles. By adjusting these principles during RL training, we gain precise control over preferences, influencing RL-trained policies and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method produces the AI assistant Dromedary-2, surpassing state-of-the-art AI systems with only 6 exemplars for in-context learning and 31 human-defined principles. We open-source the code and model weights, encouraging further research in aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.']",abstract_chunked," Supervised Fine-Tuning (SFT) on response demonstrations coupled with Reinforcement Learning from Human Feedback (RLHF) offers a robust paradigm for aligning AI agents based on large language models (LLMs). However, the reliance on high-quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in-distribution preferences is challenging. This paper introduces SALMON (Self-ALignMent with principlefOllowiNg reward models), a novel approach to align base language models with minimal human supervision. SALMON utilizes a principle-following reward model, trained on synthetic preference data, to generate reward scores based on human-defined principles. By adjusting these principles during RL training, we gain precise control over preferences, influencing RL-trained policies and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method produces the AI assistant Dromedary-2, surpassing state-of-the-art AI systems with only 6 exemplars for in-context learning and 31 human-defined principles. We open-source the code and model weights, encouraging further research in aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight."
15,GPT-3.5,"["" This paper addresses the challenge of learning representations for entities and relations in knowledge graphs, with the goal of predicting missing links. The effectiveness of this task hinges on the model's capacity to capture and infer intricate patterns within relations. We introduce RotatE, a novel knowledge graph embedding approach designed to model and infer diverse relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation from the source entity to the target entity in the complex vector space. Furthermore, we propose a novel self-adversarial negative sampling technique to efficiently and effectively train the RotatE model. Experimental results across multiple benchmark knowledge graphs demonstrate that RotatE is not only scalable but also adept at inferring and modeling various relation patterns, surpassing existing state-of-the-art models in link prediction.""]",abstract_chunked," This paper addresses the challenge of learning representations for entities and relations in knowledge graphs, with the goal of predicting missing links. The effectiveness of this task hinges on the model's capacity to capture and infer intricate patterns within relations. We introduce RotatE, a novel knowledge graph embedding approach designed to model and infer diverse relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation from the source entity to the target entity in the complex vector space. Furthermore, we propose a novel self-adversarial negative sampling technique to efficiently and effectively train the RotatE model. Experimental results across multiple benchmark knowledge graphs demonstrate that RotatE is not only scalable but also adept at inferring and modeling various relation patterns, surpassing existing state-of-the-art models in link prediction."
16,GPT-3.5,"["" This paper introduces a novel paradigm, RECITation-augmented gEneration (RECITE), aimed at enhancing the accuracy of factual knowledge generation in Large Language Models (LLMs) without relying on external corpuses. Departing from conventional retrieval-augmented language models, RECITE employs a recite-and-answer approach, where relevant passages are sampled from the LLMs' internal memory before generating final responses. The proposed RECITE paradigm demonstrates significant efficacy in knowledge-intensive Natural Language Processing (NLP) tasks, achieving state-of-the-art performance in various closed-book question answering (CBQA) scenarios. Through comprehensive experiments on four pre-trained models (PaLM, UL2, OPT, and Codex) across three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA), we validate the effectiveness of RECITE. The code for our approach is publicly available for further exploration.""]",abstract_chunked," This paper introduces a novel paradigm, RECITation-augmented gEneration (RECITE), aimed at enhancing the accuracy of factual knowledge generation in Large Language Models (LLMs) without relying on external corpuses. Departing from conventional retrieval-augmented language models, RECITE employs a recite-and-answer approach, where relevant passages are sampled from the LLMs' internal memory before generating final responses. The proposed RECITE paradigm demonstrates significant efficacy in knowledge-intensive Natural Language Processing (NLP) tasks, achieving state-of-the-art performance in various closed-book question answering (CBQA) scenarios. Through comprehensive experiments on four pre-trained models (PaLM, UL2, OPT, and Codex) across three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA), we validate the effectiveness of RECITE. The code for our approach is publicly available for further exploration."
17,GPT-3.5,"["" Recent advances in Natural Language Processing (NLP) have seen significant success through the utilization of large pre-trained models with millions of parameters. However, the deployment of such models to resource-limited mobile devices is hindered by their substantial size and high latency. This paper introduces MobileBERT, a solution designed to compress and accelerate the widely used BERT model. MobileBERT retains the task-agnostic nature of the original BERT, allowing it to be applied to various NLP tasks through straightforward fine-tuning. Essentially, MobileBERT is a streamlined version of BERTLARGE, featuring bottleneck structures and a meticulously balanced combination of self-attentions and feed-forward networks. The training process involves the creation of a specially designed teacher model, an inverted-bottleneck incorporated BERTLARGE model, from which knowledge is transferred to MobileBERT. Empirical studies demonstrate that MobileBERT achieves a 4.3× reduction in size and a 5.5× improvement in speed compared to BERTBASE while delivering competitive results on established benchmarks. In particular, on GLUE's natural language inference tasks, MobileBERT attains a GLUE score of 77.7 (0.6 lower than BERTBASE) with a latency of 62 ms on a Pixel 4 phone. Additionally, on the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE).""]",abstract_chunked," Recent advances in Natural Language Processing (NLP) have seen significant success through the utilization of large pre-trained models with millions of parameters. However, the deployment of such models to resource-limited mobile devices is hindered by their substantial size and high latency. This paper introduces MobileBERT, a solution designed to compress and accelerate the widely used BERT model. MobileBERT retains the task-agnostic nature of the original BERT, allowing it to be applied to various NLP tasks through straightforward fine-tuning. Essentially, MobileBERT is a streamlined version of BERTLARGE, featuring bottleneck structures and a meticulously balanced combination of self-attentions and feed-forward networks. The training process involves the creation of a specially designed teacher model, an inverted-bottleneck incorporated BERTLARGE model, from which knowledge is transferred to MobileBERT. Empirical studies demonstrate that MobileBERT achieves a 4.3× reduction in size and a 5.5× improvement in speed compared to BERTBASE while delivering competitive results on established benchmarks. In particular, on GLUE's natural language inference tasks, MobileBERT attains a GLUE score of 77.7 (0.6 lower than BERTBASE) with a latency of 62 ms on a Pixel 4 phone. Additionally, on the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE)."
18,GPT-3.5,"["" Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present PEER, a collaborative language model trained to imitate the entire writing process. PEER can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of PEER are trained to infill various parts of the writing process, leveraging self-training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate PEER's strong performance across diverse domains and editing tasks.""]",abstract_chunked," Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present PEER, a collaborative language model trained to imitate the entire writing process. PEER can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of PEER are trained to infill various parts of the writing process, leveraging self-training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate PEER's strong performance across diverse domains and editing tasks."
19,GPT-3.5,"["" The success of pretraining deep language models in NLP has been remarkable, but recent findings highlight their struggle to comprehend rare words. Addressing this limitation, we present BERTRAM, an architecture built upon BERT that excels in generating high-quality embeddings for rare words. Inspired by strategies employed in static word embeddings, BERTRAM enables the interaction between the surface form and contexts of a word within a deep architecture. Integrating BERTRAM into BERT yields substantial performance improvements, particularly in the representation of rare and medium-frequency words. This enhancement is evident across a rare word probing task and three downstream tasks, showcasing BERTRAM's effectiveness in improving the treatment of rare words in pretrained language models.""]",abstract_chunked," The success of pretraining deep language models in NLP has been remarkable, but recent findings highlight their struggle to comprehend rare words. Addressing this limitation, we present BERTRAM, an architecture built upon BERT that excels in generating high-quality embeddings for rare words. Inspired by strategies employed in static word embeddings, BERTRAM enables the interaction between the surface form and contexts of a word within a deep architecture. Integrating BERTRAM into BERT yields substantial performance improvements, particularly in the representation of rare and medium-frequency words. This enhancement is evident across a rare word probing task and three downstream tasks, showcasing BERTRAM's effectiveness in improving the treatment of rare words in pretrained language models."
20,GPT-3.5,"[' While some natural language processing (NLP) tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Pattern-Exploiting Training (PET), a novel semi-supervised training approach that combines unsupervised and supervised learning. PET reformulates input examples as cloze-style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, PET demonstrates superior performance compared to both supervised training and strong semi-supervised approaches, particularly in low-resource settings.']",abstract_chunked," While some natural language processing (NLP) tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Pattern-Exploiting Training (PET), a novel semi-supervised training approach that combines unsupervised and supervised learning. PET reformulates input examples as cloze-style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, PET demonstrates superior performance compared to both supervised training and strong semi-supervised approaches, particularly in low-resource settings."
21,GPT-3.5,"[' Word embeddings are integral to high-performing natural language processing (NLP) systems, yet learning effective representations for novel words remains challenging, especially for words absent in the training data. Existing approaches either focus on the surface-form of novel words or their contextual usage. In this paper, we introduce an architecture that combines information from both surface-form and context, demonstrating substantial improvements in embedding quality. Our model achieves state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, making it easily integrable into any NLP system to enhance its ability to handle novel words.']",abstract_chunked," Word embeddings are integral to high-performing natural language processing (NLP) systems, yet learning effective representations for novel words remains challenging, especially for words absent in the training data. Existing approaches either focus on the surface-form of novel words or their contextual usage. In this paper, we introduce an architecture that combines information from both surface-form and context, demonstrating substantial improvements in embedding quality. Our model achieves state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, making it easily integrable into any NLP system to enhance its ability to handle novel words."
22,GPT-3.5,"[' This paper introduces an innovative approach to obtaining high-quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models (PLMs). Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high-performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique.']",abstract_chunked," This paper introduces an innovative approach to obtaining high-quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models (PLMs). Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high-performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique."
23,GPT-3.5,"[' This paper explores the synergy between pretrained language models and simple task descriptions to address unsupervised tasks and enhance few-shot learning in text classification. The proposed approach, GENPET, leverages pattern-exploiting training, originally designed for classification tasks, to enable text generation. We demonstrate the efficacy of GENPET across various summarization and headline generation datasets, showcasing its consistent improvements over robust baselines in few-shot scenarios. The method addresses challenges such as finding easily understandable task descriptions for pretrained models and ensuring their effective utilization, while also implementing measures to mitigate overfitting. GENPET emerges as a promising solution to enhance data efficiency in generative settings, offering a novel perspective on combining task descriptions and example-based learning for text generation.']",abstract_chunked," This paper explores the synergy between pretrained language models and simple task descriptions to address unsupervised tasks and enhance few-shot learning in text classification. The proposed approach, GENPET, leverages pattern-exploiting training, originally designed for classification tasks, to enable text generation. We demonstrate the efficacy of GENPET across various summarization and headline generation datasets, showcasing its consistent improvements over robust baselines in few-shot scenarios. The method addresses challenges such as finding easily understandable task descriptions for pretrained models and ensuring their effective utilization, while also implementing measures to mitigate overfitting. GENPET emerges as a promising solution to enhance data efficiency in generative settings, offering a novel perspective on combining task descriptions and example-based learning for text generation."
24,Aman Madaan,"[' Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%.2']",abstract_chunked," Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%.2"
25,Aman Madaan,"[' Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.']",abstract_chunked," Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach."
26,Aman Madaan,"[' The waning of Moore’s Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program’s performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, we use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI’s CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5ˆ for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10ˆ smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.']",abstract_chunked," The waning of Moore’s Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program’s performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, we use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI’s CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5ˆ for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10ˆ smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code."
27,Aman Madaan,"[' We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches “serialize” the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the fewshot setting. Our code and data are available at https://github.com/madaan/ CoCoGen .']",abstract_chunked," We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches “serialize” the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the fewshot setting. Our code and data are available at https://github.com/madaan/ CoCoGen ."
28,Aman Madaan,"[' In the past decade, we witnessed dramatic gains in natural language processing and an\nunprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (COT) prompting. Specifically, COT pushes the performance of large language models in a few-shot setup by\naugmenting the prompts with intermediate steps. Despite impressive results across various\ntasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT-based few-shot prompting\nmechanisms in large language models. We first systematically identify and define the\nkey components of a prompt: symbols, patterns, and text. Then, we devise and conduct\nan exhaustive set of deliberated experiments across four different tasks, by querying the\nmodel with counterfactual prompts where only one of these components is altered. Our\nexperiments across three models—PaLM, GPT-3, and CODEX—reveal several surprising\nfindings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success\nof COT. Second, our results conclude that the primary role of intermediate steps may not\nbe to facilitate learning “how” to solve a task.', 'The intermediate steps are rather a beacon\nfor the model to realize “what” symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to “trick” the model into forming sentences\nthat resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis\nreveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and\npatterns enforce task understanding and direct text generation. Such systematic understanding of COT enables us to devise a concise chain of thought, dubbed as CCOT, where\ntext and patterns are pruned by over 20%, only retaining their key roles. We achieve this\nreduction in the number of tokens while delivering on par or slightly higher solve task rate.']",abstract_chunked," In the past decade, we witnessed dramatic gains in natural language processing and an
unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (COT) prompting. Specifically, COT pushes the performance of large language models in a few-shot setup by
augmenting the prompts with intermediate steps. Despite impressive results across various
tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT-based few-shot prompting
mechanisms in large language models. We first systematically identify and define the
key components of a prompt: symbols, patterns, and text. Then, we devise and conduct
an exhaustive set of deliberated experiments across four different tasks, by querying the
model with counterfactual prompts where only one of these components is altered. Our
experiments across three models—PaLM, GPT-3, and CODEX—reveal several surprising
findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success
of COT. Second, our results conclude that the primary role of intermediate steps may not
be to facilitate learning “how” to solve a task."
29,Aman Madaan,"[' In the past decade, we witnessed dramatic gains in natural language processing and an\nunprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (COT) prompting. Specifically, COT pushes the performance of large language models in a few-shot setup by\naugmenting the prompts with intermediate steps. Despite impressive results across various\ntasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT-based few-shot prompting\nmechanisms in large language models. We first systematically identify and define the\nkey components of a prompt: symbols, patterns, and text. Then, we devise and conduct\nan exhaustive set of deliberated experiments across four different tasks, by querying the\nmodel with counterfactual prompts where only one of these components is altered. Our\nexperiments across three models—PaLM, GPT-3, and CODEX—reveal several surprising\nfindings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success\nof COT. Second, our results conclude that the primary role of intermediate steps may not\nbe to facilitate learning “how” to solve a task.', 'The intermediate steps are rather a beacon\nfor the model to realize “what” symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to “trick” the model into forming sentences\nthat resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis\nreveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and\npatterns enforce task understanding and direct text generation. Such systematic understanding of COT enables us to devise a concise chain of thought, dubbed as CCOT, where\ntext and patterns are pruned by over 20%, only retaining their key roles. We achieve this\nreduction in the number of tokens while delivering on par or slightly higher solve task rate.']",abstract_chunked,"The intermediate steps are rather a beacon
for the model to realize “what” symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to “trick” the model into forming sentences
that resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis
reveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and
patterns enforce task understanding and direct text generation. Such systematic understanding of COT enables us to devise a concise chain of thought, dubbed as CCOT, where
text and patterns are pruned by over 20%, only retaining their key roles. We achieve this
reduction in the number of tokens while delivering on par or slightly higher solve task rate."
30,Aman Madaan,"[' Machine learning systems typically apply the\nsame model to both easy and tough cases. This is\nin stark contrast with humans, who tend to evoke\neither fast (instinctive) or slow (analytical) thinking process, depending on the difficulty of the\nproblem—a property called the dual-process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual-process\ntheory of mind. Depending on the difficulty of\ngraph completion at the current step, the system either calls a FAST (weaker) module or a\nSLOW (stronger) module for the task. These modules have identical architectures, but vary in the\nnumber of parameters and consequently differ\nin generative power. Experiments on real-world\ngraphs show that FLOWGEN can successfully generate graphs similar to those generated by a single\nlarge model, while being up to 2x faster.']",abstract_chunked," Machine learning systems typically apply the
same model to both easy and tough cases. This is
in stark contrast with humans, who tend to evoke
either fast (instinctive) or slow (analytical) thinking process, depending on the difficulty of the
problem—a property called the dual-process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual-process
theory of mind. Depending on the difficulty of
graph completion at the current step, the system either calls a FAST (weaker) module or a
SLOW (stronger) module for the task. These modules have identical architectures, but vary in the
number of parameters and consequently differ
in generative power. Experiments on real-world
graphs show that FLOWGEN can successfully generate graphs similar to those generated by a single
large model, while being up to 2x faster."
31,Aman Madaan,"[' Conditional set generation learns a mapping\nfrom an input sequence of tokens to a set. Several NLP tasks, such as entity typing and\ndialogue emotion tagging, are instances of\nset generation. SEQ2SEQ models, a popular choice for set generation, treat a set as\na sequence and do not fully leverage its key\nproperties, namely order-invariance and cardinality. We propose a novel algorithm for\neffectively sampling informative orders over\nthe combinatorial space of label orders. We\njointly model the set cardinality and output\nby prepending the set size and taking advantage of the autoregressive factorization used\nby SEQ2SEQ models. Our method is a modelindependent data augmentation approach that\nendows any SEQ2SEQ model with the signals\nof order-invariance and cardinality. Training a\nSEQ2SEQ model on this augmented data (without any additional annotations) gets an average\nrelative improvement of 20% on four benchmark datasets across various models: BARTbase, T5-11B, and GPT3-175B.']",abstract_chunked," Conditional set generation learns a mapping
from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and
dialogue emotion tagging, are instances of
set generation. SEQ2SEQ models, a popular choice for set generation, treat a set as
a sequence and do not fully leverage its key
properties, namely order-invariance and cardinality. We propose a novel algorithm for
effectively sampling informative orders over
the combinatorial space of label orders. We
jointly model the set cardinality and output
by prepending the set size and taking advantage of the autoregressive factorization used
by SEQ2SEQ models. Our method is a modelindependent data augmentation approach that
endows any SEQ2SEQ model with the signals
of order-invariance and cardinality. Training a
SEQ2SEQ model on this augmented data (without any additional annotations) gets an average
relative improvement of 20% on four benchmark datasets across various models: BARTbase, T5-11B, and GPT3-175B."
32,Aman Madaan,"[' Large LMs such as GPT-3 are powerful, but\ncan commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly\ninterpret ""What word is similar to good?"" to\nmean a homophone, while the user intended\na synonym. Our goal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be\nprohibitively costly. We pair GPT-3 with a\ngrowing memory of recorded cases where the\nmodel misunderstood the user’s intents, along\nwith user feedback for clarification. Such\na memory allows our system to produce enhanced prompts for any new query based on\nthe user feedback for error correction on similar cases in the past. On four tasks (two lexical\ntasks, two advanced ethical reasoning tasks),\nwe show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained\nLMs.']",abstract_chunked," Large LMs such as GPT-3 are powerful, but
can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly
interpret ""What word is similar to good?"" to
mean a homophone, while the user intended
a synonym. Our goal is to effectively correct such errors via user interactions with the
system but without retraining, which will be
prohibitively costly. We pair GPT-3 with a
growing memory of recorded cases where the
model misunderstood the user’s intents, along
with user feedback for clarification. Such
a memory allows our system to produce enhanced prompts for any new query based on
the user feedback for error correction on similar cases in the past. On four tasks (two lexical
tasks, two advanced ethical reasoning tasks),
we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT3. Our approach is a step towards the low-cost
utility enhancement for very large pre-trained
LMs."
33,Aman Madaan,"[' Defeasible reasoning is the mode of reasoning\nwhere conclusions can be overturned by taking\ninto account new evidence. Existing cognitive\nscience literature on defeasible reasoning suggests that a person forms a mental model of the\nproblem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the\nquestion scenario before answering a defeasible query. Our approach is, given a question,\nto have a model first create a graph of relevant\ninfluences, and then leverage that graph as an\nadditional input when answering the question. Our system, CURIOUS, achieves a new stateof-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by\nguiding a system to “think about” a question\nand explicitly model the scenario, rather than\nanswering reflexively.']",abstract_chunked," Defeasible reasoning is the mode of reasoning
where conclusions can be overturned by taking
into account new evidence. Existing cognitive
science literature on defeasible reasoning suggests that a person forms a mental model of the
problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the
question scenario before answering a defeasible query. Our approach is, given a question,
to have a model first create a graph of relevant
influences, and then leverage that graph as an
additional input when answering the question. Our system, CURIOUS, achieves a new stateof-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by
guiding a system to “think about” a question
and explicitly model the scenario, rather than
answering reflexively."
34,Aman Madaan,"[' Defeasible reasoning is a mode of reasoning\nwhere conclusions can be overturned by taking into account new evidence. A commonly\nused method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference\ngraphs through transfer learning from a related\nNLP task that shares the kind of reasoning that\ninference graphs support. Through automated\nmetrics and human evaluation, we find that our\nmethod generates meaningful graphs for the\ndefeasible inference task. Human accuracy on\nthis task improves by 20% by consulting the\ngenerated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning.']",abstract_chunked," Defeasible reasoning is a mode of reasoning
where conclusions can be overturned by taking into account new evidence. A commonly
used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference
graphs through transfer learning from a related
NLP task that shares the kind of reasoning that
inference graphs support. Through automated
metrics and human evaluation, we find that our
method generates meaningful graphs for the
defeasible inference task. Human accuracy on
this task improves by 20% by consulting the
generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning."
35,Aman Madaan,"[' A class of explainable NLP models for reasoning tasks support their decisions by generating free-form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce MERCURIE- an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains.']",abstract_chunked," A class of explainable NLP models for reasoning tasks support their decisions by generating free-form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce MERCURIE- an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains."
36,Aman Madaan,"[' Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a “what-if” Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning']",abstract_chunked," Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a “what-if” Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning"
37,Aman Madaan,"[' This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with humanannotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the systeminduced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-ofdomain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting']",abstract_chunked," This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with humanannotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the systeminduced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-ofdomain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting"
38,Aman Madaan,"[' This paper introduces a new task of politeness\ntransfer which involves converting non-polite\nsentences to polite sentences while preserving\nthe meaning. We also provide a dataset of\nmore than 1.39 million instances automatically\nlabeled for politeness to encourage benchmark\nevaluations on this new task. We design a tag\nand generate pipeline that identifies stylistic attributes and subsequently generates a sentence\nin the target style while preserving most of the\nsource content. For politeness as well as five\nother transfer tasks, our model outperforms the\nstate-of-the-art methods on automatic metrics\nfor content preservation, with a comparable\nor better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer\naccuracy across all the six style transfer tasks. The data and code is located at https://\ngithub.com/tag-and-generate/']",abstract_chunked," This paper introduces a new task of politeness
transfer which involves converting non-polite
sentences to polite sentences while preserving
the meaning. We also provide a dataset of
more than 1.39 million instances automatically
labeled for politeness to encourage benchmark
evaluations on this new task. We design a tag
and generate pipeline that identifies stylistic attributes and subsequently generates a sentence
in the target style while preserving most of the
source content. For politeness as well as five
other transfer tasks, our model outperforms the
state-of-the-art methods on automatic metrics
for content preservation, with a comparable
or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer
accuracy across all the six style transfer tasks. The data and code is located at https://
github.com/tag-and-generate/"
39,Aman Madaan,"[' We propose a method of curating high-quality comparable training data\nfor low-resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the\nsource and target languages by getting captions for such images in both\nlanguages independently. Human evaluations on the English-Hindi comparable corpora created with our method show that 81.1% of the pairs\nare acceptable translations, and only 2.47% of the pairs are not translations at all. We further establish the potential of the dataset collected\nthrough our approach by experimenting on two downstream tasks – machine translation and dictionary extraction. All code and data are available\nat https://github.com/madaan/PML4DC-Comparable-Data-Collection']",abstract_chunked," We propose a method of curating high-quality comparable training data
for low-resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the
source and target languages by getting captions for such images in both
languages independently. Human evaluations on the English-Hindi comparable corpora created with our method show that 81.1% of the pairs
are acceptable translations, and only 2.47% of the pairs are not translations at all. We further establish the potential of the dataset collected
through our approach by experimenting on two downstream tasks – machine translation and dictionary extraction. All code and data are available
at https://github.com/madaan/PML4DC-Comparable-Data-Collection"
40,Aman Madaan,"[' We study a novel task of numerical relation extraction with\nthe goal of extracting relations where one of the arguments\nis a number or a quantity (e.g., atomic number(Aluminium,\n13), inflation rate(India, 10.9%)). This task presents peculiar\nchallenges not found in standard Information Extraction (IE),\nsuch as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: (1) NumberRule, a rule based extractor, and (2) NumberTron, a probabilistic graphical model. We find that both\nsystems dramatically outperform MultiR, a state-of-the-art\nnon-numerical IE model, obtaining up to 25 points F-score\nimprovement.']",abstract_chunked," We study a novel task of numerical relation extraction with
the goal of extracting relations where one of the arguments
is a number or a quantity (e.g., atomic number(Aluminium,
13), inflation rate(India, 10.9%)). This task presents peculiar
challenges not found in standard Information Extraction (IE),
such as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: (1) NumberRule, a rule based extractor, and (2) NumberTron, a probabilistic graphical model. We find that both
systems dramatically outperform MultiR, a state-of-the-art
non-numerical IE model, obtaining up to 25 points F-score
improvement."
41,Hugo Touvron,"[' In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.']",abstract_chunked," In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs."
42,Hugo Touvron,"[' We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly available datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community.']",abstract_chunked," We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B
parameters. We train our models on trillions
of tokens, and show that it is possible to train
state-of-the-art models using publicly available datasets exclusively, without resorting
to proprietary and inaccessible datasets. In
particular, LLaMA-13B outperforms GPT-3
(175B) on most benchmarks, and LLaMA65B is competitive with the best models,
Chinchilla-70B and PaLM-540B. We release
all our models to the research community."
43,Hugo Touvron,"[' We introduce submodel co-training, a regularization\nmethod related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each\nsample we implicitly instantiate two altered networks, “submodels”, with stochastic depth: we activate only a subset\nof the layers. Each network serves as a soft teacher to the\nother, by providing a loss that complements the regular loss\nprovided by the one-hot label. Our approach, dubbed “cosub”, uses a single set of weights, and does not involve a\npre-trained external model or temporal averaging. Experimentally, we show that submodel co-training is\neffective to train backbones for recognition tasks such as\nimage classification and semantic segmentation. Our approach is compatible with multiple architectures, including\nRegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training\nstrategy improves their results in comparable settings. For\ninstance, a ViT-B pretrained with cosub on ImageNet-21k\nobtains 87.4% top-1 acc. @448 on ImageNet-val.']",abstract_chunked," We introduce submodel co-training, a regularization
method related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each
sample we implicitly instantiate two altered networks, “submodels”, with stochastic depth: we activate only a subset
of the layers. Each network serves as a soft teacher to the
other, by providing a loss that complements the regular loss
provided by the one-hot label. Our approach, dubbed “cosub”, uses a single set of weights, and does not involve a
pre-trained external model or temporal averaging. Experimentally, we show that submodel co-training is
effective to train backbones for recognition tasks such as
image classification and semantic segmentation. Our approach is compatible with multiple architectures, including
RegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training
strategy improves their results in comparable settings. For
instance, a ViT-B pretrained with cosub on ImageNet-21k
obtains 87.4% top-1 acc. @448 on ImageNet-val."
44,Hugo Touvron,"[' A Vision Transformer (ViT) is a simple neural architecture amenable to serve\nseveral computer vision tasks. It has limited built-in architectural priors, in\ncontrast to more recent architectures that incorporate priors either about the\ninput data or of specific tasks. Recent works show that ViTs benefit from selfsupervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the supervised training of ViTs. Our procedure\nbuilds upon and simplifies a recipe introduced for training ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations,\ncloser to the practice in self-supervised learning. Our evaluations on Image\nclassification (ImageNet-1k with and without pre-training on ImageNet-21k),\ntransfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It\nalso reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better\nbaselines for recent self-supervised approaches demonstrated on ViT.']",abstract_chunked," A Vision Transformer (ViT) is a simple neural architecture amenable to serve
several computer vision tasks. It has limited built-in architectural priors, in
contrast to more recent architectures that incorporate priors either about the
input data or of specific tasks. Recent works show that ViTs benefit from selfsupervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the supervised training of ViTs. Our procedure
builds upon and simplifies a recipe introduced for training ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations,
closer to the practice in self-supervised learning. Our evaluations on Image
classification (ImageNet-1k with and without pre-training on ImageNet-21k),
transfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It
also reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better
baselines for recent self-supervised approaches demonstrated on ViT."
45,Hugo Touvron,"[' After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing stateof-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.']",abstract_chunked," After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing stateof-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets."
46,Hugo Touvron,"[' Nowadays, machine learning and more particularly deep learning have an increasing impact in our society. This field has become prevalent, for instance in natural language processing where it has led to concrete applications to hate speech detection and document summarization. Sim- ilarly for computer vision, it enables better image interpretation, medical diagnosis, and major steps towards autonomous driving. Deep learning success is often associated with emblematic architectures, like AlexNet [123], ResNet [94] or GPT [155]. These successes were also powered by well-designed optimisation procedures, which are usually not the main focus of discussions. In image classification, the ImageNet [168] challenge was an accelerator for the development of new architectures but also for new optimisation recipes. In this thesis, we discuss the interactions between architectures and training procedures. We study more specifically Transformers [204] architecture applied for visual understanding. Cur- rently, transformers training procedures are less mature than those employed with convolutional networks (convnets). However, training is key to overcoming the limited architectural priors of transformers. For this reason, we focus on training procedures capable of obtaining interesting performance for transformers or even simpler architectures close to the multi-layer perceptron. We start by studying the possibility of learning with coarse labels through a modification of the training procedure. We then study different kinds of architectures for computer vision. We study their features, their advantages, their drawbacks and how to train them. Finally, we study the impact of the interaction between architecture and training process. All our approaches are evalu- ated in image classification on ImageNet and in transfer learning. We also evaluate our methods on additional tasks such as semantic segmentation.']",abstract_chunked," Nowadays, machine learning and more particularly deep learning have an increasing impact in our society. This field has become prevalent, for instance in natural language processing where it has led to concrete applications to hate speech detection and document summarization. Sim- ilarly for computer vision, it enables better image interpretation, medical diagnosis, and major steps towards autonomous driving. Deep learning success is often associated with emblematic architectures, like AlexNet [123], ResNet [94] or GPT [155]. These successes were also powered by well-designed optimisation procedures, which are usually not the main focus of discussions. In image classification, the ImageNet [168] challenge was an accelerator for the development of new architectures but also for new optimisation recipes. In this thesis, we discuss the interactions between architectures and training procedures. We study more specifically Transformers [204] architecture applied for visual understanding. Cur- rently, transformers training procedures are less mature than those employed with convolutional networks (convnets). However, training is key to overcoming the limited architectural priors of transformers. For this reason, we focus on training procedures capable of obtaining interesting performance for transformers or even simpler architectures close to the multi-layer perceptron. We start by studying the possibility of learning with coarse labels through a modification of the training procedure. We then study different kinds of architectures for computer vision. We study their features, their advantages, their drawbacks and how to train them. Finally, we study the impact of the interaction between architecture and training process. All our approaches are evalu- ated in image classification on ImageNet and in transfer learning. We also evaluate our methods on additional tasks such as semantic segmentation."
47,Hugo Touvron,"[' We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attentionbased aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.']",abstract_chunked," We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attentionbased aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection."
48,Hugo Touvron,"[' Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions\nof images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers by\ntraining on Imagenet only. We train them on a single computer in less than\n3 days. Our reference vision transformer (86M parameters) achieves top-1\naccuracy of 83.1% (single-crop) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet\n(where we obtain up to 85.2% accuracy) and when transferring to other\ntasks. We share our code and models.']",abstract_chunked," Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions
of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers by
training on Imagenet only. We train them on a single computer in less than
3 days. Our reference vision transformer (86M parameters) achieves top-1
accuracy of 83.1% (single-crop) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to
transformers. It relies on a distillation token ensuring that the student
learns from the teacher through attention. We show the interest of this
token-based distillation, especially when using a convnet as a teacher. This
leads us to report results competitive with convnets for both Imagenet
(where we obtain up to 85.2% accuracy) and when transferring to other
tasks. We share our code and models."
49,Hugo Touvron,"[' We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity tradeoffs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.']",abstract_chunked," We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity tradeoffs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library."
50,Hugo Touvron,"[' Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers\nhas been little studied so far. In this work, we build and optimize deeper\ntransformer networks for image classification. In particular, we investigate\nthe interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models\nwhose performance does not saturate early with more depth, for instance\nwe obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet\nwith Reassessed labels and Imagenet-V2 / match frequency, in the setting\nwith no additional training data. We share our code and models.']",abstract_chunked," Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers
has been little studied so far. In this work, we build and optimize deeper
transformer networks for image classification. In particular, we investigate
the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models
whose performance does not saturate early with more depth, for instance
we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet
with Reassessed labels and Imagenet-V2 / match frequency, in the setting
with no additional training data. We share our code and models."
51,Hugo Touvron,"[' This paper tackles the problem of learning a finer representation than the one provided by training labels. This\nenables fine-grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest-neighbor classifier\nobjective, and an instance loss inspired by self-supervised\nlearning. By jointly leveraging the coarse labels and the underlying fine-grained latent space, it significantly improves\nthe accuracy of category-level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than\nthat available at train time. It also improves the accuracy\nfor transfer learning tasks to fine-grained datasets, thereby\nestablishing the new state of the art on five public benchmarks, like iNaturalist-2018.']",abstract_chunked," This paper tackles the problem of learning a finer representation than the one provided by training labels. This
enables fine-grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest-neighbor classifier
objective, and an instance loss inspired by self-supervised
learning. By jointly leveraging the coarse labels and the underlying fine-grained latent space, it significantly improves
the accuracy of category-level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than
that available at train time. It also improves the accuracy
for transfer learning tasks to fine-grained datasets, thereby
establishing the new state of the art on five public benchmarks, like iNaturalist-2018."
52,Hugo Touvron,"[' We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising,\ndeblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a\nresidual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training\nschedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number\nof weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with\nthe number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance\nof our model is comparable or better than CycleGAN with significantly fewer parameters.']",abstract_chunked," We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising,
deblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a
residual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training
schedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number
of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with
the number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance
of our model is comparable or better than CycleGAN with significantly fewer parameters."
53,Hugo Touvron,"[' Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date']",abstract_chunked," Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date"
54,Hugo Touvron,"[' The growing availability of large neuroimaging databases\noffers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these\ndatabases may be prone to several sources of variability (e.g.,\nage, gender, acquisition parameters,...). These nuisance variables can hamper the performance of a classification method\nand can even lead to misinterpret its behavior. We focus in\nthis paper on how to account for data coming from different\ndatabases. First, we present experiments on simulated data\nthat illustrate how interactions with other confounds such as\nage can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust\ndata and evaluate them in the real scenario of a ComputerAided Diagnosis (CAD) system that discriminates healthy\nfrom Alzheimer’s Disease (AD) subjects based on volumetric\ncharacteristics derived from structural MRI.']",abstract_chunked," The growing availability of large neuroimaging databases
offers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these
databases may be prone to several sources of variability (e.g.,
age, gender, acquisition parameters,...). These nuisance variables can hamper the performance of a classification method
and can even lead to misinterpret its behavior. We focus in
this paper on how to account for data coming from different
databases. First, we present experiments on simulated data
that illustrate how interactions with other confounds such as
age can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust
data and evaluate them in the real scenario of a ComputerAided Diagnosis (CAD) system that discriminates healthy
from Alzheimer’s Disease (AD) subjects based on volumetric
characteristics derived from structural MRI."
55,Hugo Touvron,"[' Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top-1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date.']",abstract_chunked," Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top-1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date."
56,Zhiqing Sun,"[' Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principlefOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.']",abstract_chunked," Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principlefOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight."
57,Zhiqing Sun,"[' Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in “hallucination”, generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at.']",abstract_chunked," Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in “hallucination”, generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at."
58,Zhiqing Sun,"[' Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of the AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user’s queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary . With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning), Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings. We have open-sourced the code, LoRA weights of Dromedary, and our synthetic training data to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, reduced biases, and improved controllability']",abstract_chunked," Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of the AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user’s queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary . With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning), Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings. We have open-sourced the code, LoRA weights of Dromedary, and our synthetic training data to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, reduced biases, and improved controllability"
59,Zhiqing Sun,"[' Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark. Our code is available at.']",abstract_chunked," Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark. Our code is available at."
60,Zhiqing Sun,"[' Numerical simulation of non-linear partial differential equations plays a crucial role in modeling physical science and engineering phenomena, such as weather, climate, and aerodynamics. Recent Machine Learning (ML) models trained on low-resolution spatio-temporal signals have shown new promises in capturing important dynamics in high-resolution signals, under the condition that the models can effectively recover the missing details. However, this study shows that significant information is often lost in the lowresolution down-sampled features. To address such issues, we propose a new approach, namely Temporal Stencil Modeling (TSM), which combines the strengths of advanced time-series sequence modeling (with the HiPPO features) and state-of-the-art neural PDE solvers (with learnable stencil modeling). TSM aims to recover the lost information from the PDE trajectories and can be regarded as a temporal generalization of classic finite volume methods such as WENO. Our experimental results show that TSM achieves the new state-of-the-art simulation accuracy for 2-D incompressible Navier-Stokes turbulent flows: it significantly outperforms the previously reported best results by 19.9% in terms of the highly-correlated duration time and reduces the inference latency into 80%. We also show a strong generalization ability of the proposed method to various out-of-distribution turbulent flow settings. Our code is available at https: //github.com/Edward-Sun/TSM-PDE.']",abstract_chunked," Numerical simulation of non-linear partial differential equations plays a crucial role in modeling physical science and engineering phenomena, such as weather, climate, and aerodynamics. Recent Machine Learning (ML) models trained on low-resolution spatio-temporal signals have shown new promises in capturing important dynamics in high-resolution signals, under the condition that the models can effectively recover the missing details. However, this study shows that significant information is often lost in the lowresolution down-sampled features. To address such issues, we propose a new approach, namely Temporal Stencil Modeling (TSM), which combines the strengths of advanced time-series sequence modeling (with the HiPPO features) and state-of-the-art neural PDE solvers (with learnable stencil modeling). TSM aims to recover the lost information from the PDE trajectories and can be regarded as a temporal generalization of classic finite volume methods such as WENO. Our experimental results show that TSM achieves the new state-of-the-art simulation accuracy for 2-D incompressible Navier-Stokes turbulent flows: it significantly outperforms the previously reported best results by 19.9% in terms of the highly-correlated duration time and reduces the inference latency into 80%. We also show a strong generalization ability of the proposed method to various out-of-distribution turbulent flow settings. Our code is available at https: //github.com/Edward-Sun/TSM-PDE."
61,Zhiqing Sun,"[' We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at.']",abstract_chunked," We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at."
62,Zhiqing Sun,"[' Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learning-to-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants']",abstract_chunked," Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learning-to-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants"
63,Zhiqing Sun,"[' DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly out-perform DETR and other baselines in terms of detection accuracy. Code is released at.']",abstract_chunked," DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly out-perform DETR and other baselines in terms of detection accuracy. Code is released at."
64,Zhiqing Sun,"[' Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3× smaller and 5.5× faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE)']",abstract_chunked," Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3× smaller and 5.5× faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE)"
65,Zhiqing Sun,"[' Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.']",abstract_chunked," Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available."
66,Zhiqing Sun,"[' Autoregressive (AR) models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non-autoregressive (NAR) models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi-modality in sequence generation. This paper proposes a new approach that jointly optimizes both AR and NAR models in a unified Expectation-Maximization (EM) framework. In the E-step, an AR model learns to approximate the regularized posterior of the NAR model. In the M-step, the NAR model is updated on the new posterior and selects the training examples for the next AR model. This iterative process can effectively guide the system to remove the multi-modality in the output sequences. To our knowledge, this is the first EM approach to NAR sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.']",abstract_chunked," Autoregressive (AR) models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non-autoregressive (NAR) models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi-modality in sequence generation. This paper proposes a new approach that jointly optimizes both AR and NAR models in a unified Expectation-Maximization (EM) framework. In the E-step, an AR model learns to approximate the regularized posterior of the NAR model. In the M-step, the NAR model is updated on the new posterior and selects the training examples for the next AR model. This iterative process can effectively guide the system to remove the multi-modality in the output sequences. To our knowledge, this is the first EM approach to NAR sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency."
67,Zhiqing Sun,"[' Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8∼14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models.']",abstract_chunked," Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8∼14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models."
68,Zhiqing Sun,"[' Keyphrase extraction from documents is useful to a variety of applications such as information retrieval and document summarization. This paper presents an end-to-end method called DivGraphPointer for extracting a set of diversified keyphrases from a document. DivGraphPointer combines the advantages of traditional graph-based ranking methods and recent neural network-based approaches. Specifically, given a document, a word graph is constructed from the document based on word proximity and is encoded with graph convolutional networks, which effectively capture document-level word salience by modeling long-range dependency between words in the document and aggregating multiple appearances of identical words into one node. Furthermore, we propose a diversified point network to generate a set of diverse keyphrases out of the word graph in the decoding process. Experimental results on five benchmark data sets show that our proposed method significantly outperforms the existing state-of-the-art approaches.']",abstract_chunked," Keyphrase extraction from documents is useful to a variety of applications such as information retrieval and document summarization. This paper presents an end-to-end method called DivGraphPointer for extracting a set of diversified keyphrases from a document. DivGraphPointer combines the advantages of traditional graph-based ranking methods and recent neural network-based approaches. Specifically, given a document, a word graph is constructed from the document based on word proximity and is encoded with graph convolutional networks, which effectively capture document-level word salience by modeling long-range dependency between words in the document and aggregating multiple appearances of identical words into one node. Furthermore, we propose a diversified point network to generate a set of diverse keyphrases out of the word graph in the decoding process. Experimental results on five benchmark data sets show that our proposed method significantly outperforms the existing state-of-the-art approaches."
69,Zhiqing Sun,"[' We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.']",abstract_chunked," We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction."
70,Zhiqing Sun,"[' Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.']",abstract_chunked," Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff."
71,Timo Schick,"[' Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with “task descriptions” in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.']",abstract_chunked," Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with “task descriptions” in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin."
72,Timo Schick,"[' When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.']",abstract_chunked," When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models."
73,Timo Schick,"[' Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.']",abstract_chunked," Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."
74,Timo Schick,"[' When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.']",abstract_chunked," When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction."
75,Timo Schick,"[' A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language model’s abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand-crafted label-to-word mappings.']",abstract_chunked," A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language model’s abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand-crafted label-to-word mappings."
76,Timo Schick,"[' To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pre-training objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.']",abstract_chunked," To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pre-training objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets."
77,Timo Schick,"[' Pretraining deep neural network architectures with a language modeling objective has brought large improvements for many natural language processing tasks. Exemplified by BERT, a recently proposed such architecture, we demonstrate that despite being trained on huge amounts of data, deep language models still struggle to understand rare words. To fix this problem, we adapt Attentive Mimicking, a method that was designed to explicitly learn embeddings for rare words, to deep language models. In order to make this possible, we introduce one-token approximation, a procedure that enables us to use Attentive Mimicking even when the underlying language model uses subword-based tokenization, i.e., it does not assign embeddings to all words. To evaluate our method, we create a novel dataset that tests the ability of language models to capture semantic properties of words without any task-specific fine-tuning. Using this dataset, we show that adding our adapted version of Attentive Mimicking to BERT does substantially improve its understanding of rare words.']",abstract_chunked," Pretraining deep neural network architectures with a language modeling objective has brought large improvements for many natural language processing tasks. Exemplified by BERT, a recently proposed such architecture, we demonstrate that despite being trained on huge amounts of data, deep language models still struggle to understand rare words. To fix this problem, we adapt Attentive Mimicking, a method that was designed to explicitly learn embeddings for rare words, to deep language models. In order to make this possible, we introduce one-token approximation, a procedure that enables us to use Attentive Mimicking even when the underlying language model uses subword-based tokenization, i.e., it does not assign embeddings to all words. To evaluate our method, we create a novel dataset that tests the ability of language models to capture semantic properties of words without any task-specific fine-tuning. Using this dataset, we show that adding our adapted version of Attentive Mimicking to BERT does substantially improve its understanding of rare words."
78,Timo Schick,"[' Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few-shot results for a wide range of text classification tasks when combined with gradient-based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Pattern-Exploiting Training (PET), a recently proposed few-shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of PET gives consistent improvements over a strong baseline in few-shot settings.']",abstract_chunked," Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few-shot results for a wide range of text classification tasks when combined with gradient-based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Pattern-Exploiting Training (PET), a recently proposed few-shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of PET gives consistent improvements over a strong baseline in few-shot settings."
79,Timo Schick,"[' Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GENPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GENPET gives consistent improvements over strong baselines in few-shot\nsettings.']",abstract_chunked," Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GENPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GENPET gives consistent improvements over strong baselines in few-shot
settings."
80,Timo Schick,"[' Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today’s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER’s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.']",abstract_chunked," Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today’s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER’s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks."
81,Timo Schick,"[' Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word’s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium-frequency range.']",abstract_chunked," Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word’s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium-frequency range."
82,Timo Schick,"[' Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch¨utze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.']",abstract_chunked," Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch¨utze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks."
83,Timo Schick,"[' Prompt-based approaches are strong at fewshot learning. However, Perez et al. (2021) have recently cast doubt on their performance because they had difficulty getting good results in a “true” few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of PET, a method that combines textual instructions with examplebased finetuning. We show that, if correctly configured, PET performs strongly in a true few-shot setting, i.e., without a dev set. Crucial for this strong performance is PET’s ability to intelligently handle multiple prompts. We then put our findings to a real-world test by running PET on RAFT, a benchmark of tasks taken directly from realistic NLP applications for which no labeled dev or test sets are available. PET achieves a new state of the art on RAFT and performs close to nonexpert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners like PET excel at true few-shot learning and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities.']",abstract_chunked," Prompt-based approaches are strong at fewshot learning. However, Perez et al. (2021) have recently cast doubt on their performance because they had difficulty getting good results in a “true” few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of PET, a method that combines textual instructions with examplebased finetuning. We show that, if correctly configured, PET performs strongly in a true few-shot setting, i.e., without a dev set. Crucial for this strong performance is PET’s ability to intelligently handle multiple prompts. We then put our findings to a real-world test by running PET on RAFT, a benchmark of tasks taken directly from realistic NLP applications for which no labeled dev or test sets are available. PET achieves a new state of the art on RAFT and performs close to nonexpert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners like PET excel at true few-shot learning and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities."
84,Timo Schick,"[' Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for novel words on the fly, i.e., for words that did not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: (i) learning an embedding from the novel word’s surface-form (e.g., subword n-grams) and (ii) learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information – surface-form and context – and show that it results in large increases in embedding quality. Our architecture obtains state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words.']",abstract_chunked," Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for novel words on the fly, i.e., for words that did not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: (i) learning an embedding from the novel word’s surface-form (e.g., subword n-grams) and (ii) learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information – surface-form and context – and show that it results in large increases in embedding quality. Our architecture obtains state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words."
85,Timo Schick,"[' This work addresses the task of generating English sentences from Abstract Meaning Representation (AMR) graphs. To cope with this task, we transform each input AMR graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an algorithm that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Bleu score of 27.4 on the LDC2014T12 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data.']",abstract_chunked," This work addresses the task of generating English sentences from Abstract Meaning Representation (AMR) graphs. To cope with this task, we transform each input AMR graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an algorithm that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Bleu score of 27.4 on the LDC2014T12 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data."
0,GPT-3.5,"[' The availability of diverse Large Language Models (LLMs) through cloud API providers offers a spectrum of choices, but the challenge lies in efficiently utilizing these options to strike a balance between computational cost and performance optimization. This work presents AutoMix, a novel approach addressing this challenge by intelligently directing queries to larger LLMs based on the reliability of outputs from a smaller LM. The key innovation lies in a few-shot self-verification mechanism, eliminating the need for extensive training to estimate output reliability. Recognizing the potential noise in verifications, AutoMix incorporates a meta verifier to refine the accuracy of these assessments. The experimental evaluation, conducted on LLAMA2-13/70B across five context-grounded reasoning datasets, demonstrates the superiority of AutoMix over established baselines, showcasing a remarkable up to 89% enhancement in the incremental benefit per cost.']",intro_chunked," The availability of diverse Large Language Models (LLMs) through cloud API providers offers a spectrum of choices, but the challenge lies in efficiently utilizing these options to strike a balance between computational cost and performance optimization. This work presents AutoMix, a novel approach addressing this challenge by intelligently directing queries to larger LLMs based on the reliability of outputs from a smaller LM. The key innovation lies in a few-shot self-verification mechanism, eliminating the need for extensive training to estimate output reliability. Recognizing the potential noise in verifications, AutoMix incorporates a meta verifier to refine the accuracy of these assessments. The experimental evaluation, conducted on LLAMA2-13/70B across five context-grounded reasoning datasets, demonstrates the superiority of AutoMix over established baselines, showcasing a remarkable up to 89% enhancement in the incremental benefit per cost."
1,GPT-3.5,"[' Large Language Models (LLMs) have exhibited remarkable capabilities in natural language processing tasks. However, their performance is not infallible, often producing suboptimal outputs in the first attempt. This motivates the development of SELF-REFINE, an innovative approach aimed at refining LLM outputs through an iterative feedback loop. Inspired by human writing refinement processes, SELF-REFINE leverages a single LLM for the entire feedback and improvement pipeline. Unlike existing methods, our approach circumvents the need for additional supervised training data, specialized training, or reinforcement learning. In this paper, we explore the efficacy of SELF-REFINE across a spectrum of 7 diverse tasks, spanning conversational contexts to intricate mathematical reasoning. Utilizing cutting-edge LLMs, including GPT-3.5 and GPT-4, we present compelling evidence that SELF-REFINE consistently outperforms conventional one-step generation, showcasing an average improvement of around 20% in task performance. Our work underscores the untapped potential for refining even the most advanced LLMs at test-time with a straightforward, standalone strategy.']",intro_chunked," Large Language Models (LLMs) have exhibited remarkable capabilities in natural language processing tasks. However, their performance is not infallible, often producing suboptimal outputs in the first attempt. This motivates the development of SELF-REFINE, an innovative approach aimed at refining LLM outputs through an iterative feedback loop. Inspired by human writing refinement processes, SELF-REFINE leverages a single LLM for the entire feedback and improvement pipeline. Unlike existing methods, our approach circumvents the need for additional supervised training data, specialized training, or reinforcement learning. In this paper, we explore the efficacy of SELF-REFINE across a spectrum of 7 diverse tasks, spanning conversational contexts to intricate mathematical reasoning. Utilizing cutting-edge LLMs, including GPT-3.5 and GPT-4, we present compelling evidence that SELF-REFINE consistently outperforms conventional one-step generation, showcasing an average improvement of around 20% in task performance. Our work underscores the untapped potential for refining even the most advanced LLMs at test-time with a straightforward, standalone strategy."
2,GPT-3.5,"[' Machine learning models commonly lack the nuanced adaptability demonstrated by human cognitive processes. Humans instinctively switch between fast and slow thinking modes based on the difficulty of a task, as articulated in the dual-process theory of mind. Addressing this disparity, we present FLOWGEN, a novel graph generation model inspired by the dual-process theory. FLOWGEN incorporates both FAST (weaker) and SLOW (stronger) modules, sharing architectural similarities but diverging in parameter count and generative capabilities. The model dynamically selects the appropriate module based on the difficulty of the current graph completion step. Through empirical evaluations on real-world graphs, FLOWGEN exhibits the capability to generate graphs comparable to those produced by a single large model, with the added advantage of being up to 2x faster.']",intro_chunked," Machine learning models commonly lack the nuanced adaptability demonstrated by human cognitive processes. Humans instinctively switch between fast and slow thinking modes based on the difficulty of a task, as articulated in the dual-process theory of mind. Addressing this disparity, we present FLOWGEN, a novel graph generation model inspired by the dual-process theory. FLOWGEN incorporates both FAST (weaker) and SLOW (stronger) modules, sharing architectural similarities but diverging in parameter count and generative capabilities. The model dynamically selects the appropriate module based on the difficulty of the current graph completion step. Through empirical evaluations on real-world graphs, FLOWGEN exhibits the capability to generate graphs comparable to those produced by a single large model, with the added advantage of being up to 2x faster."
3,GPT-3.5,"[' Training data scarcity remains a formidable challenge in low-resource language domains. In response, we propose a novel method for curating high-quality comparable training data, leveraging the capabilities of monolingual annotators. Our methodology revolves around the selection of a judicious set of images that act as linguistic pivots between source and target languages. By independently eliciting captions for these images in both languages, we establish a robust and comparable corpus. To assess the efficacy of our approach, we conduct human evaluations on the English-Hindi comparable corpora, revealing an impressive 81.1% acceptability rate for translation pairs, with a mere 2.47% identified as non-translations. Additionally, we explore the potential of the curated dataset through experiments on two downstream tasksâ€”machine translation and dictionary extraction.']",intro_chunked," Training data scarcity remains a formidable challenge in low-resource language domains. In response, we propose a novel method for curating high-quality comparable training data, leveraging the capabilities of monolingual annotators. Our methodology revolves around the selection of a judicious set of images that act as linguistic pivots between source and target languages. By independently eliciting captions for these images in both languages, we establish a robust and comparable corpus. To assess the efficacy of our approach, we conduct human evaluations on the English-Hindi comparable corpora, revealing an impressive 81.1% acceptability rate for translation pairs, with a mere 2.47% identified as non-translations. Additionally, we explore the potential of the curated dataset through experiments on two downstream tasksâ€”machine translation and dictionary extraction."
4,GPT-3.5,"[' In the realm of natural language processing, this paper pioneers the task of politeness transfer – a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 1.39 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag-and-generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state-of-the-art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open-sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research.']",intro_chunked," In the realm of natural language processing, this paper pioneers the task of politeness transfer – a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 1.39 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag-and-generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state-of-the-art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open-sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research."
5,GPT-3.5,"[' Comprehending dynamic processes necessitates the ability to analyze events and comprehend their intricate interconnections. This paper introduces EIGEN, an innovative approach leveraging pre-trained language models for the generation of event influences. By considering context, the nature of influence, and the position in a reasoning chain, EIGEN excels in producing high-quality event influence representations. We address the current gap in research with the introduction of a dedicated dataset, providing a standardized platform for evaluating methods in the realm of event influence generation. Through rigorous evaluation against strong baselines, EIGEN emerges as a superior solution, not only in terms of automated metrics but also in human assessments of generated content, affirming its effectiveness in capturing both reference closeness and relevance.']",intro_chunked," Comprehending dynamic processes necessitates the ability to analyze events and comprehend their intricate interconnections. This paper introduces EIGEN, an innovative approach leveraging pre-trained language models for the generation of event influences. By considering context, the nature of influence, and the position in a reasoning chain, EIGEN excels in producing high-quality event influence representations. We address the current gap in research with the introduction of a dedicated dataset, providing a standardized platform for evaluating methods in the realm of event influence generation. Through rigorous evaluation against strong baselines, EIGEN emerges as a superior solution, not only in terms of automated metrics but also in human assessments of generated content, affirming its effectiveness in capturing both reference closeness and relevance."
6,GPT-3.5,"[' Convolutional Neural Networks (CNNs) have long reigned supreme in the realm of image classification, but recent breakthroughs have demonstrated the potential of transformers in this domain. This work delves into the unexplored territory of optimizing image transformers for large-scale image classification tasks. The primary focus is on understanding and enhancing the interplay between the architecture and optimization techniques of dedicated transformers. Two crucial architectural modifications are introduced, each contributing significantly to the accuracy of deep transformers. These changes not only address early saturation issues associated with increased model depth but also result in models that outperform existing state-of-the-art solutions on ImageNet. The achieved 86.5% top-1 accuracy without external data showcases the effectiveness of the proposed approach in achieving superior performance with reduced computational complexity and model parameters.']",intro_chunked," Convolutional Neural Networks (CNNs) have long reigned supreme in the realm of image classification, but recent breakthroughs have demonstrated the potential of transformers in this domain. This work delves into the unexplored territory of optimizing image transformers for large-scale image classification tasks. The primary focus is on understanding and enhancing the interplay between the architecture and optimization techniques of dedicated transformers. Two crucial architectural modifications are introduced, each contributing significantly to the accuracy of deep transformers. These changes not only address early saturation issues associated with increased model depth but also result in models that outperform existing state-of-the-art solutions on ImageNet. The achieved 86.5% top-1 accuracy without external data showcases the effectiveness of the proposed approach in achieving superior performance with reduced computational complexity and model parameters."
7,GPT-3.5,"[' Addressing unpaired image-to-image translation tasks, such as style or class transfer, denoising, and more, requires efficient architectures that balance simplicity and effectiveness. In this work, we present a novel approach starting from a fixed-weight image autoencoder architecture. Our method introduces task-specific residual blocks operating in the latent space, employed iteratively until the target domain is reached. To counteract the exponentiation effect inherent in iterative processes, a specific training schedule is implemented. At the testing phase, our approach stands out due to its limited number of weight parameters and a compositional design allowing the modulation of transformation strength based on the number of iterations. This proves particularly advantageous when the characteristics of the noise to be suppressed are uncertain. We provide experimental evidence supporting the efficacy of our method, demonstrating its utility for various transformations, and showcasing comparable or superior performance to CycleGAN while utilizing significantly fewer parameters.']",intro_chunked," Addressing unpaired image-to-image translation tasks, such as style or class transfer, denoising, and more, requires efficient architectures that balance simplicity and effectiveness. In this work, we present a novel approach starting from a fixed-weight image autoencoder architecture. Our method introduces task-specific residual blocks operating in the latent space, employed iteratively until the target domain is reached. To counteract the exponentiation effect inherent in iterative processes, a specific training schedule is implemented. At the testing phase, our approach stands out due to its limited number of weight parameters and a compositional design allowing the modulation of transformation strength based on the number of iterations. This proves particularly advantageous when the characteristics of the noise to be suppressed are uncertain. We provide experimental evidence supporting the efficacy of our method, demonstrating its utility for various transformations, and showcasing comparable or superior performance to CycleGAN while utilizing significantly fewer parameters."
8,GPT-3.5,"[' In response to the growing demand for sophisticated language models, we present Llama 2, a significant advancement in the landscape of pretrained and fine-tuned large language models (LLMs). Llama 2 encompasses a diverse range of models, spanning from 7 billion to 70 billion parameters. Notably, our focus lies on Llama 2-Chat, a set of fine-tuned models tailored for dialogue-based applications. Through rigorous benchmark evaluations, we establish the superior performance of our models compared to existing open-source chat models. Human evaluations, particularly in terms of helpfulness and safety, suggest the potential suitability of Llama 2-Chat as a substitute for closed-source alternatives. This paper outlines our approach to fine-tuning and safety enhancements, aiming to facilitate knowledge-sharing within the community and foster responsible advancements in LLM development.']",intro_chunked," In response to the growing demand for sophisticated language models, we present Llama 2, a significant advancement in the landscape of pretrained and fine-tuned large language models (LLMs). Llama 2 encompasses a diverse range of models, spanning from 7 billion to 70 billion parameters. Notably, our focus lies on Llama 2-Chat, a set of fine-tuned models tailored for dialogue-based applications. Through rigorous benchmark evaluations, we establish the superior performance of our models compared to existing open-source chat models. Human evaluations, particularly in terms of helpfulness and safety, suggest the potential suitability of Llama 2-Chat as a substitute for closed-source alternatives. This paper outlines our approach to fine-tuning and safety enhancements, aiming to facilitate knowledge-sharing within the community and foster responsible advancements in LLM development."
9,GPT-3.5,"["" The growing availability of extensive neuroimaging databases has opened up remarkable opportunities for advancing machine learning algorithms. However, the inherent variability in these databases, arising from factors such as age, gender, and acquisition parameters, poses significant challenges to classification methods. These nuisance variables not only affect performance but can also introduce complexities in interpreting the model's behavior. This paper addresses the crucial issue of handling data from different databases. Initial experiments, conducted on simulated data, illustrate the potential pitfalls associated with interactions between data and confounding variables, particularly age. Subsequently, we delve into a comparison of three strategies for data adjustment, with a specific focus on their application in a real-world scenario involving a Computer-Aided Diagnosis (CAD) system. This system aims to distinguish between healthy and Alzheimer's Disease (AD) subjects, utilizing volumetric characteristics derived from structural MRI.""]",intro_chunked," The growing availability of extensive neuroimaging databases has opened up remarkable opportunities for advancing machine learning algorithms. However, the inherent variability in these databases, arising from factors such as age, gender, and acquisition parameters, poses significant challenges to classification methods. These nuisance variables not only affect performance but can also introduce complexities in interpreting the model's behavior. This paper addresses the crucial issue of handling data from different databases. Initial experiments, conducted on simulated data, illustrate the potential pitfalls associated with interactions between data and confounding variables, particularly age. Subsequently, we delve into a comparison of three strategies for data adjustment, with a specific focus on their application in a real-world scenario involving a Computer-Aided Diagnosis (CAD) system. This system aims to distinguish between healthy and Alzheimer's Disease (AD) subjects, utilizing volumetric characteristics derived from structural MRI."
10,GPT-3.5,"["" The ubiquity of machine learning, particularly deep learning, has ushered in a transformative era across diverse societal realms. Noteworthy applications in natural language processing and computer vision have demonstrated the prowess of emblematic architectures like AlexNet, ResNet, and GPT. While these architectures have rightfully garnered attention, the complementary role of well-designed optimization procedures remains a critical, albeit often overlooked, component of their success. This thesis embarks on an exploration of the intricate dynamics between architectures and training procedures, with a specific focus on the application of Transformer architectures to visual understanding. Unlike their convolutional counterparts, transformers' training procedures are in a nascent stage, prompting an imperative need for refinement. The pivotal role of training in mitigating the architectural constraints of transformers serves as the guiding principle for our investigation. The research journey unfolds by delving into the potential of learning with coarse labels, introducing a modification to the training procedure. Subsequent chapters undertake a comprehensive study of various computer vision architectures, dissecting their attributes, advantages, drawbacks, and the nuances of effective training methodologies. This exploration ultimately leads to an analysis of the symbiotic relationship between architecture and training processes.""]",intro_chunked," The ubiquity of machine learning, particularly deep learning, has ushered in a transformative era across diverse societal realms. Noteworthy applications in natural language processing and computer vision have demonstrated the prowess of emblematic architectures like AlexNet, ResNet, and GPT. While these architectures have rightfully garnered attention, the complementary role of well-designed optimization procedures remains a critical, albeit often overlooked, component of their success. This thesis embarks on an exploration of the intricate dynamics between architectures and training procedures, with a specific focus on the application of Transformer architectures to visual understanding. Unlike their convolutional counterparts, transformers' training procedures are in a nascent stage, prompting an imperative need for refinement. The pivotal role of training in mitigating the architectural constraints of transformers serves as the guiding principle for our investigation. The research journey unfolds by delving into the potential of learning with coarse labels, introducing a modification to the training procedure. Subsequent chapters undertake a comprehensive study of various computer vision architectures, dissecting their attributes, advantages, drawbacks, and the nuances of effective training methodologies. This exploration ultimately leads to an analysis of the symbiotic relationship between architecture and training processes."
11,GPT-3.5,"[' Traditional convolutional neural networks (CNNs) have been the cornerstone of image classification architectures, but this paper proposes a paradigm shift with ResMLP. Built solely upon multi-layer perceptrons, ResMLP introduces a straightforward yet highly effective residual network. The architecture alternates between two key components: a linear layer facilitating interaction among image patches across channels, and a two-layer feed-forward network promoting independent channel interaction per patch. This design choice, coupled with contemporary training strategies encompassing robust data augmentation and optional distillation, yields unexpectedly favorable results on the ImageNet dataset. Moreover, this paper extends the application of ResMLP to self-supervised learning, showcasing its ability to learn meaningful representations without explicit supervision. By reducing reliance on labeled datasets, ResMLP demonstrates versatility and robustness across diverse training scenarios. The paper also explores the adaptability of ResMLP to machine translation, achieving surprisingly competitive results in this domain.']",intro_chunked," Traditional convolutional neural networks (CNNs) have been the cornerstone of image classification architectures, but this paper proposes a paradigm shift with ResMLP. Built solely upon multi-layer perceptrons, ResMLP introduces a straightforward yet highly effective residual network. The architecture alternates between two key components: a linear layer facilitating interaction among image patches across channels, and a two-layer feed-forward network promoting independent channel interaction per patch. This design choice, coupled with contemporary training strategies encompassing robust data augmentation and optional distillation, yields unexpectedly favorable results on the ImageNet dataset. Moreover, this paper extends the application of ResMLP to self-supervised learning, showcasing its ability to learn meaningful representations without explicit supervision. By reducing reliance on labeled datasets, ResMLP demonstrates versatility and robustness across diverse training scenarios. The paper also explores the adaptability of ResMLP to machine translation, achieving surprisingly competitive results in this domain."
12,GPT-3.5,"[' The dynamic landscape of Knowledge Graph Completion (KGC) has witnessed a proliferation of advanced techniques, each vying for superiority in predicting missing links within expansive knowledge graphs. Notably, recent publications have showcased seemingly exceptional performance, surpassing established state-of-the-art methods. However, a critical examination reveals that the reported advancements may be linked to the adoption of inappropriate evaluation protocols. In this paper, we address this issue by proposing a simple yet robust evaluation protocol. This protocol aims to rectify the inadequacies of existing evaluation methodologies and, crucially, is designed to handle model bias effectively. We embark on extensive experiments to assess the performance of several existing KGC methods using our proposed protocol. The openness of our reproducible code further enhances the transparency and replicability of our findings.']",intro_chunked," The dynamic landscape of Knowledge Graph Completion (KGC) has witnessed a proliferation of advanced techniques, each vying for superiority in predicting missing links within expansive knowledge graphs. Notably, recent publications have showcased seemingly exceptional performance, surpassing established state-of-the-art methods. However, a critical examination reveals that the reported advancements may be linked to the adoption of inappropriate evaluation protocols. In this paper, we address this issue by proposing a simple yet robust evaluation protocol. This protocol aims to rectify the inadequacies of existing evaluation methodologies and, crucially, is designed to handle model bias effectively. We embark on extensive experiments to assess the performance of several existing KGC methods using our proposed protocol. The openness of our reproducible code further enhances the transparency and replicability of our findings."
13,GPT-3.5,"[' Autoregressive sequence models have demonstrated state-of-the-art performance in diverse domains such as machine translation. Despite their prowess, these models suffer from substantial inference latency due to their autoregressive factorization nature. Non-autoregressive sequence models have been introduced as a solution to reduce inference time, but their assumption of conditional independence during token decoding often leads to output inconsistencies and inferior accuracy compared to autoregressive counterparts. This paper proposes a novel approach to enhance decoding consistency and reduce inference cost simultaneously by integrating a structured inference module into non-autoregressive models. We introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models and incorporate a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation demonstrate that our model achieves significantly improved translation performance compared to previous non-autoregressive models, with minimal additional latency (8âˆ¼14ms). Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and closely approaches the accuracy of purely autoregressive models, trailing by only 0.61 in BLEU.']",intro_chunked," Autoregressive sequence models have demonstrated state-of-the-art performance in diverse domains such as machine translation. Despite their prowess, these models suffer from substantial inference latency due to their autoregressive factorization nature. Non-autoregressive sequence models have been introduced as a solution to reduce inference time, but their assumption of conditional independence during token decoding often leads to output inconsistencies and inferior accuracy compared to autoregressive counterparts. This paper proposes a novel approach to enhance decoding consistency and reduce inference cost simultaneously by integrating a structured inference module into non-autoregressive models. We introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models and incorporate a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation demonstrate that our model achieves significantly improved translation performance compared to previous non-autoregressive models, with minimal additional latency (8âˆ¼14ms). Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and closely approaches the accuracy of purely autoregressive models, trailing by only 0.61 in BLEU."
14,GPT-3.5,"[' Supervised Fine-Tuning (SFT) in conjunction with Reinforcement Learning from Human Feedback (RLHF) has proven effective for aligning AI agents based on large language models (LLMs). However, the necessity for high-quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in-distribution preferences is non-trivial. This paper presents SALMON (Self-ALignMent with principlefOllowiNg reward models), a pioneering approach to aligning base language models with minimal human supervision. Central to SALMON is a principle-following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human-defined principles. By adjusting these principles during RL training, we achieve precise control over preferences, influencing the behavior of RL-trained policies, and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method results in the creation of Dromedary-2, an AI assistant outperforming state-of-the-art systems with only 6 exemplars for in-context learning and 31 human-defined principles.']",intro_chunked," Supervised Fine-Tuning (SFT) in conjunction with Reinforcement Learning from Human Feedback (RLHF) has proven effective for aligning AI agents based on large language models (LLMs). However, the necessity for high-quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in-distribution preferences is non-trivial. This paper presents SALMON (Self-ALignMent with principlefOllowiNg reward models), a pioneering approach to aligning base language models with minimal human supervision. Central to SALMON is a principle-following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human-defined principles. By adjusting these principles during RL training, we achieve precise control over preferences, influencing the behavior of RL-trained policies, and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method results in the creation of Dromedary-2, an AI assistant outperforming state-of-the-art systems with only 6 exemplars for in-context learning and 31 human-defined principles."
15,GPT-3.5,"["" Learning effective representations of entities and relations in knowledge graphs is pivotal for predicting missing links. The success of this task crucially depends on the model's ability to capture and infer intricate patterns within relations. In response, we present RotatE, a novel knowledge graph embedding approach specifically designed to model and infer a range of relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation in the complex vector space, providing a versatile framework for capturing diverse relation characteristics. Additionally, we introduce a novel self-adversarial negative sampling technique, enhancing the efficiency and effectiveness of RotatE model training. Through comprehensive experiments on multiple benchmark knowledge graphs, we demonstrate that RotatE is not only scalable but also excels in inferring and modeling various relation patterns, surpassing the performance of existing state-of-the-art models in link prediction.""]",intro_chunked," Learning effective representations of entities and relations in knowledge graphs is pivotal for predicting missing links. The success of this task crucially depends on the model's ability to capture and infer intricate patterns within relations. In response, we present RotatE, a novel knowledge graph embedding approach specifically designed to model and infer a range of relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation in the complex vector space, providing a versatile framework for capturing diverse relation characteristics. Additionally, we introduce a novel self-adversarial negative sampling technique, enhancing the efficiency and effectiveness of RotatE model training. Through comprehensive experiments on multiple benchmark knowledge graphs, we demonstrate that RotatE is not only scalable but also excels in inferring and modeling various relation patterns, surpassing the performance of existing state-of-the-art models in link prediction."
16,GPT-3.5,"["" In the realm of Large Language Models (LLMs), the quest for more accurate factual knowledge generation has led to the development of RECITation-augmented gEneration (RECITE). In contrast to traditional retrieval-augmented models, RECITE pioneers a recite-first approach, where pertinent passages are drawn from the LLMs' internal memory through sampling before the final answers are generated. This innovative strategy is tailored for knowledge-intensive Natural Language Processing (NLP) tasks, specifically closed-book question answering (CBQA). Our motivation stems from the need to reduce reliance on external corpuses and enhance the model's autonomy in generating precise and contextually relevant information. In this paper, we present RECITE as a powerful paradigm by demonstrating its applicability across diverse CBQA tasks, including Natural Questions, TriviaQA, and HotpotQA. We conduct extensive experiments employing four pre-trained models—PaLM, UL2, OPT, and Codex—to validate RECITE's effectiveness. The results showcase that the recite-and-answer scheme consistently achieves new state-of-the-art performance, establishing RECITE as a promising avenue for advancing factual knowledge generation within LLMs.""]",intro_chunked," In the realm of Large Language Models (LLMs), the quest for more accurate factual knowledge generation has led to the development of RECITation-augmented gEneration (RECITE). In contrast to traditional retrieval-augmented models, RECITE pioneers a recite-first approach, where pertinent passages are drawn from the LLMs' internal memory through sampling before the final answers are generated. This innovative strategy is tailored for knowledge-intensive Natural Language Processing (NLP) tasks, specifically closed-book question answering (CBQA). Our motivation stems from the need to reduce reliance on external corpuses and enhance the model's autonomy in generating precise and contextually relevant information. In this paper, we present RECITE as a powerful paradigm by demonstrating its applicability across diverse CBQA tasks, including Natural Questions, TriviaQA, and HotpotQA. We conduct extensive experiments employing four pre-trained models—PaLM, UL2, OPT, and Codex—to validate RECITE's effectiveness. The results showcase that the recite-and-answer scheme consistently achieves new state-of-the-art performance, establishing RECITE as a promising avenue for advancing factual knowledge generation within LLMs."
17,GPT-3.5,"[' Natural Language Processing (NLP) has made remarkable strides, leveraging massive pre-trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces MobileBERT, a solution that addresses these challenges by compressing and accelerating the popular BERT model. Maintaining the versatility of the original BERT, MobileBERT is designed to be task-agnostic, allowing seamless application to diverse NLP tasks with minimal fine-tuning. The architecture of MobileBERT draws inspiration from BERTLARGE but incorporates bottleneck structures and a finely tuned balance between self-attentions and feed-forward networks. The training strategy involves a specially crafted teacher model, an inverted-bottleneck incorporated BERTLARGE, from which knowledge is transferred to MobileBERT. Empirical results demonstrate the efficiency of MobileBERT, achieving a 4.3× reduction in size and a 5.5× acceleration compared to BERTBASE, while still maintaining competitive performance on established benchmarks.']",intro_chunked," Natural Language Processing (NLP) has made remarkable strides, leveraging massive pre-trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces MobileBERT, a solution that addresses these challenges by compressing and accelerating the popular BERT model. Maintaining the versatility of the original BERT, MobileBERT is designed to be task-agnostic, allowing seamless application to diverse NLP tasks with minimal fine-tuning. The architecture of MobileBERT draws inspiration from BERTLARGE but incorporates bottleneck structures and a finely tuned balance between self-attentions and feed-forward networks. The training strategy involves a specially crafted teacher model, an inverted-bottleneck incorporated BERTLARGE, from which knowledge is transferred to MobileBERT. Empirical results demonstrate the efficiency of MobileBERT, achieving a 4.3× reduction in size and a 5.5× acceleration compared to BERTBASE, while still maintaining competitive performance on established benchmarks."
18,GPT-3.5,"["" Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce PEER, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of PEER to cover various parts of the writing process, leveraging self-training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEER's applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEER's robust performance across a range of domains and editing tasks.""]",intro_chunked," Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce PEER, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of PEER to cover various parts of the writing process, leveraging self-training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEER's applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEER's robust performance across a range of domains and editing tasks."
19,GPT-3.5,"["" Pretraining deep language models, exemplified by BERT, has yielded significant advancements in natural language processing (NLP). However, recent observations underscore the challenge these models face in comprehending rare words. This work addresses the issue by introducing BERTRAM, an innovative architecture based on BERT designed to generate high-quality embeddings for rare words. Drawing inspiration from strategies applied to static word embeddings, BERTRAM facilitates the interaction between a word's surface form and contexts within a deep architecture. When integrated into BERT, BERTRAM achieves substantial performance boosts, particularly in enhancing representations of rare and medium-frequency words. This improvement is demonstrated across various tasks, including a rare word probing task and three downstream applications, highlighting BERTRAM's efficacy in enhancing the treatment of rare words within pretrained language models.""]",intro_chunked," Pretraining deep language models, exemplified by BERT, has yielded significant advancements in natural language processing (NLP). However, recent observations underscore the challenge these models face in comprehending rare words. This work addresses the issue by introducing BERTRAM, an innovative architecture based on BERT designed to generate high-quality embeddings for rare words. Drawing inspiration from strategies applied to static word embeddings, BERTRAM facilitates the interaction between a word's surface form and contexts within a deep architecture. When integrated into BERT, BERTRAM achieves substantial performance boosts, particularly in enhancing representations of rare and medium-frequency words. This improvement is demonstrated across various tasks, including a rare word probing task and three downstream applications, highlighting BERTRAM's efficacy in enhancing the treatment of rare words within pretrained language models."
20,GPT-3.5,"[' The prospect of solving natural language processing (NLP) tasks in a fully unsupervised manner by providing pretrained language models with task descriptions has been explored but often falls short of supervised counterparts. In this work, we introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that seeks to bridge the gap between unsupervised and supervised learning. PET transforms input examples into cloze-style phrases, serving as guidance for language models in understanding the underlying task. These phrases are then leveraged to assign soft labels to a substantial set of unlabeled examples. Finally, standard supervised training is applied to the resulting training set. Across diverse tasks and languages, PET showcases superior performance compared to both supervised training and robust semi-supervised approaches, particularly demonstrating efficacy in low-resource settings.']",intro_chunked," The prospect of solving natural language processing (NLP) tasks in a fully unsupervised manner by providing pretrained language models with task descriptions has been explored but often falls short of supervised counterparts. In this work, we introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that seeks to bridge the gap between unsupervised and supervised learning. PET transforms input examples into cloze-style phrases, serving as guidance for language models in understanding the underlying task. These phrases are then leveraged to assign soft labels to a substantial set of unlabeled examples. Finally, standard supervised training is applied to the resulting training set. Across diverse tasks and languages, PET showcases superior performance compared to both supervised training and robust semi-supervised approaches, particularly demonstrating efficacy in low-resource settings."
21,GPT-3.5,"[' Effective representation learning for novel words in natural language processing (NLP) systems remains a significant challenge. Existing approaches tackle this issue by either focusing on the surface-form or contextual usage of novel words. In this paper, we present an innovative architecture that leverages both surface-form and context information to enhance the quality of word embeddings. Our model achieves state-of-the-art results on benchmark datasets such as Definitional Nonce and Contextual Rare Words. Crucially, our approach requires only an embedding set and an unlabeled corpus for training, offering a versatile solution that can be seamlessly integrated into any existing NLP system, augmenting its capability to handle novel words.']",intro_chunked," Effective representation learning for novel words in natural language processing (NLP) systems remains a significant challenge. Existing approaches tackle this issue by either focusing on the surface-form or contextual usage of novel words. In this paper, we present an innovative architecture that leverages both surface-form and context information to enhance the quality of word embeddings. Our model achieves state-of-the-art results on benchmark datasets such as Definitional Nonce and Contextual Rare Words. Crucially, our approach requires only an embedding set and an unlabeled corpus for training, offering a versatile solution that can be seamlessly integrated into any existing NLP system, augmenting its capability to handle novel words."
22,GPT-3.5,"[' The quest for high-quality sentence embeddings has been marked by the dichotomy between augmenting pretrained language models (PLMs) with supplementary pretraining objectives and relying on finetuning with large labeled datasets. While the latter typically outperforms the former, the formidable challenge lies in the human effort required to generate expansive, suitable datasets. This paper presents a groundbreaking alternative, demonstrating how the generative capabilities of large and high-performing PLMs can be harnessed to overcome this challenge. By generating entire datasets of labeled text pairs de novo, our approach eliminates the need for labeled data, finetuning, or modifications to pretraining objectives. This innovative strategy facilitates the finetuning of smaller and more efficient models, achieving superior performance compared to established baselines on diverse semantic textual similarity datasets.']",intro_chunked," The quest for high-quality sentence embeddings has been marked by the dichotomy between augmenting pretrained language models (PLMs) with supplementary pretraining objectives and relying on finetuning with large labeled datasets. While the latter typically outperforms the former, the formidable challenge lies in the human effort required to generate expansive, suitable datasets. This paper presents a groundbreaking alternative, demonstrating how the generative capabilities of large and high-performing PLMs can be harnessed to overcome this challenge. By generating entire datasets of labeled text pairs de novo, our approach eliminates the need for labeled data, finetuning, or modifications to pretraining objectives. This innovative strategy facilitates the finetuning of smaller and more efficient models, achieving superior performance compared to established baselines on diverse semantic textual similarity datasets."
23,GPT-3.5,"[' The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text-related tasks and improving few-shot learning outcomes. This paper introduces GENPET, a method rooted in pattern-exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few-shot text generation.']",intro_chunked," The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text-related tasks and improving few-shot learning outcomes. This paper introduces GENPET, a method rooted in pattern-exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few-shot text generation."
24,Aman Madaan,"[' Human problem-solving inherently follows a multi-step process: generate a solution, verify its\nvalidity, and refine it further based on verification outcomes. The emulation of this self-refinement\nand reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan\net al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,\ndemonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the\nintrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable\ndata) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step\nreasoning), motivate an alternative approach of model switching. Model switching iteratively queries\nover models of disparate sizes and capabilities, verifying feedback at each step and determining\nwhether to accept the output or route to a more capable, albeit computationally intensive, model (Liu\net al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly\nfor each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,\n2022).', 'However, modern LLM often provide access solely through black-box APIs, restricting\ndirect model optimization and adaptability due to the unavailability of fine-tuning capabilities and\nweight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM\nAPIs, circumventing the necessity for separate models or logits access by adopting few-shot learning\nstrategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies\nfor each step of problem-solving: solution generation, verification, and routing, all assuming we only\nhave access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model\nrouting, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified\nearly. This consideration allows AutoMix to judiciously allocate computational resources, preventing\nunwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with\nthe provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan\net al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic\nlife"" would be flagged as inconsistent.', 'However, recognizing that self-verification can sometimes\nbe inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability\nof the initial verification. The meta-verifier acts as a secondary check, providing an additional layer\nof confidence assessment to ensure that the decision to route a task to a larger or smaller model is\nwell-founded. In summary, our contributions are:\n• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating\na solution, verifying the solution, and switching to a larger language model, everything without\naccess to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps\nimprove the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure\nthat quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets\nusing the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large\n(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing\nthe incremental benefit per cost by up to 89%.']",intro_chunked," Human problem-solving inherently follows a multi-step process: generate a solution, verify its
validity, and refine it further based on verification outcomes. The emulation of this self-refinement
and reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan
et al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,
demonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the
intrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable
data) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step
reasoning), motivate an alternative approach of model switching. Model switching iteratively queries
over models of disparate sizes and capabilities, verifying feedback at each step and determining
whether to accept the output or route to a more capable, albeit computationally intensive, model (Liu
et al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly
for each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,
2022)."
25,Aman Madaan,"[' Human problem-solving inherently follows a multi-step process: generate a solution, verify its\nvalidity, and refine it further based on verification outcomes. The emulation of this self-refinement\nand reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan\net al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,\ndemonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the\nintrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable\ndata) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step\nreasoning), motivate an alternative approach of model switching. Model switching iteratively queries\nover models of disparate sizes and capabilities, verifying feedback at each step and determining\nwhether to accept the output or route to a more capable, albeit computationally intensive, model (Liu\net al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly\nfor each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,\n2022).', 'However, modern LLM often provide access solely through black-box APIs, restricting\ndirect model optimization and adaptability due to the unavailability of fine-tuning capabilities and\nweight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM\nAPIs, circumventing the necessity for separate models or logits access by adopting few-shot learning\nstrategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies\nfor each step of problem-solving: solution generation, verification, and routing, all assuming we only\nhave access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model\nrouting, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified\nearly. This consideration allows AutoMix to judiciously allocate computational resources, preventing\nunwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with\nthe provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan\net al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic\nlife"" would be flagged as inconsistent.', 'However, recognizing that self-verification can sometimes\nbe inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability\nof the initial verification. The meta-verifier acts as a secondary check, providing an additional layer\nof confidence assessment to ensure that the decision to route a task to a larger or smaller model is\nwell-founded. In summary, our contributions are:\n• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating\na solution, verifying the solution, and switching to a larger language model, everything without\naccess to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps\nimprove the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure\nthat quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets\nusing the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large\n(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing\nthe incremental benefit per cost by up to 89%.']",intro_chunked,"However, modern LLM often provide access solely through black-box APIs, restricting
direct model optimization and adaptability due to the unavailability of fine-tuning capabilities and
weight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM
APIs, circumventing the necessity for separate models or logits access by adopting few-shot learning
strategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies
for each step of problem-solving: solution generation, verification, and routing, all assuming we only
have access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model
routing, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified
early. This consideration allows AutoMix to judiciously allocate computational resources, preventing
unwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with
the provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan
et al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic
life"" would be flagged as inconsistent."
26,Aman Madaan,"[' Human problem-solving inherently follows a multi-step process: generate a solution, verify its\nvalidity, and refine it further based on verification outcomes. The emulation of this self-refinement\nand reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan\net al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,\ndemonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the\nintrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable\ndata) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step\nreasoning), motivate an alternative approach of model switching. Model switching iteratively queries\nover models of disparate sizes and capabilities, verifying feedback at each step and determining\nwhether to accept the output or route to a more capable, albeit computationally intensive, model (Liu\net al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly\nfor each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,\n2022).', 'However, modern LLM often provide access solely through black-box APIs, restricting\ndirect model optimization and adaptability due to the unavailability of fine-tuning capabilities and\nweight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM\nAPIs, circumventing the necessity for separate models or logits access by adopting few-shot learning\nstrategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies\nfor each step of problem-solving: solution generation, verification, and routing, all assuming we only\nhave access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model\nrouting, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified\nearly. This consideration allows AutoMix to judiciously allocate computational resources, preventing\nunwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with\nthe provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan\net al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic\nlife"" would be flagged as inconsistent.', 'However, recognizing that self-verification can sometimes\nbe inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability\nof the initial verification. The meta-verifier acts as a secondary check, providing an additional layer\nof confidence assessment to ensure that the decision to route a task to a larger or smaller model is\nwell-founded. In summary, our contributions are:\n• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating\na solution, verifying the solution, and switching to a larger language model, everything without\naccess to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps\nimprove the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure\nthat quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets\nusing the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large\n(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing\nthe incremental benefit per cost by up to 89%.']",intro_chunked,"However, recognizing that self-verification can sometimes
be inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability
of the initial verification. The meta-verifier acts as a secondary check, providing an additional layer
of confidence assessment to ensure that the decision to route a task to a larger or smaller model is
well-founded. In summary, our contributions are:
• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating
a solution, verifying the solution, and switching to a larger language model, everything without
access to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps
improve the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure
that quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets
using the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large
(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing
the incremental benefit per cost by up to 89%."
27,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked," Although large language models (LLMs) can generate coherent outputs, they often fall short in
addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such
as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program
readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may
benefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved
one—to ensure that the desired quality is achieved. Iterative refinement typically involves training
a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models
require large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),
which may not always be feasible to obtain. These limitations underscore the need for an effective
refinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;
Flower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating
an initial draft and subsequently refining it based on self-provided feedback."
28,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked,"For example, when drafting an email to request a document from a colleague, an individual may initially write a direct
request such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the
potential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data
at your earliest convenience?"". When writing code, a programmer may implement an initial “quick
and dirty” implementation, and then, upon reflection, refactor their code to a solution that is more
efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement
without additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get
feedback. Then, the feedback is passed back to the same model to refine the previously-generated
draft. This process is repeated either for a specified number of iterations or until M determines that
no further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to
both generate feedback and incorporate the feedback into an improved draft."
29,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked,"Figure 1 illustrates the
high-level idea, that SELF-REFINE uses the same underlying language model to generate feedback
and refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural
language and source-code generation. We show that SELF-REFINE outperforms direct generation
from strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang
et al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,
SELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code
models such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which
is easily extensible to other LLMs. In essence, our results show that even when an LLM cannot
generate an optimal output on its first try, the LLM can often provide useful feedback and improve
its own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs
from a single model without any additional training, via iterative (self-)feedback and refinement"
30,Aman Madaan,"[' Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited\nor the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing\ncompiler, which iteratively applies optimizations as an intermediate step in generating machine code (Aho et al.,\n2007). Despite the impressive progress in optimizing compilers, programmers are still generally responsible for\nnumerous performance considerations. Experienced programmers can often shave off microseconds from an\nalready optimized code. However, such improvements typically come after laborious consideration of factors\nsuch as algorithm selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models (LLMs) have shown great potential in a variety of software-related tasks ranging from\ncompetitive programming (Li et al., 2022), code completion/editing (Fried et al., 2022), to clone detection,\ndefect detection, and much more (Lu et al., 2021; Xu et al., 2022; Zhou et al., 2022; Austin et al., 2021). Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the\nefficiency of programs while operating at the level of high-level programming languages like Python or C++? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or\nvery non-trivial to expect from an optimizing compiler or search procedure.', 'If so, large language models may\nhave an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating\na dataset of Performance-Improving Edits, PIE. We collect trajectories of programs written by the same user,\nwhere we track a single programmer’s submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us\nwith pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize\nimpact on the program’s runtime. We show that PIE allows us to improve the efficacy of large language models for code optimization. Specifically,\nwe investigate the effects of using PIE for few-shot prompting on CODEX and fine-tuning models like CODEGEN. We see noticeable improvements in all experiments using PIE. Ultimately, these models can successfully\nperform substantial (up to 2.5ˆ) code optimization for Python and C++ for many examples (up to 50% of the\ntest set). We also demonstrate that CODEGEN and CODEGEN trained on PIE can, in some cases, match the perfromance of CODEX while being up to 10 X smaller in size than CODEX.']",intro_chunked," Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited
or the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing
compiler, which iteratively applies optimizations as an intermediate step in generating machine code (Aho et al.,
2007). Despite the impressive progress in optimizing compilers, programmers are still generally responsible for
numerous performance considerations. Experienced programmers can often shave off microseconds from an
already optimized code. However, such improvements typically come after laborious consideration of factors
such as algorithm selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models (LLMs) have shown great potential in a variety of software-related tasks ranging from
competitive programming (Li et al., 2022), code completion/editing (Fried et al., 2022), to clone detection,
defect detection, and much more (Lu et al., 2021; Xu et al., 2022; Zhou et al., 2022; Austin et al., 2021). Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the
efficiency of programs while operating at the level of high-level programming languages like Python or C++? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or
very non-trivial to expect from an optimizing compiler or search procedure."
31,Aman Madaan,"[' Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited\nor the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing\ncompiler, which iteratively applies optimizations as an intermediate step in generating machine code (Aho et al.,\n2007). Despite the impressive progress in optimizing compilers, programmers are still generally responsible for\nnumerous performance considerations. Experienced programmers can often shave off microseconds from an\nalready optimized code. However, such improvements typically come after laborious consideration of factors\nsuch as algorithm selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models (LLMs) have shown great potential in a variety of software-related tasks ranging from\ncompetitive programming (Li et al., 2022), code completion/editing (Fried et al., 2022), to clone detection,\ndefect detection, and much more (Lu et al., 2021; Xu et al., 2022; Zhou et al., 2022; Austin et al., 2021). Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the\nefficiency of programs while operating at the level of high-level programming languages like Python or C++? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or\nvery non-trivial to expect from an optimizing compiler or search procedure.', 'If so, large language models may\nhave an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating\na dataset of Performance-Improving Edits, PIE. We collect trajectories of programs written by the same user,\nwhere we track a single programmer’s submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us\nwith pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize\nimpact on the program’s runtime. We show that PIE allows us to improve the efficacy of large language models for code optimization. Specifically,\nwe investigate the effects of using PIE for few-shot prompting on CODEX and fine-tuning models like CODEGEN. We see noticeable improvements in all experiments using PIE. Ultimately, these models can successfully\nperform substantial (up to 2.5ˆ) code optimization for Python and C++ for many examples (up to 50% of the\ntest set). We also demonstrate that CODEGEN and CODEGEN trained on PIE can, in some cases, match the perfromance of CODEX while being up to 10 X smaller in size than CODEX.']",intro_chunked,"If so, large language models may
have an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating
a dataset of Performance-Improving Edits, PIE. We collect trajectories of programs written by the same user,
where we track a single programmer’s submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us
with pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize
impact on the program’s runtime. We show that PIE allows us to improve the efficacy of large language models for code optimization. Specifically,
we investigate the effects of using PIE for few-shot prompting on CODEX and fine-tuning models like CODEGEN. We see noticeable improvements in all experiments using PIE. Ultimately, these models can successfully
perform substantial (up to 2.5ˆ) code optimization for Python and C++ for many examples (up to 50% of the
test set). We also demonstrate that CODEGEN and CODEGEN trained on PIE can, in some cases, match the perfromance of CODEX while being up to 10 X smaller in size than CODEX."
32,Aman Madaan,"[' The growing capabilities of large pre-trained language models (LLMs) for generating text have enabled their successful application in a variety of\ntasks, including summarization, translation, and\nquestion-answering (Wang et al., 2019; Raffel et al.,\n2019; Brown et al., 2020; Chowdhery et al., 2022). Nevertheless, while employing LLMs for natural language (NL) tasks is straightforward, a major remaining challenge is how to leverage LLMs\nfor structured commonsense reasoning, including\ntasks such as generating event graphs (Tandon et al.,\n2019), reasoning graphs (Madaan et al., 2021a),\nscripts (Sakaguchi et al., 2021), and argument explanation graphs (Saha et al., 2021). Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured\noutput given a natural language input. This family\nof tasks relies on the natural language knowledge\nlearned by the LLM, but it also requires complex\nstructured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format\nof a problem. Specifically, the structure to be generated (e.g., a graph or a table) is converted, or\n“serialized”, into text. Such conversions include\n“flattening” the graph into a list of node pairs (Figure 1d), or into a specification language such as\nDOT (Figure 1c; Gansner et al., 2006).', 'While converting the structured output into text\nhas shown promising results (Rajagopal et al.,\n2021; Madaan and Yang, 2021), LLMs struggle\nto generate these “unnatural” outputs: LMs are\nprimarily pre-trained on free-form text, and these\nserialized structured outputs strongly diverge from\nthe majority of the pre-training data. Further, for\nnatural language, semantically relevant words are\ntypically found within a small span, whereas neighboring nodes in a graph might be pushed farther\napart when representing a graph as a flat string. Thus, a language model which was trained on\nnatural language text is likely to fail to capture\nthe topology of the graph. Consequently, using\nLLMs for graph generation typically requires a\nlarge amount of task-specific training data, and\ntheir generated outputs show structural errors and\nsemantic inconsistencies, which need to be further fixed either manually or by using a secondary\ndownstream model (Madaan et al., 2021b). Despite these struggles, the recent success of\nlarge-language models of code (Code-LLMs; Chen\net al., 2021b; Xu et al., 2022) for tasks such as\ncode generation from natural language (Austin\net al., 2021; Nijkamp et al., 2022), code completion (Fried et al., 2022), and code translation (Wang et al.) show that Code-LLMs are able to perform complex reasoning on structured data such\nas programs.', 'Thus, instead of forcing LLMs of\nnatural language (NL-LLMs) to be fine-tuned on\nstructured commonsense data, an easier way to\nclose the discrepancy between the pre-training data\n(free-form text) and the task-specific data (commonsense reasoning graphs) is to adapt LLMs that\nwere pre-trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language\nmodels of code are good structured commonsense\nreasoners. Further, we show that Code-LLMs can\nbe even better structured reasoners than NL-LLMs,\nwhen converting the desired output graph into a format similar to that observed in the code pre-training\ndata. We call our method COCOGEN: models\nof Code for Commonsense Generation, and it is\ndemonstrated in Figure 1. Our contributions are as follows:\n1. We highlight the insight that Code-LLMs\nare better structured commonsense reasoners\nthan NL-LLMs, when representing the desired\ngraph prediction as code. 2. We propose COCOGEN: a method for\nleveraging LLMs of code for structured\ncommonsense generation. 3. We perform an extensive evaluation across\nthree structured commonsense generation\ntasks and demonstrate that COCOGEN vastly\noutperforms NL-LLMs, either fine-tuned or\nfew-shot tested, while controlling for the num-\nber of downstream task examples. 4. We perform a thorough ablation study, which\nshows the role of data formatting, model size,\nand the number of few-shot examples.']",intro_chunked," The growing capabilities of large pre-trained language models (LLMs) for generating text have enabled their successful application in a variety of
tasks, including summarization, translation, and
question-answering (Wang et al., 2019; Raffel et al.,
2019; Brown et al., 2020; Chowdhery et al., 2022). Nevertheless, while employing LLMs for natural language (NL) tasks is straightforward, a major remaining challenge is how to leverage LLMs
for structured commonsense reasoning, including
tasks such as generating event graphs (Tandon et al.,
2019), reasoning graphs (Madaan et al., 2021a),
scripts (Sakaguchi et al., 2021), and argument explanation graphs (Saha et al., 2021). Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured
output given a natural language input. This family
of tasks relies on the natural language knowledge
learned by the LLM, but it also requires complex
structured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format
of a problem. Specifically, the structure to be generated (e.g., a graph or a table) is converted, or
“serialized”, into text. Such conversions include
“flattening” the graph into a list of node pairs (Figure 1d), or into a specification language such as
DOT (Figure 1c; Gansner et al., 2006)."
33,Aman Madaan,"[' The growing capabilities of large pre-trained language models (LLMs) for generating text have enabled their successful application in a variety of\ntasks, including summarization, translation, and\nquestion-answering (Wang et al., 2019; Raffel et al.,\n2019; Brown et al., 2020; Chowdhery et al., 2022). Nevertheless, while employing LLMs for natural language (NL) tasks is straightforward, a major remaining challenge is how to leverage LLMs\nfor structured commonsense reasoning, including\ntasks such as generating event graphs (Tandon et al.,\n2019), reasoning graphs (Madaan et al., 2021a),\nscripts (Sakaguchi et al., 2021), and argument explanation graphs (Saha et al., 2021). Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured\noutput given a natural language input. This family\nof tasks relies on the natural language knowledge\nlearned by the LLM, but it also requires complex\nstructured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format\nof a problem. Specifically, the structure to be generated (e.g., a graph or a table) is converted, or\n“serialized”, into text. Such conversions include\n“flattening” the graph into a list of node pairs (Figure 1d), or into a specification language such as\nDOT (Figure 1c; Gansner et al., 2006).', 'While converting the structured output into text\nhas shown promising results (Rajagopal et al.,\n2021; Madaan and Yang, 2021), LLMs struggle\nto generate these “unnatural” outputs: LMs are\nprimarily pre-trained on free-form text, and these\nserialized structured outputs strongly diverge from\nthe majority of the pre-training data. Further, for\nnatural language, semantically relevant words are\ntypically found within a small span, whereas neighboring nodes in a graph might be pushed farther\napart when representing a graph as a flat string. Thus, a language model which was trained on\nnatural language text is likely to fail to capture\nthe topology of the graph. Consequently, using\nLLMs for graph generation typically requires a\nlarge amount of task-specific training data, and\ntheir generated outputs show structural errors and\nsemantic inconsistencies, which need to be further fixed either manually or by using a secondary\ndownstream model (Madaan et al., 2021b). Despite these struggles, the recent success of\nlarge-language models of code (Code-LLMs; Chen\net al., 2021b; Xu et al., 2022) for tasks such as\ncode generation from natural language (Austin\net al., 2021; Nijkamp et al., 2022), code completion (Fried et al., 2022), and code translation (Wang et al.) show that Code-LLMs are able to perform complex reasoning on structured data such\nas programs.', 'Thus, instead of forcing LLMs of\nnatural language (NL-LLMs) to be fine-tuned on\nstructured commonsense data, an easier way to\nclose the discrepancy between the pre-training data\n(free-form text) and the task-specific data (commonsense reasoning graphs) is to adapt LLMs that\nwere pre-trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language\nmodels of code are good structured commonsense\nreasoners. Further, we show that Code-LLMs can\nbe even better structured reasoners than NL-LLMs,\nwhen converting the desired output graph into a format similar to that observed in the code pre-training\ndata. We call our method COCOGEN: models\nof Code for Commonsense Generation, and it is\ndemonstrated in Figure 1. Our contributions are as follows:\n1. We highlight the insight that Code-LLMs\nare better structured commonsense reasoners\nthan NL-LLMs, when representing the desired\ngraph prediction as code. 2. We propose COCOGEN: a method for\nleveraging LLMs of code for structured\ncommonsense generation. 3. We perform an extensive evaluation across\nthree structured commonsense generation\ntasks and demonstrate that COCOGEN vastly\noutperforms NL-LLMs, either fine-tuned or\nfew-shot tested, while controlling for the num-\nber of downstream task examples. 4. We perform a thorough ablation study, which\nshows the role of data formatting, model size,\nand the number of few-shot examples.']",intro_chunked,"While converting the structured output into text
has shown promising results (Rajagopal et al.,
2021; Madaan and Yang, 2021), LLMs struggle
to generate these “unnatural” outputs: LMs are
primarily pre-trained on free-form text, and these
serialized structured outputs strongly diverge from
the majority of the pre-training data. Further, for
natural language, semantically relevant words are
typically found within a small span, whereas neighboring nodes in a graph might be pushed farther
apart when representing a graph as a flat string. Thus, a language model which was trained on
natural language text is likely to fail to capture
the topology of the graph. Consequently, using
LLMs for graph generation typically requires a
large amount of task-specific training data, and
their generated outputs show structural errors and
semantic inconsistencies, which need to be further fixed either manually or by using a secondary
downstream model (Madaan et al., 2021b). Despite these struggles, the recent success of
large-language models of code (Code-LLMs; Chen
et al., 2021b; Xu et al., 2022) for tasks such as
code generation from natural language (Austin
et al., 2021; Nijkamp et al., 2022), code completion (Fried et al., 2022), and code translation (Wang et al.) show that Code-LLMs are able to perform complex reasoning on structured data such
as programs."
34,Aman Madaan,"[' The growing capabilities of large pre-trained language models (LLMs) for generating text have enabled their successful application in a variety of\ntasks, including summarization, translation, and\nquestion-answering (Wang et al., 2019; Raffel et al.,\n2019; Brown et al., 2020; Chowdhery et al., 2022). Nevertheless, while employing LLMs for natural language (NL) tasks is straightforward, a major remaining challenge is how to leverage LLMs\nfor structured commonsense reasoning, including\ntasks such as generating event graphs (Tandon et al.,\n2019), reasoning graphs (Madaan et al., 2021a),\nscripts (Sakaguchi et al., 2021), and argument explanation graphs (Saha et al., 2021). Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured\noutput given a natural language input. This family\nof tasks relies on the natural language knowledge\nlearned by the LLM, but it also requires complex\nstructured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format\nof a problem. Specifically, the structure to be generated (e.g., a graph or a table) is converted, or\n“serialized”, into text. Such conversions include\n“flattening” the graph into a list of node pairs (Figure 1d), or into a specification language such as\nDOT (Figure 1c; Gansner et al., 2006).', 'While converting the structured output into text\nhas shown promising results (Rajagopal et al.,\n2021; Madaan and Yang, 2021), LLMs struggle\nto generate these “unnatural” outputs: LMs are\nprimarily pre-trained on free-form text, and these\nserialized structured outputs strongly diverge from\nthe majority of the pre-training data. Further, for\nnatural language, semantically relevant words are\ntypically found within a small span, whereas neighboring nodes in a graph might be pushed farther\napart when representing a graph as a flat string. Thus, a language model which was trained on\nnatural language text is likely to fail to capture\nthe topology of the graph. Consequently, using\nLLMs for graph generation typically requires a\nlarge amount of task-specific training data, and\ntheir generated outputs show structural errors and\nsemantic inconsistencies, which need to be further fixed either manually or by using a secondary\ndownstream model (Madaan et al., 2021b). Despite these struggles, the recent success of\nlarge-language models of code (Code-LLMs; Chen\net al., 2021b; Xu et al., 2022) for tasks such as\ncode generation from natural language (Austin\net al., 2021; Nijkamp et al., 2022), code completion (Fried et al., 2022), and code translation (Wang et al.) show that Code-LLMs are able to perform complex reasoning on structured data such\nas programs.', 'Thus, instead of forcing LLMs of\nnatural language (NL-LLMs) to be fine-tuned on\nstructured commonsense data, an easier way to\nclose the discrepancy between the pre-training data\n(free-form text) and the task-specific data (commonsense reasoning graphs) is to adapt LLMs that\nwere pre-trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language\nmodels of code are good structured commonsense\nreasoners. Further, we show that Code-LLMs can\nbe even better structured reasoners than NL-LLMs,\nwhen converting the desired output graph into a format similar to that observed in the code pre-training\ndata. We call our method COCOGEN: models\nof Code for Commonsense Generation, and it is\ndemonstrated in Figure 1. Our contributions are as follows:\n1. We highlight the insight that Code-LLMs\nare better structured commonsense reasoners\nthan NL-LLMs, when representing the desired\ngraph prediction as code. 2. We propose COCOGEN: a method for\nleveraging LLMs of code for structured\ncommonsense generation. 3. We perform an extensive evaluation across\nthree structured commonsense generation\ntasks and demonstrate that COCOGEN vastly\noutperforms NL-LLMs, either fine-tuned or\nfew-shot tested, while controlling for the num-\nber of downstream task examples. 4. We perform a thorough ablation study, which\nshows the role of data formatting, model size,\nand the number of few-shot examples.']",intro_chunked,"Thus, instead of forcing LLMs of
natural language (NL-LLMs) to be fine-tuned on
structured commonsense data, an easier way to
close the discrepancy between the pre-training data
(free-form text) and the task-specific data (commonsense reasoning graphs) is to adapt LLMs that
were pre-trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language
models of code are good structured commonsense
reasoners. Further, we show that Code-LLMs can
be even better structured reasoners than NL-LLMs,
when converting the desired output graph into a format similar to that observed in the code pre-training
data. We call our method COCOGEN: models
of Code for Commonsense Generation, and it is
demonstrated in Figure 1. Our contributions are as follows:
1. We highlight the insight that Code-LLMs
are better structured commonsense reasoners
than NL-LLMs, when representing the desired
graph prediction as code. 2. We propose COCOGEN: a method for
leveraging LLMs of code for structured
commonsense generation. 3. We perform an extensive evaluation across
three structured commonsense generation
tasks and demonstrate that COCOGEN vastly
outperforms NL-LLMs, either fine-tuned or
few-shot tested, while controlling for the num-
ber of downstream task examples. 4. We perform a thorough ablation study, which
shows the role of data formatting, model size,
and the number of few-shot examples."
35,Aman Madaan,"[' The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.', '(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),\ndeliberately sketched as controlled studies. First, we identify key components of an example in few-shot\nprompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping\nall but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).', 'Finally, we elicit\nmeaningful findings via conducting a systematic and qualitative analysis of the performance divergence\nbetween different prompt queries. Our experiments on four diverse reasoning tasks and across three large\nlanguage models—PaLM, GPT-3, and CODEX, reveal several surprising findings:\n- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In\naddition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the\ncorrectness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute\nchiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain\ncorrect outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays\na vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense\nknowledge), and patterns help reinforce task understanding, enabling the language model to generate text\nthat helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this\ninterplay between text and patterns—COT helps a language model in imitating the prompt and generating\nthe right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated\nby applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key\nrole in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a\nset of key design principles is an important challenge. To this end, we distill our findings to create concise\nprompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without\nnegative repercussions on the task solve rate.']",intro_chunked," The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al."
36,Aman Madaan,"[' The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.', '(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),\ndeliberately sketched as controlled studies. First, we identify key components of an example in few-shot\nprompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping\nall but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).', 'Finally, we elicit\nmeaningful findings via conducting a systematic and qualitative analysis of the performance divergence\nbetween different prompt queries. Our experiments on four diverse reasoning tasks and across three large\nlanguage models—PaLM, GPT-3, and CODEX, reveal several surprising findings:\n- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In\naddition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the\ncorrectness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute\nchiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain\ncorrect outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays\na vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense\nknowledge), and patterns help reinforce task understanding, enabling the language model to generate text\nthat helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this\ninterplay between text and patterns—COT helps a language model in imitating the prompt and generating\nthe right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated\nby applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key\nrole in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a\nset of key design principles is an important challenge. To this end, we distill our findings to create concise\nprompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without\nnegative repercussions on the task solve rate.']",intro_chunked,"(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),
deliberately sketched as controlled studies. First, we identify key components of an example in few-shot
prompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping
all but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets)."
37,Aman Madaan,"[' The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.', '(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),\ndeliberately sketched as controlled studies. First, we identify key components of an example in few-shot\nprompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping\nall but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).', 'Finally, we elicit\nmeaningful findings via conducting a systematic and qualitative analysis of the performance divergence\nbetween different prompt queries. Our experiments on four diverse reasoning tasks and across three large\nlanguage models—PaLM, GPT-3, and CODEX, reveal several surprising findings:\n- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In\naddition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the\ncorrectness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute\nchiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain\ncorrect outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays\na vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense\nknowledge), and patterns help reinforce task understanding, enabling the language model to generate text\nthat helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this\ninterplay between text and patterns—COT helps a language model in imitating the prompt and generating\nthe right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated\nby applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key\nrole in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a\nset of key design principles is an important challenge. To this end, we distill our findings to create concise\nprompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without\nnegative repercussions on the task solve rate.']",intro_chunked,"Finally, we elicit
meaningful findings via conducting a systematic and qualitative analysis of the performance divergence
between different prompt queries. Our experiments on four diverse reasoning tasks and across three large
language models—PaLM, GPT-3, and CODEX, reveal several surprising findings:
- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In
addition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the
correctness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute
chiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain
correct outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays
a vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense
knowledge), and patterns help reinforce task understanding, enabling the language model to generate text
that helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this
interplay between text and patterns—COT helps a language model in imitating the prompt and generating
the right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated
by applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key
role in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a
set of key design principles is an important challenge. To this end, we distill our findings to create concise
prompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without
negative repercussions on the task solve rate."
38,Aman Madaan,"[' Graphs provide a rich abstraction for a wide range of tasks\nincluding molecular design (De Cao & Kipf, 2018; Samanta\net al., 2019; Lim et al., 2020), temporal and commonsense\nreasoning (Madaan & Yang, 2021; Madaan et al., 2021;\nSakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout\ndesign (Mi et al., 2021). Developing generative models of\ngraphs is is therefore an important classical problem, which\nhas seen renewed interest with the success of deep learning\nmodels. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit\nmodels, implicit generative models do not explicitly model\nthe distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and\nhave recently shown state of the art results for generative\nmodeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of\ngraphs currently use identical model complexity and computational strength while generating graphs. However, since\nthese models are constructive by design (i.e., they build a\ngraph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of\nreasoning. For example, generating a 2-hop neighborhood\nfrequently seen during training might be easier than generating a novel 4-hop neighborhood.', 'Indeed, it has long been posited (Posner & Snyder, 1975;\nShiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;\nKahneman, 2003; Frankish, 2010) that humans frequently\nuse differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)\n203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different\nlevel of difficulty for a human solver. The answer to 2*2\nwill almost instinctively come to most, while solving 19*3\nwill require more careful thinking. Specifically, Stanovich\n(2000) propose to divide mental processing as being done\nby two metaphorical systems referred by them as System\n1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for\nSystems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a\ncombination of fast and slow reasoning systems in diverse\nareas of Machine Learning (Anthony et al., 2017; Mujika\net al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model\nthat is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into\nthe problem of learning to generate walks.', 'Generating\nwalks provides a setting where identifying the easy and\nchallenging portions is easier: starting from a given node,\nthe model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating\nsuch walks then gradually increases for two reasons. First,\nconditioning on increasingly longer contexts is required\nfor generating longer walks. Second, as the length of the\nwalks exceeds the length seen during training, a model is\nforced to create neighborhoods not seen during the training:\na task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty\nby dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1\nprovides an overview of our approach. FLOWGEN method\nachieves the same results as using the SLOW method alone\non three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer\nmodel, similar to the architectures used by the popular GPT2\nmodels. Using transformers allows us to easily instantiate\nfast and slow versions of the same model by varying the\nnumber of layers. In contrast to the state-of-the-art methods\nfor generative modeling of graphs that use either an implicit\nmodel (e.g., GANs as done by Bojchevski et al. (2018)),\nexplicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and\nleverage graph-aware decoding methods (You et al., 2018),\nour method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. ).']",intro_chunked," Graphs provide a rich abstraction for a wide range of tasks
including molecular design (De Cao & Kipf, 2018; Samanta
et al., 2019; Lim et al., 2020), temporal and commonsense
reasoning (Madaan & Yang, 2021; Madaan et al., 2021;
Sakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout
design (Mi et al., 2021). Developing generative models of
graphs is is therefore an important classical problem, which
has seen renewed interest with the success of deep learning
models. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit
models, implicit generative models do not explicitly model
the distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and
have recently shown state of the art results for generative
modeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of
graphs currently use identical model complexity and computational strength while generating graphs. However, since
these models are constructive by design (i.e., they build a
graph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of
reasoning. For example, generating a 2-hop neighborhood
frequently seen during training might be easier than generating a novel 4-hop neighborhood."
39,Aman Madaan,"[' Graphs provide a rich abstraction for a wide range of tasks\nincluding molecular design (De Cao & Kipf, 2018; Samanta\net al., 2019; Lim et al., 2020), temporal and commonsense\nreasoning (Madaan & Yang, 2021; Madaan et al., 2021;\nSakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout\ndesign (Mi et al., 2021). Developing generative models of\ngraphs is is therefore an important classical problem, which\nhas seen renewed interest with the success of deep learning\nmodels. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit\nmodels, implicit generative models do not explicitly model\nthe distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and\nhave recently shown state of the art results for generative\nmodeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of\ngraphs currently use identical model complexity and computational strength while generating graphs. However, since\nthese models are constructive by design (i.e., they build a\ngraph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of\nreasoning. For example, generating a 2-hop neighborhood\nfrequently seen during training might be easier than generating a novel 4-hop neighborhood.', 'Indeed, it has long been posited (Posner & Snyder, 1975;\nShiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;\nKahneman, 2003; Frankish, 2010) that humans frequently\nuse differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)\n203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different\nlevel of difficulty for a human solver. The answer to 2*2\nwill almost instinctively come to most, while solving 19*3\nwill require more careful thinking. Specifically, Stanovich\n(2000) propose to divide mental processing as being done\nby two metaphorical systems referred by them as System\n1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for\nSystems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a\ncombination of fast and slow reasoning systems in diverse\nareas of Machine Learning (Anthony et al., 2017; Mujika\net al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model\nthat is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into\nthe problem of learning to generate walks.', 'Generating\nwalks provides a setting where identifying the easy and\nchallenging portions is easier: starting from a given node,\nthe model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating\nsuch walks then gradually increases for two reasons. First,\nconditioning on increasingly longer contexts is required\nfor generating longer walks. Second, as the length of the\nwalks exceeds the length seen during training, a model is\nforced to create neighborhoods not seen during the training:\na task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty\nby dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1\nprovides an overview of our approach. FLOWGEN method\nachieves the same results as using the SLOW method alone\non three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer\nmodel, similar to the architectures used by the popular GPT2\nmodels. Using transformers allows us to easily instantiate\nfast and slow versions of the same model by varying the\nnumber of layers. In contrast to the state-of-the-art methods\nfor generative modeling of graphs that use either an implicit\nmodel (e.g., GANs as done by Bojchevski et al. (2018)),\nexplicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and\nleverage graph-aware decoding methods (You et al., 2018),\nour method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. ).']",intro_chunked,"Indeed, it has long been posited (Posner & Snyder, 1975;
Shiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;
Kahneman, 2003; Frankish, 2010) that humans frequently
use differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)
203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different
level of difficulty for a human solver. The answer to 2*2
will almost instinctively come to most, while solving 19*3
will require more careful thinking. Specifically, Stanovich
(2000) propose to divide mental processing as being done
by two metaphorical systems referred by them as System
1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for
Systems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a
combination of fast and slow reasoning systems in diverse
areas of Machine Learning (Anthony et al., 2017; Mujika
et al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model
that is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into
the problem of learning to generate walks."
40,Aman Madaan,"[' Graphs provide a rich abstraction for a wide range of tasks\nincluding molecular design (De Cao & Kipf, 2018; Samanta\net al., 2019; Lim et al., 2020), temporal and commonsense\nreasoning (Madaan & Yang, 2021; Madaan et al., 2021;\nSakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout\ndesign (Mi et al., 2021). Developing generative models of\ngraphs is is therefore an important classical problem, which\nhas seen renewed interest with the success of deep learning\nmodels. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit\nmodels, implicit generative models do not explicitly model\nthe distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and\nhave recently shown state of the art results for generative\nmodeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of\ngraphs currently use identical model complexity and computational strength while generating graphs. However, since\nthese models are constructive by design (i.e., they build a\ngraph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of\nreasoning. For example, generating a 2-hop neighborhood\nfrequently seen during training might be easier than generating a novel 4-hop neighborhood.', 'Indeed, it has long been posited (Posner & Snyder, 1975;\nShiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;\nKahneman, 2003; Frankish, 2010) that humans frequently\nuse differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)\n203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different\nlevel of difficulty for a human solver. The answer to 2*2\nwill almost instinctively come to most, while solving 19*3\nwill require more careful thinking. Specifically, Stanovich\n(2000) propose to divide mental processing as being done\nby two metaphorical systems referred by them as System\n1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for\nSystems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a\ncombination of fast and slow reasoning systems in diverse\nareas of Machine Learning (Anthony et al., 2017; Mujika\net al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model\nthat is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into\nthe problem of learning to generate walks.', 'Generating\nwalks provides a setting where identifying the easy and\nchallenging portions is easier: starting from a given node,\nthe model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating\nsuch walks then gradually increases for two reasons. First,\nconditioning on increasingly longer contexts is required\nfor generating longer walks. Second, as the length of the\nwalks exceeds the length seen during training, a model is\nforced to create neighborhoods not seen during the training:\na task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty\nby dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1\nprovides an overview of our approach. FLOWGEN method\nachieves the same results as using the SLOW method alone\non three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer\nmodel, similar to the architectures used by the popular GPT2\nmodels. Using transformers allows us to easily instantiate\nfast and slow versions of the same model by varying the\nnumber of layers. In contrast to the state-of-the-art methods\nfor generative modeling of graphs that use either an implicit\nmodel (e.g., GANs as done by Bojchevski et al. (2018)),\nexplicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and\nleverage graph-aware decoding methods (You et al., 2018),\nour method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. ).']",intro_chunked,"Generating
walks provides a setting where identifying the easy and
challenging portions is easier: starting from a given node,
the model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating
such walks then gradually increases for two reasons. First,
conditioning on increasingly longer contexts is required
for generating longer walks. Second, as the length of the
walks exceeds the length seen during training, a model is
forced to create neighborhoods not seen during the training:
a task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty
by dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1
provides an overview of our approach. FLOWGEN method
achieves the same results as using the SLOW method alone
on three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer
model, similar to the architectures used by the popular GPT2
models. Using transformers allows us to easily instantiate
fast and slow versions of the same model by varying the
number of layers. In contrast to the state-of-the-art methods
for generative modeling of graphs that use either an implicit
model (e.g., GANs as done by Bojchevski et al. (2018)),
explicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and
leverage graph-aware decoding methods (You et al., 2018),
our method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. )."
41,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked," Conditional set generation is the task of modeling
the distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several
NLP tasks are instances of set generation, including
open-entity typing (Choi et al., 2018; Dai et al.,
2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng
et al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning
paradigm have encouraged a formulation of set
generation as a SEQ2SEQ generation task (Vinyals
et al., 2016; Yang et al., 2018; Meng et al., 2019;
Ju et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not
explicitly account for two key properties of a set
output: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation
treats a set as a sequence, assuming an arbitrary
order between the elements it outputs. Similarly,
the cardinality of sets is ignored, as the number of
elements to be generated is typically not modeled."
42,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked,"Prior work has highlighted the importance of
these two properties for set output through loss
functions that encourage order invariance (Ye et al.,
2021), exhaustive search over the label space
for finding an optimal order (Qin et al., 2019;
Rezatofighi et al., 2018; Vinyals et al., 2016), and
post-processing the output (Nag Chowdhury et al.,
2016). Despite the progress, several important gaps
remain. First, exhaustive search does not scale with
large output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is
still not explicitly modeled in the SEQ2SEQ setting
despite being an essential aspect for a set. Finally,
architectural modifications required for specialized
set-generation techniques might not be viable for
modern large-language models. We address these challenges with a novel data
augmentation strategy. Specifically, we take advantage of the auto-regressive factorization used
by SEQ2SEQ models and (i) impose an informative
order over the label space, and (ii) explicitly model
cardinality. First, the label sets are converted to
sequences using informative orders by grouping
labels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the
edges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance."
43,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked,"Specifically, sequences obtained via topological
traversals of this graph allow independent labels to
appear at different locations in the sequence, while restricting order for dependent labels. Next, we
jointly model a set with its cardinality by simply
prepending the set size to the output sequence. This
strategy aligns with the current trend of very large
language models which do not lend themselves to
architectural modifications but increasingly rely on
the informativeness of the inputs (Yang et al., 2020;
Liu et al., 2021). Figure 1 illustrates the key intuitions behind our
method using sample task where given an input
x (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more
meaningful, consider a case where one of the emotions is joy, which leads to a more general emotion
of pride. After first generating joy, the model can
generate pride with certainty (joy leads to pride in
all samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]
is thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,
joy contains two sub-emotions, and love contains
one."
44,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked,"A model that first predicts the number of
sub-emotions can be more precise and avoid overgeneration, a significant challenge with language
generation models (Welleck et al., 2020; Fu et al.,
2021). We efficiently sample such informative orders from the combinatorial space of all possible
orders and jointly model cardinality by leveraging
the auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly
modeling the cardinality and augmenting the
training data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show
that our method serves as a better proposal
distribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with
no additional annotations or architecture
changes."
45,Aman Madaan,"[' Language models are now better than ever before at\ngenerating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense\nis in misunderstanding a user’s intent. The typical\nremedy of retraining with more data is prohibitive\ndue to the cost and infrastructure requirements. In\nsuch cases, even if users repeatedly observe the\nmodel making a mistake, there are no avenues to\nprovide feedback to the model to make it more\naccurate and personalized over time. Our goal is to allow users to correct such errors\ndirectly through interaction, and without retraining by injecting the knowledge required to correct the\nmodel’s misunderstanding. Building upon the recent success of injecting commonsense in the input\n(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in\nthe input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with\na growing memory of cases where the model misunderstood user’s intent and was provided with\ncorrective feedback. This feedback is question dependent, and thus the prompt for each sample is\nedited to adapt to the input. In this sense, our\nwork can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing\nthe prompts.', 'Our work adds interactivity to prompt\nengineering as it involves dynamically updating the\nprompt for every instance. Figure 1 presents a sample interaction between a\nuser and GPT-3 that our setup enables. The model\nwas asked for a similar word. However, the model’s\n(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task\ninstruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,\nnote that such instructional correction is feasible\neven if the user does not know the correct answer to\ntheir question, as they are critiquing the model’s understanding of their intent, rather than the answers\nthemselves. Thus, our setup does not require the\nusers to be experts at tasks being solved, another\nadvantage of our approach. Further, it is desirable to have a system that can\nleverage past feedback on new, unseen examples\nfor prompt-editing. We maintain a memory M of\nsuch feedback as a set of key-value pairs, where the\nkey is a misunderstood question, and the value is\nthe user’s feedback to correct that misunderstanding. Given a new question, we check if the model\nhas made a mistake on a similar question earlier,\nby querying the memory for a similar question.', 'If\nfound, append the corresponding feedback to the\nquestion prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive\nreminding in psychology (Jacoby and Wahlheim,\n2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for\nthe system and provides representative implementations for each component. We then demonstrate\nthe system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure\n1), (2) word scrambling (e.g., anagrams), (3) ethical\nreasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)\nethics reasoning with user feedback being natural\nlanguage. We find that in all cases, GPT-3’s accuracy significantly increases with time, without\nretraining, as our approach enables it to use corrective feedback from earlier examples to avoid\nsimilar misunderstandings on future examples. In\nsummary, our contributions are:• We show that a large model like GPT-3 can be\nimproved after deployment, without retraining,\nthrough a memory-assisted architecture. • Our implementation, MemPrompt, is the first\ndemonstration that this is possible - this is an important step forward for real use of LMs, and the\npaper sets out a general architecture that others can\nbuild on, a specific implementation, and detailed\nevaluation on multiple tasks.']",intro_chunked," Language models are now better than ever before at
generating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense
is in misunderstanding a user’s intent. The typical
remedy of retraining with more data is prohibitive
due to the cost and infrastructure requirements. In
such cases, even if users repeatedly observe the
model making a mistake, there are no avenues to
provide feedback to the model to make it more
accurate and personalized over time. Our goal is to allow users to correct such errors
directly through interaction, and without retraining by injecting the knowledge required to correct the
model’s misunderstanding. Building upon the recent success of injecting commonsense in the input
(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in
the input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with
a growing memory of cases where the model misunderstood user’s intent and was provided with
corrective feedback. This feedback is question dependent, and thus the prompt for each sample is
edited to adapt to the input. In this sense, our
work can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing
the prompts."
46,Aman Madaan,"[' Language models are now better than ever before at\ngenerating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense\nis in misunderstanding a user’s intent. The typical\nremedy of retraining with more data is prohibitive\ndue to the cost and infrastructure requirements. In\nsuch cases, even if users repeatedly observe the\nmodel making a mistake, there are no avenues to\nprovide feedback to the model to make it more\naccurate and personalized over time. Our goal is to allow users to correct such errors\ndirectly through interaction, and without retraining by injecting the knowledge required to correct the\nmodel’s misunderstanding. Building upon the recent success of injecting commonsense in the input\n(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in\nthe input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with\na growing memory of cases where the model misunderstood user’s intent and was provided with\ncorrective feedback. This feedback is question dependent, and thus the prompt for each sample is\nedited to adapt to the input. In this sense, our\nwork can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing\nthe prompts.', 'Our work adds interactivity to prompt\nengineering as it involves dynamically updating the\nprompt for every instance. Figure 1 presents a sample interaction between a\nuser and GPT-3 that our setup enables. The model\nwas asked for a similar word. However, the model’s\n(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task\ninstruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,\nnote that such instructional correction is feasible\neven if the user does not know the correct answer to\ntheir question, as they are critiquing the model’s understanding of their intent, rather than the answers\nthemselves. Thus, our setup does not require the\nusers to be experts at tasks being solved, another\nadvantage of our approach. Further, it is desirable to have a system that can\nleverage past feedback on new, unseen examples\nfor prompt-editing. We maintain a memory M of\nsuch feedback as a set of key-value pairs, where the\nkey is a misunderstood question, and the value is\nthe user’s feedback to correct that misunderstanding. Given a new question, we check if the model\nhas made a mistake on a similar question earlier,\nby querying the memory for a similar question.', 'If\nfound, append the corresponding feedback to the\nquestion prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive\nreminding in psychology (Jacoby and Wahlheim,\n2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for\nthe system and provides representative implementations for each component. We then demonstrate\nthe system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure\n1), (2) word scrambling (e.g., anagrams), (3) ethical\nreasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)\nethics reasoning with user feedback being natural\nlanguage. We find that in all cases, GPT-3’s accuracy significantly increases with time, without\nretraining, as our approach enables it to use corrective feedback from earlier examples to avoid\nsimilar misunderstandings on future examples. In\nsummary, our contributions are:• We show that a large model like GPT-3 can be\nimproved after deployment, without retraining,\nthrough a memory-assisted architecture. • Our implementation, MemPrompt, is the first\ndemonstration that this is possible - this is an important step forward for real use of LMs, and the\npaper sets out a general architecture that others can\nbuild on, a specific implementation, and detailed\nevaluation on multiple tasks.']",intro_chunked,"Our work adds interactivity to prompt
engineering as it involves dynamically updating the
prompt for every instance. Figure 1 presents a sample interaction between a
user and GPT-3 that our setup enables. The model
was asked for a similar word. However, the model’s
(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task
instruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,
note that such instructional correction is feasible
even if the user does not know the correct answer to
their question, as they are critiquing the model’s understanding of their intent, rather than the answers
themselves. Thus, our setup does not require the
users to be experts at tasks being solved, another
advantage of our approach. Further, it is desirable to have a system that can
leverage past feedback on new, unseen examples
for prompt-editing. We maintain a memory M of
such feedback as a set of key-value pairs, where the
key is a misunderstood question, and the value is
the user’s feedback to correct that misunderstanding. Given a new question, we check if the model
has made a mistake on a similar question earlier,
by querying the memory for a similar question."
47,Aman Madaan,"[' Language models are now better than ever before at\ngenerating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense\nis in misunderstanding a user’s intent. The typical\nremedy of retraining with more data is prohibitive\ndue to the cost and infrastructure requirements. In\nsuch cases, even if users repeatedly observe the\nmodel making a mistake, there are no avenues to\nprovide feedback to the model to make it more\naccurate and personalized over time. Our goal is to allow users to correct such errors\ndirectly through interaction, and without retraining by injecting the knowledge required to correct the\nmodel’s misunderstanding. Building upon the recent success of injecting commonsense in the input\n(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in\nthe input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with\na growing memory of cases where the model misunderstood user’s intent and was provided with\ncorrective feedback. This feedback is question dependent, and thus the prompt for each sample is\nedited to adapt to the input. In this sense, our\nwork can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing\nthe prompts.', 'Our work adds interactivity to prompt\nengineering as it involves dynamically updating the\nprompt for every instance. Figure 1 presents a sample interaction between a\nuser and GPT-3 that our setup enables. The model\nwas asked for a similar word. However, the model’s\n(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task\ninstruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,\nnote that such instructional correction is feasible\neven if the user does not know the correct answer to\ntheir question, as they are critiquing the model’s understanding of their intent, rather than the answers\nthemselves. Thus, our setup does not require the\nusers to be experts at tasks being solved, another\nadvantage of our approach. Further, it is desirable to have a system that can\nleverage past feedback on new, unseen examples\nfor prompt-editing. We maintain a memory M of\nsuch feedback as a set of key-value pairs, where the\nkey is a misunderstood question, and the value is\nthe user’s feedback to correct that misunderstanding. Given a new question, we check if the model\nhas made a mistake on a similar question earlier,\nby querying the memory for a similar question.', 'If\nfound, append the corresponding feedback to the\nquestion prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive\nreminding in psychology (Jacoby and Wahlheim,\n2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for\nthe system and provides representative implementations for each component. We then demonstrate\nthe system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure\n1), (2) word scrambling (e.g., anagrams), (3) ethical\nreasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)\nethics reasoning with user feedback being natural\nlanguage. We find that in all cases, GPT-3’s accuracy significantly increases with time, without\nretraining, as our approach enables it to use corrective feedback from earlier examples to avoid\nsimilar misunderstandings on future examples. In\nsummary, our contributions are:• We show that a large model like GPT-3 can be\nimproved after deployment, without retraining,\nthrough a memory-assisted architecture. • Our implementation, MemPrompt, is the first\ndemonstration that this is possible - this is an important step forward for real use of LMs, and the\npaper sets out a general architecture that others can\nbuild on, a specific implementation, and detailed\nevaluation on multiple tasks.']",intro_chunked,"If
found, append the corresponding feedback to the
question prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive
reminding in psychology (Jacoby and Wahlheim,
2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for
the system and provides representative implementations for each component. We then demonstrate
the system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure
1), (2) word scrambling (e.g., anagrams), (3) ethical
reasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)
ethics reasoning with user feedback being natural
language. We find that in all cases, GPT-3’s accuracy significantly increases with time, without
retraining, as our approach enables it to use corrective feedback from earlier examples to avoid
similar misunderstandings on future examples. In
summary, our contributions are:• We show that a large model like GPT-3 can be
improved after deployment, without retraining,
through a memory-assisted architecture. • Our implementation, MemPrompt, is the first
demonstration that this is possible - this is an important step forward for real use of LMs, and the
paper sets out a general architecture that others can
build on, a specific implementation, and detailed
evaluation on multiple tasks."
48,Aman Madaan,"[' Defeasible inference is a mode of reasoning where\nadditional information can modify conclusions\n(Koons, 2017). Here we consider the specific formulation and challenge in Rudinger et al. (2020):\nGiven that some premise P plausibly implies a hypothesis H, does new information that the situation\nis S weaken or strengthen the conclusion H? For\nexample, consider the premise “The drinking glass\nfell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow”\nhere weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans\nwith an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan et al.,\n2021), which we use in this paper, draws connections between the P, H, and S through mediating events. This can be seen as a mental model of the\nquestion scenario before answering the question\n(Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario\nwith inference graphs help machines in defeasible\nreasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then,\nuse that graph as an additional input when answering the defeasible reasoning query. Our proposed\nsystem, CURIOUS, comprises a graph generation\nmodule and a graph encoding module to use the\ngenerated graph for the query (Figure 2).', 'To generate inference graphs, we build upon past\nwork that uses a sequence to sequence approach\n(Madaan et al., 2021). However, our analysis revealed that the graphs can often be erroneous, and\nCURIOUS also includes an error correction module\nto generate higher quality inference graphs. This\nwas important because we found that better graphs\nare more helpful in the downstream QA task. The generated inference graph is then used for\nthe QA task on three existing defeasible inference\ndatasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015),\nδ-SOCIAL (reasoning about social norms) (Forbes\net al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way\nthe graph is encoded for input is important. If we\nsimply augment the question with the generated\ngraphs, there are some gains on all datasets. However, the accuracy improves substantially across\nall datasets with a more judicious encoding of the\ngraph-augmented question that accounts for interactions between the graph nodes. To achieve this,\nwe use the mixture of experts approach (Jacobs\net al., 1991) to include a mixture of experts layers\nduring encoding, enabling the ability to attend to\nspecific nodes while capturing their interactions\nselectively. In summary, our contributiion is in drawing on\nthe idea of an inference graph from cognitive science to show benefits in a defeasible inference QA\ntask. Using an error correction module in the graph\ngeneration process, and a judicious encoding of the\ngraph augmented question, CURIOUS achieves a\nnew state-of-the-art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to “think about""\na question before answering can improve performance.']",intro_chunked," Defeasible inference is a mode of reasoning where
additional information can modify conclusions
(Koons, 2017). Here we consider the specific formulation and challenge in Rudinger et al. (2020):
Given that some premise P plausibly implies a hypothesis H, does new information that the situation
is S weaken or strengthen the conclusion H? For
example, consider the premise “The drinking glass
fell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow”
here weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans
with an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan et al.,
2021), which we use in this paper, draws connections between the P, H, and S through mediating events. This can be seen as a mental model of the
question scenario before answering the question
(Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario
with inference graphs help machines in defeasible
reasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then,
use that graph as an additional input when answering the defeasible reasoning query. Our proposed
system, CURIOUS, comprises a graph generation
module and a graph encoding module to use the
generated graph for the query (Figure 2)."
49,Aman Madaan,"[' Defeasible inference is a mode of reasoning where\nadditional information can modify conclusions\n(Koons, 2017). Here we consider the specific formulation and challenge in Rudinger et al. (2020):\nGiven that some premise P plausibly implies a hypothesis H, does new information that the situation\nis S weaken or strengthen the conclusion H? For\nexample, consider the premise “The drinking glass\nfell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow”\nhere weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans\nwith an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan et al.,\n2021), which we use in this paper, draws connections between the P, H, and S through mediating events. This can be seen as a mental model of the\nquestion scenario before answering the question\n(Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario\nwith inference graphs help machines in defeasible\nreasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then,\nuse that graph as an additional input when answering the defeasible reasoning query. Our proposed\nsystem, CURIOUS, comprises a graph generation\nmodule and a graph encoding module to use the\ngenerated graph for the query (Figure 2).', 'To generate inference graphs, we build upon past\nwork that uses a sequence to sequence approach\n(Madaan et al., 2021). However, our analysis revealed that the graphs can often be erroneous, and\nCURIOUS also includes an error correction module\nto generate higher quality inference graphs. This\nwas important because we found that better graphs\nare more helpful in the downstream QA task. The generated inference graph is then used for\nthe QA task on three existing defeasible inference\ndatasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015),\nδ-SOCIAL (reasoning about social norms) (Forbes\net al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way\nthe graph is encoded for input is important. If we\nsimply augment the question with the generated\ngraphs, there are some gains on all datasets. However, the accuracy improves substantially across\nall datasets with a more judicious encoding of the\ngraph-augmented question that accounts for interactions between the graph nodes. To achieve this,\nwe use the mixture of experts approach (Jacobs\net al., 1991) to include a mixture of experts layers\nduring encoding, enabling the ability to attend to\nspecific nodes while capturing their interactions\nselectively. In summary, our contributiion is in drawing on\nthe idea of an inference graph from cognitive science to show benefits in a defeasible inference QA\ntask. Using an error correction module in the graph\ngeneration process, and a judicious encoding of the\ngraph augmented question, CURIOUS achieves a\nnew state-of-the-art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to “think about""\na question before answering can improve performance.']",intro_chunked,"To generate inference graphs, we build upon past
work that uses a sequence to sequence approach
(Madaan et al., 2021). However, our analysis revealed that the graphs can often be erroneous, and
CURIOUS also includes an error correction module
to generate higher quality inference graphs. This
was important because we found that better graphs
are more helpful in the downstream QA task. The generated inference graph is then used for
the QA task on three existing defeasible inference
datasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015),
δ-SOCIAL (reasoning about social norms) (Forbes
et al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way
the graph is encoded for input is important. If we
simply augment the question with the generated
graphs, there are some gains on all datasets. However, the accuracy improves substantially across
all datasets with a more judicious encoding of the
graph-augmented question that accounts for interactions between the graph nodes. To achieve this,
we use the mixture of experts approach (Jacobs
et al., 1991) to include a mixture of experts layers
during encoding, enabling the ability to attend to
specific nodes while capturing their interactions
selectively. In summary, our contributiion is in drawing on
the idea of an inference graph from cognitive science to show benefits in a defeasible inference QA
task. Using an error correction module in the graph
generation process, and a judicious encoding of the
graph augmented question, CURIOUS achieves a
new state-of-the-art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to “think about""
a question before answering can improve performance."
50,Aman Madaan,"[' Defeasible inference (Rudinger et al., 2020) is a\nmode of reasoning in which given a premise P\n(Rob went for a hike), a hypothesis H (Rob saw\nan elephant, it was pink) may be weakened or\noverturned in light of new evidence i.e., an update U (Rob often has hallucinations). Given the\nnon-monotonic nature of this reasoning, humans\nfind it challenging to master this task (Morgan,\n2004). This problem has been widely studied in\nclassical AI through logic (Israel, 1980; McCarthy,\n1981), and in cognitive science through argumentative models (Pollock, 1987). A prominent approach\nis to support defeasible inference through argumentations by constructing an inference graph (Pollock,\n2009). Despite their prominence (Bentahar et al., 2010),\nargumentative models are not scalable because an\ninference graph needs to be handcrafted for every\nexample. Recently, Rudinger et al.', '(2020) proposed\ntwo auxiliary tasks related to defeasible inference:\n(i) an NLI task to predict whether an update U\nwould weaken or strengthen a hypothesis H, and\n(ii) a generative task to generate an update U given\na premise P and a hypothesis H. However, this\nonly addresses a part of the problem because their\ninference is still not supported by the line of reasoning that a human typically uses to solve this\ntask, namely mediators (e.g., hallucinations can be\ndeceptive) and contextualizers (some elephants can\nhave mutated gene which makes them look different) that are inherently embedded in an inference\ngraph, limiting their utility for humans (figure 1).In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive\nscience and provide a computational model to make\ntheir generation scalable. Training such a model\nwould require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in NLP (Tandon et al., 2019),\nwhere the reasoning is supported by a graph that we\nfind has similarities with the kind of reasoning that\nan inference graph supports. We train a model that\ncan learn from the NLP task and effectively transfer\nit to generate inference graphs. Such transfer learning is made possible due to the powerful seq-to-seq\nneural language models that did not exist before. The contributions of this paper are the answers\nto the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In\n§2, we show that we can effectively construct\nmeaningful graphs using transfer learning. Can our generated graphs help improve human performance? In §3, we show that humans\nleverage generated graphs to improve their performance on a previously reported benchmark.']",intro_chunked," Defeasible inference (Rudinger et al., 2020) is a
mode of reasoning in which given a premise P
(Rob went for a hike), a hypothesis H (Rob saw
an elephant, it was pink) may be weakened or
overturned in light of new evidence i.e., an update U (Rob often has hallucinations). Given the
non-monotonic nature of this reasoning, humans
find it challenging to master this task (Morgan,
2004). This problem has been widely studied in
classical AI through logic (Israel, 1980; McCarthy,
1981), and in cognitive science through argumentative models (Pollock, 1987). A prominent approach
is to support defeasible inference through argumentations by constructing an inference graph (Pollock,
2009). Despite their prominence (Bentahar et al., 2010),
argumentative models are not scalable because an
inference graph needs to be handcrafted for every
example. Recently, Rudinger et al."
51,Aman Madaan,"[' Defeasible inference (Rudinger et al., 2020) is a\nmode of reasoning in which given a premise P\n(Rob went for a hike), a hypothesis H (Rob saw\nan elephant, it was pink) may be weakened or\noverturned in light of new evidence i.e., an update U (Rob often has hallucinations). Given the\nnon-monotonic nature of this reasoning, humans\nfind it challenging to master this task (Morgan,\n2004). This problem has been widely studied in\nclassical AI through logic (Israel, 1980; McCarthy,\n1981), and in cognitive science through argumentative models (Pollock, 1987). A prominent approach\nis to support defeasible inference through argumentations by constructing an inference graph (Pollock,\n2009). Despite their prominence (Bentahar et al., 2010),\nargumentative models are not scalable because an\ninference graph needs to be handcrafted for every\nexample. Recently, Rudinger et al.', '(2020) proposed\ntwo auxiliary tasks related to defeasible inference:\n(i) an NLI task to predict whether an update U\nwould weaken or strengthen a hypothesis H, and\n(ii) a generative task to generate an update U given\na premise P and a hypothesis H. However, this\nonly addresses a part of the problem because their\ninference is still not supported by the line of reasoning that a human typically uses to solve this\ntask, namely mediators (e.g., hallucinations can be\ndeceptive) and contextualizers (some elephants can\nhave mutated gene which makes them look different) that are inherently embedded in an inference\ngraph, limiting their utility for humans (figure 1).In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive\nscience and provide a computational model to make\ntheir generation scalable. Training such a model\nwould require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in NLP (Tandon et al., 2019),\nwhere the reasoning is supported by a graph that we\nfind has similarities with the kind of reasoning that\nan inference graph supports. We train a model that\ncan learn from the NLP task and effectively transfer\nit to generate inference graphs. Such transfer learning is made possible due to the powerful seq-to-seq\nneural language models that did not exist before. The contributions of this paper are the answers\nto the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In\n§2, we show that we can effectively construct\nmeaningful graphs using transfer learning. Can our generated graphs help improve human performance? In §3, we show that humans\nleverage generated graphs to improve their performance on a previously reported benchmark.']",intro_chunked,"(2020) proposed
two auxiliary tasks related to defeasible inference:
(i) an NLI task to predict whether an update U
would weaken or strengthen a hypothesis H, and
(ii) a generative task to generate an update U given
a premise P and a hypothesis H. However, this
only addresses a part of the problem because their
inference is still not supported by the line of reasoning that a human typically uses to solve this
task, namely mediators (e.g., hallucinations can be
deceptive) and contextualizers (some elephants can
have mutated gene which makes them look different) that are inherently embedded in an inference
graph, limiting their utility for humans (figure 1).In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive
science and provide a computational model to make
their generation scalable. Training such a model
would require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in NLP (Tandon et al., 2019),
where the reasoning is supported by a graph that we
find has similarities with the kind of reasoning that
an inference graph supports. We train a model that
can learn from the NLP task and effectively transfer
it to generate inference graphs. Such transfer learning is made possible due to the powerful seq-to-seq
neural language models that did not exist before. The contributions of this paper are the answers
to the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In
§2, we show that we can effectively construct
meaningful graphs using transfer learning. Can our generated graphs help improve human performance? In §3, we show that humans
leverage generated graphs to improve their performance on a previously reported benchmark."
52,Aman Madaan,"[' Interactive Machine Learning allows humans to give feedback to the models, often leading to improved accuracy (Fails and Olsen, 2003; Raghavan, 2006; Settles, 2011). Interactive systems for NLP have used human-in-the-loop style interactions for helping refugee settlement (Brown and Grinter, 2016), aligning topic models (Yuan et al., 2018) and enhancing bilingual word embeddings (Yuan et al., 2020). Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback (e.g., on its explanation) without retraining. One line of prior approaches (interactive semantic parsing approach) (Elgohary et al., 2021; Wang et al., 2016) parse natural language user feedback into a set of edit operations, which can then be ∗ authors contributed equally to this work. Ordering determined by dice rolling. 1We release a dataset of over 450k graphs for defeasible reasoning generated by our system at https://tinyurl. com/mercurie. executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is specific to a semantic parsing schema and has to be specialized, i.e., directly mapped to specific instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a specific set of edit operations expressed in natural language.', 'However, real-world human feedback is often imprecise and not directly actionable. Another line of prior approaches (interactive reasoning approach) (Talmor et al., 2020) explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box – so the human does not know what the model’s internal belief is and how it will change based on the feedback. These two lines of prior approaches inspire this paper – we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation (similar to the interactive semantic parsing approach). We do this while relaxing the assumptions of the parsing approach – our feedback does not have a task-specific structure, and it is not assumed to be actionable (similar to the interactive reasoning approach). We introduce MERCURIE, a pipeline system with two components, a previously trained neural model M and a graph corrector G. It takes as input any previously trained neural model M capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure (for example, that some nodes are inconsistent with the rest of the graph). As output, it produces a better explanation structure. The contributions of this work are: • We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that MERCURIE can improve the consistency of explanation structures by up to 40% (§4). • We also show downstream task (defeasible inference (Rudinger et al., 2020)) improvement for all domains by at least 1.2 points on accuracy (§6).']",intro_chunked," Interactive Machine Learning allows humans to give feedback to the models, often leading to improved accuracy (Fails and Olsen, 2003; Raghavan, 2006; Settles, 2011). Interactive systems for NLP have used human-in-the-loop style interactions for helping refugee settlement (Brown and Grinter, 2016), aligning topic models (Yuan et al., 2018) and enhancing bilingual word embeddings (Yuan et al., 2020). Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback (e.g., on its explanation) without retraining. One line of prior approaches (interactive semantic parsing approach) (Elgohary et al., 2021; Wang et al., 2016) parse natural language user feedback into a set of edit operations, which can then be ∗ authors contributed equally to this work. Ordering determined by dice rolling. 1We release a dataset of over 450k graphs for defeasible reasoning generated by our system at https://tinyurl. com/mercurie. executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is specific to a semantic parsing schema and has to be specialized, i.e., directly mapped to specific instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a specific set of edit operations expressed in natural language."
53,Aman Madaan,"[' Interactive Machine Learning allows humans to give feedback to the models, often leading to improved accuracy (Fails and Olsen, 2003; Raghavan, 2006; Settles, 2011). Interactive systems for NLP have used human-in-the-loop style interactions for helping refugee settlement (Brown and Grinter, 2016), aligning topic models (Yuan et al., 2018) and enhancing bilingual word embeddings (Yuan et al., 2020). Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback (e.g., on its explanation) without retraining. One line of prior approaches (interactive semantic parsing approach) (Elgohary et al., 2021; Wang et al., 2016) parse natural language user feedback into a set of edit operations, which can then be ∗ authors contributed equally to this work. Ordering determined by dice rolling. 1We release a dataset of over 450k graphs for defeasible reasoning generated by our system at https://tinyurl. com/mercurie. executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is specific to a semantic parsing schema and has to be specialized, i.e., directly mapped to specific instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a specific set of edit operations expressed in natural language.', 'However, real-world human feedback is often imprecise and not directly actionable. Another line of prior approaches (interactive reasoning approach) (Talmor et al., 2020) explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box – so the human does not know what the model’s internal belief is and how it will change based on the feedback. These two lines of prior approaches inspire this paper – we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation (similar to the interactive semantic parsing approach). We do this while relaxing the assumptions of the parsing approach – our feedback does not have a task-specific structure, and it is not assumed to be actionable (similar to the interactive reasoning approach). We introduce MERCURIE, a pipeline system with two components, a previously trained neural model M and a graph corrector G. It takes as input any previously trained neural model M capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure (for example, that some nodes are inconsistent with the rest of the graph). As output, it produces a better explanation structure. The contributions of this work are: • We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that MERCURIE can improve the consistency of explanation structures by up to 40% (§4). • We also show downstream task (defeasible inference (Rudinger et al., 2020)) improvement for all domains by at least 1.2 points on accuracy (§6).']",intro_chunked,"However, real-world human feedback is often imprecise and not directly actionable. Another line of prior approaches (interactive reasoning approach) (Talmor et al., 2020) explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box – so the human does not know what the model’s internal belief is and how it will change based on the feedback. These two lines of prior approaches inspire this paper – we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation (similar to the interactive semantic parsing approach). We do this while relaxing the assumptions of the parsing approach – our feedback does not have a task-specific structure, and it is not assumed to be actionable (similar to the interactive reasoning approach). We introduce MERCURIE, a pipeline system with two components, a previously trained neural model M and a graph corrector G. It takes as input any previously trained neural model M capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure (for example, that some nodes are inconsistent with the rest of the graph). As output, it produces a better explanation structure. The contributions of this work are: • We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that MERCURIE can improve the consistency of explanation structures by up to 40% (§4). • We also show downstream task (defeasible inference (Rudinger et al., 2020)) improvement for all domains by at least 1.2 points on accuracy (§6)."
54,Aman Madaan,"[' Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.', 'The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1.', 'In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning']",intro_chunked," Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events."
55,Aman Madaan,"[' Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.', 'The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1.', 'In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning']",intro_chunked,"The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1."
56,Aman Madaan,"[' Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.', 'The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1.', 'In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning']",intro_chunked,"In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning"
57,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked," Temporal reasoning is crucial for analyzing the interactions among complex events and producing
coherent interpretations of text data (Duran et al.,
2007). There is a rich body of research on the
use of temporal information in a variety of important application domains, including topic detection
and tracking (Makkonen et al., 2003), information
extraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://
github.com/madaan/temporal-graph-gen
sis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the
temporal ordering among events, where the nodes
are the individual events, and the edges capture
temporal relationships such as “before”, “after” or
“simultaneous”. Representative work on automated
extraction of such graphs from textual documents
includes the early work by Chambers and Jurafsky
(2009), where the focus is on the construction of
event chains from a collection of documents, and
the more recent CAEVO (Chambers et al., 2014) and
Cogcomptime (Ning et al., 2018), which extract
a graph for each input document instead. These
methods focus on rule-based and statistical submodules to extract verb-centered events and the
temporal relations among them."
58,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked,"As an emerging
area of NLP, large scale pre-trained language models have made strides in addressing challenging
tasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog
generation (Budzianowski and Vulic´, 2019). These
systems typically fine-tune large language models
on a corpus of a task-specific dataset. However,
these techniques have not been investigated for
temporal graph extraction. This paper focuses on the problem of generation
of an event-level temporal graph for each document, and we refer to this task as contextualized
graph generation. We address this open challenge
by proposing a novel reformulation of the task as a
sequence-to-sequence mapping problem (Sutskever
et al., 2014), which enables us to leverage large pretrained models for our task. Further, different from
existing methods, our proposed approach is completely end-to-end and eliminates the need for a
pipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which
is a prerequisite to our main goal: the difficulty of
obtaining a large quantity of training graphs with human-annotated events and temporal relations. To
this end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps
for pruning and noise reduction."
59,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked,"We then encode
the graph in each training pair as a string in the
graph representation format DOT, transforming the
text-to-graph mapping into sequence-to-sequence
mapping. We fine-tune GPT-2 on this dataset of
document-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document
and the generated graph by our system. In summary, our main contributions are:
1. We present the first investigation on using
large pre-trained language models for contextualized temporal event graph generation by
proposing a new formulation of the problem
as a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large
collection of human-annotated graphs, which
is crucial for effective fine-tuning of pretrained models, by automatically producing a
collection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different
models) and a hand-labeled, out-of-domain
dataset (TimeBank-Dense), show the advantage of our proposed approach over strong
baselines. Further, we show that our approach
can help in generating plausible answers for
open ended-temporal questions in a reading
comprehension dataset, Torque (Ning et al.,
2020)."
60,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked," Politeness plays a crucial role in social interaction,
and is closely tied with power dynamics, social
distance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative
to use the appropriate level of politeness for smooth
communication in conversations (Coppock, 2005),
organizational settings like emails (Peterson et al.,
2011), memos, official documents, and many other
settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we
study the task of converting non-polite sentences
to polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,
2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a
style transfer task, and we argue that defining it is
cumbersome. While native speakers of a language
and cohabitants of a region have a good working
understanding of the phenomenon of politeness
for everyday conversation, pinning it down as a
definition is non-trivial (Meier, 1995). There are
primarily two reasons for this complexity. First, as
noted by (Brown et al., 1987), the phenomenon of
politeness is rich and multifaceted."
61,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked,"Second, politeness of a sentence depends on the culture, language,
and social structure of both the speaker and the addressed person. For instance, while using “please”
in requests made to the closest friends is common
amongst the native speakers of North American
English, such an act would be considered awkward,
if not rude, in the Arab culture (Kad´ ar and Mills ´ ,
2011). We circumscribe the scope of politeness for the
purpose of this study as follows: First, we adopt
the data driven definition of politeness proposed by
(Danescu-Niculescu-Mizil et al., 2013). Second,
we base our experiments on a dataset derived from
the Enron corpus (Klimt and Yang, 2004) which
consists of email exchanges in an American corporation. Thus, we restrict our attention to the notion
of politeness as widely accepted by the speakers of
North American English in a formal setting. Even after framing politeness transfer as a task,
there are additional challenges involved that differentiate politeness from other styles. Consider a
common directive in formal communication, “send
me the data”. While the sentence is not impolite, a rephrasing “could you please send me the
data” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings
out a distinct characteristic of politeness."
62,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked,"It is easy
to pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite
sentences, are typically riddled with curse words
and insulting phrases. While interesting, such cases
can typically be neutralized using lexicons. For
our study, we focus on the task of transferring the
non-polite sentences to polite sentences, where we
simply define non-politeness to be the absence of
both politeness and impoliteness. Note that this
is in stark contrast with the standard style transfer
tasks, which involve transferring a sentence from a
well-defined style polarity to the other (like positive
to negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the
words or phrases which belong to the original style
and replaces them with a tag token. If the sentence
has no style attributes, as in the case for politeness
transfer, the tagger adds the tag token in positions
where phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style."
63,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked,"Additionally, unlike previous systems, the outputs
of the intermediate steps in our system are fully
realized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target
style, our model won’t add any stylistic markers
and thus would allow the input to flow as is. We evaluate our model on politeness transfer as
well as 5 additional tasks described in prior work
(Shen et al., 2017; Prabhumoye et al., 2018; Li
et al., 2018) on content preservation, fluency and
style transfer accuracy. Both automatic and human
evaluations show that our model beats the stateof-the-art methods in content preservation, while
either matching or improving the transfer accuracy
across six different style transfer tasks(§5). The
results show that our technique is effective across a
broad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves
upon several of its limitations as described in (§2). Our main contribution is the design of politeness
transfer task. To this end, we provide a large dataset
of nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/
politeness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)
which are annotated as requests. To the best of our knowledge, we are the first to undertake politeness
as a style transfer task. In the process, we highlight an important class of problems wherein the
transfer involves going from a neutral style to the
target style. Finally, we design a “tag and generate”
pipeline that is particularly well suited for tasks like
politeness, while being general enough to match
or beat the performance of the existing systems on
popular style transfer tasks."
64,Aman Madaan,"[' Machine translation (MT) is a natural language processing task that aims to automatically\ntranslate text from a source language into a target language. Current state-of-the-art methods for MT are based on neural network architectures (Barrault et al., 2019), and often\nrequire large parallel corpora (i.e. the same text in two or more languages) for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which\ntranslators are readily available, for many low-resource languages pairs it is challenging to\nfind any speakers sufficiently proficient in both the source and target languages. We propose\na method to create data for training machine translation systems in such situations. Our\nmethod relies on obtaining captions for images depicting concepts that are relevant in most\nlanguages in the world. The captions are collected individually in each of the source and\ntarget languages, removing the requirement for the human annotators to be well-versed in\nboth languages. The captions for the same image are then paired to create training data.', 'Unlike existing methods (Hewitt et al., 2018; Bergsma & Van Durme, 2011; Singhal et al.,\n2019; Hitschler et al., 2016; Chen et al., 2019) aimed at leveraging multimodal information\nfor creating comparable corpus, our method does not rely on the existence of large resources\navailable in both the languages. Our goal is to propose a solution for the cold start scenario:\none in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates\ndata that are comparable, rather than strictly parallel. Nevertheless, comparable corpora\nhave been proven useful for machine translation and related applications (Munteanu et al.,\n2004; Abdul-Rauf & Schwenk, 2009; Irvine & Callison-Burch, 2013). In this work, we\nevaluate the utility of our collected data as a replacement for parallel corpora in two ways: • We show that bilingual speakers (of the source and target languages) judge the\ndataset as containing over 81% acceptable translation pairs. • We demonstrate that the data collected via our method has the potential of being\nused for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of parallel corpus creation\nand show that it is significantly more cost-efficient.', 'We apply our proposed method and evaluation techniques to a specific language pair: Hindi\nas the source language and English the target. Hindi is chosen as a testbed because, although it has over 520 million speakers (Chandramouli & General, 2011), far fewer parallel\ncorpora exist as compared to other widely spoken languages like French, German, Spanish,\netc. (Kunchukuttan et al., 2017). We note that our annotation tools and methodology are\nnot language-specific and can be easily adapted to any very low-resource setting with two\nassumptions: i) availability of speakers, and ii) a writing system. If the language uses a\nnovel character set not present in unicode, we can create a mapping from existing unicode\nsymbols to those in the language. If (digitally) literate speakers are not available, or if the\nlanguage lacks a working orthography (most of the world’s languages are indeed oral (Simons & Fennig, 2017)) we could instead collect spoken input, to be transcribed to a phonetic\ntranscription, with tools such as Adams et al. (2019) or Li et al. (2020).']",intro_chunked," Machine translation (MT) is a natural language processing task that aims to automatically
translate text from a source language into a target language. Current state-of-the-art methods for MT are based on neural network architectures (Barrault et al., 2019), and often
require large parallel corpora (i.e. the same text in two or more languages) for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which
translators are readily available, for many low-resource languages pairs it is challenging to
find any speakers sufficiently proficient in both the source and target languages. We propose
a method to create data for training machine translation systems in such situations. Our
method relies on obtaining captions for images depicting concepts that are relevant in most
languages in the world. The captions are collected individually in each of the source and
target languages, removing the requirement for the human annotators to be well-versed in
both languages. The captions for the same image are then paired to create training data."
65,Aman Madaan,"[' Machine translation (MT) is a natural language processing task that aims to automatically\ntranslate text from a source language into a target language. Current state-of-the-art methods for MT are based on neural network architectures (Barrault et al., 2019), and often\nrequire large parallel corpora (i.e. the same text in two or more languages) for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which\ntranslators are readily available, for many low-resource languages pairs it is challenging to\nfind any speakers sufficiently proficient in both the source and target languages. We propose\na method to create data for training machine translation systems in such situations. Our\nmethod relies on obtaining captions for images depicting concepts that are relevant in most\nlanguages in the world. The captions are collected individually in each of the source and\ntarget languages, removing the requirement for the human annotators to be well-versed in\nboth languages. The captions for the same image are then paired to create training data.', 'Unlike existing methods (Hewitt et al., 2018; Bergsma & Van Durme, 2011; Singhal et al.,\n2019; Hitschler et al., 2016; Chen et al., 2019) aimed at leveraging multimodal information\nfor creating comparable corpus, our method does not rely on the existence of large resources\navailable in both the languages. Our goal is to propose a solution for the cold start scenario:\none in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates\ndata that are comparable, rather than strictly parallel. Nevertheless, comparable corpora\nhave been proven useful for machine translation and related applications (Munteanu et al.,\n2004; Abdul-Rauf & Schwenk, 2009; Irvine & Callison-Burch, 2013). In this work, we\nevaluate the utility of our collected data as a replacement for parallel corpora in two ways: • We show that bilingual speakers (of the source and target languages) judge the\ndataset as containing over 81% acceptable translation pairs. • We demonstrate that the data collected via our method has the potential of being\nused for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of parallel corpus creation\nand show that it is significantly more cost-efficient.', 'We apply our proposed method and evaluation techniques to a specific language pair: Hindi\nas the source language and English the target. Hindi is chosen as a testbed because, although it has over 520 million speakers (Chandramouli & General, 2011), far fewer parallel\ncorpora exist as compared to other widely spoken languages like French, German, Spanish,\netc. (Kunchukuttan et al., 2017). We note that our annotation tools and methodology are\nnot language-specific and can be easily adapted to any very low-resource setting with two\nassumptions: i) availability of speakers, and ii) a writing system. If the language uses a\nnovel character set not present in unicode, we can create a mapping from existing unicode\nsymbols to those in the language. If (digitally) literate speakers are not available, or if the\nlanguage lacks a working orthography (most of the world’s languages are indeed oral (Simons & Fennig, 2017)) we could instead collect spoken input, to be transcribed to a phonetic\ntranscription, with tools such as Adams et al. (2019) or Li et al. (2020).']",intro_chunked,"Unlike existing methods (Hewitt et al., 2018; Bergsma & Van Durme, 2011; Singhal et al.,
2019; Hitschler et al., 2016; Chen et al., 2019) aimed at leveraging multimodal information
for creating comparable corpus, our method does not rely on the existence of large resources
available in both the languages. Our goal is to propose a solution for the cold start scenario:
one in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates
data that are comparable, rather than strictly parallel. Nevertheless, comparable corpora
have been proven useful for machine translation and related applications (Munteanu et al.,
2004; Abdul-Rauf & Schwenk, 2009; Irvine & Callison-Burch, 2013). In this work, we
evaluate the utility of our collected data as a replacement for parallel corpora in two ways: • We show that bilingual speakers (of the source and target languages) judge the
dataset as containing over 81% acceptable translation pairs. • We demonstrate that the data collected via our method has the potential of being
used for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of parallel corpus creation
and show that it is significantly more cost-efficient."
66,Aman Madaan,"[' Machine translation (MT) is a natural language processing task that aims to automatically\ntranslate text from a source language into a target language. Current state-of-the-art methods for MT are based on neural network architectures (Barrault et al., 2019), and often\nrequire large parallel corpora (i.e. the same text in two or more languages) for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which\ntranslators are readily available, for many low-resource languages pairs it is challenging to\nfind any speakers sufficiently proficient in both the source and target languages. We propose\na method to create data for training machine translation systems in such situations. Our\nmethod relies on obtaining captions for images depicting concepts that are relevant in most\nlanguages in the world. The captions are collected individually in each of the source and\ntarget languages, removing the requirement for the human annotators to be well-versed in\nboth languages. The captions for the same image are then paired to create training data.', 'Unlike existing methods (Hewitt et al., 2018; Bergsma & Van Durme, 2011; Singhal et al.,\n2019; Hitschler et al., 2016; Chen et al., 2019) aimed at leveraging multimodal information\nfor creating comparable corpus, our method does not rely on the existence of large resources\navailable in both the languages. Our goal is to propose a solution for the cold start scenario:\none in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates\ndata that are comparable, rather than strictly parallel. Nevertheless, comparable corpora\nhave been proven useful for machine translation and related applications (Munteanu et al.,\n2004; Abdul-Rauf & Schwenk, 2009; Irvine & Callison-Burch, 2013). In this work, we\nevaluate the utility of our collected data as a replacement for parallel corpora in two ways: • We show that bilingual speakers (of the source and target languages) judge the\ndataset as containing over 81% acceptable translation pairs. • We demonstrate that the data collected via our method has the potential of being\nused for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of parallel corpus creation\nand show that it is significantly more cost-efficient.', 'We apply our proposed method and evaluation techniques to a specific language pair: Hindi\nas the source language and English the target. Hindi is chosen as a testbed because, although it has over 520 million speakers (Chandramouli & General, 2011), far fewer parallel\ncorpora exist as compared to other widely spoken languages like French, German, Spanish,\netc. (Kunchukuttan et al., 2017). We note that our annotation tools and methodology are\nnot language-specific and can be easily adapted to any very low-resource setting with two\nassumptions: i) availability of speakers, and ii) a writing system. If the language uses a\nnovel character set not present in unicode, we can create a mapping from existing unicode\nsymbols to those in the language. If (digitally) literate speakers are not available, or if the\nlanguage lacks a working orthography (most of the world’s languages are indeed oral (Simons & Fennig, 2017)) we could instead collect spoken input, to be transcribed to a phonetic\ntranscription, with tools such as Adams et al. (2019) or Li et al. (2020).']",intro_chunked,"We apply our proposed method and evaluation techniques to a specific language pair: Hindi
as the source language and English the target. Hindi is chosen as a testbed because, although it has over 520 million speakers (Chandramouli & General, 2011), far fewer parallel
corpora exist as compared to other widely spoken languages like French, German, Spanish,
etc. (Kunchukuttan et al., 2017). We note that our annotation tools and methodology are
not language-specific and can be easily adapted to any very low-resource setting with two
assumptions: i) availability of speakers, and ii) a writing system. If the language uses a
novel character set not present in unicode, we can create a mapping from existing unicode
symbols to those in the language. If (digitally) literate speakers are not available, or if the
language lacks a working orthography (most of the world’s languages are indeed oral (Simons & Fennig, 2017)) we could instead collect spoken input, to be transcribed to a phonetic
transcription, with tools such as Adams et al. (2019) or Li et al. (2020)."
67,Aman Madaan,"[' While there is a long history of relation extraction systems\nin the NLP literature (e.g., (ARPA 1991; Soderland 1999;\nHoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which\nthe arguments are non-numerical. These include real world\nentities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression\ntypes such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,\ni.e., relations involving general numeric arguments such as\npopulation, area, atomic number, inflation rate, or boiling\npoint. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information\nfrom text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present\nseveral peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,\nin which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that\nsentence.', 'The signal from distant supervision becomes much\nweaker for numerical relations since there can be a much\nlarger number of reasons why a certain number is present\nin the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),\na state-of-the-art IE system, obtained an F-score of under\n20, hardly acceptable for real tasks. Secondly, numbers have\nunits and their semantics is important. Thirdly, numbers may\nbe written at different rounding levels necessitating partial\nmatching techniques. Lastly, numerical relations allow for\nsentences which describe the change in the argument value\nfrom the last measurement, instead of the argument value\nitself. In response, we develop two numerical relation extractors\nthat incorporate these observations . Both extractors expect\nminimal human supervision in the form of the unit of the\nrelation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that\nlooks for occurrences of specific numerical relation based\npatterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords\nto learn new keywords and patterns and can also leverage\nany existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries.', 'We\ncompile a knowledge-base using geopolitical data from\nWorld Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a\nslightly higher precision as compared to NumberRule. Both\nsystems massively outperform MultiR model (and its simple\nextensions) obtaining 17–25 point F-score improvements. We release our code1\nand other resources for further research. Overall, we make the following contributions in this\npaper:\n• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this\ntask compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract\na numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns\nwhile also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher\nrecall and F-score than NumberRule, and both systems\noutperform the MultiR model as well as a recall oriented\nbaseline by wide margins.']",intro_chunked," While there is a long history of relation extraction systems
in the NLP literature (e.g., (ARPA 1991; Soderland 1999;
Hoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which
the arguments are non-numerical. These include real world
entities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression
types such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,
i.e., relations involving general numeric arguments such as
population, area, atomic number, inflation rate, or boiling
point. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information
from text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present
several peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,
in which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that
sentence."
68,Aman Madaan,"[' While there is a long history of relation extraction systems\nin the NLP literature (e.g., (ARPA 1991; Soderland 1999;\nHoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which\nthe arguments are non-numerical. These include real world\nentities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression\ntypes such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,\ni.e., relations involving general numeric arguments such as\npopulation, area, atomic number, inflation rate, or boiling\npoint. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information\nfrom text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present\nseveral peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,\nin which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that\nsentence.', 'The signal from distant supervision becomes much\nweaker for numerical relations since there can be a much\nlarger number of reasons why a certain number is present\nin the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),\na state-of-the-art IE system, obtained an F-score of under\n20, hardly acceptable for real tasks. Secondly, numbers have\nunits and their semantics is important. Thirdly, numbers may\nbe written at different rounding levels necessitating partial\nmatching techniques. Lastly, numerical relations allow for\nsentences which describe the change in the argument value\nfrom the last measurement, instead of the argument value\nitself. In response, we develop two numerical relation extractors\nthat incorporate these observations . Both extractors expect\nminimal human supervision in the form of the unit of the\nrelation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that\nlooks for occurrences of specific numerical relation based\npatterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords\nto learn new keywords and patterns and can also leverage\nany existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries.', 'We\ncompile a knowledge-base using geopolitical data from\nWorld Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a\nslightly higher precision as compared to NumberRule. Both\nsystems massively outperform MultiR model (and its simple\nextensions) obtaining 17–25 point F-score improvements. We release our code1\nand other resources for further research. Overall, we make the following contributions in this\npaper:\n• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this\ntask compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract\na numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns\nwhile also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher\nrecall and F-score than NumberRule, and both systems\noutperform the MultiR model as well as a recall oriented\nbaseline by wide margins.']",intro_chunked,"The signal from distant supervision becomes much
weaker for numerical relations since there can be a much
larger number of reasons why a certain number is present
in the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),
a state-of-the-art IE system, obtained an F-score of under
20, hardly acceptable for real tasks. Secondly, numbers have
units and their semantics is important. Thirdly, numbers may
be written at different rounding levels necessitating partial
matching techniques. Lastly, numerical relations allow for
sentences which describe the change in the argument value
from the last measurement, instead of the argument value
itself. In response, we develop two numerical relation extractors
that incorporate these observations . Both extractors expect
minimal human supervision in the form of the unit of the
relation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that
looks for occurrences of specific numerical relation based
patterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords
to learn new keywords and patterns and can also leverage
any existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries."
69,Aman Madaan,"[' While there is a long history of relation extraction systems\nin the NLP literature (e.g., (ARPA 1991; Soderland 1999;\nHoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which\nthe arguments are non-numerical. These include real world\nentities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression\ntypes such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,\ni.e., relations involving general numeric arguments such as\npopulation, area, atomic number, inflation rate, or boiling\npoint. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information\nfrom text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present\nseveral peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,\nin which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that\nsentence.', 'The signal from distant supervision becomes much\nweaker for numerical relations since there can be a much\nlarger number of reasons why a certain number is present\nin the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),\na state-of-the-art IE system, obtained an F-score of under\n20, hardly acceptable for real tasks. Secondly, numbers have\nunits and their semantics is important. Thirdly, numbers may\nbe written at different rounding levels necessitating partial\nmatching techniques. Lastly, numerical relations allow for\nsentences which describe the change in the argument value\nfrom the last measurement, instead of the argument value\nitself. In response, we develop two numerical relation extractors\nthat incorporate these observations . Both extractors expect\nminimal human supervision in the form of the unit of the\nrelation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that\nlooks for occurrences of specific numerical relation based\npatterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords\nto learn new keywords and patterns and can also leverage\nany existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries.', 'We\ncompile a knowledge-base using geopolitical data from\nWorld Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a\nslightly higher precision as compared to NumberRule. Both\nsystems massively outperform MultiR model (and its simple\nextensions) obtaining 17–25 point F-score improvements. We release our code1\nand other resources for further research. Overall, we make the following contributions in this\npaper:\n• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this\ntask compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract\na numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns\nwhile also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher\nrecall and F-score than NumberRule, and both systems\noutperform the MultiR model as well as a recall oriented\nbaseline by wide margins.']",intro_chunked,"We
compile a knowledge-base using geopolitical data from
World Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a
slightly higher precision as compared to NumberRule. Both
systems massively outperform MultiR model (and its simple
extensions) obtaining 17–25 point F-score improvements. We release our code1
and other resources for further research. Overall, we make the following contributions in this
paper:
• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this
task compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract
a numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns
while also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher
recall and F-score than NumberRule, and both systems
outperform the MultiR model as well as a recall oriented
baseline by wide margins."
70,Hugo Touvron,"[' Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety.', 'This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.', 'We are releasing the following models to the general public for research and commercial use‡\n:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide¶ and code examples‖\nto facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3. The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).']",intro_chunked," Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in
complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized
domains such as programming and creative writing. They enable interaction with humans through intuitive
chat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training
methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,
followed by alignment with human preferences via techniques such as Reinforcement Learning with Human
Feedback (RLHF). Although the training methodology is simple, high computational requirements have
limited the development of LLMs to a few players. There have been public releases of pretrained LLMs
(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that
match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla
(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such
as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human
preferences, which greatly enhances their usability and safety."
71,Hugo Touvron,"[' Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety.', 'This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.', 'We are releasing the following models to the general public for research and commercial use‡\n:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide¶ and code examples‖\nto facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3. The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).']",intro_chunked,"This step can require significant costs in
compute and human annotation, and is often not transparent or easily reproducible, limiting progress within
the community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and
Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,
Llama 2-Chat models generally perform better than existing open-source models. They also appear to
be on par with some of the closed-source models, at least on the human evaluations we performed (see
Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data
annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,
this paper contributes a thorough description of our fine-tuning methodology and approach to improving
LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and
continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as
the emergence of tool usage and temporal organization of knowledge."
72,Hugo Touvron,"[' Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety.', 'This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.', 'We are releasing the following models to the general public for research and commercial use‡\n:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide¶ and code examples‖\nto facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3. The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).']",intro_chunked,"We are releasing the following models to the general public for research and commercial use‡
:
1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also
increased the size of the pretraining corpus by 40%, doubled the context length of the model, and
adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with
7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper
but are not releasing.§
2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release
variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,
Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;
Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover
all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform
safety testing and tuning tailored to their specific applications of the model. We provide a responsible use
guide¶ and code examples‖
to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of
our responsible release strategy can be found in Section 5.3. The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology
(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related
work (Section 6), and conclusions (Section 7)."
73,Hugo Touvron,"[' Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties first appeared when scaling models to a\nsufficient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that\nmore parameters will lead to better performance. However, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale. In this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of performance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al.', '(2022) recommends\ntraining a 10B model on 200B tokens, we find\nthat the performance of a 7B model continues to\nimprove even after 1T tokens. The focus of this work is to train a series of\nlanguage models that achieve the best possible performance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA, ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10× smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work compatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview\nof the modifications we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.']",intro_chunked," Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a
few examples (Brown et al., 2020). These few-shot
properties first appeared when scaling models to a
sufficient size (Kaplan et al., 2020), resulting in a
line of work that focuses on further scaling these
models (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that
more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022)
shows that, for a given compute budget, the best
performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. (2022) is to determine how to best
scale the dataset and model sizes for a particular
training compute budget. However, this objective
disregards the inference budget, which becomes
critical when serving a language model at scale. In this context, given a target level of performance,
the preferred model is not the fastest to train but the
fastest at inference, and although it may be cheaper
to train a large model to reach a certain level of performance, a smaller one trained longer will
ultimately be cheaper at inference. For instance,
although Hoffmann et al."
74,Hugo Touvron,"[' Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties first appeared when scaling models to a\nsufficient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that\nmore parameters will lead to better performance. However, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale. In this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of performance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al.', '(2022) recommends\ntraining a 10B model on 200B tokens, we find\nthat the performance of a 7B model continues to\nimprove even after 1T tokens. The focus of this work is to train a series of\nlanguage models that achieve the best possible performance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA, ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10× smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work compatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview\nof the modifications we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.']",intro_chunked,"(2022) recommends
training a 10B model on 200B tokens, we find
that the performance of a 7B model continues to
improve even after 1T tokens. The focus of this work is to train a series of
language models that achieve the best possible performance at various inference budgets, by training
on more tokens than what is typically used. The
resulting models, called LLaMA, ranges from 7B
to 65B parameters with competitive performance
compared to the best existing LLMs. For instance,
LLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10× smaller. We believe that
this model will help democratize the access and
study of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter
model is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only
use publicly available data, making our work compatible with open-sourcing, while most existing
models rely on data which is either not publicly
available or undocumented (e.g. “Books – 2TB” or
“Social media conversations”). There exist some
exceptions, notably OPT (Zhang et al., 2022),
GPT-NeoX (Black et al., 2022), BLOOM (Scao
et al., 2022) and GLM (Zeng et al., 2022), but none
that are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview
of the modifications we made to the transformer
architecture (Vaswani et al., 2017), as well as our
training method. We then report the performance of
our models and compare with others LLMs on a set
of standard benchmarks. Finally, we expose some
of the biases and toxicity encoded in our models,
using some of the most recent benchmarks from
the responsible AI community."
75,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked," Although the fundamental ideas of deep trainable neural
networks have been around for decades, only recently have
barriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of
these barriers are related to non-convex optimization in one
way or another, which is central to the success of modern
neural networks. The optimization challenges have been
addressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization
of very deep networks. An exceptionally successful design
principle is using residual connections [24, 25]. Although
this does not change the expressiveness of the functions that
the network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very
deep networks. Another key element to the optimization is
the importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet
dataset [11], and the popularization of transfer learning with
pre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions
of parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address
this issue."
76,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"Data-augmentation strategies, including those
mixing different images like Mixup [61] and CutMix [60],
have proven to provide a complementary data-driven form
of regularization. More recently, multiple works propose
to resort to self-supervised pre-training. These approaches
rely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)
auto-encoders [5, 22, 16], which were popular in the early
deep learning literature [7, 19, 27]. Similarly, contrastive
approaches [23, 9] provide a richer supervision less prone to
a supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,
possibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over
learning from the data directly. In contrast to traditional
distillation, co-distillation does not require pre-training a
(strong) teacher. Instead, a pool of models supervise each
other. Practically, it faces several limitations, including the
difficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights."
77,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider
a single target model to be trained, and we instantiate two
submodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can
backpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel
serves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the
submodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss
compared to the label loss, and our experiments show that
it significantly increases the final model accuracy. This co-training across different submodels, which we
refer to as cosub, can be regarded as a massive co-training
between 2
L models that share a common set of parameters,
where L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all
models. With a layer drop-rate set to 0.5, for instance for
a ViT-H model, all submodels are equiprobable, and then it
amounts to averaging the weights of 2
2×32 models."
78,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"Our contributions can be summarized as follows:
• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and
fine-tuning it at resolution 448, we obtain 87.4% top-1
accuracy on Imagenet-val. • We provide an efficient implementation to subsample
models on the fly. It is a simple yet effective variation
of stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models
by themselves even with significant trimming, similar
to LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures
(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from
scratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the
DeiT repository."
79,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked," After their vast success in NLP, transformers models [55] and their derivatives
are increasingly popular in computer vision. They are increasingly used in image
classification [13], detection & segmentation [3], video analysis, etc. In particular,
the vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative
to convolutional architectures. This supports the adoption of transformers as a
general architecture able to learn convolutions as well as longer range operations
through the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,
41] implicitly offer built-in translation invariance. As a result their training does
not have to learn this prior. It is therefore not surprising that hybrid architectures
that include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,
transformers have to learn about the structure of images while optimizing the
model such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks
in the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently
train vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al."
80,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"[13], the training procedures are mostly
variants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions
and a pyramid structure. These new designs, while being particularly effective
for some tasks, are less general. One difficult question to address is whether the
improved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training
have raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer
architecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as
transfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like
BeiT [2] is due to the training, e.g."
81,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general
implicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art
on fully supervised and self-supervised approaches, with new insights regarding
data-augmentation. We propose new training recipes for vision transformers on
ImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In
2
particular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the
training of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for
self-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train
vision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping
when pre-training on a larger set like ImageNet-21k."
82,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it
also has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k
than when pre-training at resolution 224 × 224 (256 tokens). This is also less
demanding at pre-training time, as there are 70% fewer tokens. From this
perspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making
another step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et
al. [48]. As a result, we obtain a competitive performance in image classification
and segmentation, even when compared to recent popular architectures such as
SwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below
we point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with
supervised training procedure reported in the literature at resolution 224×224."
83,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"• Our training procedure for ImageNet-1k allow us to train a billion-parameter
ViT-H (52 layers) without any hyper-parameter adaptation, just using the
same stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,
i.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of
GPUs required and the training time for ViT-H, making it effectively possible
to train such models without a reduced amount of resources. This is thanks
to our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with
BerT-like self-supervised approaches [2, 19] with their default setting and
when using the same level of annotations and less epochs, both for the tasks
of image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance
trade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better
to another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task."
84,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked," Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers."
85,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked,"Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task."
86,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked,"This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions."
87,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked,"Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21]."
88,Hugo Touvron,"[' Computer vision is about understanding how to obtain a high-level representation of images\nand videos. High-level representations are obtained by projecting the image into a vector space\nwith a certain structure that makes it easier to extract the information needed to interpret the\nimage. This allows complex tasks such as recognising concepts in an image or performing action\nrecognition in video. While it is easy for a human to recognise a given concept, it is hard to design\nan algorithm that would do the same. Indeed, when we see a cat, we know quite easily that it is\na cat, but it is almost impossible to describe in an algorithmic way all the steps that lead us to\nthis conclusion. Early approaches were based on handcrafted image representations, i.e manually\ndesigned and relying on expert knowledge. The most emblematic strategy is certainly the Bagof-Words (BoW) method, which encodes and pools local features on visual dictionaries. BoW\nwas the state-of-the-art approach for image classification in the 2000s. Inspired by information\nretrieval [170], the pioneering work [138] introduced a BoW scheme for image representation\nusing a color dictionary, extended to Gabor feature dictionary by [74], and finally popularized\nusing SIFT features (Scale-Invariant Feature Transform [137]) for visual recognition [43, 179]. In\nthe 2010s, we then witnessed the emergence of deep learning methods, which gradually overtook\nall traditional computer vision approaches. The computer vision field includes many tasks such as image classification, detection or segmentation (see Figure 1.1 for an illustration). Today, the gold standard approaches to solve these\ntasks are based on deep learning. This thesis falls within this context. In the following, we detail\nthe basics of deep learning for vision and position our contributions.']",intro_chunked," Computer vision is about understanding how to obtain a high-level representation of images
and videos. High-level representations are obtained by projecting the image into a vector space
with a certain structure that makes it easier to extract the information needed to interpret the
image. This allows complex tasks such as recognising concepts in an image or performing action
recognition in video. While it is easy for a human to recognise a given concept, it is hard to design
an algorithm that would do the same. Indeed, when we see a cat, we know quite easily that it is
a cat, but it is almost impossible to describe in an algorithmic way all the steps that lead us to
this conclusion. Early approaches were based on handcrafted image representations, i.e manually
designed and relying on expert knowledge. The most emblematic strategy is certainly the Bagof-Words (BoW) method, which encodes and pools local features on visual dictionaries. BoW
was the state-of-the-art approach for image classification in the 2000s. Inspired by information
retrieval [170], the pioneering work [138] introduced a BoW scheme for image representation
using a color dictionary, extended to Gabor feature dictionary by [74], and finally popularized
using SIFT features (Scale-Invariant Feature Transform [137]) for visual recognition [43, 179]. In
the 2010s, we then witnessed the emergence of deep learning methods, which gradually overtook
all traditional computer vision approaches. The computer vision field includes many tasks such as image classification, detection or segmentation (see Figure 1.1 for an illustration). Today, the gold standard approaches to solve these
tasks are based on deep learning. This thesis falls within this context. In the following, we detail
the basics of deep learning for vision and position our contributions."
89,Hugo Touvron,"[' Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches.', 'Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.', 'This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 .']",intro_chunked," Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches."
90,Hugo Touvron,"[' Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches.', 'Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.', 'This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 .']",intro_chunked,"Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network."
91,Hugo Touvron,"[' Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches.', 'Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.', 'This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 .']",intro_chunked,"This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 ."
92,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked," Convolutional neural networks have been the main design paradigm for image
understanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,
namely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest
in architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled
image dataset (JFT-300M [46], 300 millions images). The paper concluded that
transformers “do not generalize well when trained on insufficient amounts of data”,
and the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two
to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)
that is competitive with convnets having a similar number of parameters and
efficiency. It uses Imagenet as the sole training set."
93,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked,"We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements
included in the timm library [55]. With our Data-efficient image Transformers
(DeiT), we report large improvements over previous results, see Figure 1. Our
ablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce
a token-based strategy, specific to transformers and denoted by DeiT, and
show that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:
• We show that our neural networks that contains no convolutional layer
can achieve competitive results against the state of the art on ImageNet
with no external data. They are learned on a single node with 4 GPUs in
three days1
. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,
which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the
transformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin."
94,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked,"• Interestingly, with our distillation, image transformers learn more from a
convnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to
different downstream tasks such as fine-grained classification, on several
popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,
Stanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,
and focus on transformers for image classification in Section 3. We introduce
our distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent
transformers, as well as a comparative evaluation of our transformer-specific
distillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the
key ingredients involved in DeiT. We conclude in Section 7."
95,Hugo Touvron,"[' Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.', 'We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation.', 'To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.']",intro_chunked," Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier."
96,Hugo Touvron,"[' Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.', 'We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation.', 'To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.']",intro_chunked,"We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation."
97,Hugo Touvron,"[' Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.', 'We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation.', 'To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.']",intro_chunked,"To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation."
98,Hugo Touvron,"[' Residual architectures are prominent in computer vision since the advent of\nResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function\ng and R define how the network updates the input x at layer l. The function g is typically identity, while\nR is the main building block of the network: many variants in the literature essentially differ on how\none defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and\narchitecture design. As pointed out by He et al. [27], residual networks do not\noffer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]\nthe importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network\nalternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original\narchitecture of Vaswani et al. [66], except the LayerNorm is applied before the\nblock (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et\nal.', '[13] adopt this choice with LayerNorm for training deeper transformers for\nvarious media, including for image generation where they train transformers\nwith 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,\n34, 75]. In Section 2, we revisit this topic for transformer architectures solving\nimage classification problems. Examples of approaches closely related to ours\ninclude Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to\nimprove the training of deeper architecture compared to current methods for\nimage transformers. Formally, we add a learnable diagonal matrix on output of\neach residual block, initialized close to (but not at) 0. Adding this simple layer\nafter each residual block improves the training dynamic, allowing us to train\ndeeper high-capacity image transformers that benefit from depth. We refer to\nthis approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2.', 'It is akin to an encoder/decoder architecture,\nin which we explicitly separate the transformer layers involving self-attention\nbetween patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear\nclassifier. This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54], our model is on par with the state\nof the art (86.5%) while requiring less FLOPs (329B vs 377B) and having\nless parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We\ndiscuss related works along this paper and in the dedicated Section 6, before\nwe conclude in Section 7. The appendices contain some variations we have\ntried during our exploration.']",intro_chunked," Residual architectures are prominent in computer vision since the advent of
ResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function
g and R define how the network updates the input x at layer l. The function g is typically identity, while
R is the main building block of the network: many variants in the literature essentially differ on how
one defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and
architecture design. As pointed out by He et al. [27], residual networks do not
offer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]
the importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network
alternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original
architecture of Vaswani et al. [66], except the LayerNorm is applied before the
block (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et
al."
99,Hugo Touvron,"[' Residual architectures are prominent in computer vision since the advent of\nResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function\ng and R define how the network updates the input x at layer l. The function g is typically identity, while\nR is the main building block of the network: many variants in the literature essentially differ on how\none defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and\narchitecture design. As pointed out by He et al. [27], residual networks do not\noffer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]\nthe importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network\nalternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original\narchitecture of Vaswani et al. [66], except the LayerNorm is applied before the\nblock (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et\nal.', '[13] adopt this choice with LayerNorm for training deeper transformers for\nvarious media, including for image generation where they train transformers\nwith 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,\n34, 75]. In Section 2, we revisit this topic for transformer architectures solving\nimage classification problems. Examples of approaches closely related to ours\ninclude Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to\nimprove the training of deeper architecture compared to current methods for\nimage transformers. Formally, we add a learnable diagonal matrix on output of\neach residual block, initialized close to (but not at) 0. Adding this simple layer\nafter each residual block improves the training dynamic, allowing us to train\ndeeper high-capacity image transformers that benefit from depth. We refer to\nthis approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2.', 'It is akin to an encoder/decoder architecture,\nin which we explicitly separate the transformer layers involving self-attention\nbetween patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear\nclassifier. This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54], our model is on par with the state\nof the art (86.5%) while requiring less FLOPs (329B vs 377B) and having\nless parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We\ndiscuss related works along this paper and in the dedicated Section 6, before\nwe conclude in Section 7. The appendices contain some variations we have\ntried during our exploration.']",intro_chunked,"[13] adopt this choice with LayerNorm for training deeper transformers for
various media, including for image generation where they train transformers
with 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,
34, 75]. In Section 2, we revisit this topic for transformer architectures solving
image classification problems. Examples of approaches closely related to ours
include Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to
improve the training of deeper architecture compared to current methods for
image transformers. Formally, we add a learnable diagonal matrix on output of
each residual block, initialized close to (but not at) 0. Adding this simple layer
after each residual block improves the training dynamic, allowing us to train
deeper high-capacity image transformers that benefit from depth. We refer to
this approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2."
100,Hugo Touvron,"[' Residual architectures are prominent in computer vision since the advent of\nResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function\ng and R define how the network updates the input x at layer l. The function g is typically identity, while\nR is the main building block of the network: many variants in the literature essentially differ on how\none defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and\narchitecture design. As pointed out by He et al. [27], residual networks do not\noffer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]\nthe importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network\nalternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original\narchitecture of Vaswani et al. [66], except the LayerNorm is applied before the\nblock (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et\nal.', '[13] adopt this choice with LayerNorm for training deeper transformers for\nvarious media, including for image generation where they train transformers\nwith 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,\n34, 75]. In Section 2, we revisit this topic for transformer architectures solving\nimage classification problems. Examples of approaches closely related to ours\ninclude Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to\nimprove the training of deeper architecture compared to current methods for\nimage transformers. Formally, we add a learnable diagonal matrix on output of\neach residual block, initialized close to (but not at) 0. Adding this simple layer\nafter each residual block improves the training dynamic, allowing us to train\ndeeper high-capacity image transformers that benefit from depth. We refer to\nthis approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2.', 'It is akin to an encoder/decoder architecture,\nin which we explicitly separate the transformer layers involving self-attention\nbetween patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear\nclassifier. This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54], our model is on par with the state\nof the art (86.5%) while requiring less FLOPs (329B vs 377B) and having\nless parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We\ndiscuss related works along this paper and in the dedicated Section 6, before\nwe conclude in Section 7. The appendices contain some variations we have\ntried during our exploration.']",intro_chunked,"It is akin to an encoder/decoder architecture,
in which we explicitly separate the transformer layers involving self-attention
between patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear
classifier. This explicit separation avoids the contradictory objective of guiding
the attention process while processing the class embedding. We refer to this
new architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and
complementary of our approaches:
• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands
of parameters to the network at training time (negligible w.r.t. the total
number of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional
training data. On ImageNet1k-val [54], our model is on par with the state
of the art (86.5%) while requiring less FLOPs (329B vs 377B) and having
less parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We
discuss related works along this paper and in the dedicated Section 6, before
we conclude in Section 7. The appendices contain some variations we have
tried during our exploration."
101,Hugo Touvron,"[' Image classification now achieves a performance\nthat meets many application needs [27, 37, 54]. In\npractice however, the dataset and labels available at\ntraining time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for\nfine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to\nfind enough images of rare classes, and annotating\nthem precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states\nthat: “Manually labeling a large number of images with\nthe presence or absence of 19,794 different classes is not\nfeasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases\ndue to the assisting algorithm. Being able to get strong\nclassification and image retrieval performance on fine\nconcepts using only coarse labels at training time can\ncircumvents the issue, liberating the data collection\nprocess from the quirks of a rigid fine-grained taxonomy.', 'In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:\nGiven a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their\nfine-grained semantic similarity to a new query image\noutside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we\nuse a non-parametric kNN classifier [61] for on-the-fly\nclassification, i.e. without training on the fine-grained\nlabels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build\nupon recent works [3, 62] that exploits two losses to\naddress both image classification and instance recognition, leveraging the “free” annotations provided by\nmultiple data augmentations of a same instance, in the\nspirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly\ninfer coarse labels even when classifying for a finer\ngranularity. For this purpose, we propose a simple\nmethod that exploits both a coarse classifier and image\nembeddings to improve fine-grained category-level\nretrieval.', 'This strategy outperforms existing works\nthat exploit coarse labels at training time but do not\nexplicitly rely on them when retrieving finer-grained\nconcepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following\ncontributions:\n• We propose a method that learns a representation\nat a finer granularity than the one offered by the\nannotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is\nstill +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the\ncoarse level. A byproduct of our study is a very\nstrong kNN-classifier on Imagenet: Grafit with\nResNet-50 trunk reaches 79.6% top-1 accuracy at\nresolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates\nbetter at a finer granularity. Everything being\nequal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the\nart on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],\nFood101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method\nin Section 3. Section 4 compares our approach against\nbaselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one\nlearned by a vanilla cross-entropy loss. Appendix B\ncomplements our experimental section 4 with more\ndetailed results. Appendix C provides visual results\nassociated with different levels of training/testing\ngranularities.']",intro_chunked," Image classification now achieves a performance
that meets many application needs [27, 37, 54]. In
practice however, the dataset and labels available at
training time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for
fine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to
find enough images of rare classes, and annotating
them precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states
that: “Manually labeling a large number of images with
the presence or absence of 19,794 different classes is not
feasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases
due to the assisting algorithm. Being able to get strong
classification and image retrieval performance on fine
concepts using only coarse labels at training time can
circumvents the issue, liberating the data collection
process from the quirks of a rigid fine-grained taxonomy."
102,Hugo Touvron,"[' Image classification now achieves a performance\nthat meets many application needs [27, 37, 54]. In\npractice however, the dataset and labels available at\ntraining time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for\nfine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to\nfind enough images of rare classes, and annotating\nthem precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states\nthat: “Manually labeling a large number of images with\nthe presence or absence of 19,794 different classes is not\nfeasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases\ndue to the assisting algorithm. Being able to get strong\nclassification and image retrieval performance on fine\nconcepts using only coarse labels at training time can\ncircumvents the issue, liberating the data collection\nprocess from the quirks of a rigid fine-grained taxonomy.', 'In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:\nGiven a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their\nfine-grained semantic similarity to a new query image\noutside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we\nuse a non-parametric kNN classifier [61] for on-the-fly\nclassification, i.e. without training on the fine-grained\nlabels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build\nupon recent works [3, 62] that exploits two losses to\naddress both image classification and instance recognition, leveraging the “free” annotations provided by\nmultiple data augmentations of a same instance, in the\nspirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly\ninfer coarse labels even when classifying for a finer\ngranularity. For this purpose, we propose a simple\nmethod that exploits both a coarse classifier and image\nembeddings to improve fine-grained category-level\nretrieval.', 'This strategy outperforms existing works\nthat exploit coarse labels at training time but do not\nexplicitly rely on them when retrieving finer-grained\nconcepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following\ncontributions:\n• We propose a method that learns a representation\nat a finer granularity than the one offered by the\nannotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is\nstill +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the\ncoarse level. A byproduct of our study is a very\nstrong kNN-classifier on Imagenet: Grafit with\nResNet-50 trunk reaches 79.6% top-1 accuracy at\nresolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates\nbetter at a finer granularity. Everything being\nequal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the\nart on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],\nFood101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method\nin Section 3. Section 4 compares our approach against\nbaselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one\nlearned by a vanilla cross-entropy loss. Appendix B\ncomplements our experimental section 4 with more\ndetailed results. Appendix C provides visual results\nassociated with different levels of training/testing\ngranularities.']",intro_chunked,"In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:
Given a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their
fine-grained semantic similarity to a new query image
outside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we
use a non-parametric kNN classifier [61] for on-the-fly
classification, i.e. without training on the fine-grained
labels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build
upon recent works [3, 62] that exploits two losses to
address both image classification and instance recognition, leveraging the “free” annotations provided by
multiple data augmentations of a same instance, in the
spirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly
infer coarse labels even when classifying for a finer
granularity. For this purpose, we propose a simple
method that exploits both a coarse classifier and image
embeddings to improve fine-grained category-level
retrieval."
103,Hugo Touvron,"[' Image classification now achieves a performance\nthat meets many application needs [27, 37, 54]. In\npractice however, the dataset and labels available at\ntraining time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for\nfine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to\nfind enough images of rare classes, and annotating\nthem precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states\nthat: “Manually labeling a large number of images with\nthe presence or absence of 19,794 different classes is not\nfeasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases\ndue to the assisting algorithm. Being able to get strong\nclassification and image retrieval performance on fine\nconcepts using only coarse labels at training time can\ncircumvents the issue, liberating the data collection\nprocess from the quirks of a rigid fine-grained taxonomy.', 'In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:\nGiven a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their\nfine-grained semantic similarity to a new query image\noutside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we\nuse a non-parametric kNN classifier [61] for on-the-fly\nclassification, i.e. without training on the fine-grained\nlabels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build\nupon recent works [3, 62] that exploits two losses to\naddress both image classification and instance recognition, leveraging the “free” annotations provided by\nmultiple data augmentations of a same instance, in the\nspirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly\ninfer coarse labels even when classifying for a finer\ngranularity. For this purpose, we propose a simple\nmethod that exploits both a coarse classifier and image\nembeddings to improve fine-grained category-level\nretrieval.', 'This strategy outperforms existing works\nthat exploit coarse labels at training time but do not\nexplicitly rely on them when retrieving finer-grained\nconcepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following\ncontributions:\n• We propose a method that learns a representation\nat a finer granularity than the one offered by the\nannotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is\nstill +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the\ncoarse level. A byproduct of our study is a very\nstrong kNN-classifier on Imagenet: Grafit with\nResNet-50 trunk reaches 79.6% top-1 accuracy at\nresolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates\nbetter at a finer granularity. Everything being\nequal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the\nart on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],\nFood101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method\nin Section 3. Section 4 compares our approach against\nbaselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one\nlearned by a vanilla cross-entropy loss. Appendix B\ncomplements our experimental section 4 with more\ndetailed results. Appendix C provides visual results\nassociated with different levels of training/testing\ngranularities.']",intro_chunked,"This strategy outperforms existing works
that exploit coarse labels at training time but do not
explicitly rely on them when retrieving finer-grained
concepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following
contributions:
• We propose a method that learns a representation
at a finer granularity than the one offered by the
annotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly
classification on ImageNet. This improvement is
still +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the
coarse level. A byproduct of our study is a very
strong kNN-classifier on Imagenet: Grafit with
ResNet-50 trunk reaches 79.6% top-1 accuracy at
resolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates
better at a finer granularity. Everything being
equal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the
art on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],
Food101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method
in Section 3. Section 4 compares our approach against
baselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one
learned by a vanilla cross-entropy loss. Appendix B
complements our experimental section 4 with more
detailed results. Appendix C provides visual results
associated with different levels of training/testing
granularities."
104,Hugo Touvron,"[' Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as\nsupported by the universal approximation theorem [15, 31, 12]. More precisely, the theorem states that stacking a number of\nbasic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions\non the non-linear basic blocks. Studies on non-linear complex holomorphic functions involved in escape-time fractals showed that iterating simple non-linear\nfunctions can also construct arbitrarily complex landscapes [2]. These functions are complex in the sense that their iso-surfaces\nare made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting\nfunction. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes. Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates\na single building block in the latent space of an auto-encoder. We focus on image translation tasks, that can be trained from\neither paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods [5, 41, 32].', 'In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and\nthe other for output domain B. Therefore we do not have access to any parallel data [8], which is a realistic scenario in\nmany applications, e.g., image restoration. We train a function FORMULA, such that the output b is\nindiscernible from images of B. Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this\ncompositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and\nstyle transfer, where the user may want to select the best rendering. This “Powers of layers” (PoL) mechanism is illustrated in\nFigure 1 in the category transfer context (horse to zebra). Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it\nsuitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend\nof current state-of-the-art works to specialize the architecture and to increase its complexity and number of parameters [10, 40, 7]. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all\nthings being equal otherwise, for the original set of image-to-image translation tasks proposed in their papers, as well as for\ndenoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive\nresults confirmed by objective and psycho-visual metrics, illustrated by visualizations.']",intro_chunked," Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as
supported by the universal approximation theorem [15, 31, 12]. More precisely, the theorem states that stacking a number of
basic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions
on the non-linear basic blocks. Studies on non-linear complex holomorphic functions involved in escape-time fractals showed that iterating simple non-linear
functions can also construct arbitrarily complex landscapes [2]. These functions are complex in the sense that their iso-surfaces
are made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting
function. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes. Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates
a single building block in the latent space of an auto-encoder. We focus on image translation tasks, that can be trained from
either paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods [5, 41, 32]."
105,Hugo Touvron,"[' Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as\nsupported by the universal approximation theorem [15, 31, 12]. More precisely, the theorem states that stacking a number of\nbasic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions\non the non-linear basic blocks. Studies on non-linear complex holomorphic functions involved in escape-time fractals showed that iterating simple non-linear\nfunctions can also construct arbitrarily complex landscapes [2]. These functions are complex in the sense that their iso-surfaces\nare made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting\nfunction. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes. Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates\na single building block in the latent space of an auto-encoder. We focus on image translation tasks, that can be trained from\neither paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods [5, 41, 32].', 'In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and\nthe other for output domain B. Therefore we do not have access to any parallel data [8], which is a realistic scenario in\nmany applications, e.g., image restoration. We train a function FORMULA, such that the output b is\nindiscernible from images of B. Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this\ncompositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and\nstyle transfer, where the user may want to select the best rendering. This “Powers of layers” (PoL) mechanism is illustrated in\nFigure 1 in the category transfer context (horse to zebra). Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it\nsuitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend\nof current state-of-the-art works to specialize the architecture and to increase its complexity and number of parameters [10, 40, 7]. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all\nthings being equal otherwise, for the original set of image-to-image translation tasks proposed in their papers, as well as for\ndenoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive\nresults confirmed by objective and psycho-visual metrics, illustrated by visualizations.']",intro_chunked,"In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and
the other for output domain B. Therefore we do not have access to any parallel data [8], which is a realistic scenario in
many applications, e.g., image restoration. We train a function FORMULA, such that the output b is
indiscernible from images of B. Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this
compositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and
style transfer, where the user may want to select the best rendering. This “Powers of layers” (PoL) mechanism is illustrated in
Figure 1 in the category transfer context (horse to zebra). Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it
suitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend
of current state-of-the-art works to specialize the architecture and to increase its complexity and number of parameters [10, 40, 7]. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all
things being equal otherwise, for the original set of image-to-image translation tasks proposed in their papers, as well as for
denoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive
results confirmed by objective and psycho-visual metrics, illustrated by visualizations."
106,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.', 'This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]']",intro_chunked," Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8]."
107,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.', 'This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]']",intro_chunked,"In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time."
108,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.', 'This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]']",intro_chunked,"This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]"
109,Hugo Touvron,"[' The growing availability of large neuroimaging databases offers a unique opportunity to collect considerable amount of\ndata to train artificial intelligence algorithms. A side effect\nis that it inevitably leads to the introduction of new biases related especially to heterogeneous protocol guidelines in terms\nof subject enrollment and acquisition parameters [1, 2]. In\nthis paper, we investigate several strategies to account for data\ncoming from different databases. This is exemplified through\nthe design of a CAD system that discriminates healthy from\nAlzheimer’s Disease (AD) subjects based on volumetric characteristics derived from structural MRI. A nuisance variable is an external factor which is not of\ninterest for the study, but which causes an increase in variability within groups. In our case, nuisance variables can be\ndatabase, age, gender and Estimated Total Intracranial Volume (ETIV). A particular subcategory of nuisance variables\nare the confounding variables (or confounds) which also have\na direct effect on the target variable (i.e., the diagnosis for a\nCAD system). For instance, gender is a confounding variable\nfor AD diagnosis since almost two-thirds of the individuals diagnosed with AD are women [3]. We also define the concept of artificial confound, which is a nuisance variable that\nshould theoretically not have an effect on the target variable,\nbut which in practice has one because it has been sampled\nnon uniformly.', 'For instance, the database should not theoretically have an impact on AD diagnosis, but since the ratio\nof AD vs healthy subjects can be very different according to\nthe database, an artificial link is created. Note that such artificial confounds are as far as possible avoided in prospective\nstudy by collecting data stratified into groups that have the\nsame distribution for the confounds. Nonetheless, it may frequently occur when merging several databases that may have\nheterogeneous recruitment schemes in terms of age distribution, gender ratio, or pathology prevalence. Particular attention should be paid to both natural and artificial confounds since they can introduce a positive bias into\nthe performance of a classifier and can lead to misinterpret\nits behavior. Several papers already addressed how to handle confounds such as age and gender in neuroimaging study\n[4, 5, 6]. Here, we investigate more specifically how to account for data coming from multiple databases and the interplay that occurs with other confounds such as age and gender. First, we present experiments on simulated data that illustrate\nhow interactions with other confounds such as age can make\nthe handling of data coming from multiple databases an intricate problem. Then, we compare three strategies to adjust\ndata and evaluate them in the real scenario of a CAD system\ndedicated to Alzheimer’s disease.']",intro_chunked," The growing availability of large neuroimaging databases offers a unique opportunity to collect considerable amount of
data to train artificial intelligence algorithms. A side effect
is that it inevitably leads to the introduction of new biases related especially to heterogeneous protocol guidelines in terms
of subject enrollment and acquisition parameters [1, 2]. In
this paper, we investigate several strategies to account for data
coming from different databases. This is exemplified through
the design of a CAD system that discriminates healthy from
Alzheimer’s Disease (AD) subjects based on volumetric characteristics derived from structural MRI. A nuisance variable is an external factor which is not of
interest for the study, but which causes an increase in variability within groups. In our case, nuisance variables can be
database, age, gender and Estimated Total Intracranial Volume (ETIV). A particular subcategory of nuisance variables
are the confounding variables (or confounds) which also have
a direct effect on the target variable (i.e., the diagnosis for a
CAD system). For instance, gender is a confounding variable
for AD diagnosis since almost two-thirds of the individuals diagnosed with AD are women [3]. We also define the concept of artificial confound, which is a nuisance variable that
should theoretically not have an effect on the target variable,
but which in practice has one because it has been sampled
non uniformly."
110,Hugo Touvron,"[' The growing availability of large neuroimaging databases offers a unique opportunity to collect considerable amount of\ndata to train artificial intelligence algorithms. A side effect\nis that it inevitably leads to the introduction of new biases related especially to heterogeneous protocol guidelines in terms\nof subject enrollment and acquisition parameters [1, 2]. In\nthis paper, we investigate several strategies to account for data\ncoming from different databases. This is exemplified through\nthe design of a CAD system that discriminates healthy from\nAlzheimer’s Disease (AD) subjects based on volumetric characteristics derived from structural MRI. A nuisance variable is an external factor which is not of\ninterest for the study, but which causes an increase in variability within groups. In our case, nuisance variables can be\ndatabase, age, gender and Estimated Total Intracranial Volume (ETIV). A particular subcategory of nuisance variables\nare the confounding variables (or confounds) which also have\na direct effect on the target variable (i.e., the diagnosis for a\nCAD system). For instance, gender is a confounding variable\nfor AD diagnosis since almost two-thirds of the individuals diagnosed with AD are women [3]. We also define the concept of artificial confound, which is a nuisance variable that\nshould theoretically not have an effect on the target variable,\nbut which in practice has one because it has been sampled\nnon uniformly.', 'For instance, the database should not theoretically have an impact on AD diagnosis, but since the ratio\nof AD vs healthy subjects can be very different according to\nthe database, an artificial link is created. Note that such artificial confounds are as far as possible avoided in prospective\nstudy by collecting data stratified into groups that have the\nsame distribution for the confounds. Nonetheless, it may frequently occur when merging several databases that may have\nheterogeneous recruitment schemes in terms of age distribution, gender ratio, or pathology prevalence. Particular attention should be paid to both natural and artificial confounds since they can introduce a positive bias into\nthe performance of a classifier and can lead to misinterpret\nits behavior. Several papers already addressed how to handle confounds such as age and gender in neuroimaging study\n[4, 5, 6]. Here, we investigate more specifically how to account for data coming from multiple databases and the interplay that occurs with other confounds such as age and gender. First, we present experiments on simulated data that illustrate\nhow interactions with other confounds such as age can make\nthe handling of data coming from multiple databases an intricate problem. Then, we compare three strategies to adjust\ndata and evaluate them in the real scenario of a CAD system\ndedicated to Alzheimer’s disease.']",intro_chunked,"For instance, the database should not theoretically have an impact on AD diagnosis, but since the ratio
of AD vs healthy subjects can be very different according to
the database, an artificial link is created. Note that such artificial confounds are as far as possible avoided in prospective
study by collecting data stratified into groups that have the
same distribution for the confounds. Nonetheless, it may frequently occur when merging several databases that may have
heterogeneous recruitment schemes in terms of age distribution, gender ratio, or pathology prevalence. Particular attention should be paid to both natural and artificial confounds since they can introduce a positive bias into
the performance of a classifier and can lead to misinterpret
its behavior. Several papers already addressed how to handle confounds such as age and gender in neuroimaging study
[4, 5, 6]. Here, we investigate more specifically how to account for data coming from multiple databases and the interplay that occurs with other confounds such as age and gender. First, we present experiments on simulated data that illustrate
how interactions with other confounds such as age can make
the handling of data coming from multiple databases an intricate problem. Then, we compare three strategies to adjust
data and evaluate them in the real scenario of a CAD system
dedicated to Alzheimer’s disease."
111,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs.', 'For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].']",intro_chunked," Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8]."
112,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs.', 'For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].']",intro_chunked,"In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs."
113,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs.', 'For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].']",intro_chunked,"For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]."
114,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked," The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy
(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022)."
115,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"Our vision is to define a
few general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are
comprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),
where the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over
the model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang
et al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still
lag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,
methods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai
et al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage
feedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of
the models that have already undergone RLHF training. That is, these RLAIF methods inherit the
heavy dependency on the human-annotated preferences in the RLHF warm-up stage."
116,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"This leads to
a pivotal research question:
• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their
general alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach
namely SALMON. At the heart of our approach lies the introduction of the principle-following
(also termed instruction-following) reward model. Pioneering in its nature, this reward model is
adept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently
generating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,
2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the
resulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of
the final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection
of online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to
counteract reward hacking (Pan et al., 2022). This complication emerges when the policy model
exploits weaknesses in the reward model, producing inflated scores that do not accurately reflect
model performance. In SALMON, we can address this issue by simply crafting principles explicitly
2
Preprint
Write a story
about dromedaries."
117,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"Sampled prompts
RM-RLHF
RM-SALMON
RLHF (Ouyang et al., 2022)
SFT
SFT-generated responses
RM-RLAIF
RLAIF (Bai et al., 2022)
SALMON (Ours)
Principle Aggregating
AI-labeled preferences
human-labeled preferences
SFT
SFT
AI-labeled preferences Principle-following reward model
Stand-alone reward model
Stand-alone reward model
Reward Score
Prompt + Response
Principles
Reward Score
Prompt + Response
Reward Score
Prompt + Response
Principles
Principles
Human Annotator
In general, SFT denotes the
Supervised Fine-Tuned model, but it
can also be RLHF-trained in RLAIF. General
Alignment
Safety
Alignment
General
Alignment
Figure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and
SALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give
high scores to generally good responses, while the principle-following reward model in SALMON
is trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1
reward hacking patterns in model outputs, such as self-praising at the
end of the response. Additionally, we found that we are able to emphasize distinct aspects of the
alignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)
by customizing the preference principles."
118,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"Our methodology also proved effective in reducing the
occurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by
crafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to
a diverse range of language models without collecting any model-specific human preference data
(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include
principle-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations
(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron
et al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),
our method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from
scratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a
combined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-
Chat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response
demonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human
supervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1."
119,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked," Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment."
120,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked,"One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models."
121,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked,"Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at."
122,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked," The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries."
123,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems."
124,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3."
125,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1."
126,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch."
127,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values."
128,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked," Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022)."
129,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked,"Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task."
130,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked,"Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few ( N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image."
131,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked,"The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes."
132,Zhiqing Sun,"[' Complex physical systems described by non-linear partial\ndifferential equations (PDEs) are ubiquitous throughout the\n1Carnegie Mellon University, Pittsburgh, PA 15213, USA\n2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems\nin aeronautics (Rhie & Chow, 1983), medicine (Sallam &\nHwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations\n(Courant et al., 1967). Solving most equations of importance\nis usually computationally intractable with direct numerical\nsimulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE\nsolvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov\net al. 2021; Brandstetter et al. 2021, inter alia) have shown\nthat end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike\nclassical finite differences, finite volumes, finite elements,\nor pseudo-spectral methods that require a smooth variation\non the high-resolution meshes for guaranteed convergence,\nneural solvers do not rely on such conditions and are able\nto model the underlying physics with under-resolved low\nresolutions and produce high-quality simulations with significantly reduced computational cost.', 'The power of learnable PDE solvers is usually believed to\ncome from the super-resolution ability of neural networks,\nwhich means that the machine learning model is capable of\nrecovering the missing details based on the coarse features\n(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly\ntraining a super-resolution model, and then find that since\nlow-resolution down-sampling of the field can lead to some\ninformation loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the\ntrajectories and the temporal feature encoding scheme are\ncrucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of\ntwo worlds: stencil learning (i.e., Learned Interpolation in\nKochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO\n(Gu et al., 2020) as a state-of-the-art time series sequence\nmodel. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux\nwithin a finite volume method framework. As illustrated in\nFig.', '1, TSM can be regarded as a temporal generalization\nof classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned\ninterpolation solvers (Kochkov et al., 2021), both of which\nadaptively weight or interpolate the stencils based on the\nlatest states only. On the other hand, in TSM we use the\nHiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity\non each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation\ncoefficients, while the stencil learning framework ensures\nthat the neural system’s prediction exactly conserves the\nConservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize\nthe temporal bundling technique (Brandstetter et al., 2021)\nto avoid over-fitting and improve the prediction latency for\nTSM. Following the precedent work in the field (Li et al., 2020c;\nKochkov et al., 2021; Brandstetter et al., 2021), we evaluate\nthe proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing\nequation for turbulent flows with the conservation of mass\nand momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows\ncan achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and\ndifferent Reynolds numbers.']",intro_chunked," Complex physical systems described by non-linear partial
differential equations (PDEs) are ubiquitous throughout the
1Carnegie Mellon University, Pittsburgh, PA 15213, USA
2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems
in aeronautics (Rhie & Chow, 1983), medicine (Sallam &
Hwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations
(Courant et al., 1967). Solving most equations of importance
is usually computationally intractable with direct numerical
simulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE
solvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov
et al. 2021; Brandstetter et al. 2021, inter alia) have shown
that end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike
classical finite differences, finite volumes, finite elements,
or pseudo-spectral methods that require a smooth variation
on the high-resolution meshes for guaranteed convergence,
neural solvers do not rely on such conditions and are able
to model the underlying physics with under-resolved low
resolutions and produce high-quality simulations with significantly reduced computational cost."
133,Zhiqing Sun,"[' Complex physical systems described by non-linear partial\ndifferential equations (PDEs) are ubiquitous throughout the\n1Carnegie Mellon University, Pittsburgh, PA 15213, USA\n2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems\nin aeronautics (Rhie & Chow, 1983), medicine (Sallam &\nHwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations\n(Courant et al., 1967). Solving most equations of importance\nis usually computationally intractable with direct numerical\nsimulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE\nsolvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov\net al. 2021; Brandstetter et al. 2021, inter alia) have shown\nthat end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike\nclassical finite differences, finite volumes, finite elements,\nor pseudo-spectral methods that require a smooth variation\non the high-resolution meshes for guaranteed convergence,\nneural solvers do not rely on such conditions and are able\nto model the underlying physics with under-resolved low\nresolutions and produce high-quality simulations with significantly reduced computational cost.', 'The power of learnable PDE solvers is usually believed to\ncome from the super-resolution ability of neural networks,\nwhich means that the machine learning model is capable of\nrecovering the missing details based on the coarse features\n(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly\ntraining a super-resolution model, and then find that since\nlow-resolution down-sampling of the field can lead to some\ninformation loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the\ntrajectories and the temporal feature encoding scheme are\ncrucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of\ntwo worlds: stencil learning (i.e., Learned Interpolation in\nKochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO\n(Gu et al., 2020) as a state-of-the-art time series sequence\nmodel. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux\nwithin a finite volume method framework. As illustrated in\nFig.', '1, TSM can be regarded as a temporal generalization\nof classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned\ninterpolation solvers (Kochkov et al., 2021), both of which\nadaptively weight or interpolate the stencils based on the\nlatest states only. On the other hand, in TSM we use the\nHiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity\non each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation\ncoefficients, while the stencil learning framework ensures\nthat the neural system’s prediction exactly conserves the\nConservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize\nthe temporal bundling technique (Brandstetter et al., 2021)\nto avoid over-fitting and improve the prediction latency for\nTSM. Following the precedent work in the field (Li et al., 2020c;\nKochkov et al., 2021; Brandstetter et al., 2021), we evaluate\nthe proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing\nequation for turbulent flows with the conservation of mass\nand momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows\ncan achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and\ndifferent Reynolds numbers.']",intro_chunked,"The power of learnable PDE solvers is usually believed to
come from the super-resolution ability of neural networks,
which means that the machine learning model is capable of
recovering the missing details based on the coarse features
(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly
training a super-resolution model, and then find that since
low-resolution down-sampling of the field can lead to some
information loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the
trajectories and the temporal feature encoding scheme are
crucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of
two worlds: stencil learning (i.e., Learned Interpolation in
Kochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO
(Gu et al., 2020) as a state-of-the-art time series sequence
model. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux
within a finite volume method framework. As illustrated in
Fig."
134,Zhiqing Sun,"[' Complex physical systems described by non-linear partial\ndifferential equations (PDEs) are ubiquitous throughout the\n1Carnegie Mellon University, Pittsburgh, PA 15213, USA\n2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems\nin aeronautics (Rhie & Chow, 1983), medicine (Sallam &\nHwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations\n(Courant et al., 1967). Solving most equations of importance\nis usually computationally intractable with direct numerical\nsimulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE\nsolvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov\net al. 2021; Brandstetter et al. 2021, inter alia) have shown\nthat end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike\nclassical finite differences, finite volumes, finite elements,\nor pseudo-spectral methods that require a smooth variation\non the high-resolution meshes for guaranteed convergence,\nneural solvers do not rely on such conditions and are able\nto model the underlying physics with under-resolved low\nresolutions and produce high-quality simulations with significantly reduced computational cost.', 'The power of learnable PDE solvers is usually believed to\ncome from the super-resolution ability of neural networks,\nwhich means that the machine learning model is capable of\nrecovering the missing details based on the coarse features\n(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly\ntraining a super-resolution model, and then find that since\nlow-resolution down-sampling of the field can lead to some\ninformation loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the\ntrajectories and the temporal feature encoding scheme are\ncrucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of\ntwo worlds: stencil learning (i.e., Learned Interpolation in\nKochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO\n(Gu et al., 2020) as a state-of-the-art time series sequence\nmodel. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux\nwithin a finite volume method framework. As illustrated in\nFig.', '1, TSM can be regarded as a temporal generalization\nof classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned\ninterpolation solvers (Kochkov et al., 2021), both of which\nadaptively weight or interpolate the stencils based on the\nlatest states only. On the other hand, in TSM we use the\nHiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity\non each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation\ncoefficients, while the stencil learning framework ensures\nthat the neural system’s prediction exactly conserves the\nConservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize\nthe temporal bundling technique (Brandstetter et al., 2021)\nto avoid over-fitting and improve the prediction latency for\nTSM. Following the precedent work in the field (Li et al., 2020c;\nKochkov et al., 2021; Brandstetter et al., 2021), we evaluate\nthe proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing\nequation for turbulent flows with the conservation of mass\nand momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows\ncan achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and\ndifferent Reynolds numbers.']",intro_chunked,"1, TSM can be regarded as a temporal generalization
of classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned
interpolation solvers (Kochkov et al., 2021), both of which
adaptively weight or interpolate the stencils based on the
latest states only. On the other hand, in TSM we use the
HiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity
on each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation
coefficients, while the stencil learning framework ensures
that the neural system’s prediction exactly conserves the
Conservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize
the temporal bundling technique (Brandstetter et al., 2021)
to avoid over-fitting and improve the prediction latency for
TSM. Following the precedent work in the field (Li et al., 2020c;
Kochkov et al., 2021; Brandstetter et al., 2021), we evaluate
the proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing
equation for turbulent flows with the conservation of mass
and momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows
can achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and
different Reynolds numbers."
135,Zhiqing Sun,"[' Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.', 'The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.', 'We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).']",intro_chunked," Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs."
136,Zhiqing Sun,"[' Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.', 'The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.', 'We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).']",intro_chunked,"The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge."
137,Zhiqing Sun,"[' Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.', 'The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.', 'We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).']",intro_chunked,"We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018)."
138,Zhiqing Sun,"[' The Transformer architecture (Vaswani et al., 2017) has been successfully applied to various tasks, including natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2020), and time series forecasting (Zhou et al., 2020; Wu et al., 2021). Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) (Kitaev et al., 2020; Daras et al., 2020) and mini-batch spherical k-means (Roy et al., 2021; Wang et al., 2020a). The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.', 'The effectiveness of those ANN approaches rely on the assumption that the (transformed) query and key vectors should lie in the same space, which could be sub-optimal in dealing with different sparsity patterns, as analyzed in this paper (Section 3.2). Besides, the hash functions in LSH are randomized and data-agnostic, which cannot fully utilize the rich information in real-world data distributions. In this paper, we address the above limitations of existing ANN-based methods for attention sparsification. Firstly, we analyze two imbalance issues in LSH-produced sparse attention patterns, i.e., unbalanced hash bucket sizes and unbalanced query-key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that ANN-derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning-to-Hash Attention (LHA), for dynamic attention sparsification with enhanced model expressiveness. LHA directly optimizes our newly defined\nattention utility metric in an end-to-end manner via separate learnable hash functions for queries and\nkeys, respectively. As for reducing the computational complexity in the training phase, LHA uses\nunbiased kernelized attention techniques (Choromanski et al., 2020; Peng et al., 2021) to efficiently\napproximate the attention utilities. Similar to other sparse attention models (Kitaev et al., 2020;\nRoy et al., 2021), LHA reduces the overall complexity of self-attention from O(N2\n) to O(N1.5\n)\nfor sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for\nlanguage modeling, natural language understanding, and Long-Range-Arena show that LHA achieves\nbetter performance compared to strong transformer baselines.']",intro_chunked," The Transformer architecture (Vaswani et al., 2017) has been successfully applied to various tasks, including natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2020), and time series forecasting (Zhou et al., 2020; Wu et al., 2021). Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) (Kitaev et al., 2020; Daras et al., 2020) and mini-batch spherical k-means (Roy et al., 2021; Wang et al., 2020a). The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability."
139,Zhiqing Sun,"[' The Transformer architecture (Vaswani et al., 2017) has been successfully applied to various tasks, including natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2020), and time series forecasting (Zhou et al., 2020; Wu et al., 2021). Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) (Kitaev et al., 2020; Daras et al., 2020) and mini-batch spherical k-means (Roy et al., 2021; Wang et al., 2020a). The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.', 'The effectiveness of those ANN approaches rely on the assumption that the (transformed) query and key vectors should lie in the same space, which could be sub-optimal in dealing with different sparsity patterns, as analyzed in this paper (Section 3.2). Besides, the hash functions in LSH are randomized and data-agnostic, which cannot fully utilize the rich information in real-world data distributions. In this paper, we address the above limitations of existing ANN-based methods for attention sparsification. Firstly, we analyze two imbalance issues in LSH-produced sparse attention patterns, i.e., unbalanced hash bucket sizes and unbalanced query-key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that ANN-derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning-to-Hash Attention (LHA), for dynamic attention sparsification with enhanced model expressiveness. LHA directly optimizes our newly defined\nattention utility metric in an end-to-end manner via separate learnable hash functions for queries and\nkeys, respectively. As for reducing the computational complexity in the training phase, LHA uses\nunbiased kernelized attention techniques (Choromanski et al., 2020; Peng et al., 2021) to efficiently\napproximate the attention utilities. Similar to other sparse attention models (Kitaev et al., 2020;\nRoy et al., 2021), LHA reduces the overall complexity of self-attention from O(N2\n) to O(N1.5\n)\nfor sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for\nlanguage modeling, natural language understanding, and Long-Range-Arena show that LHA achieves\nbetter performance compared to strong transformer baselines.']",intro_chunked,"The effectiveness of those ANN approaches rely on the assumption that the (transformed) query and key vectors should lie in the same space, which could be sub-optimal in dealing with different sparsity patterns, as analyzed in this paper (Section 3.2). Besides, the hash functions in LSH are randomized and data-agnostic, which cannot fully utilize the rich information in real-world data distributions. In this paper, we address the above limitations of existing ANN-based methods for attention sparsification. Firstly, we analyze two imbalance issues in LSH-produced sparse attention patterns, i.e., unbalanced hash bucket sizes and unbalanced query-key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that ANN-derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning-to-Hash Attention (LHA), for dynamic attention sparsification with enhanced model expressiveness. LHA directly optimizes our newly defined
attention utility metric in an end-to-end manner via separate learnable hash functions for queries and
keys, respectively. As for reducing the computational complexity in the training phase, LHA uses
unbiased kernelized attention techniques (Choromanski et al., 2020; Peng et al., 2021) to efficiently
approximate the attention utilities. Similar to other sparse attention models (Kitaev et al., 2020;
Roy et al., 2021), LHA reduces the overall complexity of self-attention from O(N2
) to O(N1.5
)
for sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for
language modeling, natural language understanding, and Long-Range-Arena show that LHA achieves
better performance compared to strong transformer baselines."
140,Zhiqing Sun,"[' Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state-of-the-art neural detectors [21, 24, 19, 25, 38, 26, 11] are developed in a detect-and-merge fashion that is, instead of directly optimizing the predicted set in an end-to-end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post-processing step (e.g., “non-maximum suppression” or NMS) for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end-to-end and arguably sub-optimal.DEtection TRansformer (DETR) [2] is recently proposed as the first fully end-to-end object detector. It usesTransformer [32] to directly output a final set of predictions without further post-processing. However, it takes extra-long training time to converge. For example, the popular Faster RCNN model [26] only requires about 30 epochs to convergence, but DETR needs 500 epochs, which takes at least 10 days on 8 V100 GPUs.', 'Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like Transformer-based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETR’s optimization difficulty we conduct extensive experiments and find that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder-only version of DETR by removing the cross-attention module. We find that the encoder-only DETR yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETR’s Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only DETR with feature pyramids [18]. Specifically, we present TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN), which are inspired by a classic one-stage detector FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respectively. A novel Feature of Interest (FoI) selection mechanism is developed in TSP-FCOS to help Transformer encoder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20] the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.']",intro_chunked," Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state-of-the-art neural detectors [21, 24, 19, 25, 38, 26, 11] are developed in a detect-and-merge fashion that is, instead of directly optimizing the predicted set in an end-to-end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post-processing step (e.g., “non-maximum suppression” or NMS) for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end-to-end and arguably sub-optimal.DEtection TRansformer (DETR) [2] is recently proposed as the first fully end-to-end object detector. It usesTransformer [32] to directly output a final set of predictions without further post-processing. However, it takes extra-long training time to converge. For example, the popular Faster RCNN model [26] only requires about 30 epochs to convergence, but DETR needs 500 epochs, which takes at least 10 days on 8 V100 GPUs."
141,Zhiqing Sun,"[' Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state-of-the-art neural detectors [21, 24, 19, 25, 38, 26, 11] are developed in a detect-and-merge fashion that is, instead of directly optimizing the predicted set in an end-to-end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post-processing step (e.g., “non-maximum suppression” or NMS) for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end-to-end and arguably sub-optimal.DEtection TRansformer (DETR) [2] is recently proposed as the first fully end-to-end object detector. It usesTransformer [32] to directly output a final set of predictions without further post-processing. However, it takes extra-long training time to converge. For example, the popular Faster RCNN model [26] only requires about 30 epochs to convergence, but DETR needs 500 epochs, which takes at least 10 days on 8 V100 GPUs.', 'Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like Transformer-based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETR’s optimization difficulty we conduct extensive experiments and find that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder-only version of DETR by removing the cross-attention module. We find that the encoder-only DETR yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETR’s Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only DETR with feature pyramids [18]. Specifically, we present TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN), which are inspired by a classic one-stage detector FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respectively. A novel Feature of Interest (FoI) selection mechanism is developed in TSP-FCOS to help Transformer encoder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20] the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.']",intro_chunked,"Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like Transformer-based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETR’s optimization difficulty we conduct extensive experiments and find that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder-only version of DETR by removing the cross-attention module. We find that the encoder-only DETR yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETR’s Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only DETR with feature pyramids [18]. Specifically, we present TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN), which are inspired by a classic one-stage detector FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respectively. A novel Feature of Interest (FoI) selection mechanism is developed in TSP-FCOS to help Transformer encoder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20] the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy."
142,Zhiqing Sun,"[' The NLP community has witnessed a revolution of\npre-training self-supervised models. These models\nusually have hundreds of millions of parameters\n(Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)\n∗This work was done when the first author was an intern\nat Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP,\nBERT suffers from the heavy model size and high\nlatency, making it impractical for resource-limited\nmobile devices to deploy the power of BERT in\nmobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models\n(Turc et al., 2019; Tang et al., 2019; Sun et al.,\n2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a\nmodel that can be generically fine-tuned on different downstream NLP tasks as the original BERT\ndoes. In this paper, we propose MobileBERT to\nfill this gap. In practice, task-agnostic compression\nof BERT is desirable. Task-specific compression\nneeds to first fine-tune the original large BERT\nmodel into a task-specific teacher and then distill.', 'Such a process is much more complicated (Wu\net al., 2019) and costly than directly fine-tuning a\ntask-agnostic compact model. At first glance, it may seem straightforward to\nobtain a task-agnostic compact BERT. For example,\none may just take a narrower or shallower version\nof BERT, and train it until convergence by minimizing a convex combination of the prediction loss\nand distillation loss (Turc et al., 2019; Sun et al.,\n2019). Unfortunately, empirical results show that\nsuch a straightforward approach results in significant accuracy loss (Turc et al., 2019). This may not\nbe that surprising. It is well-known that shallow\nnetworks usually do not have enough representation power while narrow and deep networks are\ndifficult to train. Our MobileBERT is designed to be as deep as\nBERTLARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self-attentions and feed-forward networks (Figure 1). To train MobileBERT, a deep\nand thin model, we first train a specially designed\nteacher model, an inverted-bottleneck incorporated\nBERTLARGE model (IB-BERT). Then, we conduct\nknowledge transfer from IB-BERT to MobileBERT. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations1\nshow that MobileBERT\nis 4.3× smaller and 5.5× faster than BERTBASE,\nwhile it can still achieve competitive results on\nwell-known NLP benchmarks. On the natural language inference tasks of GLUE, MobileBERT can\nachieve a GLUE score of 77.7, which is only 0.6\nlower than BERTBASE, with a latency of 62 ms on\na Pixel 4 phone. On the SQuAD v1.1/v2.0 question\nanswering task, MobileBER obtains a dev F1 score\nof 90.3/80.2, which is even 1.5/2.1 higher than\nBERTBASE.']",intro_chunked," The NLP community has witnessed a revolution of
pre-training self-supervised models. These models
usually have hundreds of millions of parameters
(Peters et al., 2018; Radford et al., 2018; Devlin
et al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)
∗This work was done when the first author was an intern
at Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP,
BERT suffers from the heavy model size and high
latency, making it impractical for resource-limited
mobile devices to deploy the power of BERT in
mobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models
(Turc et al., 2019; Tang et al., 2019; Sun et al.,
2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a
model that can be generically fine-tuned on different downstream NLP tasks as the original BERT
does. In this paper, we propose MobileBERT to
fill this gap. In practice, task-agnostic compression
of BERT is desirable. Task-specific compression
needs to first fine-tune the original large BERT
model into a task-specific teacher and then distill."
143,Zhiqing Sun,"[' The NLP community has witnessed a revolution of\npre-training self-supervised models. These models\nusually have hundreds of millions of parameters\n(Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)\n∗This work was done when the first author was an intern\nat Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP,\nBERT suffers from the heavy model size and high\nlatency, making it impractical for resource-limited\nmobile devices to deploy the power of BERT in\nmobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models\n(Turc et al., 2019; Tang et al., 2019; Sun et al.,\n2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a\nmodel that can be generically fine-tuned on different downstream NLP tasks as the original BERT\ndoes. In this paper, we propose MobileBERT to\nfill this gap. In practice, task-agnostic compression\nof BERT is desirable. Task-specific compression\nneeds to first fine-tune the original large BERT\nmodel into a task-specific teacher and then distill.', 'Such a process is much more complicated (Wu\net al., 2019) and costly than directly fine-tuning a\ntask-agnostic compact model. At first glance, it may seem straightforward to\nobtain a task-agnostic compact BERT. For example,\none may just take a narrower or shallower version\nof BERT, and train it until convergence by minimizing a convex combination of the prediction loss\nand distillation loss (Turc et al., 2019; Sun et al.,\n2019). Unfortunately, empirical results show that\nsuch a straightforward approach results in significant accuracy loss (Turc et al., 2019). This may not\nbe that surprising. It is well-known that shallow\nnetworks usually do not have enough representation power while narrow and deep networks are\ndifficult to train. Our MobileBERT is designed to be as deep as\nBERTLARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self-attentions and feed-forward networks (Figure 1). To train MobileBERT, a deep\nand thin model, we first train a specially designed\nteacher model, an inverted-bottleneck incorporated\nBERTLARGE model (IB-BERT). Then, we conduct\nknowledge transfer from IB-BERT to MobileBERT. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations1\nshow that MobileBERT\nis 4.3× smaller and 5.5× faster than BERTBASE,\nwhile it can still achieve competitive results on\nwell-known NLP benchmarks. On the natural language inference tasks of GLUE, MobileBERT can\nachieve a GLUE score of 77.7, which is only 0.6\nlower than BERTBASE, with a latency of 62 ms on\na Pixel 4 phone. On the SQuAD v1.1/v2.0 question\nanswering task, MobileBER obtains a dev F1 score\nof 90.3/80.2, which is even 1.5/2.1 higher than\nBERTBASE.']",intro_chunked,"Such a process is much more complicated (Wu
et al., 2019) and costly than directly fine-tuning a
task-agnostic compact model. At first glance, it may seem straightforward to
obtain a task-agnostic compact BERT. For example,
one may just take a narrower or shallower version
of BERT, and train it until convergence by minimizing a convex combination of the prediction loss
and distillation loss (Turc et al., 2019; Sun et al.,
2019). Unfortunately, empirical results show that
such a straightforward approach results in significant accuracy loss (Turc et al., 2019). This may not
be that surprising. It is well-known that shallow
networks usually do not have enough representation power while narrow and deep networks are
difficult to train. Our MobileBERT is designed to be as deep as
BERTLARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self-attentions and feed-forward networks (Figure 1). To train MobileBERT, a deep
and thin model, we first train a specially designed
teacher model, an inverted-bottleneck incorporated
BERTLARGE model (IB-BERT). Then, we conduct
knowledge transfer from IB-BERT to MobileBERT. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations1
show that MobileBERT
is 4.3× smaller and 5.5× faster than BERTBASE,
while it can still achieve competitive results on
well-known NLP benchmarks. On the natural language inference tasks of GLUE, MobileBERT can
achieve a GLUE score of 77.7, which is only 0.6
lower than BERTBASE, with a latency of 62 ms on
a Pixel 4 phone. On the SQuAD v1.1/v2.0 question
answering task, MobileBER obtains a dev F1 score
of 90.3/80.2, which is even 1.5/2.1 higher than
BERTBASE."
144,Zhiqing Sun,"[' Real-world knowledge bases are usually expressed as multi-relational graphs, which are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. However, real-word knowledge bases are usually incomplete (Dong et al., 2014), which motivates the research of automatically predicting missing links. A popular approach for Knowledge Graph Completion (KGC) is to embed entities and relations into continuous vector or matrix space, and use a well-designed score function f (h, r, t) to measure the plausibility of the triplet (h, r, t). Most of the previous methods use translation distance based (Bordes et al., 2013; Wang et al., 2014; Xiao et al., 2016; Sun et al., 2019) and semantic matching based (Nickel and Tresp, 2013; Yang et al., 2014; Nickel et al., 2016; Trouillon et al., 2016; scoring functions which are easy to analyze. However, recently, a vast number of neural network-based methods have been proposed. They have complex score functions which utilize black-box neural networks including Convolutional Neural Networks (CNNs) (Dettmers et al., 2018; Nguyen et al., 2018), Recurrent Neural Networks (RNNs) (Lin et al., 2015; Wang et al., 2018), Graph Neural Networks (GNNs) (Schlichtkrull et al., 2017; Shang et al., 2019), and Capsule Networks (Nguyen et al., 2019).', 'While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re-examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.']",intro_chunked," Real-world knowledge bases are usually expressed as multi-relational graphs, which are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. However, real-word knowledge bases are usually incomplete (Dong et al., 2014), which motivates the research of automatically predicting missing links. A popular approach for Knowledge Graph Completion (KGC) is to embed entities and relations into continuous vector or matrix space, and use a well-designed score function f (h, r, t) to measure the plausibility of the triplet (h, r, t). Most of the previous methods use translation distance based (Bordes et al., 2013; Wang et al., 2014; Xiao et al., 2016; Sun et al., 2019) and semantic matching based (Nickel and Tresp, 2013; Yang et al., 2014; Nickel et al., 2016; Trouillon et al., 2016; scoring functions which are easy to analyze. However, recently, a vast number of neural network-based methods have been proposed. They have complex score functions which utilize black-box neural networks including Convolutional Neural Networks (CNNs) (Dettmers et al., 2018; Nguyen et al., 2018), Recurrent Neural Networks (RNNs) (Lin et al., 2015; Wang et al., 2018), Graph Neural Networks (GNNs) (Schlichtkrull et al., 2017; Shang et al., 2019), and Capsule Networks (Nguyen et al., 2019)."
145,Zhiqing Sun,"[' Real-world knowledge bases are usually expressed as multi-relational graphs, which are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. However, real-word knowledge bases are usually incomplete (Dong et al., 2014), which motivates the research of automatically predicting missing links. A popular approach for Knowledge Graph Completion (KGC) is to embed entities and relations into continuous vector or matrix space, and use a well-designed score function f (h, r, t) to measure the plausibility of the triplet (h, r, t). Most of the previous methods use translation distance based (Bordes et al., 2013; Wang et al., 2014; Xiao et al., 2016; Sun et al., 2019) and semantic matching based (Nickel and Tresp, 2013; Yang et al., 2014; Nickel et al., 2016; Trouillon et al., 2016; scoring functions which are easy to analyze. However, recently, a vast number of neural network-based methods have been proposed. They have complex score functions which utilize black-box neural networks including Convolutional Neural Networks (CNNs) (Dettmers et al., 2018; Nguyen et al., 2018), Recurrent Neural Networks (RNNs) (Lin et al., 2015; Wang et al., 2018), Graph Neural Networks (GNNs) (Schlichtkrull et al., 2017; Shang et al., 2019), and Capsule Networks (Nguyen et al., 2019).', 'While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re-examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.']",intro_chunked,"While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re-examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at."
146,Zhiqing Sun,"[' State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences.', 'Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model.', 'We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.']",intro_chunked," State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences."
147,Zhiqing Sun,"[' State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences.', 'Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model.', 'We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.']",intro_chunked,"Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model."
148,Zhiqing Sun,"[' State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences.', 'Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model.', 'We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.']",intro_chunked,"We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly."
149,Zhiqing Sun,"[' Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently.', 'Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.', 'However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts.']",intro_chunked," Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently."
150,Zhiqing Sun,"[' Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently.', 'Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.', 'However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts.']",intro_chunked,"Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations."
151,Zhiqing Sun,"[' Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently.', 'Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.', 'However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts.']",intro_chunked,"However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts."
152,Zhiqing Sun,"[' Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.', 'Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.', 'Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.']",intro_chunked," Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones."
153,Zhiqing Sun,"[' Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.', 'Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.', 'Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.']",intro_chunked,"Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted."
154,Zhiqing Sun,"[' Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.', 'Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.', 'Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.']",intro_chunked,"Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases."
155,Zhiqing Sun,"[' Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.', 'For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.', 'Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.', 'To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.']",intro_chunked," Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts."
156,Zhiqing Sun,"[' Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.', 'For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.', 'Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.', 'To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.']",intro_chunked,"For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding."
157,Zhiqing Sun,"[' Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.', 'For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.', 'Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.', 'To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.']",intro_chunked,"Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory."
158,Zhiqing Sun,"[' Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.', 'For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.', 'Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.', 'To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.']",intro_chunked,"To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks."
159,Zhiqing Sun,"[' Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013).', 'There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets.']",intro_chunked," Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013)."
160,Zhiqing Sun,"[' Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013).', 'There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets.']",intro_chunked,"There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets."
161,Timo Schick,"[' Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real-world uses of NLP to have only a small number of labeled examples, making few-shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text: T1: This was the best pizza I’ve ever had. T2: You can get better sushi for half the price. T3: Pizza was average. Not worth the price. Furthermore, imagine we are told that the labels of T1 and T2 are l and l′, respectively, and we are asked to infer the correct label for T3. Based only on these examples, this is impossible because plausible justifications can be found for both l and l′. However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l′ to T3.', 'This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, i.e., a textual explanation that helps us understand what the task is about. With the rise of pretrained language models (PLMs) such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the PLM predict continuations that solve the task (Radford et al., 2019; Puri and Catanzaro, 2019). So far, this idea has mostly been considered in zero-shot scenarios where no training data is available at all. In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few-shot settings: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that uses natural language patterns to reformulate input examples into cloze-style phrases. As illustrated in Figure 1, PET works in three steps: First, for each pattern a separate PLM is finetuned on a small training set T . The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft-labeled dataset. We also devise iPET, an iterative variant of PET in which this process is repeated with increasing training set sizes.On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, PET and iPET substantially outperform unsupervised approaches, supervised training and strong semi-supervised baselines.']",intro_chunked," Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real-world uses of NLP to have only a small number of labeled examples, making few-shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text: T1: This was the best pizza I’ve ever had. T2: You can get better sushi for half the price. T3: Pizza was average. Not worth the price. Furthermore, imagine we are told that the labels of T1 and T2 are l and l′, respectively, and we are asked to infer the correct label for T3. Based only on these examples, this is impossible because plausible justifications can be found for both l and l′. However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l′ to T3."
162,Timo Schick,"[' Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real-world uses of NLP to have only a small number of labeled examples, making few-shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text: T1: This was the best pizza I’ve ever had. T2: You can get better sushi for half the price. T3: Pizza was average. Not worth the price. Furthermore, imagine we are told that the labels of T1 and T2 are l and l′, respectively, and we are asked to infer the correct label for T3. Based only on these examples, this is impossible because plausible justifications can be found for both l and l′. However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l′ to T3.', 'This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, i.e., a textual explanation that helps us understand what the task is about. With the rise of pretrained language models (PLMs) such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the PLM predict continuations that solve the task (Radford et al., 2019; Puri and Catanzaro, 2019). So far, this idea has mostly been considered in zero-shot scenarios where no training data is available at all. In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few-shot settings: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that uses natural language patterns to reformulate input examples into cloze-style phrases. As illustrated in Figure 1, PET works in three steps: First, for each pattern a separate PLM is finetuned on a small training set T . The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft-labeled dataset. We also devise iPET, an iterative variant of PET in which this process is repeated with increasing training set sizes.On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, PET and iPET substantially outperform unsupervised approaches, supervised training and strong semi-supervised baselines.']",intro_chunked,"This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, i.e., a textual explanation that helps us understand what the task is about. With the rise of pretrained language models (PLMs) such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the PLM predict continuations that solve the task (Radford et al., 2019; Puri and Catanzaro, 2019). So far, this idea has mostly been considered in zero-shot scenarios where no training data is available at all. In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few-shot settings: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that uses natural language patterns to reformulate input examples into cloze-style phrases. As illustrated in Figure 1, PET works in three steps: First, for each pattern a separate PLM is finetuned on a small training set T . The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft-labeled dataset. We also devise iPET, an iterative variant of PET in which this process is repeated with increasing training set sizes.On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, PET and iPET substantially outperform unsupervised approaches, supervised training and strong semi-supervised baselines."
163,Timo Schick,"[' Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018; Devlin et al., 2019; Liuet al., 2019; Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model’s output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is __”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Schütze, 2021). Recently, Brown et al. (2020) introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as LM problems, GPT-3 achieves near state-of-the-art results for some SuperGLUE (Wang et al., 2019) tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed.', 'While being straightforward to use, this method has two major drawbacks: It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern-exploiting training (PET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way. In this work, we adapt PET for tasks that require predicting multiple tokens. We then show that incombination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET’s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET’s “green” properties, we see ourwork as an important contribution to an environmentally sound NLP.']",intro_chunked," Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018; Devlin et al., 2019; Liuet al., 2019; Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model’s output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is __”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Schütze, 2021). Recently, Brown et al. (2020) introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as LM problems, GPT-3 achieves near state-of-the-art results for some SuperGLUE (Wang et al., 2019) tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed."
164,Timo Schick,"[' Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018; Devlin et al., 2019; Liuet al., 2019; Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model’s output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is __”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Schütze, 2021). Recently, Brown et al. (2020) introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as LM problems, GPT-3 achieves near state-of-the-art results for some SuperGLUE (Wang et al., 2019) tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed.', 'While being straightforward to use, this method has two major drawbacks: It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern-exploiting training (PET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way. In this work, we adapt PET for tasks that require predicting multiple tokens. We then show that incombination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET’s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET’s “green” properties, we see ourwork as an important contribution to an environmentally sound NLP.']",intro_chunked,"While being straightforward to use, this method has two major drawbacks: It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern-exploiting training (PET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way. In this work, we adapt PET for tasks that require predicting multiple tokens. We then show that incombination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET’s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET’s “green” properties, we see ourwork as an important contribution to an environmentally sound NLP."
165,Timo Schick,"[' Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scaling. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022). A simple way to overcome these limitations of\ntoday’s language models is to give them the ability to use external tools such as search engines,\ncalculators, or calendars. However, existing approaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-specific settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.', 'Therefore, we propose Toolformer, a model that\nlearns to use tools in a novel way, which fulfills the\nfollowing desiderata:\n• The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations. This is important not only because of the costs associated\nwith such annotations, but also because what\nhumans find useful may be different from\nwhat a model finds useful. • The LM should not lose any of its generality\nand should be able to decide for itself when\nand how to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspecific tasks. Our approach for achieving these goals is based\non the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Schütze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\nfinetune the LM itself on the API calls that it considers useful.', 'As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a variety of tools, and to choose for themselves which\ntool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the first place. This ensures that the model does not lose any\nof its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) and']",intro_chunked," Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent
capabilities (Wei et al., 2022). However, all of
these models have several inherent limitations that
can at best be partially addressed by further scaling. These limitations include an inability to access
up-to-date information on recent events (Komeili
et al., 2022) and the related tendency to hallucinate
facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin
et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an
unawareness of the progression of time (Dhingra
et al., 2022). A simple way to overcome these limitations of
today’s language models is to give them the ability to use external tools such as search engines,
calculators, or calendars. However, existing approaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-specific settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs."
166,Timo Schick,"[' Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scaling. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022). A simple way to overcome these limitations of\ntoday’s language models is to give them the ability to use external tools such as search engines,\ncalculators, or calendars. However, existing approaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-specific settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.', 'Therefore, we propose Toolformer, a model that\nlearns to use tools in a novel way, which fulfills the\nfollowing desiderata:\n• The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations. This is important not only because of the costs associated\nwith such annotations, but also because what\nhumans find useful may be different from\nwhat a model finds useful. • The LM should not lose any of its generality\nand should be able to decide for itself when\nand how to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspecific tasks. Our approach for achieving these goals is based\non the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Schütze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\nfinetune the LM itself on the API calls that it considers useful.', 'As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a variety of tools, and to choose for themselves which\ntool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the first place. This ensures that the model does not lose any\nof its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) and']",intro_chunked,"Therefore, we propose Toolformer, a model that
learns to use tools in a novel way, which fulfills the
following desiderata:
• The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. This is important not only because of the costs associated
with such annotations, but also because what
humans find useful may be different from
what a model finds useful. • The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. In contrast to
existing approaches, this enables a much more
comprehensive use of tools that is not tied to
specific tasks. Our approach for achieving these goals is based
on the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate
entire datasets from scratch (Schick and Schütze,
2021b; Honovich et al., 2022; Wang et al., 2022):
Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
finetune the LM itself on the API calls that it considers useful."
167,Timo Schick,"[' Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scaling. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022). A simple way to overcome these limitations of\ntoday’s language models is to give them the ability to use external tools such as search engines,\ncalculators, or calendars. However, existing approaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-specific settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.', 'Therefore, we propose Toolformer, a model that\nlearns to use tools in a novel way, which fulfills the\nfollowing desiderata:\n• The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations. This is important not only because of the costs associated\nwith such annotations, but also because what\nhumans find useful may be different from\nwhat a model finds useful. • The LM should not lose any of its generality\nand should be able to decide for itself when\nand how to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspecific tasks. Our approach for achieving these goals is based\non the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Schütze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\nfinetune the LM itself on the API calls that it considers useful.', 'As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a variety of tools, and to choose for themselves which\ntool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the first place. This ensures that the model does not lose any\nof its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) and']",intro_chunked,"As illustrated in Figure 1, through
this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which
tool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset
that was used to pretrain a model in the first place. This ensures that the model does not lose any
of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after
learning to use tools, Toolformer, which is based
on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much
stronger zero-shot results, clearly outperforming a
much larger GPT-3 model (Brown et al., 2020) and"
168,Timo Schick,"[' Pretraining neural networks using a language modeling objective leads to large improvements across\na variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019). With model sizes continually increasing\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are\ntypically based on crawls from the internet that are\nonly filtered with some basic rules (Radford et al.,\n2019; Raffel et al., 2020). As a consequence, they\ncontain non-negligible amounts of text exhibiting\nbiases that are undesirable or outright harmful for\nmany potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such\ndata pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;\nBasta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned\nwords (Raffel et al., 2020) fall short of mitigating\nthis problem for at least two reasons. First, they do\nnot reliably keep language models from generating\nbiased text: Examples in Figure 1 show that biased text can easily be generated by using only words\nthat are, by themselves, completely unproblematic.', 'As many such words are important words of the\nEnglish vocabulary and thus needed for meaningful\ntext generation, they should not be included in a list\nof banned words. Secondly, banning words also\nprevents language models from gaining knowledge\nof topics related to the banned words, which may\nbe necessary for some applications.2\nIt is therefore inherently difficult to ban words without doing\nharm to a model’s capabilities. Building training datasets with more care and\ndeliberation, an alternative solution discussed by\nBender et al. (2021), is important, especially for\nimproving linguistic and cultural diversity in online\nand other forms of communication. However, for\nlarge language models that are available for common global languages, it is desirable to also have\nother mechanisms to address bias because dataset\ncuration and documentation is extremely resource\nintensive, given the amount of data required. It\ncan also necessitate building different training sets\nand, accordingly, training different models for each\ndesired behavior, which can result in high environmental impact (Strubell et al., 2019).', 'In this paper, we therefore propose an approach\nthat, instead of trusting that a model will implicitly learn desired behaviors from the training data,\nmakes explicit how we expect it to behave at test\ntime: If the model is told which biases are undesired – and it is able to discern their presence –,\nit should be able to avoid them even if they are\npresent in some of the texts it has been trained on. As it is a necessary condition for this approach, we\nfirst explore whether language models are able to\ndetect when their own outputs exhibit undesirable\nattributes, based only on their internal knowledge –\na process to which we refer as self-diagnosis. We\nthen investigate whether this ability can be used\nto perform self-debiasing, i.e., whether language\nmodels can use this knowledge to discard undesired\nbehaviors in a fully unsupervised fashion. To this\nend, we propose a decoding algorithm that reduces\nthe probability of a model producing biased text,\nrequiring nothing more than a textual description\nof the undesired behavior, which can be as simple\nas a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in\nparticular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find\nthat their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in\nNLP']",intro_chunked," Pretraining neural networks using a language modeling objective leads to large improvements across
a variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,
2019). With model sizes continually increasing
(Radford et al., 2019; Raffel et al., 2020; Brown
et al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are
typically based on crawls from the internet that are
only filtered with some basic rules (Radford et al.,
2019; Raffel et al., 2020). As a consequence, they
contain non-negligible amounts of text exhibiting
biases that are undesirable or outright harmful for
many potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such
data pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;
Basta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned
words (Raffel et al., 2020) fall short of mitigating
this problem for at least two reasons. First, they do
not reliably keep language models from generating
biased text: Examples in Figure 1 show that biased text can easily be generated by using only words
that are, by themselves, completely unproblematic."
169,Timo Schick,"[' Pretraining neural networks using a language modeling objective leads to large improvements across\na variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019). With model sizes continually increasing\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are\ntypically based on crawls from the internet that are\nonly filtered with some basic rules (Radford et al.,\n2019; Raffel et al., 2020). As a consequence, they\ncontain non-negligible amounts of text exhibiting\nbiases that are undesirable or outright harmful for\nmany potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such\ndata pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;\nBasta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned\nwords (Raffel et al., 2020) fall short of mitigating\nthis problem for at least two reasons. First, they do\nnot reliably keep language models from generating\nbiased text: Examples in Figure 1 show that biased text can easily be generated by using only words\nthat are, by themselves, completely unproblematic.', 'As many such words are important words of the\nEnglish vocabulary and thus needed for meaningful\ntext generation, they should not be included in a list\nof banned words. Secondly, banning words also\nprevents language models from gaining knowledge\nof topics related to the banned words, which may\nbe necessary for some applications.2\nIt is therefore inherently difficult to ban words without doing\nharm to a model’s capabilities. Building training datasets with more care and\ndeliberation, an alternative solution discussed by\nBender et al. (2021), is important, especially for\nimproving linguistic and cultural diversity in online\nand other forms of communication. However, for\nlarge language models that are available for common global languages, it is desirable to also have\nother mechanisms to address bias because dataset\ncuration and documentation is extremely resource\nintensive, given the amount of data required. It\ncan also necessitate building different training sets\nand, accordingly, training different models for each\ndesired behavior, which can result in high environmental impact (Strubell et al., 2019).', 'In this paper, we therefore propose an approach\nthat, instead of trusting that a model will implicitly learn desired behaviors from the training data,\nmakes explicit how we expect it to behave at test\ntime: If the model is told which biases are undesired – and it is able to discern their presence –,\nit should be able to avoid them even if they are\npresent in some of the texts it has been trained on. As it is a necessary condition for this approach, we\nfirst explore whether language models are able to\ndetect when their own outputs exhibit undesirable\nattributes, based only on their internal knowledge –\na process to which we refer as self-diagnosis. We\nthen investigate whether this ability can be used\nto perform self-debiasing, i.e., whether language\nmodels can use this knowledge to discard undesired\nbehaviors in a fully unsupervised fashion. To this\nend, we propose a decoding algorithm that reduces\nthe probability of a model producing biased text,\nrequiring nothing more than a textual description\nof the undesired behavior, which can be as simple\nas a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in\nparticular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find\nthat their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in\nNLP']",intro_chunked,"As many such words are important words of the
English vocabulary and thus needed for meaningful
text generation, they should not be included in a list
of banned words. Secondly, banning words also
prevents language models from gaining knowledge
of topics related to the banned words, which may
be necessary for some applications.2
It is therefore inherently difficult to ban words without doing
harm to a model’s capabilities. Building training datasets with more care and
deliberation, an alternative solution discussed by
Bender et al. (2021), is important, especially for
improving linguistic and cultural diversity in online
and other forms of communication. However, for
large language models that are available for common global languages, it is desirable to also have
other mechanisms to address bias because dataset
curation and documentation is extremely resource
intensive, given the amount of data required. It
can also necessitate building different training sets
and, accordingly, training different models for each
desired behavior, which can result in high environmental impact (Strubell et al., 2019)."
170,Timo Schick,"[' Pretraining neural networks using a language modeling objective leads to large improvements across\na variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019). With model sizes continually increasing\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are\ntypically based on crawls from the internet that are\nonly filtered with some basic rules (Radford et al.,\n2019; Raffel et al., 2020). As a consequence, they\ncontain non-negligible amounts of text exhibiting\nbiases that are undesirable or outright harmful for\nmany potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such\ndata pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;\nBasta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned\nwords (Raffel et al., 2020) fall short of mitigating\nthis problem for at least two reasons. First, they do\nnot reliably keep language models from generating\nbiased text: Examples in Figure 1 show that biased text can easily be generated by using only words\nthat are, by themselves, completely unproblematic.', 'As many such words are important words of the\nEnglish vocabulary and thus needed for meaningful\ntext generation, they should not be included in a list\nof banned words. Secondly, banning words also\nprevents language models from gaining knowledge\nof topics related to the banned words, which may\nbe necessary for some applications.2\nIt is therefore inherently difficult to ban words without doing\nharm to a model’s capabilities. Building training datasets with more care and\ndeliberation, an alternative solution discussed by\nBender et al. (2021), is important, especially for\nimproving linguistic and cultural diversity in online\nand other forms of communication. However, for\nlarge language models that are available for common global languages, it is desirable to also have\nother mechanisms to address bias because dataset\ncuration and documentation is extremely resource\nintensive, given the amount of data required. It\ncan also necessitate building different training sets\nand, accordingly, training different models for each\ndesired behavior, which can result in high environmental impact (Strubell et al., 2019).', 'In this paper, we therefore propose an approach\nthat, instead of trusting that a model will implicitly learn desired behaviors from the training data,\nmakes explicit how we expect it to behave at test\ntime: If the model is told which biases are undesired – and it is able to discern their presence –,\nit should be able to avoid them even if they are\npresent in some of the texts it has been trained on. As it is a necessary condition for this approach, we\nfirst explore whether language models are able to\ndetect when their own outputs exhibit undesirable\nattributes, based only on their internal knowledge –\na process to which we refer as self-diagnosis. We\nthen investigate whether this ability can be used\nto perform self-debiasing, i.e., whether language\nmodels can use this knowledge to discard undesired\nbehaviors in a fully unsupervised fashion. To this\nend, we propose a decoding algorithm that reduces\nthe probability of a model producing biased text,\nrequiring nothing more than a textual description\nof the undesired behavior, which can be as simple\nas a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in\nparticular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find\nthat their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in\nNLP']",intro_chunked,"In this paper, we therefore propose an approach
that, instead of trusting that a model will implicitly learn desired behaviors from the training data,
makes explicit how we expect it to behave at test
time: If the model is told which biases are undesired – and it is able to discern their presence –,
it should be able to avoid them even if they are
present in some of the texts it has been trained on. As it is a necessary condition for this approach, we
first explore whether language models are able to
detect when their own outputs exhibit undesirable
attributes, based only on their internal knowledge –
a process to which we refer as self-diagnosis. We
then investigate whether this ability can be used
to perform self-debiasing, i.e., whether language
models can use this knowledge to discard undesired
behaviors in a fully unsupervised fashion. To this
end, we propose a decoding algorithm that reduces
the probability of a model producing biased text,
requiring nothing more than a textual description
of the undesired behavior, which can be as simple
as a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in
particular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find
that their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in
NLP"
171,Timo Schick,"[' Pretraining language models on large corpora has led to improvements on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia), but learning to solve tasks from only a few examples remains a challenging problem. As small datasets are common for many realworld applications of NLP, solving this challenge is crucial to enable broad applicability. A promising direction for many tasks is to reformulate them (e.g., by appending an instruction such as “translate into French”) so that they can directly be solved by a pretrained language model (Radford et al., 2019; Schick and Schutze, 2020a; Brown et al., 2020). The key idea of P ¨ ET (Schick and Schutze, 2020a), one ¨ such approach aimed at text classification, is to rephrase each input as a cloze question for which the language model’s prediction can somehow be mapped to a label; an example is illustrated in Figure 1. While PET achieves remarkable results with little or no labeled training data, manually defining the required mapping between a language model’s predictions and labels is difficult as it requires both taskspecific knowledge and an understanding of the language model’s inner workings to identify words that it understands sufficiently well. In this work, we show how this mapping can be obtained automatically, removing the need for expert knowledge: We introduce PET with Automatic Labels (PETAL), a simple approach for identifying words that can serve as proxies for labels given small amounts of training data. At its core, our approach breaks the intractable problem of finding the mapping that maximizes the likelihood of the training data into several manageable subproblems. Integrating our approach into PET significantly outperforms regular supervised training and almost matches the performance of PET with a manually defined mapping']",intro_chunked," Pretraining language models on large corpora has led to improvements on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia), but learning to solve tasks from only a few examples remains a challenging problem. As small datasets are common for many realworld applications of NLP, solving this challenge is crucial to enable broad applicability. A promising direction for many tasks is to reformulate them (e.g., by appending an instruction such as “translate into French”) so that they can directly be solved by a pretrained language model (Radford et al., 2019; Schick and Schutze, 2020a; Brown et al., 2020). The key idea of P ¨ ET (Schick and Schutze, 2020a), one ¨ such approach aimed at text classification, is to rephrase each input as a cloze question for which the language model’s prediction can somehow be mapped to a label; an example is illustrated in Figure 1. While PET achieves remarkable results with little or no labeled training data, manually defining the required mapping between a language model’s predictions and labels is difficult as it requires both taskspecific knowledge and an understanding of the language model’s inner workings to identify words that it understands sufficiently well. In this work, we show how this mapping can be obtained automatically, removing the need for expert knowledge: We introduce PET with Automatic Labels (PETAL), a simple approach for identifying words that can serve as proxies for labels given small amounts of training data. At its core, our approach breaks the intractable problem of finding the mapping that maximizes the likelihood of the training data into several manageable subproblems. Integrating our approach into PET significantly outperforms regular supervised training and almost matches the performance of PET with a manually defined mapping"
172,Timo Schick,"[' While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise un-supervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. To alleviate both problems, we explore a novel approach to obtaining high-quality sentence embeddings: We mimic the creation of NLI datasets by human crowdworkers (Bowman et al., 2015; Williams et al., 2018), but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models.', 'Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like GPT-3 (Brown et al., 2020) without requiring any updates to their parameters. As illustrated in Figure 1, our approach is based on recent methods for providing instructions to PLMs (e.g., Radford et al., 2019; Brown et al., 2020; Schick and Schütze, 2020, 2021a). We use the self-debiasing approach of Schick et al. (2021) to ensure that each generated text pair is not only a good fit for a given similarity label, but also not a good fit for other labels. We refer to our method as Datasets from Instructions (DINO). In summary, our contributions are as follows: We introduce DINO, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions. We release STS- (read as “STS-Dino”), the first textual similarity dataset generated completely automatically, without any human annotation effort. We show that Sentence-RoBERTa (Reimers and Gurevych, 2019) trained on STS- out-performs strong baselines on several semantic textual similarity datasets.']",intro_chunked," While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise un-supervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. To alleviate both problems, we explore a novel approach to obtaining high-quality sentence embeddings: We mimic the creation of NLI datasets by human crowdworkers (Bowman et al., 2015; Williams et al., 2018), but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models."
173,Timo Schick,"[' While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise un-supervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. To alleviate both problems, we explore a novel approach to obtaining high-quality sentence embeddings: We mimic the creation of NLI datasets by human crowdworkers (Bowman et al., 2015; Williams et al., 2018), but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models.', 'Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like GPT-3 (Brown et al., 2020) without requiring any updates to their parameters. As illustrated in Figure 1, our approach is based on recent methods for providing instructions to PLMs (e.g., Radford et al., 2019; Brown et al., 2020; Schick and Schütze, 2020, 2021a). We use the self-debiasing approach of Schick et al. (2021) to ensure that each generated text pair is not only a good fit for a given similarity label, but also not a good fit for other labels. We refer to our method as Datasets from Instructions (DINO). In summary, our contributions are as follows: We introduce DINO, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions. We release STS- (read as “STS-Dino”), the first textual similarity dataset generated completely automatically, without any human annotation effort. We show that Sentence-RoBERTa (Reimers and Gurevych, 2019) trained on STS- out-performs strong baselines on several semantic textual similarity datasets.']",intro_chunked,"Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like GPT-3 (Brown et al., 2020) without requiring any updates to their parameters. As illustrated in Figure 1, our approach is based on recent methods for providing instructions to PLMs (e.g., Radford et al., 2019; Brown et al., 2020; Schick and Schütze, 2020, 2021a). We use the self-debiasing approach of Schick et al. (2021) to ensure that each generated text pair is not only a good fit for a given similarity label, but also not a good fit for other labels. We refer to our method as Datasets from Instructions (DINO). In summary, our contributions are as follows: We introduce DINO, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions. We release STS- (read as “STS-Dino”), the first textual similarity dataset generated completely automatically, without any human annotation effort. We show that Sentence-RoBERTa (Reimers and Gurevych, 2019) trained on STS- out-performs strong baselines on several semantic textual similarity datasets."
174,Timo Schick,"[' Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.', 'The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.', 'However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.']",intro_chunked," Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”."
175,Timo Schick,"[' Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.', 'The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.', 'However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.']",intro_chunked,"The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings."
176,Timo Schick,"[' Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.', 'The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.', 'However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.']",intro_chunked,"However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings."
177,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches often reduce the amount of training data required, they still do not perform well if only a handful of examples is available for the downstream task, which is common for real-word uses of NLP. In such few-shot settings, however, significant gains are possible by proceeding the other way around: Instead of making pretraining more similar to a downstream task, we can reformulate the task itself to make it more similar to the pretraining objective. For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2020a).', 'Besides making pretraining and finetuning more similar, this approach has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. Examples in Figure 1 demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task. While this idea even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020), it only unfolds its full potential when combined with gradient-based training on a handful of abeled examples (Schick and Schütze, 2020b). In particular, Pattern-Exploiting Training (PET) – an approach proposed by Schick and Schütze (2020a) that combines task descriptions with learning from examples – performs strongly for various few-shot text classification datasets. However, it can only be applied to classification tasks and is therefore not applicable to any problems that require the generation of text sequences. In this paper, we adapt PET to train generative models on text generation tasks. In particular, we propose several modifications that enable us to fine-tune a pretrained PEGASUS model (Zhang et al., 2020) with PET. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with our adapted version of PET clearly outperforms regular finetuning. In summary, our contributions are as follows: We describe how PET can be modified for finetuning generative language models for sequence generation tasks. We show that training PEGASUS with PET outperforms regular finetuning across a large set of tasks and training set sizes. We analyze the factors contributing to PET’s strong performance.']",intro_chunked," Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches often reduce the amount of training data required, they still do not perform well if only a handful of examples is available for the downstream task, which is common for real-word uses of NLP. In such few-shot settings, however, significant gains are possible by proceeding the other way around: Instead of making pretraining more similar to a downstream task, we can reformulate the task itself to make it more similar to the pretraining objective. For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2020a)."
178,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches often reduce the amount of training data required, they still do not perform well if only a handful of examples is available for the downstream task, which is common for real-word uses of NLP. In such few-shot settings, however, significant gains are possible by proceeding the other way around: Instead of making pretraining more similar to a downstream task, we can reformulate the task itself to make it more similar to the pretraining objective. For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2020a).', 'Besides making pretraining and finetuning more similar, this approach has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. Examples in Figure 1 demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task. While this idea even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020), it only unfolds its full potential when combined with gradient-based training on a handful of abeled examples (Schick and Schütze, 2020b). In particular, Pattern-Exploiting Training (PET) – an approach proposed by Schick and Schütze (2020a) that combines task descriptions with learning from examples – performs strongly for various few-shot text classification datasets. However, it can only be applied to classification tasks and is therefore not applicable to any problems that require the generation of text sequences. In this paper, we adapt PET to train generative models on text generation tasks. In particular, we propose several modifications that enable us to fine-tune a pretrained PEGASUS model (Zhang et al., 2020) with PET. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with our adapted version of PET clearly outperforms regular finetuning. In summary, our contributions are as follows: We describe how PET can be modified for finetuning generative language models for sequence generation tasks. We show that training PEGASUS with PET outperforms regular finetuning across a large set of tasks and training set sizes. We analyze the factors contributing to PET’s strong performance.']",intro_chunked,"Besides making pretraining and finetuning more similar, this approach has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. Examples in Figure 1 demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task. While this idea even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020), it only unfolds its full potential when combined with gradient-based training on a handful of abeled examples (Schick and Schütze, 2020b). In particular, Pattern-Exploiting Training (PET) – an approach proposed by Schick and Schütze (2020a) that combines task descriptions with learning from examples – performs strongly for various few-shot text classification datasets. However, it can only be applied to classification tasks and is therefore not applicable to any problems that require the generation of text sequences. In this paper, we adapt PET to train generative models on text generation tasks. In particular, we propose several modifications that enable us to fine-tune a pretrained PEGASUS model (Zhang et al., 2020) with PET. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with our adapted version of PET clearly outperforms regular finetuning. In summary, our contributions are as follows: We describe how PET can be modified for finetuning generative language models for sequence generation tasks. We show that training PEGASUS with PET outperforms regular finetuning across a large set of tasks and training set sizes. We analyze the factors contributing to PET’s strong performance."
179,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.', 'For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization.', 'We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.']",intro_chunked," Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective."
180,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.', 'For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization.', 'We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.']",intro_chunked,"For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization."
181,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.', 'For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization.', 'We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.']",intro_chunked,"We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components."
182,Timo Schick,"[' Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;\nRaffel et al., 2020; Brown et al., 2020; Rae et al.,\n2021; Zhang et al., 2022; Chowdhery et al., 2022,\ni.a.). However, the way these models operate—\nproducing outputs in a single pass from left to\nright—differs strongly from the iterative process\nby which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond\nthat, they are hard to control (Korbak et al., 2022)\nand verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the\nability to explain their intentions. All of this makes\nit very difficult for humans to collaborate with such\nmodels for writing coherent, factual texts. To address these shortcomings of existing LMs,\nwe propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process.', 'As illustrated in Figure 1, PEER operates in several steps\nthat aim to mirror the human writing process: For\na given text, either a user or the model itself can\nplan an action to be applied, for example by means\nof a natural language instruction. This plan is then\nrealized by an edit, which the model can explain\nboth in form of a textual comment and by pointing\nto references used; this is enabled by augmenting\neach input text with retrieved passages containing\npotentially relevant background information. We\nrepeat these steps until the text is in a satisfactory\nstate that does not require any further updates. This\niterative approach does not only enable the model\nto decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it\nalso allows humans to intervene at any time and\nsteer the model in the right direction, either by providing it with their own plans and comments or by\nmaking edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),\nwe use Wikipedia as our main source of edits and\nassociated comments, which we use as proxies for\nplans and explanations.', 'In contrast to this prior\nwork, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:\nIt should be capable of following human-written\ninstructions for updating texts in any domain. To\nachieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in\nsequential order, but also to infill various parts; for\nexample, given an edited text and a set of relevant\ndocuments, we teach it to produce the original version of this text before it was edited. This enables\nus to use self-training techniques (e.g., Yarowsky,\n1995; Sennrich et al., 2016; He et al., 2020a; Schick\nand Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We\nshow that this substantially improves PEER along\nseveral axes, including its ability to edit texts in any\ndomain, to understand human-written instructions,\nand to explain its actions. In summary, our contributions are as follows:\n• We introduce PEER, a collaborative language\nmodel trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities\nessential for collaborative writing. • For different tasks related to editing texts, we\nshow that PEER clearly outperforms various\nbaselines and analyze factors leading to its\nstrong performance. • To facilitate further research on collaborative\nLMs, we release a variety of PEER models as\nwell as the data and code used to train them.']",intro_chunked," Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;
Raffel et al., 2020; Brown et al., 2020; Rae et al.,
2021; Zhang et al., 2022; Chowdhery et al., 2022,
i.a.). However, the way these models operate—
producing outputs in a single pass from left to
right—differs strongly from the iterative process
by which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond
that, they are hard to control (Korbak et al., 2022)
and verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the
ability to explain their intentions. All of this makes
it very difficult for humans to collaborate with such
models for writing coherent, factual texts. To address these shortcomings of existing LMs,
we propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process."
183,Timo Schick,"[' Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;\nRaffel et al., 2020; Brown et al., 2020; Rae et al.,\n2021; Zhang et al., 2022; Chowdhery et al., 2022,\ni.a.). However, the way these models operate—\nproducing outputs in a single pass from left to\nright—differs strongly from the iterative process\nby which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond\nthat, they are hard to control (Korbak et al., 2022)\nand verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the\nability to explain their intentions. All of this makes\nit very difficult for humans to collaborate with such\nmodels for writing coherent, factual texts. To address these shortcomings of existing LMs,\nwe propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process.', 'As illustrated in Figure 1, PEER operates in several steps\nthat aim to mirror the human writing process: For\na given text, either a user or the model itself can\nplan an action to be applied, for example by means\nof a natural language instruction. This plan is then\nrealized by an edit, which the model can explain\nboth in form of a textual comment and by pointing\nto references used; this is enabled by augmenting\neach input text with retrieved passages containing\npotentially relevant background information. We\nrepeat these steps until the text is in a satisfactory\nstate that does not require any further updates. This\niterative approach does not only enable the model\nto decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it\nalso allows humans to intervene at any time and\nsteer the model in the right direction, either by providing it with their own plans and comments or by\nmaking edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),\nwe use Wikipedia as our main source of edits and\nassociated comments, which we use as proxies for\nplans and explanations.', 'In contrast to this prior\nwork, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:\nIt should be capable of following human-written\ninstructions for updating texts in any domain. To\nachieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in\nsequential order, but also to infill various parts; for\nexample, given an edited text and a set of relevant\ndocuments, we teach it to produce the original version of this text before it was edited. This enables\nus to use self-training techniques (e.g., Yarowsky,\n1995; Sennrich et al., 2016; He et al., 2020a; Schick\nand Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We\nshow that this substantially improves PEER along\nseveral axes, including its ability to edit texts in any\ndomain, to understand human-written instructions,\nand to explain its actions. In summary, our contributions are as follows:\n• We introduce PEER, a collaborative language\nmodel trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities\nessential for collaborative writing. • For different tasks related to editing texts, we\nshow that PEER clearly outperforms various\nbaselines and analyze factors leading to its\nstrong performance. • To facilitate further research on collaborative\nLMs, we release a variety of PEER models as\nwell as the data and code used to train them.']",intro_chunked,"As illustrated in Figure 1, PEER operates in several steps
that aim to mirror the human writing process: For
a given text, either a user or the model itself can
plan an action to be applied, for example by means
of a natural language instruction. This plan is then
realized by an edit, which the model can explain
both in form of a textual comment and by pointing
to references used; this is enabled by augmenting
each input text with retrieved passages containing
potentially relevant background information. We
repeat these steps until the text is in a satisfactory
state that does not require any further updates. This
iterative approach does not only enable the model
to decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it
also allows humans to intervene at any time and
steer the model in the right direction, either by providing it with their own plans and comments or by
making edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),
we use Wikipedia as our main source of edits and
associated comments, which we use as proxies for
plans and explanations."
184,Timo Schick,"[' Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;\nRaffel et al., 2020; Brown et al., 2020; Rae et al.,\n2021; Zhang et al., 2022; Chowdhery et al., 2022,\ni.a.). However, the way these models operate—\nproducing outputs in a single pass from left to\nright—differs strongly from the iterative process\nby which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond\nthat, they are hard to control (Korbak et al., 2022)\nand verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the\nability to explain their intentions. All of this makes\nit very difficult for humans to collaborate with such\nmodels for writing coherent, factual texts. To address these shortcomings of existing LMs,\nwe propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process.', 'As illustrated in Figure 1, PEER operates in several steps\nthat aim to mirror the human writing process: For\na given text, either a user or the model itself can\nplan an action to be applied, for example by means\nof a natural language instruction. This plan is then\nrealized by an edit, which the model can explain\nboth in form of a textual comment and by pointing\nto references used; this is enabled by augmenting\neach input text with retrieved passages containing\npotentially relevant background information. We\nrepeat these steps until the text is in a satisfactory\nstate that does not require any further updates. This\niterative approach does not only enable the model\nto decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it\nalso allows humans to intervene at any time and\nsteer the model in the right direction, either by providing it with their own plans and comments or by\nmaking edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),\nwe use Wikipedia as our main source of edits and\nassociated comments, which we use as proxies for\nplans and explanations.', 'In contrast to this prior\nwork, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:\nIt should be capable of following human-written\ninstructions for updating texts in any domain. To\nachieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in\nsequential order, but also to infill various parts; for\nexample, given an edited text and a set of relevant\ndocuments, we teach it to produce the original version of this text before it was edited. This enables\nus to use self-training techniques (e.g., Yarowsky,\n1995; Sennrich et al., 2016; He et al., 2020a; Schick\nand Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We\nshow that this substantially improves PEER along\nseveral axes, including its ability to edit texts in any\ndomain, to understand human-written instructions,\nand to explain its actions. In summary, our contributions are as follows:\n• We introduce PEER, a collaborative language\nmodel trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities\nessential for collaborative writing. • For different tasks related to editing texts, we\nshow that PEER clearly outperforms various\nbaselines and analyze factors leading to its\nstrong performance. • To facilitate further research on collaborative\nLMs, we release a variety of PEER models as\nwell as the data and code used to train them.']",intro_chunked,"In contrast to this prior
work, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:
It should be capable of following human-written
instructions for updating texts in any domain. To
achieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in
sequential order, but also to infill various parts; for
example, given an edited text and a set of relevant
documents, we teach it to produce the original version of this text before it was edited. This enables
us to use self-training techniques (e.g., Yarowsky,
1995; Sennrich et al., 2016; He et al., 2020a; Schick
and Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We
show that this substantially improves PEER along
several axes, including its ability to edit texts in any
domain, to understand human-written instructions,
and to explain its actions. In summary, our contributions are as follows:
• We introduce PEER, a collaborative language
model trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities
essential for collaborative writing. • For different tasks related to editing texts, we
show that PEER clearly outperforms various
baselines and analyze factors leading to its
strong performance. • To facilitate further research on collaborative
LMs, we release a variety of PEER models as
well as the data and code used to train them."
185,Timo Schick,"[' Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavi- cencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to finetune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g.', 'Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019) recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a word’s contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn – by way of self-attention – to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking (AM). Our contributions are as follows: (i) We introduce the attentive mimicking model. It produces high-quality embeddings for rare and medium-frequency words by attending to the most informative contexts. (ii) We propose a novel evaluation method based on VecMap (Artetxe et al., 2018) that allows us to easily evaluate the embedding quality of low- and medium-frequency words. (iii) We show that attentive mimicking improves word\nembeddings on various datasets.']",intro_chunked," Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavi- cencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to finetune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g."
186,Timo Schick,"[' Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavi- cencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to finetune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g.', 'Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019) recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a word’s contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn – by way of self-attention – to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking (AM). Our contributions are as follows: (i) We introduce the attentive mimicking model. It produces high-quality embeddings for rare and medium-frequency words by attending to the most informative contexts. (ii) We propose a novel evaluation method based on VecMap (Artetxe et al., 2018) that allows us to easily evaluate the embedding quality of low- and medium-frequency words. (iii) We show that attentive mimicking improves word\nembeddings on various datasets.']",intro_chunked,"Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019) recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a word’s contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn – by way of self-attention – to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking (AM). Our contributions are as follows: (i) We introduce the attentive mimicking model. It produces high-quality embeddings for rare and medium-frequency words by attending to the most informative contexts. (ii) We propose a novel evaluation method based on VecMap (Artetxe et al., 2018) that allows us to easily evaluate the embedding quality of low- and medium-frequency words. (iii) We show that attentive mimicking improves word
embeddings on various datasets."
187,Timo Schick,"[' As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations\nhave been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.', 'It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.', 'Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.']",intro_chunked," As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations
have been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information."
188,Timo Schick,"[' As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations\nhave been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.', 'It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.', 'Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.']",intro_chunked,"It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible."
189,Timo Schick,"[' As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations\nhave been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.', 'It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.', 'Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.']",intro_chunked,"Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT."
190,Timo Schick,"[' With pretrained language models (LMs) getting\never larger (Radford et al., 2019; Raffel et al., 2020;\nBrown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method\nfor few-shot text classification (e.g., Jiang et al.,\n2020; Schick and Schütze, 2021a,c; Brown et al.,\n2020; Wei et al., 2021; Sanh et al., 2021). The\nkey idea is to give an LM access to descriptive\nnames for all possible outputs and to short prompts\nexplaining the task to be solved. In settings where\nat most a few dozen examples are available, this\nsimple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;\nGao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong\nfew-shot performance of instruction-based approaches, arguing in particular that the considered\nsettings are often not true few-shot settings (Perez\net al., 2021; Logan IV et al., 2021) mainly for\ntwo reasons: For one, some approaches (e.g., Xie\net al., 2019; Zhang et al., 2020; Chen et al., 2020;\nTam et al., 2021) make use of large development\nsets to optimize hyperparameters. Beyond that, it\nis argued that manually designed instructions require manual tuning on development sets to achieve\nstrong performance (Perez et al., 2021; Logan IV\net al., 2021).', 'Indeed, performance can vary largely\n– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and\nSchütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh\net al., 2021). Even separate from this problem, the\nneed for human involvement is generally seen as a\nhuge drawback of manually designed instructions\n(Shin et al., 2020; Lester et al., 2021). Thus, several\nrecent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao\net al., 2021; Hambardzumyan et al., 2021; Li and\nLiang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when\ncorrectly configured, prompt-based approaches\nachieve strong performance even in true few-shot\nsettings and that there is no problem in using manually designed instructions per se. On the opposite,\nsuch instructions are often relatively easy to specify\nif one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific\nknowledge, and if properly used, they consistently\nimprove model performance in few-shot settings.', 'To provide empirical support for these claims, we\nrevisit PET (Schick and Schütze, 2021a) – a method\nfor combining instructions with example-based\nfinetuning whose key feature is that it allows users\nto specify multiple instructions for a single task\n– and thoroughly examine its performance with\nhuman-made instructions in true few-shot settings. In order to simulate a real-world scenario as best\nas possible, we proceed in two steps: First, we conduct an extensive study of PET using three English\nacademic datasets to analyze its ability to perform\ntrue few-shot learning in a controlled environment\nand to derive best practices regarding the choice of\ninstructions and other hyperparameters. We then\nput our findings to the test and evaluate PET on a\nlarge variety of different real-world tasks from the\nRAFT benchmark (Alex et al., 2021), for which no\nlabeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines\non this dataset and comes surprisingly close to the\nperformance of non-expert humans (see Figure 1),\ndemonstrating that instruction-based learning can\nsuccessfully be applied to real-world tasks in true\nfew-shot settings. In summary, the main contributions of this work\nare as follows:\n• We investigate the performance of PET for\nvarious models, tasks and training set sizes,\nits ability to cope with different instructions\nand its robustness to hyperparameter choices\nin true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant\nfor efficient classification in scenarios with\nmany different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in\ntrue few-shot settings.']",intro_chunked," With pretrained language models (LMs) getting
ever larger (Radford et al., 2019; Raffel et al., 2020;
Brown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method
for few-shot text classification (e.g., Jiang et al.,
2020; Schick and Schütze, 2021a,c; Brown et al.,
2020; Wei et al., 2021; Sanh et al., 2021). The
key idea is to give an LM access to descriptive
names for all possible outputs and to short prompts
explaining the task to be solved. In settings where
at most a few dozen examples are available, this
simple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;
Gao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong
few-shot performance of instruction-based approaches, arguing in particular that the considered
settings are often not true few-shot settings (Perez
et al., 2021; Logan IV et al., 2021) mainly for
two reasons: For one, some approaches (e.g., Xie
et al., 2019; Zhang et al., 2020; Chen et al., 2020;
Tam et al., 2021) make use of large development
sets to optimize hyperparameters. Beyond that, it
is argued that manually designed instructions require manual tuning on development sets to achieve
strong performance (Perez et al., 2021; Logan IV
et al., 2021)."
191,Timo Schick,"[' With pretrained language models (LMs) getting\never larger (Radford et al., 2019; Raffel et al., 2020;\nBrown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method\nfor few-shot text classification (e.g., Jiang et al.,\n2020; Schick and Schütze, 2021a,c; Brown et al.,\n2020; Wei et al., 2021; Sanh et al., 2021). The\nkey idea is to give an LM access to descriptive\nnames for all possible outputs and to short prompts\nexplaining the task to be solved. In settings where\nat most a few dozen examples are available, this\nsimple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;\nGao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong\nfew-shot performance of instruction-based approaches, arguing in particular that the considered\nsettings are often not true few-shot settings (Perez\net al., 2021; Logan IV et al., 2021) mainly for\ntwo reasons: For one, some approaches (e.g., Xie\net al., 2019; Zhang et al., 2020; Chen et al., 2020;\nTam et al., 2021) make use of large development\nsets to optimize hyperparameters. Beyond that, it\nis argued that manually designed instructions require manual tuning on development sets to achieve\nstrong performance (Perez et al., 2021; Logan IV\net al., 2021).', 'Indeed, performance can vary largely\n– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and\nSchütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh\net al., 2021). Even separate from this problem, the\nneed for human involvement is generally seen as a\nhuge drawback of manually designed instructions\n(Shin et al., 2020; Lester et al., 2021). Thus, several\nrecent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao\net al., 2021; Hambardzumyan et al., 2021; Li and\nLiang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when\ncorrectly configured, prompt-based approaches\nachieve strong performance even in true few-shot\nsettings and that there is no problem in using manually designed instructions per se. On the opposite,\nsuch instructions are often relatively easy to specify\nif one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific\nknowledge, and if properly used, they consistently\nimprove model performance in few-shot settings.', 'To provide empirical support for these claims, we\nrevisit PET (Schick and Schütze, 2021a) – a method\nfor combining instructions with example-based\nfinetuning whose key feature is that it allows users\nto specify multiple instructions for a single task\n– and thoroughly examine its performance with\nhuman-made instructions in true few-shot settings. In order to simulate a real-world scenario as best\nas possible, we proceed in two steps: First, we conduct an extensive study of PET using three English\nacademic datasets to analyze its ability to perform\ntrue few-shot learning in a controlled environment\nand to derive best practices regarding the choice of\ninstructions and other hyperparameters. We then\nput our findings to the test and evaluate PET on a\nlarge variety of different real-world tasks from the\nRAFT benchmark (Alex et al., 2021), for which no\nlabeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines\non this dataset and comes surprisingly close to the\nperformance of non-expert humans (see Figure 1),\ndemonstrating that instruction-based learning can\nsuccessfully be applied to real-world tasks in true\nfew-shot settings. In summary, the main contributions of this work\nare as follows:\n• We investigate the performance of PET for\nvarious models, tasks and training set sizes,\nits ability to cope with different instructions\nand its robustness to hyperparameter choices\nin true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant\nfor efficient classification in scenarios with\nmany different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in\ntrue few-shot settings.']",intro_chunked,"Indeed, performance can vary largely
– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and
Schütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh
et al., 2021). Even separate from this problem, the
need for human involvement is generally seen as a
huge drawback of manually designed instructions
(Shin et al., 2020; Lester et al., 2021). Thus, several
recent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao
et al., 2021; Hambardzumyan et al., 2021; Li and
Liang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when
correctly configured, prompt-based approaches
achieve strong performance even in true few-shot
settings and that there is no problem in using manually designed instructions per se. On the opposite,
such instructions are often relatively easy to specify
if one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific
knowledge, and if properly used, they consistently
improve model performance in few-shot settings."
192,Timo Schick,"[' With pretrained language models (LMs) getting\never larger (Radford et al., 2019; Raffel et al., 2020;\nBrown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method\nfor few-shot text classification (e.g., Jiang et al.,\n2020; Schick and Schütze, 2021a,c; Brown et al.,\n2020; Wei et al., 2021; Sanh et al., 2021). The\nkey idea is to give an LM access to descriptive\nnames for all possible outputs and to short prompts\nexplaining the task to be solved. In settings where\nat most a few dozen examples are available, this\nsimple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;\nGao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong\nfew-shot performance of instruction-based approaches, arguing in particular that the considered\nsettings are often not true few-shot settings (Perez\net al., 2021; Logan IV et al., 2021) mainly for\ntwo reasons: For one, some approaches (e.g., Xie\net al., 2019; Zhang et al., 2020; Chen et al., 2020;\nTam et al., 2021) make use of large development\nsets to optimize hyperparameters. Beyond that, it\nis argued that manually designed instructions require manual tuning on development sets to achieve\nstrong performance (Perez et al., 2021; Logan IV\net al., 2021).', 'Indeed, performance can vary largely\n– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and\nSchütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh\net al., 2021). Even separate from this problem, the\nneed for human involvement is generally seen as a\nhuge drawback of manually designed instructions\n(Shin et al., 2020; Lester et al., 2021). Thus, several\nrecent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao\net al., 2021; Hambardzumyan et al., 2021; Li and\nLiang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when\ncorrectly configured, prompt-based approaches\nachieve strong performance even in true few-shot\nsettings and that there is no problem in using manually designed instructions per se. On the opposite,\nsuch instructions are often relatively easy to specify\nif one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific\nknowledge, and if properly used, they consistently\nimprove model performance in few-shot settings.', 'To provide empirical support for these claims, we\nrevisit PET (Schick and Schütze, 2021a) – a method\nfor combining instructions with example-based\nfinetuning whose key feature is that it allows users\nto specify multiple instructions for a single task\n– and thoroughly examine its performance with\nhuman-made instructions in true few-shot settings. In order to simulate a real-world scenario as best\nas possible, we proceed in two steps: First, we conduct an extensive study of PET using three English\nacademic datasets to analyze its ability to perform\ntrue few-shot learning in a controlled environment\nand to derive best practices regarding the choice of\ninstructions and other hyperparameters. We then\nput our findings to the test and evaluate PET on a\nlarge variety of different real-world tasks from the\nRAFT benchmark (Alex et al., 2021), for which no\nlabeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines\non this dataset and comes surprisingly close to the\nperformance of non-expert humans (see Figure 1),\ndemonstrating that instruction-based learning can\nsuccessfully be applied to real-world tasks in true\nfew-shot settings. In summary, the main contributions of this work\nare as follows:\n• We investigate the performance of PET for\nvarious models, tasks and training set sizes,\nits ability to cope with different instructions\nand its robustness to hyperparameter choices\nin true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant\nfor efficient classification in scenarios with\nmany different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in\ntrue few-shot settings.']",intro_chunked,"To provide empirical support for these claims, we
revisit PET (Schick and Schütze, 2021a) – a method
for combining instructions with example-based
finetuning whose key feature is that it allows users
to specify multiple instructions for a single task
– and thoroughly examine its performance with
human-made instructions in true few-shot settings. In order to simulate a real-world scenario as best
as possible, we proceed in two steps: First, we conduct an extensive study of PET using three English
academic datasets to analyze its ability to perform
true few-shot learning in a controlled environment
and to derive best practices regarding the choice of
instructions and other hyperparameters. We then
put our findings to the test and evaluate PET on a
large variety of different real-world tasks from the
RAFT benchmark (Alex et al., 2021), for which no
labeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines
on this dataset and comes surprisingly close to the
performance of non-expert humans (see Figure 1),
demonstrating that instruction-based learning can
successfully be applied to real-world tasks in true
few-shot settings. In summary, the main contributions of this work
are as follows:
• We investigate the performance of PET for
various models, tasks and training set sizes,
its ability to cope with different instructions
and its robustness to hyperparameter choices
in true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant
for efficient classification in scenarios with
many different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in
true few-shot settings."
193,Timo Schick,"[' Distributed word representations (or embeddings) are a\nfoundational aspect of many natural language processing\nsystems; they have successfully been used for a wide vari-\nety of different tasks (Goldberg 2016). The idea behind em-\nbeddings is to assign to each word a low-dimensional, real-\nvalued vector representing its meaning. In particular, neural\nnetwork based approaches such as the skipgram and cbow\nmodels introduced by Mikolov et al. (2013) have gained in-\ncreasing popularity over the last few years. Despite their success, an important problem with current\napproaches to learning embeddings is that they require many\nobservations of a word for its embedding to become reliable;\nas a consequence, they struggle with small corpora and in-\nfrequent words (Ataman and Federico 2018). Furthermore,\nas models are typically trained with a fixed vocabulary, they\nlack the ability to assign vectors to novel, out-of-vocabulary\n(OOV) words once training is complete. n recent times, several ways have been proposed to over-\ncome these limitations and to extend word embedding mod-\nels with the ability to obtain representations of previously\nunseen words on the fly. These approaches can roughly be\ndivided into two directions: (i) the usage of subword in-\nformation, i.e., exploiting information that can be extracted\nfrom the surface-form of the word and (ii) the usage of\ncontext information.', 'The first direction aims to obtain good\nembeddings for novel words by looking at their characters\n(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-\ndou et al. 2013; Luong, Socher, and Manning 2013; Cot-\nterell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;\nSalle and Villavicencio 2018). Naturally, this direction is\nespecially well-suited for languages with rich morphology\n(Gerz et al. 2018). The second, context-based direction\ntries to infer embeddings for novel words from the words\nsurrounding them (Lazaridou, Marelli, and Baroni 2017;\nHerbelot and Baroni 2017; Khodak et al. 2018). Both direc-\ntions show promising results on various benchmarks. How-\never, for both purely surface-form-based and purely context-\nbased approaches, there are many cases in which they are\nhighly unlikely to succeed in obtaining meaningful embed-\ndings. As an example, suppose that we encounter the fol-\nlowing three words – highlighted in bold letters – as novel\nwords in the given contexts:\n(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons\nup the front. (3) Unlike the grapefruit, the pomelo has very little impor-\ntance in the marketplace.', 'In sentence (1), the context is of almost no help for de-\ntermining the meaning of the novel word, but we can de-\nduce its meaning without great difficulty from an analy-\nsis of the morphemes “un”, “employ” and “able”. For sen-\ntence (2), the reverse is true: While the novel word’s mor-\nphemes give no indication that it is a piece of clothing, this\ninformation can easily be derived from the context in which\nit occurs. Perhaps most interesting is sentence (3): Both the\nclose occurrence of the word “grapefruit” and the fact that\nthe novel word’s morphemes resemble words like “pome”,\n“pomegranate” and “melon” are indicative of the fact that it\nmay be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a\npretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an\napproach to cover a wide range of novel words, it is essen-\ntial to make use of all available information. In this work,\nwe therefore propose an architecture that, given a new word,\ncaptures both its subword structure and all available context\ninformation and combines them to obtain a high-quality em-\nbedding.', 'To this end, we first infer two distinct embeddings,\none incorporating the word’s inner structure and one captur-\ning its context, and then combine them into a unified word\nembedding. Importantly, both embeddings and their compo-\nsition function are learned jointly, allowing each embedding\nto rely on its counterpart whenever its available informa-\ntion is not sufficient. In a similar fashion to work by Pinter,\nGuthrie, and Eisenstein (2017) and Khodak et al. (2018),\nour approach is not trained from scratch, but instead makes\nuse of preexisting word embeddings and aims to reconstruct\nthese embeddings. This allows for a much faster learning\nprocess and enables us to easily combine our approach with\nany existing word embedding model, regardless of its inter-\nnal structure. Our approach is able to generate embeddings for OOV\nwords even from only a single observation with high accu-\nracy in many cases and outperforms previous work on the\nDefinitional Nonce dataset (Herbelot and Baroni 2017) and\nthe Contextual Rare Words dataset (Khodak et al. 2018). To\nthe best of our knowledge, this is the first work that jointly\nuses surface-form and context information to obtain repre-\nsentations for novel words. In summary, our contributions are as follows:\nWe propose a new model for learning embeddings for\nnovel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –\nwhich only used one of these two sources of information\n– by a large margin. Our model is designed in a way which allows it to eas-\nily be integrated into existing systems. It therefore has\nthe potential to enhance the capability of any NLP sys-\ntem that uses distributed word representations to handle\nnovel words.']",intro_chunked," Distributed word representations (or embeddings) are a
foundational aspect of many natural language processing
systems; they have successfully been used for a wide vari-
ety of different tasks (Goldberg 2016). The idea behind em-
beddings is to assign to each word a low-dimensional, real-
valued vector representing its meaning. In particular, neural
network based approaches such as the skipgram and cbow
models introduced by Mikolov et al. (2013) have gained in-
creasing popularity over the last few years. Despite their success, an important problem with current
approaches to learning embeddings is that they require many
observations of a word for its embedding to become reliable;
as a consequence, they struggle with small corpora and in-
frequent words (Ataman and Federico 2018). Furthermore,
as models are typically trained with a fixed vocabulary, they
lack the ability to assign vectors to novel, out-of-vocabulary
(OOV) words once training is complete. n recent times, several ways have been proposed to over-
come these limitations and to extend word embedding mod-
els with the ability to obtain representations of previously
unseen words on the fly. These approaches can roughly be
divided into two directions: (i) the usage of subword in-
formation, i.e., exploiting information that can be extracted
from the surface-form of the word and (ii) the usage of
context information."
194,Timo Schick,"[' Distributed word representations (or embeddings) are a\nfoundational aspect of many natural language processing\nsystems; they have successfully been used for a wide vari-\nety of different tasks (Goldberg 2016). The idea behind em-\nbeddings is to assign to each word a low-dimensional, real-\nvalued vector representing its meaning. In particular, neural\nnetwork based approaches such as the skipgram and cbow\nmodels introduced by Mikolov et al. (2013) have gained in-\ncreasing popularity over the last few years. Despite their success, an important problem with current\napproaches to learning embeddings is that they require many\nobservations of a word for its embedding to become reliable;\nas a consequence, they struggle with small corpora and in-\nfrequent words (Ataman and Federico 2018). Furthermore,\nas models are typically trained with a fixed vocabulary, they\nlack the ability to assign vectors to novel, out-of-vocabulary\n(OOV) words once training is complete. n recent times, several ways have been proposed to over-\ncome these limitations and to extend word embedding mod-\nels with the ability to obtain representations of previously\nunseen words on the fly. These approaches can roughly be\ndivided into two directions: (i) the usage of subword in-\nformation, i.e., exploiting information that can be extracted\nfrom the surface-form of the word and (ii) the usage of\ncontext information.', 'The first direction aims to obtain good\nembeddings for novel words by looking at their characters\n(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-\ndou et al. 2013; Luong, Socher, and Manning 2013; Cot-\nterell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;\nSalle and Villavicencio 2018). Naturally, this direction is\nespecially well-suited for languages with rich morphology\n(Gerz et al. 2018). The second, context-based direction\ntries to infer embeddings for novel words from the words\nsurrounding them (Lazaridou, Marelli, and Baroni 2017;\nHerbelot and Baroni 2017; Khodak et al. 2018). Both direc-\ntions show promising results on various benchmarks. How-\never, for both purely surface-form-based and purely context-\nbased approaches, there are many cases in which they are\nhighly unlikely to succeed in obtaining meaningful embed-\ndings. As an example, suppose that we encounter the fol-\nlowing three words – highlighted in bold letters – as novel\nwords in the given contexts:\n(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons\nup the front. (3) Unlike the grapefruit, the pomelo has very little impor-\ntance in the marketplace.', 'In sentence (1), the context is of almost no help for de-\ntermining the meaning of the novel word, but we can de-\nduce its meaning without great difficulty from an analy-\nsis of the morphemes “un”, “employ” and “able”. For sen-\ntence (2), the reverse is true: While the novel word’s mor-\nphemes give no indication that it is a piece of clothing, this\ninformation can easily be derived from the context in which\nit occurs. Perhaps most interesting is sentence (3): Both the\nclose occurrence of the word “grapefruit” and the fact that\nthe novel word’s morphemes resemble words like “pome”,\n“pomegranate” and “melon” are indicative of the fact that it\nmay be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a\npretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an\napproach to cover a wide range of novel words, it is essen-\ntial to make use of all available information. In this work,\nwe therefore propose an architecture that, given a new word,\ncaptures both its subword structure and all available context\ninformation and combines them to obtain a high-quality em-\nbedding.', 'To this end, we first infer two distinct embeddings,\none incorporating the word’s inner structure and one captur-\ning its context, and then combine them into a unified word\nembedding. Importantly, both embeddings and their compo-\nsition function are learned jointly, allowing each embedding\nto rely on its counterpart whenever its available informa-\ntion is not sufficient. In a similar fashion to work by Pinter,\nGuthrie, and Eisenstein (2017) and Khodak et al. (2018),\nour approach is not trained from scratch, but instead makes\nuse of preexisting word embeddings and aims to reconstruct\nthese embeddings. This allows for a much faster learning\nprocess and enables us to easily combine our approach with\nany existing word embedding model, regardless of its inter-\nnal structure. Our approach is able to generate embeddings for OOV\nwords even from only a single observation with high accu-\nracy in many cases and outperforms previous work on the\nDefinitional Nonce dataset (Herbelot and Baroni 2017) and\nthe Contextual Rare Words dataset (Khodak et al. 2018). To\nthe best of our knowledge, this is the first work that jointly\nuses surface-form and context information to obtain repre-\nsentations for novel words. In summary, our contributions are as follows:\nWe propose a new model for learning embeddings for\nnovel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –\nwhich only used one of these two sources of information\n– by a large margin. Our model is designed in a way which allows it to eas-\nily be integrated into existing systems. It therefore has\nthe potential to enhance the capability of any NLP sys-\ntem that uses distributed word representations to handle\nnovel words.']",intro_chunked,"The first direction aims to obtain good
embeddings for novel words by looking at their characters
(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-
dou et al. 2013; Luong, Socher, and Manning 2013; Cot-
terell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;
Salle and Villavicencio 2018). Naturally, this direction is
especially well-suited for languages with rich morphology
(Gerz et al. 2018). The second, context-based direction
tries to infer embeddings for novel words from the words
surrounding them (Lazaridou, Marelli, and Baroni 2017;
Herbelot and Baroni 2017; Khodak et al. 2018). Both direc-
tions show promising results on various benchmarks. How-
ever, for both purely surface-form-based and purely context-
based approaches, there are many cases in which they are
highly unlikely to succeed in obtaining meaningful embed-
dings. As an example, suppose that we encounter the fol-
lowing three words – highlighted in bold letters – as novel
words in the given contexts:
(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons
up the front. (3) Unlike the grapefruit, the pomelo has very little impor-
tance in the marketplace."
195,Timo Schick,"[' Distributed word representations (or embeddings) are a\nfoundational aspect of many natural language processing\nsystems; they have successfully been used for a wide vari-\nety of different tasks (Goldberg 2016). The idea behind em-\nbeddings is to assign to each word a low-dimensional, real-\nvalued vector representing its meaning. In particular, neural\nnetwork based approaches such as the skipgram and cbow\nmodels introduced by Mikolov et al. (2013) have gained in-\ncreasing popularity over the last few years. Despite their success, an important problem with current\napproaches to learning embeddings is that they require many\nobservations of a word for its embedding to become reliable;\nas a consequence, they struggle with small corpora and in-\nfrequent words (Ataman and Federico 2018). Furthermore,\nas models are typically trained with a fixed vocabulary, they\nlack the ability to assign vectors to novel, out-of-vocabulary\n(OOV) words once training is complete. n recent times, several ways have been proposed to over-\ncome these limitations and to extend word embedding mod-\nels with the ability to obtain representations of previously\nunseen words on the fly. These approaches can roughly be\ndivided into two directions: (i) the usage of subword in-\nformation, i.e., exploiting information that can be extracted\nfrom the surface-form of the word and (ii) the usage of\ncontext information.', 'The first direction aims to obtain good\nembeddings for novel words by looking at their characters\n(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-\ndou et al. 2013; Luong, Socher, and Manning 2013; Cot-\nterell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;\nSalle and Villavicencio 2018). Naturally, this direction is\nespecially well-suited for languages with rich morphology\n(Gerz et al. 2018). The second, context-based direction\ntries to infer embeddings for novel words from the words\nsurrounding them (Lazaridou, Marelli, and Baroni 2017;\nHerbelot and Baroni 2017; Khodak et al. 2018). Both direc-\ntions show promising results on various benchmarks. How-\never, for both purely surface-form-based and purely context-\nbased approaches, there are many cases in which they are\nhighly unlikely to succeed in obtaining meaningful embed-\ndings. As an example, suppose that we encounter the fol-\nlowing three words – highlighted in bold letters – as novel\nwords in the given contexts:\n(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons\nup the front. (3) Unlike the grapefruit, the pomelo has very little impor-\ntance in the marketplace.', 'In sentence (1), the context is of almost no help for de-\ntermining the meaning of the novel word, but we can de-\nduce its meaning without great difficulty from an analy-\nsis of the morphemes “un”, “employ” and “able”. For sen-\ntence (2), the reverse is true: While the novel word’s mor-\nphemes give no indication that it is a piece of clothing, this\ninformation can easily be derived from the context in which\nit occurs. Perhaps most interesting is sentence (3): Both the\nclose occurrence of the word “grapefruit” and the fact that\nthe novel word’s morphemes resemble words like “pome”,\n“pomegranate” and “melon” are indicative of the fact that it\nmay be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a\npretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an\napproach to cover a wide range of novel words, it is essen-\ntial to make use of all available information. In this work,\nwe therefore propose an architecture that, given a new word,\ncaptures both its subword structure and all available context\ninformation and combines them to obtain a high-quality em-\nbedding.', 'To this end, we first infer two distinct embeddings,\none incorporating the word’s inner structure and one captur-\ning its context, and then combine them into a unified word\nembedding. Importantly, both embeddings and their compo-\nsition function are learned jointly, allowing each embedding\nto rely on its counterpart whenever its available informa-\ntion is not sufficient. In a similar fashion to work by Pinter,\nGuthrie, and Eisenstein (2017) and Khodak et al. (2018),\nour approach is not trained from scratch, but instead makes\nuse of preexisting word embeddings and aims to reconstruct\nthese embeddings. This allows for a much faster learning\nprocess and enables us to easily combine our approach with\nany existing word embedding model, regardless of its inter-\nnal structure. Our approach is able to generate embeddings for OOV\nwords even from only a single observation with high accu-\nracy in many cases and outperforms previous work on the\nDefinitional Nonce dataset (Herbelot and Baroni 2017) and\nthe Contextual Rare Words dataset (Khodak et al. 2018). To\nthe best of our knowledge, this is the first work that jointly\nuses surface-form and context information to obtain repre-\nsentations for novel words. In summary, our contributions are as follows:\nWe propose a new model for learning embeddings for\nnovel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –\nwhich only used one of these two sources of information\n– by a large margin. Our model is designed in a way which allows it to eas-\nily be integrated into existing systems. It therefore has\nthe potential to enhance the capability of any NLP sys-\ntem that uses distributed word representations to handle\nnovel words.']",intro_chunked,"In sentence (1), the context is of almost no help for de-
termining the meaning of the novel word, but we can de-
duce its meaning without great difficulty from an analy-
sis of the morphemes “un”, “employ” and “able”. For sen-
tence (2), the reverse is true: While the novel word’s mor-
phemes give no indication that it is a piece of clothing, this
information can easily be derived from the context in which
it occurs. Perhaps most interesting is sentence (3): Both the
close occurrence of the word “grapefruit” and the fact that
the novel word’s morphemes resemble words like “pome”,
“pomegranate” and “melon” are indicative of the fact that it
may be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a
pretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an
approach to cover a wide range of novel words, it is essen-
tial to make use of all available information. In this work,
we therefore propose an architecture that, given a new word,
captures both its subword structure and all available context
information and combines them to obtain a high-quality em-
bedding."
196,Timo Schick,"[' Distributed word representations (or embeddings) are a\nfoundational aspect of many natural language processing\nsystems; they have successfully been used for a wide vari-\nety of different tasks (Goldberg 2016). The idea behind em-\nbeddings is to assign to each word a low-dimensional, real-\nvalued vector representing its meaning. In particular, neural\nnetwork based approaches such as the skipgram and cbow\nmodels introduced by Mikolov et al. (2013) have gained in-\ncreasing popularity over the last few years. Despite their success, an important problem with current\napproaches to learning embeddings is that they require many\nobservations of a word for its embedding to become reliable;\nas a consequence, they struggle with small corpora and in-\nfrequent words (Ataman and Federico 2018). Furthermore,\nas models are typically trained with a fixed vocabulary, they\nlack the ability to assign vectors to novel, out-of-vocabulary\n(OOV) words once training is complete. n recent times, several ways have been proposed to over-\ncome these limitations and to extend word embedding mod-\nels with the ability to obtain representations of previously\nunseen words on the fly. These approaches can roughly be\ndivided into two directions: (i) the usage of subword in-\nformation, i.e., exploiting information that can be extracted\nfrom the surface-form of the word and (ii) the usage of\ncontext information.', 'The first direction aims to obtain good\nembeddings for novel words by looking at their characters\n(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-\ndou et al. 2013; Luong, Socher, and Manning 2013; Cot-\nterell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;\nSalle and Villavicencio 2018). Naturally, this direction is\nespecially well-suited for languages with rich morphology\n(Gerz et al. 2018). The second, context-based direction\ntries to infer embeddings for novel words from the words\nsurrounding them (Lazaridou, Marelli, and Baroni 2017;\nHerbelot and Baroni 2017; Khodak et al. 2018). Both direc-\ntions show promising results on various benchmarks. How-\never, for both purely surface-form-based and purely context-\nbased approaches, there are many cases in which they are\nhighly unlikely to succeed in obtaining meaningful embed-\ndings. As an example, suppose that we encounter the fol-\nlowing three words – highlighted in bold letters – as novel\nwords in the given contexts:\n(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons\nup the front. (3) Unlike the grapefruit, the pomelo has very little impor-\ntance in the marketplace.', 'In sentence (1), the context is of almost no help for de-\ntermining the meaning of the novel word, but we can de-\nduce its meaning without great difficulty from an analy-\nsis of the morphemes “un”, “employ” and “able”. For sen-\ntence (2), the reverse is true: While the novel word’s mor-\nphemes give no indication that it is a piece of clothing, this\ninformation can easily be derived from the context in which\nit occurs. Perhaps most interesting is sentence (3): Both the\nclose occurrence of the word “grapefruit” and the fact that\nthe novel word’s morphemes resemble words like “pome”,\n“pomegranate” and “melon” are indicative of the fact that it\nmay be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a\npretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an\napproach to cover a wide range of novel words, it is essen-\ntial to make use of all available information. In this work,\nwe therefore propose an architecture that, given a new word,\ncaptures both its subword structure and all available context\ninformation and combines them to obtain a high-quality em-\nbedding.', 'To this end, we first infer two distinct embeddings,\none incorporating the word’s inner structure and one captur-\ning its context, and then combine them into a unified word\nembedding. Importantly, both embeddings and their compo-\nsition function are learned jointly, allowing each embedding\nto rely on its counterpart whenever its available informa-\ntion is not sufficient. In a similar fashion to work by Pinter,\nGuthrie, and Eisenstein (2017) and Khodak et al. (2018),\nour approach is not trained from scratch, but instead makes\nuse of preexisting word embeddings and aims to reconstruct\nthese embeddings. This allows for a much faster learning\nprocess and enables us to easily combine our approach with\nany existing word embedding model, regardless of its inter-\nnal structure. Our approach is able to generate embeddings for OOV\nwords even from only a single observation with high accu-\nracy in many cases and outperforms previous work on the\nDefinitional Nonce dataset (Herbelot and Baroni 2017) and\nthe Contextual Rare Words dataset (Khodak et al. 2018). To\nthe best of our knowledge, this is the first work that jointly\nuses surface-form and context information to obtain repre-\nsentations for novel words. In summary, our contributions are as follows:\nWe propose a new model for learning embeddings for\nnovel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –\nwhich only used one of these two sources of information\n– by a large margin. Our model is designed in a way which allows it to eas-\nily be integrated into existing systems. It therefore has\nthe potential to enhance the capability of any NLP sys-\ntem that uses distributed word representations to handle\nnovel words.']",intro_chunked,"To this end, we first infer two distinct embeddings,
one incorporating the word’s inner structure and one captur-
ing its context, and then combine them into a unified word
embedding. Importantly, both embeddings and their compo-
sition function are learned jointly, allowing each embedding
to rely on its counterpart whenever its available informa-
tion is not sufficient. In a similar fashion to work by Pinter,
Guthrie, and Eisenstein (2017) and Khodak et al. (2018),
our approach is not trained from scratch, but instead makes
use of preexisting word embeddings and aims to reconstruct
these embeddings. This allows for a much faster learning
process and enables us to easily combine our approach with
any existing word embedding model, regardless of its inter-
nal structure. Our approach is able to generate embeddings for OOV
words even from only a single observation with high accu-
racy in many cases and outperforms previous work on the
Definitional Nonce dataset (Herbelot and Baroni 2017) and
the Contextual Rare Words dataset (Khodak et al. 2018). To
the best of our knowledge, this is the first work that jointly
uses surface-form and context information to obtain repre-
sentations for novel words. In summary, our contributions are as follows:
We propose a new model for learning embeddings for
novel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –
which only used one of these two sources of information
– by a large margin. Our model is designed in a way which allows it to eas-
ily be integrated into existing systems. It therefore has
the potential to enhance the capability of any NLP sys-
tem that uses distributed word representations to handle
novel words."
197,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked," Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph."
198,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;
Konstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see
Nivre, 2008)."
199,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input."
200,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient."
201,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7."
0,GPT-3.5,"[' AutoMix emerges as a promising solution in the realm of cloud-based LLMs, providing a strategic means to balance computational cost and performance. By leveraging a few-shot self-verification mechanism, the system accurately gauges the reliability of its outputs, eliminating the need for extensive training. The incorporation of a meta verifier further enhances the precision of these assessments, addressing the inherent noise in the verification process. Experimental results, conducted on LLAMA2-13/70B across context-grounded reasoning datasets, affirm the effectiveness of AutoMix. Notably, the approach surpasses established baselines, showcasing a substantial up to 89% improvement in the incremental benefit per cost. AutoMix stands as a valuable contribution in the pursuit of optimizing the utilization of cloud-based LLMs, promising a more cost-effective and performance-driven approach in natural language processing tasks.']",conclusion_chunked," AutoMix emerges as a promising solution in the realm of cloud-based LLMs, providing a strategic means to balance computational cost and performance. By leveraging a few-shot self-verification mechanism, the system accurately gauges the reliability of its outputs, eliminating the need for extensive training. The incorporation of a meta verifier further enhances the precision of these assessments, addressing the inherent noise in the verification process. Experimental results, conducted on LLAMA2-13/70B across context-grounded reasoning datasets, affirm the effectiveness of AutoMix. Notably, the approach surpasses established baselines, showcasing a substantial up to 89% improvement in the incremental benefit per cost. AutoMix stands as a valuable contribution in the pursuit of optimizing the utilization of cloud-based LLMs, promising a more cost-effective and performance-driven approach in natural language processing tasks."
1,GPT-3.5,"[' In this study, we introduced SELF-REFINE, a novel approach to enhance the outputs of Large Language Models (LLMs) through an iterative feedback and refinement process. By mimicking the iterative refinement process observed in human writing, SELF-REFINE utilizes a single LLM as the generator, refiner, and feedback provider, eliminating the need for additional training or specialized data. Across seven diverse tasks, ranging from dialog response generation to mathematical reasoning, we demonstrated the effectiveness of SELF-REFINE using state-of-the-art LLMs like GPT-3.5 and GPT-4. Our results consistently revealed that outputs generated with SELF-REFINE outperformed those from conventional one-step generation, with an average improvement of approximately 20% in task performance according to both human evaluations and automatic metrics. This research highlights the potential for further improving the capabilities of even the most advanced LLMs at test-time through the adoption of a simple and standalone approach like SELF-REFINE.']",conclusion_chunked," In this study, we introduced SELF-REFINE, a novel approach to enhance the outputs of Large Language Models (LLMs) through an iterative feedback and refinement process. By mimicking the iterative refinement process observed in human writing, SELF-REFINE utilizes a single LLM as the generator, refiner, and feedback provider, eliminating the need for additional training or specialized data. Across seven diverse tasks, ranging from dialog response generation to mathematical reasoning, we demonstrated the effectiveness of SELF-REFINE using state-of-the-art LLMs like GPT-3.5 and GPT-4. Our results consistently revealed that outputs generated with SELF-REFINE outperformed those from conventional one-step generation, with an average improvement of approximately 20% in task performance according to both human evaluations and automatic metrics. This research highlights the potential for further improving the capabilities of even the most advanced LLMs at test-time through the adoption of a simple and standalone approach like SELF-REFINE."
2,GPT-3.5,"[' In this work, we introduced FLOWGEN, a graph generation model that embraces the dual-process theory of mind, allowing for adaptive responses to varying task complexities. FLOWGEN employs either a FAST (weaker) or SLOW (stronger) module, each characterized by identical architectures but differing in parameter count and generative power. Our experiments on real-world graphs underscore the effectiveness of FLOWGEN, showcasing its ability to generate graphs akin to those produced by a single large model. Notably, FLOWGEN achieves this while demonstrating a speed improvement of up to 2x. This research contributes to bridging the gap between machine learning models and human cognitive processes, offering a promising avenue for developing more adaptive and efficient graph generation systems.']",conclusion_chunked," In this work, we introduced FLOWGEN, a graph generation model that embraces the dual-process theory of mind, allowing for adaptive responses to varying task complexities. FLOWGEN employs either a FAST (weaker) or SLOW (stronger) module, each characterized by identical architectures but differing in parameter count and generative power. Our experiments on real-world graphs underscore the effectiveness of FLOWGEN, showcasing its ability to generate graphs akin to those produced by a single large model. Notably, FLOWGEN achieves this while demonstrating a speed improvement of up to 2x. This research contributes to bridging the gap between machine learning models and human cognitive processes, offering a promising avenue for developing more adaptive and efficient graph generation systems."
3,GPT-3.5,"[' In this work, we introduced a method for curating high-quality comparable training data specifically designed for low-resource languages, utilizing monolingual annotators. The strategic use of carefully chosen images as linguistic pivots between source and target languages proved effective, as evidenced by human evaluations on the English-Hindi comparable corpora. Our method demonstrated an impressive 81.1% acceptability rate for translation pairs, with a minimal 2.47% identified as non-translations. To underscore the practical utility of our curated dataset, we conducted experiments on two downstream tasksâ€”machine translation and dictionary extractionâ€”revealing promising outcomes. The availability of all code and data at https://github.com/madaan/PML4DC-Comparable-Data-Collection emphasizes the transparency and reproducibility of our approach, contributing a valuable resource for researchers in low-resource language domains.']",conclusion_chunked," In this work, we introduced a method for curating high-quality comparable training data specifically designed for low-resource languages, utilizing monolingual annotators. The strategic use of carefully chosen images as linguistic pivots between source and target languages proved effective, as evidenced by human evaluations on the English-Hindi comparable corpora. Our method demonstrated an impressive 81.1% acceptability rate for translation pairs, with a minimal 2.47% identified as non-translations. To underscore the practical utility of our curated dataset, we conducted experiments on two downstream tasksâ€”machine translation and dictionary extractionâ€”revealing promising outcomes. The availability of all code and data at https://github.com/madaan/PML4DC-Comparable-Data-Collection emphasizes the transparency and reproducibility of our approach, contributing a valuable resource for researchers in low-resource language domains."
4,GPT-3.5,"[' In conclusion, this paper introduces and addresses the novel task of politeness transfer, providing a substantial dataset and a robust tag-and-generate pipeline to facilitate research in this burgeoning field of natural language processing. Our model not only outperforms state-of-the-art methods on automatic metrics related to content preservation but also demonstrates superior performance in human evaluations, showcasing its effectiveness in achieving grammaticality, meaning preservation, and transfer accuracy across a diverse set of style transfer tasks. By making our code and dataset publicly available, we encourage collaboration and invite the community to build upon our work, fostering advancements in politeness transfer and broader style transfer applications. The presented contributions aim to propel the field forward, offering valuable insights and tools for researchers and practitioners alike.']",conclusion_chunked," In conclusion, this paper introduces and addresses the novel task of politeness transfer, providing a substantial dataset and a robust tag-and-generate pipeline to facilitate research in this burgeoning field of natural language processing. Our model not only outperforms state-of-the-art methods on automatic metrics related to content preservation but also demonstrates superior performance in human evaluations, showcasing its effectiveness in achieving grammaticality, meaning preservation, and transfer accuracy across a diverse set of style transfer tasks. By making our code and dataset publicly available, we encourage collaboration and invite the community to build upon our work, fostering advancements in politeness transfer and broader style transfer applications. The presented contributions aim to propel the field forward, offering valuable insights and tools for researchers and practitioners alike."
5,GPT-3.5,"[' In conclusion, EIGEN stands as a robust and effective method for event influence generation, showcasing advancements in both automated evaluation metrics and human judgment. The introduced dataset serves as a valuable resource for future research in this domain. The demonstrated enhancement of performance on the WIQA benchmark, especially for questions involving background knowledge and multi-hop reasoning, underlines the practical utility of EIGEN. As the understanding of event influences plays a pivotal role in various applications, ranging from natural language understanding to decision support systems, EIGEN contributes significantly to advancing the state-of-the-art in this field.']",conclusion_chunked," In conclusion, EIGEN stands as a robust and effective method for event influence generation, showcasing advancements in both automated evaluation metrics and human judgment. The introduced dataset serves as a valuable resource for future research in this domain. The demonstrated enhancement of performance on the WIQA benchmark, especially for questions involving background knowledge and multi-hop reasoning, underlines the practical utility of EIGEN. As the understanding of event influences plays a pivotal role in various applications, ranging from natural language understanding to decision support systems, EIGEN contributes significantly to advancing the state-of-the-art in this field."
6,GPT-3.5,"[' In this study, we have presented a novel approach to building and optimizing deeper transformer networks for image classification, challenging the traditional dominance of convolutional neural networks. Through careful exploration of architectural modifications and optimization strategies, we have achieved substantial improvements in the accuracy of deep transformers. Our proposed model not only prevents early saturation issues but also sets a new standard in image classification, reaching 86.5% top-1 accuracy on ImageNet without external data. This result establishes the model as the current state-of-the-art solution, surpassing existing benchmarks in terms of both computational efficiency (FLOPs) and model parameters. Additionally, our model exhibits robust performance on ImageNet with Reassessed labels and ImageNet-V2/match frequency, even in the absence of additional training data. The code and models developed in this work are made publicly available, contributing to the ongoing progress and collaboration in the field of large-scale image classification with transformers.']",conclusion_chunked," In this study, we have presented a novel approach to building and optimizing deeper transformer networks for image classification, challenging the traditional dominance of convolutional neural networks. Through careful exploration of architectural modifications and optimization strategies, we have achieved substantial improvements in the accuracy of deep transformers. Our proposed model not only prevents early saturation issues but also sets a new standard in image classification, reaching 86.5% top-1 accuracy on ImageNet without external data. This result establishes the model as the current state-of-the-art solution, surpassing existing benchmarks in terms of both computational efficiency (FLOPs) and model parameters. Additionally, our model exhibits robust performance on ImageNet with Reassessed labels and ImageNet-V2/match frequency, even in the absence of additional training data. The code and models developed in this work are made publicly available, contributing to the ongoing progress and collaboration in the field of large-scale image classification with transformers."
7,GPT-3.5,"[' In conclusion, we propose a straightforward yet powerful architecture for unpaired image-to-image translation tasks, leveraging a fixed-weight image autoencoder as its foundation. The introduction of task-specific residual blocks in the latent space, applied iteratively, yields impressive results across diverse transformations, including style transfer, denoising, and more. A carefully crafted training schedule mitigates the challenges associated with iterative processes. During testing, our approach offers a compelling set of advantages, characterized by a constrained number of weight parameters and a compositional design that facilitates the adjustment of transformation strength based on the number of iterations. This adaptability proves invaluable in scenarios where the nature or magnitude of noise is uncertain. Experimental validations not only illustrate the effectiveness of our method but also showcase comparable or superior performance to CycleGAN, highlighting the potential for achieving high-quality image transformations with significantly reduced model complexity.']",conclusion_chunked," In conclusion, we propose a straightforward yet powerful architecture for unpaired image-to-image translation tasks, leveraging a fixed-weight image autoencoder as its foundation. The introduction of task-specific residual blocks in the latent space, applied iteratively, yields impressive results across diverse transformations, including style transfer, denoising, and more. A carefully crafted training schedule mitigates the challenges associated with iterative processes. During testing, our approach offers a compelling set of advantages, characterized by a constrained number of weight parameters and a compositional design that facilitates the adjustment of transformation strength based on the number of iterations. This adaptability proves invaluable in scenarios where the nature or magnitude of noise is uncertain. Experimental validations not only illustrate the effectiveness of our method but also showcase comparable or superior performance to CycleGAN, highlighting the potential for achieving high-quality image transformations with significantly reduced model complexity."
8,GPT-3.5,"[' The development of Llama 2 marks a significant contribution to the field of large language models, offering a diverse and scalable collection of pretrained and fine-tuned models. In particular, Llama 2-Chat demonstrates remarkable performance in dialogue applications, surpassing existing open-source chat models across various benchmarks. Human evaluations affirm the potential of our models as viable alternatives to closed-source counterparts in terms of both helpfulness and safety. We emphasize the importance of responsible development and knowledge-sharing in the era of large language models and provide a detailed account of our fine-tuning methodology and safety enhancements. Through the release of Llama 2 and the associated documentation, we invite the community to build upon our work, fostering collaborative efforts towards advancing the capabilities and ethical considerations of large language models.']",conclusion_chunked," The development of Llama 2 marks a significant contribution to the field of large language models, offering a diverse and scalable collection of pretrained and fine-tuned models. In particular, Llama 2-Chat demonstrates remarkable performance in dialogue applications, surpassing existing open-source chat models across various benchmarks. Human evaluations affirm the potential of our models as viable alternatives to closed-source counterparts in terms of both helpfulness and safety. We emphasize the importance of responsible development and knowledge-sharing in the era of large language models and provide a detailed account of our fine-tuning methodology and safety enhancements. Through the release of Llama 2 and the associated documentation, we invite the community to build upon our work, fostering collaborative efforts towards advancing the capabilities and ethical considerations of large language models."
9,GPT-3.5,"["" In conclusion, this paper underscores the critical challenge of integrating data from diverse neuroimaging databases, where variability due to factors like age, gender, and acquisition parameters can impact the performance of classification methods. Through experiments on simulated data, we highlight the intricate interactions between data and confounding variables, emphasizing the potential pitfalls in adjusting data from multiple sources. We compare and evaluate three strategies for data adjustment in the context of a Computer-Aided Diagnosis (CAD) system designed to discern healthy subjects from those with Alzheimer's Disease (AD). The findings contribute valuable insights into mitigating variability in large neuroimaging databases, advancing the robustness and interpretability of machine learning models in real-world applications.""]",conclusion_chunked," In conclusion, this paper underscores the critical challenge of integrating data from diverse neuroimaging databases, where variability due to factors like age, gender, and acquisition parameters can impact the performance of classification methods. Through experiments on simulated data, we highlight the intricate interactions between data and confounding variables, emphasizing the potential pitfalls in adjusting data from multiple sources. We compare and evaluate three strategies for data adjustment in the context of a Computer-Aided Diagnosis (CAD) system designed to discern healthy subjects from those with Alzheimer's Disease (AD). The findings contribute valuable insights into mitigating variability in large neuroimaging databases, advancing the robustness and interpretability of machine learning models in real-world applications."
10,GPT-3.5,"[' In the evolving landscape of deep learning, this thesis has illuminated the symbiotic relationship between architectures and training procedures, with a specific focus on the transformative potential of Transformer architectures in visual understanding. The exploration began with a nuanced investigation into the feasibility of learning with coarse labels, revealing a promising avenue for enhancing training procedures. The subsequent scrutiny of diverse computer vision architectures provided a panoramic view of their features, strengths, and challenges, alongside insights into optimal training strategies. As we navigated through this intricate landscape, the imperative role of training in unlocking the full potential of architectures became increasingly evident. Our evaluations, conducted rigorously across tasks such as image classification on ImageNet, transfer learning, and semantic segmentation, attest to the efficacy of the proposed approaches. By addressing the less mature training procedures for transformers, this research not only contributes to advancing the field but also opens avenues for the development of more robust and versatile deep learning models. In conclusion, the synergy between architecture and training processes emerges as a cornerstone for achieving breakthroughs in deep learning applications. As we continue to unravel the complexities of this dynamic interplay, the quest for refined optimization procedures remains paramount for the continued evolution of machine learning in shaping our technological future.']",conclusion_chunked," In the evolving landscape of deep learning, this thesis has illuminated the symbiotic relationship between architectures and training procedures, with a specific focus on the transformative potential of Transformer architectures in visual understanding. The exploration began with a nuanced investigation into the feasibility of learning with coarse labels, revealing a promising avenue for enhancing training procedures. The subsequent scrutiny of diverse computer vision architectures provided a panoramic view of their features, strengths, and challenges, alongside insights into optimal training strategies. As we navigated through this intricate landscape, the imperative role of training in unlocking the full potential of architectures became increasingly evident. Our evaluations, conducted rigorously across tasks such as image classification on ImageNet, transfer learning, and semantic segmentation, attest to the efficacy of the proposed approaches. By addressing the less mature training procedures for transformers, this research not only contributes to advancing the field but also opens avenues for the development of more robust and versatile deep learning models. In conclusion, the synergy between architecture and training processes emerges as a cornerstone for achieving breakthroughs in deep learning applications. As we continue to unravel the complexities of this dynamic interplay, the quest for refined optimization procedures remains paramount for the continued evolution of machine learning in shaping our technological future."
11,GPT-3.5,"["" In conclusion, ResMLP presents a compelling alternative to traditional CNN architectures for image classification. The architecture's simplicity, relying exclusively on multi-layer perceptrons, does not compromise its performance. Through systematic exploration of training strategies, including heavy data augmentation and optional distillation, ResMLP achieves a remarkable balance between accuracy and model complexity on the ImageNet dataset. Furthermore, the extension of ResMLP to self-supervised learning underscores its ability to learn meaningful representations in the absence of labeled data. This aspect enhances its applicability in scenarios where labeled datasets are scarce or costly to obtain. The adaptation of ResMLP to machine translation, with unexpectedly strong results, emphasizes the architecture's potential for broader applications beyond image classification. To facilitate further exploration and adoption, the paper provides pre-trained ResMLP models and code based on the Timm library. The promising results obtained across various tasks suggest that ResMLP is a valuable addition to the landscape of neural network architectures, offering a simple yet powerful approach to diverse machine learning challenges.""]",conclusion_chunked," In conclusion, ResMLP presents a compelling alternative to traditional CNN architectures for image classification. The architecture's simplicity, relying exclusively on multi-layer perceptrons, does not compromise its performance. Through systematic exploration of training strategies, including heavy data augmentation and optional distillation, ResMLP achieves a remarkable balance between accuracy and model complexity on the ImageNet dataset. Furthermore, the extension of ResMLP to self-supervised learning underscores its ability to learn meaningful representations in the absence of labeled data. This aspect enhances its applicability in scenarios where labeled datasets are scarce or costly to obtain. The adaptation of ResMLP to machine translation, with unexpectedly strong results, emphasizes the architecture's potential for broader applications beyond image classification. To facilitate further exploration and adoption, the paper provides pre-trained ResMLP models and code based on the Timm library. The promising results obtained across various tasks suggest that ResMLP is a valuable addition to the landscape of neural network architectures, offering a simple yet powerful approach to diverse machine learning challenges."
12,GPT-3.5,"[' In conclusion, our work sheds light on a critical concern in the realm of Knowledge Graph Completion (KGC) research, where recent publications tout unprecedented performance levels that surpass established state-of-the-art methods. Our investigation reveals that this trend can be attributed to the use of inappropriate evaluation protocols. To address this, we introduce a simple yet robust evaluation protocol designed to rectify shortcomings in existing methodologies. Crucially, our protocol is adept at handling model bias, a factor that can significantly impact final results. Through extensive experiments and evaluations of numerous existing KGC methods using our proposed protocol, we provide a comprehensive perspective on the true performance of these techniques. The public availability of our reproducible code contributes to fostering transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques, thereby advancing the reliability of research outcomes in this evolving field.']",conclusion_chunked," In conclusion, our work sheds light on a critical concern in the realm of Knowledge Graph Completion (KGC) research, where recent publications tout unprecedented performance levels that surpass established state-of-the-art methods. Our investigation reveals that this trend can be attributed to the use of inappropriate evaluation protocols. To address this, we introduce a simple yet robust evaluation protocol designed to rectify shortcomings in existing methodologies. Crucially, our protocol is adept at handling model bias, a factor that can significantly impact final results. Through extensive experiments and evaluations of numerous existing KGC methods using our proposed protocol, we provide a comprehensive perspective on the true performance of these techniques. The public availability of our reproducible code contributes to fostering transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques, thereby advancing the reliability of research outcomes in this evolving field."
13,GPT-3.5,"[' In conclusion, this paper presents a novel strategy to address the latency challenges of autoregressive sequence models during inference while preserving translation accuracy. By incorporating a structured inference module into non-autoregressive models, leveraging an efficient approximation for Conditional Random Fields (CRF), and introducing a dynamic transition technique for positional context modeling, our proposed model achieves significantly enhanced decoding consistency and improved translation performance. Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and approaches the accuracy of purely autoregressive models, showcasing its efficacy in balancing latency reduction and translation quality. The minimal additional latency (8âˆ¼14ms) positions our model as a promising solution for practical applications where both efficiency and accuracy are paramount.']",conclusion_chunked," In conclusion, this paper presents a novel strategy to address the latency challenges of autoregressive sequence models during inference while preserving translation accuracy. By incorporating a structured inference module into non-autoregressive models, leveraging an efficient approximation for Conditional Random Fields (CRF), and introducing a dynamic transition technique for positional context modeling, our proposed model achieves significantly enhanced decoding consistency and improved translation performance. Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and approaches the accuracy of purely autoregressive models, showcasing its efficacy in balancing latency reduction and translation quality. The minimal additional latency (8âˆ¼14ms) positions our model as a promising solution for practical applications where both efficiency and accuracy are paramount."
14,GPT-3.5,"[' In conclusion, SALMON represents a groundbreaking approach to aligning AI agents based on large language models, minimizing the dependency on extensive human supervision. Leveraging a principle-following reward model trained on synthetic preference data, our method achieves superior performance with minimal human-defined principles. By adjusting these principles during RL training, we gain unprecedented control over preferences, influencing RL-trained policies and obviating the need for online human preferences. Applied to the LLaMA-2-70b base language model, Dromedary-2, our AI assistant, significantly outperforms state-of-the-art systems on various benchmark datasets with minimal human intervention. We have open-sourced the code and model weights, encouraging further exploration and advancements in aligning LLM-based AI agents with heightened supervision efficiency, improved controllability, and scalable oversight.']",conclusion_chunked," In conclusion, SALMON represents a groundbreaking approach to aligning AI agents based on large language models, minimizing the dependency on extensive human supervision. Leveraging a principle-following reward model trained on synthetic preference data, our method achieves superior performance with minimal human-defined principles. By adjusting these principles during RL training, we gain unprecedented control over preferences, influencing RL-trained policies and obviating the need for online human preferences. Applied to the LLaMA-2-70b base language model, Dromedary-2, our AI assistant, significantly outperforms state-of-the-art systems on various benchmark datasets with minimal human intervention. We have open-sourced the code and model weights, encouraging further exploration and advancements in aligning LLM-based AI agents with heightened supervision efficiency, improved controllability, and scalable oversight."
15,GPT-3.5,"[' In conclusion, this paper introduces RotatE, a novel knowledge graph embedding approach tailored for the challenging task of predicting missing links. The RotatE model stands out for its ability to effectively model and infer a spectrum of relation patterns, including symmetry/antisymmetry, inversion, and composition. By representing each relation as a rotation in the complex vector space, RotatE offers a versatile framework for capturing diverse relation characteristics. Furthermore, we propose a novel self-adversarial negative sampling technique, contributing to the efficiency and effectiveness of RotatE model training. Experimental results across multiple benchmark knowledge graphs underscore the scalability and superior performance of RotatE, establishing it as a state-of-the-art model in link prediction. The capabilities of RotatE in capturing intricate relation patterns make it a valuable contribution to the field of knowledge graph embeddings.']",conclusion_chunked," In conclusion, this paper introduces RotatE, a novel knowledge graph embedding approach tailored for the challenging task of predicting missing links. The RotatE model stands out for its ability to effectively model and infer a spectrum of relation patterns, including symmetry/antisymmetry, inversion, and composition. By representing each relation as a rotation in the complex vector space, RotatE offers a versatile framework for capturing diverse relation characteristics. Furthermore, we propose a novel self-adversarial negative sampling technique, contributing to the efficiency and effectiveness of RotatE model training. Experimental results across multiple benchmark knowledge graphs underscore the scalability and superior performance of RotatE, establishing it as a state-of-the-art model in link prediction. The capabilities of RotatE in capturing intricate relation patterns make it a valuable contribution to the field of knowledge graph embeddings."
16,GPT-3.5,"[' In conclusion, RECITation-augmented gEneration (RECITE) emerges as a groundbreaking paradigm, offering a fresh perspective on enhancing factual knowledge generation in Large Language Models (LLMs). By introducing a recite-first approach, RECITE minimizes reliance on external corpuses, addressing the limitations of conventional retrieval-augmented models. Our experiments across multiple closed-book question answering (CBQA) tasks, leveraging four pre-trained models—PaLM, UL2, OPT, and Codex—underscore the potency of RECITE in achieving state-of-the-art performance. The recite-and-answer scheme introduced by RECITE not only demonstrates its efficacy in improving accuracy but also positions itself as a versatile solution applicable to various knowledge-intensive Natural Language Processing (NLP) tasks. This paper contributes not only a novel paradigm but also comprehensive empirical evidence supporting its effectiveness. As we make our code publicly available, we encourage further exploration and adoption of RECITE, anticipating its integration into the evolving landscape of large language models and knowledge-intensive NLP applications.']",conclusion_chunked," In conclusion, RECITation-augmented gEneration (RECITE) emerges as a groundbreaking paradigm, offering a fresh perspective on enhancing factual knowledge generation in Large Language Models (LLMs). By introducing a recite-first approach, RECITE minimizes reliance on external corpuses, addressing the limitations of conventional retrieval-augmented models. Our experiments across multiple closed-book question answering (CBQA) tasks, leveraging four pre-trained models—PaLM, UL2, OPT, and Codex—underscore the potency of RECITE in achieving state-of-the-art performance. The recite-and-answer scheme introduced by RECITE not only demonstrates its efficacy in improving accuracy but also positions itself as a versatile solution applicable to various knowledge-intensive Natural Language Processing (NLP) tasks. This paper contributes not only a novel paradigm but also comprehensive empirical evidence supporting its effectiveness. As we make our code publicly available, we encourage further exploration and adoption of RECITE, anticipating its integration into the evolving landscape of large language models and knowledge-intensive NLP applications."
17,GPT-3.5,"["" In this paper, we present MobileBERT, a novel approach to addressing the challenges associated with deploying large pre-trained NLP models on resource-limited mobile devices. By compressing and accelerating the widely used BERT model, MobileBERT achieves a significant reduction in size and improvement in speed without compromising performance. The task-agnostic nature of MobileBERT, inherited from the original BERT, ensures its applicability to various downstream NLP tasks through straightforward fine-tuning. The architectural enhancements, including bottleneck structures and a nuanced balance between self-attentions and feed-forward networks, contribute to the model's efficiency. Empirical studies validate the effectiveness of MobileBERT, showcasing a 4.3× size reduction and a 5.5× speedup compared to BERTBASE, with competitive results on standard benchmarks. MobileBERT's GLUE score of 77.7 on natural language inference tasks and its high performance on the SQuAD question answering task underscore its practical viability for real-world applications on mobile devices.""]",conclusion_chunked," In this paper, we present MobileBERT, a novel approach to addressing the challenges associated with deploying large pre-trained NLP models on resource-limited mobile devices. By compressing and accelerating the widely used BERT model, MobileBERT achieves a significant reduction in size and improvement in speed without compromising performance. The task-agnostic nature of MobileBERT, inherited from the original BERT, ensures its applicability to various downstream NLP tasks through straightforward fine-tuning. The architectural enhancements, including bottleneck structures and a nuanced balance between self-attentions and feed-forward networks, contribute to the model's efficiency. Empirical studies validate the effectiveness of MobileBERT, showcasing a 4.3× size reduction and a 5.5× speedup compared to BERTBASE, with competitive results on standard benchmarks. MobileBERT's GLUE score of 77.7 on natural language inference tasks and its high performance on the SQuAD question answering task underscore its practical viability for real-world applications on mobile devices."
18,GPT-3.5,"[' In conclusion, this paper introduces PEER, a collaborative language model designed to emulate the entire writing process, addressing critical limitations in existing language models related to collaborative writing scenarios. PEER exhibits the capability to generate drafts, propose edits, and provide explanations for its actions, unlocking new dimensions of controllability and adaptability. The novel approach of training multiple instances of PEER to cover various stages of the writing process, coupled with self-training techniques, enhances the quality, quantity, and diversity of training data. This empowers PEER to operate effectively in domains without edit histories and improves its ability to follow instructions, generate insightful comments, and explain its actions. Across diverse domains and editing tasks, PEER demonstrates strong and versatile performance, showcasing its potential as a valuable tool for collaborative writing processes.']",conclusion_chunked," In conclusion, this paper introduces PEER, a collaborative language model designed to emulate the entire writing process, addressing critical limitations in existing language models related to collaborative writing scenarios. PEER exhibits the capability to generate drafts, propose edits, and provide explanations for its actions, unlocking new dimensions of controllability and adaptability. The novel approach of training multiple instances of PEER to cover various stages of the writing process, coupled with self-training techniques, enhances the quality, quantity, and diversity of training data. This empowers PEER to operate effectively in domains without edit histories and improves its ability to follow instructions, generate insightful comments, and explain its actions. Across diverse domains and editing tasks, PEER demonstrates strong and versatile performance, showcasing its potential as a valuable tool for collaborative writing processes."
19,GPT-3.5,"["" In conclusion, this paper introduces BERTRAM, a powerful architecture derived from BERT, with a specific focus on addressing the challenges associated with rare words in pretrained language models. Leveraging insights from static word embeddings, BERTRAM facilitates the interaction between the surface form and contexts of words within a deep architecture. The integration of BERTRAM into BERT results in significant performance enhancements, particularly in the representation of rare and medium-frequency words. This improvement is consistently demonstrated across a rare word probing task and three downstream tasks, underscoring BERTRAM's efficacy in ameliorating the treatment of rare words within the context of pretrained language models. The findings contribute to advancing our understanding and strategies for handling rare words in contemporary NLP applications.""]",conclusion_chunked," In conclusion, this paper introduces BERTRAM, a powerful architecture derived from BERT, with a specific focus on addressing the challenges associated with rare words in pretrained language models. Leveraging insights from static word embeddings, BERTRAM facilitates the interaction between the surface form and contexts of words within a deep architecture. The integration of BERTRAM into BERT results in significant performance enhancements, particularly in the representation of rare and medium-frequency words. This improvement is consistently demonstrated across a rare word probing task and three downstream tasks, underscoring BERTRAM's efficacy in ameliorating the treatment of rare words within the context of pretrained language models. The findings contribute to advancing our understanding and strategies for handling rare words in contemporary NLP applications."
20,GPT-3.5,"[' In conclusion, this paper introduces Pattern-Exploiting Training (PET), a novel semi-supervised learning approach that effectively combines the strengths of unsupervised and supervised learning for natural language processing (NLP) tasks. By reformulating input examples as cloze-style phrases, PET provides valuable guidance to language models in understanding specific tasks. Soft labels assigned to a large set of unlabeled examples using these phrases enable effective leveraging of unlabeled data. Standard supervised training on the resulting training set leads to superior performance across various tasks and languages, outperforming both traditional supervised training and strong semi-supervised approaches. PET emerges as a promising strategy, especially in low-resource settings, showcasing its potential to enhance the efficiency and efficacy of language models in NLP applications.']",conclusion_chunked," In conclusion, this paper introduces Pattern-Exploiting Training (PET), a novel semi-supervised learning approach that effectively combines the strengths of unsupervised and supervised learning for natural language processing (NLP) tasks. By reformulating input examples as cloze-style phrases, PET provides valuable guidance to language models in understanding specific tasks. Soft labels assigned to a large set of unlabeled examples using these phrases enable effective leveraging of unlabeled data. Standard supervised training on the resulting training set leads to superior performance across various tasks and languages, outperforming both traditional supervised training and strong semi-supervised approaches. PET emerges as a promising strategy, especially in low-resource settings, showcasing its potential to enhance the efficiency and efficacy of language models in NLP applications."
21,GPT-3.5,"[' In conclusion, this paper introduces a novel architecture for learning word embeddings, specifically designed to address the challenge of effectively representing novel words in natural language processing (NLP) systems. By incorporating information from both surface-form and context, our model demonstrates significant improvements in embedding quality, as evidenced by state-of-the-art performance on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, offering a practical and easily integrable solution. This model can be seamlessly incorporated into any existing NLP system, enhancing its ability to handle novel words and thereby contributing to the adaptability and robustness of NLP applications.']",conclusion_chunked," In conclusion, this paper introduces a novel architecture for learning word embeddings, specifically designed to address the challenge of effectively representing novel words in natural language processing (NLP) systems. By incorporating information from both surface-form and context, our model demonstrates significant improvements in embedding quality, as evidenced by state-of-the-art performance on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, offering a practical and easily integrable solution. This model can be seamlessly incorporated into any existing NLP system, enhancing its ability to handle novel words and thereby contributing to the adaptability and robustness of NLP applications."
22,GPT-3.5,"[' In conclusion, this paper introduces a paradigm shift in the pursuit of high-quality sentence embeddings, circumventing the limitations associated with labeled data, finetuning, and modifications to pretraining objectives. Leveraging the generative abilities of large and high-performing pretrained language models (PLMs), we showcase a fully unsupervised approach that produces impressive results on various semantic textual similarity datasets. The demonstrated efficacy of our methodology not only highlights its potential for practical applications but also invites further exploration into the synergies between generative language models and unsupervised learning techniques. This innovative approach opens new avenues for research and development in the realm of natural language processing and representation learning.']",conclusion_chunked," In conclusion, this paper introduces a paradigm shift in the pursuit of high-quality sentence embeddings, circumventing the limitations associated with labeled data, finetuning, and modifications to pretraining objectives. Leveraging the generative abilities of large and high-performing pretrained language models (PLMs), we showcase a fully unsupervised approach that produces impressive results on various semantic textual similarity datasets. The demonstrated efficacy of our methodology not only highlights its potential for practical applications but also invites further exploration into the synergies between generative language models and unsupervised learning techniques. This innovative approach opens new avenues for research and development in the realm of natural language processing and representation learning."
23,GPT-3.5,"[' In conclusion, this paper introduces GENPET as a promising method that successfully marries pretrained language models with task descriptions for enhanced text generation capabilities. By extending the concept of pattern-exploiting training to generative tasks, GENPET overcomes challenges related to task description clarity, model utilization, and overfitting prevention. Through empirical validation on summarization and headline generation datasets, GENPET consistently outperforms robust baselines in few-shot scenarios, emphasizing its efficacy and versatility. The presented approach not only contributes to the ongoing discourse on improving data efficiency in generative settings but also opens avenues for further exploration in combining task descriptions and example-based learning for text generation tasks.']",conclusion_chunked," In conclusion, this paper introduces GENPET as a promising method that successfully marries pretrained language models with task descriptions for enhanced text generation capabilities. By extending the concept of pattern-exploiting training to generative tasks, GENPET overcomes challenges related to task description clarity, model utilization, and overfitting prevention. Through empirical validation on summarization and headline generation datasets, GENPET consistently outperforms robust baselines in few-shot scenarios, emphasizing its efficacy and versatility. The presented approach not only contributes to the ongoing discourse on improving data efficiency in generative settings but also opens avenues for further exploration in combining task descriptions and example-based learning for text generation tasks."
24,Aman Madaan,"[' AutoMix integrates black-box large language model (LLM) APIs into a multi-step problem-solving\nframework, optimizing the computational cost and performance trade-offs. AutoMix opens avenues\nfor several interesting research directions. First, while self-verification and correction are challenging\nfor LLMs in general, we find promising results using context-grounded few-shot verification, indicating that similar approaches may yield gain in other scenarios. Secondly, our work interweaves\nGood Old-Fashioned Artificial Intelligence (GOFAI) approaches with LLMs, demonstrating that the\nincorporation of a POMDP can boost the accuracy of a noisy few-shot verifier, showing the promise\nof this paradigm as an approach for improving LLMs during inference.']",conclusion_chunked," AutoMix integrates black-box large language model (LLM) APIs into a multi-step problem-solving
framework, optimizing the computational cost and performance trade-offs. AutoMix opens avenues
for several interesting research directions. First, while self-verification and correction are challenging
for LLMs in general, we find promising results using context-grounded few-shot verification, indicating that similar approaches may yield gain in other scenarios. Secondly, our work interweaves
Good Old-Fashioned Artificial Intelligence (GOFAI) approaches with LLMs, demonstrating that the
incorporation of a POMDP can boost the accuracy of a noisy few-shot verifier, showing the promise
of this paradigm as an approach for improving LLMs during inference."
25,Aman Madaan,"[' We present SELF-REFINE: a novel approach that allows large language models to iteratively provide\nself-feedback and refine their own outputs. SELF-REFINE operates within a single LLM, requiring\nneither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of\nuse of SELF-REFINE across a wide variety of tasks. By showcasing the potential of SELF-REFINE in\ndiverse tasks, our research contributes to the ongoing exploration and development of large language\nmodels, with the aim of reducing the cost of human creative processes in real-world settings. We hope that our iterative approach will help drive further research in this area. To this end, we make all\nour code, data and prompts anonymously available at https://selfrefine.info/.']",conclusion_chunked," We present SELF-REFINE: a novel approach that allows large language models to iteratively provide
self-feedback and refine their own outputs. SELF-REFINE operates within a single LLM, requiring
neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of
use of SELF-REFINE across a wide variety of tasks. By showcasing the potential of SELF-REFINE in
diverse tasks, our research contributes to the ongoing exploration and development of large language
models, with the aim of reducing the cost of human creative processes in real-world settings. We hope that our iterative approach will help drive further research in this area. To this end, we make all
our code, data and prompts anonymously available at https://selfrefine.info/."
26,Aman Madaan,"[' This work explores the capability of language models of code in generating performance-improving edits, while\nadhering to the functional correctness constraint. Our results demonstrate that such models can achieve speedup\nup to 2.5ˆ for over 25% of test programs in C++ and Python . This research is the initial step towards unlocking the potential of language models in leveraging the opportunities at the “top” of the computing stack. Particularly, we improve algorithmic efficiency and enable automatic code optimization without taking additional cycles from developers. Our promising results and the proven capability of large language models in\na variety of tasks sketch an exciting path forward to drive the computing efficiency in the post-Moore era. Nonetheless, there are many research directions and challenges that need attention, such as automatic test generation for performance triage, improving the alignment of generated code edits to target system and/or user\npreferences, adding structure to generated code edits, and tailoring code edit generation to architecture and\nhardware features.']",conclusion_chunked," This work explores the capability of language models of code in generating performance-improving edits, while
adhering to the functional correctness constraint. Our results demonstrate that such models can achieve speedup
up to 2.5ˆ for over 25% of test programs in C++ and Python . This research is the initial step towards unlocking the potential of language models in leveraging the opportunities at the “top” of the computing stack. Particularly, we improve algorithmic efficiency and enable automatic code optimization without taking additional cycles from developers. Our promising results and the proven capability of large language models in
a variety of tasks sketch an exciting path forward to drive the computing efficiency in the post-Moore era. Nonetheless, there are many research directions and challenges that need attention, such as automatic test generation for performance triage, improving the alignment of generated code edits to target system and/or user
preferences, adding structure to generated code edits, and tailoring code edit generation to architecture and
hardware features."
27,Aman Madaan,"[' We present the first work to employ large language models of code for structured commonsense generation. By converting the output commonsense structures to Python code, COCOGEN provides a simple and effective method for leveraging the code-generation abilities of Code-LLMs for structured generation. These results open a promising direction for structural commonsense reasoning. We believe that the principles and the methods presented in this paper are applicable to additional NLP tasks that require “language understanding” and structured prediction.']",conclusion_chunked," We present the first work to employ large language models of code for structured commonsense generation. By converting the output commonsense structures to Python code, COCOGEN provides a simple and effective method for leveraging the code-generation abilities of Code-LLMs for structured generation. These results open a promising direction for structural commonsense reasoning. We believe that the principles and the methods presented in this paper are applicable to additional NLP tasks that require “language understanding” and structured prediction."
28,Aman Madaan,"[' This work evaluates the capacity of COT to elevate complex reasoning in three state-of-the-arts LLMs,\nPaLM, GPT-3, and CODEX. We systematically assembled a series of controlled counterfactual experiments. Our results show the initial inklings of connection between text, patterns, and reasoning in LLMs. Our study\nindicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process. In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models\nto mold correct answers.']",conclusion_chunked," This work evaluates the capacity of COT to elevate complex reasoning in three state-of-the-arts LLMs,
PaLM, GPT-3, and CODEX. We systematically assembled a series of controlled counterfactual experiments. Our results show the initial inklings of connection between text, patterns, and reasoning in LLMs. Our study
indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process. In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models
to mold correct answers."
29,Aman Madaan,"[' Future machine learning applications will potentially have\nAPI-level access to several models of varying strengths and\ncosts of usage. In such scenarios, building systems that\ncan adapt to the difficulty of the sample will be critical for\nscale and efficiency. FLOWGEN presents a real-world use\ncase for such FAST-SLOW systems. As future work, we\nplan to explore the use of FAST-SLOW generation methods\nfor effective and adaptive language generation using largelanguage models.']",conclusion_chunked," Future machine learning applications will potentially have
API-level access to several models of varying strengths and
costs of usage. In such scenarios, building systems that
can adapt to the difficulty of the sample will be critical for
scale and efficiency. FLOWGEN presents a real-world use
case for such FAST-SLOW systems. As future work, we
plan to explore the use of FAST-SLOW generation methods
for effective and adaptive language generation using largelanguage models."
30,Aman Madaan,"[' We present SETAUG, a novel data augmentation\nmethod for conditional set generation that incorporates informative orders and adds cardinality information. Our key idea is using the most likely\norder (vs. a randomly selected order) to represent\na set as a sequence and conditioning the generation of a set on predicted cardinality. As a computationally efficient and general-purpose plug-in\ndata augmentation algorithm, SETAUG improves\nSEQ2SEQ models for set generation across a wide\nspectrum of tasks. For future work, it would be\ninteresting to investigate if the general ideas in this\nwork have applications in settings beyond set generation. Examples include generating additional data\nto improve language modeling in low-resource scenarios and determining the ideal order of in-context\nexamples in a prompt.']",conclusion_chunked," We present SETAUG, a novel data augmentation
method for conditional set generation that incorporates informative orders and adds cardinality information. Our key idea is using the most likely
order (vs. a randomly selected order) to represent
a set as a sequence and conditioning the generation of a set on predicted cardinality. As a computationally efficient and general-purpose plug-in
data augmentation algorithm, SETAUG improves
SEQ2SEQ models for set generation across a wide
spectrum of tasks. For future work, it would be
interesting to investigate if the general ideas in this
work have applications in settings beyond set generation. Examples include generating additional data
to improve language modeling in low-resource scenarios and determining the ideal order of in-context
examples in a prompt."
31,Aman Madaan,"[' We present MemPrompt, a novel, memoryenhanced GPT-3 that allows users to interact and\nimprove the model without retraining. A key insight is to have the model articulate not just its\nanswer but also its understanding of the user’s intent, providing an avenue for feedback. We show\nthat deployed systems with fixed large-language\nmodels can still be improved by interacting with end-users, potentially improving their performance\nand broadening their utility.']",conclusion_chunked," We present MemPrompt, a novel, memoryenhanced GPT-3 that allows users to interact and
improve the model without retraining. A key insight is to have the model articulate not just its
answer but also its understanding of the user’s intent, providing an avenue for feedback. We show
that deployed systems with fixed large-language
models can still be improved by interacting with end-users, potentially improving their performance
and broadening their utility."
32,Aman Madaan,"[' Cognitive science suggests that people form “mental models” of a situation to answer questions about it. Drawing on those ideas, we have presented a simple instantiation in which the situational model is an inference graph. Different from GCN-based models popular in graph learning, we use mixtureof-experts to pool graph representations. Our experiments show that MoE-based pooling can be a strong (both in terms of performance and explainability) alternative to GCN for graph-based learning for reasoning tasks. Our method establishes a new state-of-the-art on three defeasible reasoning datasets. Overall, our method shows that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reflexively']",conclusion_chunked," Cognitive science suggests that people form “mental models” of a situation to answer questions about it. Drawing on those ideas, we have presented a simple instantiation in which the situational model is an inference graph. Different from GCN-based models popular in graph learning, we use mixtureof-experts to pool graph representations. Our experiments show that MoE-based pooling can be a strong (both in terms of performance and explainability) alternative to GCN for graph-based learning for reasoning tasks. Our method establishes a new state-of-the-art on three defeasible reasoning datasets. Overall, our method shows that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reflexively"
33,Aman Madaan,"[' Our work takes the idea of using inference graphs\nfor defeasible inference and scales up its usability by automatically generating and augmenting\nthem to a downstream defeasible task that both humans and machines are known to find difficult. We\nidentify that the contextualizer and mediator nodes\nare crucial to defeasible inference, and show that\nour generated graphs generate these critical nodes effectively. Humans perform significantly better\n(20% absolute improvement) across diverse defeasible datasets and overwhelmingly attribute their\nsuccess to the mediator nodes – giving insights into\nwhat helps and why. In this case study, we show\nthat machines can fill the gaps in human knowledge\nwhen for defeasible reasoning. While we establish\nthat humans are helped by these graphs, a further\ninvestigation on how (and if) the graphs reinforced\ntheir beliefs, and what additional information in the\ngraphs was beneficial to their understanding is essential. Furthermore, a deeper understanding of the\ntrade-offs (time spent in answering these questions\nwith and without the graphs) also forms important\nfuture work.']",conclusion_chunked," Our work takes the idea of using inference graphs
for defeasible inference and scales up its usability by automatically generating and augmenting
them to a downstream defeasible task that both humans and machines are known to find difficult. We
identify that the contextualizer and mediator nodes
are crucial to defeasible inference, and show that
our generated graphs generate these critical nodes effectively. Humans perform significantly better
(20% absolute improvement) across diverse defeasible datasets and overwhelmingly attribute their
success to the mediator nodes – giving insights into
what helps and why. In this case study, we show
that machines can fill the gaps in human knowledge
when for defeasible reasoning. While we establish
that humans are helped by these graphs, a further
investigation on how (and if) the graphs reinforced
their beliefs, and what additional information in the
graphs was beneficial to their understanding is essential. Furthermore, a deeper understanding of the
trade-offs (time spent in answering these questions
with and without the graphs) also forms important
future work."
34,Aman Madaan,"[' We present MERCURIE, a system that improves the explanation structure (graphs) generated by a model without requiring expensive humanannotated feedback. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains. This work paves a new path towards exciting future research direction of constantly improving explainable NLP models by applying human feedback.']",conclusion_chunked," We present MERCURIE, a system that improves the explanation structure (graphs) generated by a model without requiring expensive humanannotated feedback. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains. This work paves a new path towards exciting future research direction of constantly improving explainable NLP models by applying human feedback."
35,Aman Madaan,"[' We define the problem of event-influence reasoning as a generation task conditioned on context, particularly exploring the efficacy of large scale pre-trained language models for the task. We use human-curated event influence graphs to train a model to generate targeted event influences grounded in a context. Our experiments with ablations and error analysis provide insights into how to effectively adapt pretrained language models for event influence generation and opens up exciting avenues for further research. Our method outperforms strong baselines on both automated and human evaluations. Furthermore, generated influences improve performance on the benchmark Question Type BERT + EIGEN BERT Exogenous 64.04 56.13 In-para 73.58 79.68 Out-of-para 90.84 89.38 Table 9: QA accuracy by question type WIQA QA task without architectural changes to the model. Future work would extend the generalizability of this method to understand more complex and volatile event influences, such as events in news articles and stock markets.']",conclusion_chunked," We define the problem of event-influence reasoning as a generation task conditioned on context, particularly exploring the efficacy of large scale pre-trained language models for the task. We use human-curated event influence graphs to train a model to generate targeted event influences grounded in a context. Our experiments with ablations and error analysis provide insights into how to effectively adapt pretrained language models for event influence generation and opens up exciting avenues for further research. Our method outperforms strong baselines on both automated and human evaluations. Furthermore, generated influences improve performance on the benchmark Question Type BERT + EIGEN BERT Exogenous 64.04 56.13 In-para 73.58 79.68 Out-of-para 90.84 89.38 Table 9: QA accuracy by question type WIQA QA task without architectural changes to the model. Future work would extend the generalizability of this method to understand more complex and volatile event influences, such as events in news articles and stock markets."
36,Aman Madaan,"[' Current methods for generating event-level temporal graphs are developed with relatively small amounts of hand-labeled data. On the other hand, the possibility of using pre-trained language models for this task has not received sufficient attention. This paper addresses this open challenge by first developing a data generation pipeline that uses existing IE/NLP/clustering techniques for automated acquisition of a large corpus of document-graph pairs, and by proposing a new formulation of the graph generation task as a sequence-to-sequence mapping task, allowing us to leverage and fine-tune pretrained language models. Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines. We plan to explore techniques for adapting largescale language models on unseen domains and at multiple granularity levels in the future.']",conclusion_chunked," Current methods for generating event-level temporal graphs are developed with relatively small amounts of hand-labeled data. On the other hand, the possibility of using pre-trained language models for this task has not received sufficient attention. This paper addresses this open challenge by first developing a data generation pipeline that uses existing IE/NLP/clustering techniques for automated acquisition of a large corpus of document-graph pairs, and by proposing a new formulation of the graph generation task as a sequence-to-sequence mapping task, allowing us to leverage and fine-tune pretrained language models. Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines. We plan to explore techniques for adapting largescale language models on unseen domains and at multiple granularity levels in the future."
37,Aman Madaan,"[' We introduce the task of politeness transfer for\nwhich we provide a dataset comprised of sentences\ncurated from email exchanges present in the Enron\ncorpus. We extend prior works (Li et al., 2018;\nSudhakar et al., 2019) on attribute transfer by introducing a simple pipeline – tag & generate which\nis an interpretable two-staged approach for content\npreserving style transfer. We believe our approach\nis the first to be robust in cases when the source is\nstyle neutral, like the “non-polite” class in the case\nof politeness transfer. Automatic and human evaluation shows that our approach outperforms other\nstate-of-the-art models on content preservation metrics while retaining (or in some cases improving)\nthe transfer accuracies.']",conclusion_chunked," We introduce the task of politeness transfer for
which we provide a dataset comprised of sentences
curated from email exchanges present in the Enron
corpus. We extend prior works (Li et al., 2018;
Sudhakar et al., 2019) on attribute transfer by introducing a simple pipeline – tag & generate which
is an interpretable two-staged approach for content
preserving style transfer. We believe our approach
is the first to be robust in cases when the source is
style neutral, like the “non-polite” class in the case
of politeness transfer. Automatic and human evaluation shows that our approach outperforms other
state-of-the-art models on content preservation metrics while retaining (or in some cases improving)
the transfer accuracies."
38,Aman Madaan,"[' In this work, we propose a method that uses images for generating high-quality comparable\ntraining data without the need for bilingual translators. More specifically, our technique\nfor image selection and crowdsourcing results in useful training data for scenarios where\nfinding annotators proficient in both the languages is challenging, as demonstrated by human\nevaluation and downstream task performance. To the best of our knowledge, we are the first to introduce the idea of crowdsourcing comparable data using images for low resource settings. The IAPR TC-12 dataset (Grubinger\net al., 2006) is one of the earliest image captioning dataset that comprises of travel images\nwith captions in English, Spanish, and German provided by tour guides. The datasets released by Elliott et al. (2016) (30,000 images with captions in German and English) and\nFunaki & Nakayama (2015) (1000 images with captions in English and Japanese) were both\nobtained with the help of professional translators. Hewitt et al. (2018) and Bergsma &\nVan Durme (2011) rely on a corpus of images associated with words (accessed via image\nsearch engines) in the languages of interest. Similarities in images are then used to induce\nbilingual lexicons. In contrast, our method is ideal for settings where absolutely no resources are available for a low resource language. Further, Singhal et al.', '(2019) use a similar\nproprietary dataset obtained via Google search to learn multilingual embeddings. Hitschler\net al. (2016) and Chen et al. (2019) improve the quality of statistical machine translation\nand bilingual lexicon induction by using large monolingual image-captioning corpora. While\ntheir work is orthogonal to ours, it underscores the fact that the dataset generated by our\nmethod can indeed boost downstream tasks. In the future, we plan to use our data creation technique on extremely low-resource languages\nand release parallel corpora that can potentially propel the use of state-of-the-art NLP\ntechniques on these languages. It would also be interesting to explore methods to quantify\nthe definition of universality and select such images for tasks like ours.']",conclusion_chunked," In this work, we propose a method that uses images for generating high-quality comparable
training data without the need for bilingual translators. More specifically, our technique
for image selection and crowdsourcing results in useful training data for scenarios where
finding annotators proficient in both the languages is challenging, as demonstrated by human
evaluation and downstream task performance. To the best of our knowledge, we are the first to introduce the idea of crowdsourcing comparable data using images for low resource settings. The IAPR TC-12 dataset (Grubinger
et al., 2006) is one of the earliest image captioning dataset that comprises of travel images
with captions in English, Spanish, and German provided by tour guides. The datasets released by Elliott et al. (2016) (30,000 images with captions in German and English) and
Funaki & Nakayama (2015) (1000 images with captions in English and Japanese) were both
obtained with the help of professional translators. Hewitt et al. (2018) and Bergsma &
Van Durme (2011) rely on a corpus of images associated with words (accessed via image
search engines) in the languages of interest. Similarities in images are then used to induce
bilingual lexicons. In contrast, our method is ideal for settings where absolutely no resources are available for a low resource language. Further, Singhal et al."
39,Aman Madaan,"[' In this work, we propose a method that uses images for generating high-quality comparable\ntraining data without the need for bilingual translators. More specifically, our technique\nfor image selection and crowdsourcing results in useful training data for scenarios where\nfinding annotators proficient in both the languages is challenging, as demonstrated by human\nevaluation and downstream task performance. To the best of our knowledge, we are the first to introduce the idea of crowdsourcing comparable data using images for low resource settings. The IAPR TC-12 dataset (Grubinger\net al., 2006) is one of the earliest image captioning dataset that comprises of travel images\nwith captions in English, Spanish, and German provided by tour guides. The datasets released by Elliott et al. (2016) (30,000 images with captions in German and English) and\nFunaki & Nakayama (2015) (1000 images with captions in English and Japanese) were both\nobtained with the help of professional translators. Hewitt et al. (2018) and Bergsma &\nVan Durme (2011) rely on a corpus of images associated with words (accessed via image\nsearch engines) in the languages of interest. Similarities in images are then used to induce\nbilingual lexicons. In contrast, our method is ideal for settings where absolutely no resources are available for a low resource language. Further, Singhal et al.', '(2019) use a similar\nproprietary dataset obtained via Google search to learn multilingual embeddings. Hitschler\net al. (2016) and Chen et al. (2019) improve the quality of statistical machine translation\nand bilingual lexicon induction by using large monolingual image-captioning corpora. While\ntheir work is orthogonal to ours, it underscores the fact that the dataset generated by our\nmethod can indeed boost downstream tasks. In the future, we plan to use our data creation technique on extremely low-resource languages\nand release parallel corpora that can potentially propel the use of state-of-the-art NLP\ntechniques on these languages. It would also be interesting to explore methods to quantify\nthe definition of universality and select such images for tasks like ours.']",conclusion_chunked,"(2019) use a similar
proprietary dataset obtained via Google search to learn multilingual embeddings. Hitschler
et al. (2016) and Chen et al. (2019) improve the quality of statistical machine translation
and bilingual lexicon induction by using large monolingual image-captioning corpora. While
their work is orthogonal to ours, it underscores the fact that the dataset generated by our
method can indeed boost downstream tasks. In the future, we plan to use our data creation technique on extremely low-resource languages
and release parallel corpora that can potentially propel the use of state-of-the-art NLP
techniques on these languages. It would also be interesting to explore methods to quantify
the definition of universality and select such images for tasks like ours."
40,Aman Madaan,"[' We present the first detailed study of the task of numerical relation extraction, in which one of the arguments of the\nrelation is a quantity. Our preliminary analysis reveals several peculiarities that make the task differently challenging\nfrom standard IE. We employ these insights into a rule-based\nsystem, NumberRule, that can extract any numerical relation given input keywords for that relation. We also develop\nNumberTron, an extension of MultiR, which employs novel\ntask-specific features and can be trained via distant supervision or other heuristic labelings. By aggregating evidence from multiple features, NumberTron produces much higher recall at comparable precision compared to NumberRule. Both systems vastly outperform baselines and non-numerical IE systems, with NumberTron yielding almost 25 point F-score improvement. A key limitation of our research is lack of temporal modeling – many numerical relations change over time. In the future, we wish to extract numerical relations along with their\ntemporal scopes. Temporal identification will likely improve\nthe effectiveness of distant supervision too.']",conclusion_chunked," We present the first detailed study of the task of numerical relation extraction, in which one of the arguments of the
relation is a quantity. Our preliminary analysis reveals several peculiarities that make the task differently challenging
from standard IE. We employ these insights into a rule-based
system, NumberRule, that can extract any numerical relation given input keywords for that relation. We also develop
NumberTron, an extension of MultiR, which employs novel
task-specific features and can be trained via distant supervision or other heuristic labelings. By aggregating evidence from multiple features, NumberTron produces much higher recall at comparable precision compared to NumberRule. Both systems vastly outperform baselines and non-numerical IE systems, with NumberTron yielding almost 25 point F-score improvement. A key limitation of our research is lack of temporal modeling – many numerical relations change over time. In the future, we wish to extract numerical relations along with their
temporal scopes. Temporal identification will likely improve
the effectiveness of distant supervision too."
41,Hugo Touvron,"[' In this study, we have introduced Llama 2, a new family of pretrained and fine-tuned models with scales of 7 billion to 70 billion parameters. These models have demonstrated their competitiveness with existing open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation sets we examined, although they still lag behind other models like GPT-4. We meticulously elaborated on the methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research, we have responsibly opened access to Llama 2 and Llama 2-Chat. As part of our ongoing commitment to transparency and safety, we plan to make further improvements to Llama 2-Chat in future work']",conclusion_chunked," In this study, we have introduced Llama 2, a new family of pretrained and fine-tuned models with scales of 7 billion to 70 billion parameters. These models have demonstrated their competitiveness with existing open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation sets we examined, although they still lag behind other models like GPT-4. We meticulously elaborated on the methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research, we have responsibly opened access to Llama 2 and Llama 2-Chat. As part of our ongoing commitment to transparency and safety, we plan to make further improvements to Llama 2-Chat in future work"
42,Hugo Touvron,"[' In this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10× smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robustness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al. (2022) that finetuning these models on instructions\nlead to promising results, and we plan to further\ninvestigate this in future work. Finally, we plan to\nrelease larger models trained on larger pretraining\ncorpora in the future, since we have seen a constant\nimprovement in performance as we were scaling.']",conclusion_chunked," In this paper, we presented a series of language
models that are released openly, and competitive
with state-of-the-art foundation models. Most
notably, LLaMA-13B outperforms GPT-3 while
being more than 10× smaller, and LLaMA-65B is
competitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, we show that it is possible
to achieve state-of-the-art performance by training
exclusively on publicly available data, without
resorting to proprietary datasets. We hope that
releasing these models to the research community
will accelerate the development of large language
models, and help efforts to improve their robustness and mitigate known issues such as toxicity and
bias. Additionally, we observed like Chung et al. (2022) that finetuning these models on instructions
lead to promising results, and we plan to further
investigate this in future work. Finally, we plan to
release larger models trained on larger pretraining
corpora in the future, since we have seen a constant
improvement in performance as we were scaling."
43,Hugo Touvron,"[' Co-training submodels (cosub) is an effective way to improve existing deep residual networks. It is straightforward\nto implement, just involving a few lines of code. It does not\nneed a pre-trained teacher, and it only maintains a single set\nof weights for the model. Extensive experimental results\non image classification, transfer learning and semantic segmentation show that cosub is overall extremely effective. It\nworks off-the-shelf and improves the state of the art for various network architectures, including convnets like Regnet.']",conclusion_chunked," Co-training submodels (cosub) is an effective way to improve existing deep residual networks. It is straightforward
to implement, just involving a few lines of code. It does not
need a pre-trained teacher, and it only maintains a single set
of weights for the model. Extensive experimental results
on image classification, transfer learning and semantic segmentation show that cosub is overall extremely effective. It
works off-the-shelf and improves the state of the art for various network architectures, including convnets like Regnet."
44,Hugo Touvron,"[' This paper makes a simple contribution: it proposes improved baselines for vision\ntransformers trained in a supervised fashion that can serve (1) either as a comparison basis for new architectures; (2) or for other training approaches such as\nthose based on self-supervised learning. We hope that this stronger baseline will\nserve the community effort in making progress on learning foundation models\nthat could serve many tasks. Our experiments have also gathered a few insights\non how to train ViT for larger models with reduced resources without hurting accuracy, allowing us to train a one-billion parameter model with 4 nodes of 8 GPUs.']",conclusion_chunked," This paper makes a simple contribution: it proposes improved baselines for vision
transformers trained in a supervised fashion that can serve (1) either as a comparison basis for new architectures; (2) or for other training approaches such as
those based on self-supervised learning. We hope that this stronger baseline will
serve the community effort in making progress on learning foundation models
that could serve many tasks. Our experiments have also gathered a few insights
on how to train ViT for larger models with reduced resources without hurting accuracy, allowing us to train a one-billion parameter model with 4 nodes of 8 GPUs."
45,Hugo Touvron,"[' In this paper, we looked at three different topics related to Vision Transformers. First, we investigated a simple but effective way to parallelize them, showing a viable alternative to increase capacity without significantly increasing the working dimensionality. Whether this simple parallel design principle can be applied to other architectures is an exploration left for future work. Second, we considered different fine-tuning strategies and showed that fine-tuning the self-attention layer is sufficient in the context of resolution fine-tuning. This can also be interesting when transferring to other downstream classification tasks, especially when finetuning large models or/and transferring to a dataset with few training images. Last, we introduced a simple patch pre-processing stem, which processes patches independently across multiple linear layers interleaved with non-linearities and patch aggregation. It is especially useful when combined with mask-based selfsupervised learning such as BeiT.']",conclusion_chunked," In this paper, we looked at three different topics related to Vision Transformers. First, we investigated a simple but effective way to parallelize them, showing a viable alternative to increase capacity without significantly increasing the working dimensionality. Whether this simple parallel design principle can be applied to other architectures is an exploration left for future work. Second, we considered different fine-tuning strategies and showed that fine-tuning the self-attention layer is sufficient in the context of resolution fine-tuning. This can also be interesting when transferring to other downstream classification tasks, especially when finetuning large models or/and transferring to a dataset with few training images. Last, we introduced a simple patch pre-processing stem, which processes patches independently across multiple linear layers interleaved with non-linearities and patch aggregation. It is especially useful when combined with mask-based selfsupervised learning such as BeiT."
46,Hugo Touvron,"[' In this chapter, we have introduced Grafit, a method to learn image representations at a\nfiner granularity than the one offered by the annotation at training time. Inspired by recent selfsupervised learning approach, we carefully design a joint learning scheme integrating instance\nand coarse-label based classification losses. For the latter one, we exploit a knn strategy but with\na dedicated process to manage the memory both at train-time and for inference at test-time. We propose two original use-cases to deeply evaluate coarse-trained fine-grained testing evaluation, for which Grafit exhibits outstanding performance. It improves the performance for fine-grained category retrieval within a coarsely annotated\ncollection. For on-the-fly kNN classification, Grafit significantly reduces the gap with a network\ntrained with fine labels. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is still +9.5% w.r.t. our own stronger baseline,\neverything being equal otherwise. Grafit also improves transfer learning: our experiments show\nthat our representation discriminates better at a finer granularity. It also translates into better\ntransfer learning performance to fine-grained datasets, outperforming the current state of the art\nwith a more efficient network.']",conclusion_chunked," In this chapter, we have introduced Grafit, a method to learn image representations at a
finer granularity than the one offered by the annotation at training time. Inspired by recent selfsupervised learning approach, we carefully design a joint learning scheme integrating instance
and coarse-label based classification losses. For the latter one, we exploit a knn strategy but with
a dedicated process to manage the memory both at train-time and for inference at test-time. We propose two original use-cases to deeply evaluate coarse-trained fine-grained testing evaluation, for which Grafit exhibits outstanding performance. It improves the performance for fine-grained category retrieval within a coarsely annotated
collection. For on-the-fly kNN classification, Grafit significantly reduces the gap with a network
trained with fine labels. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly
classification on ImageNet. This improvement is still +9.5% w.r.t. our own stronger baseline,
everything being equal otherwise. Grafit also improves transfer learning: our experiments show
that our representation discriminates better at a finer granularity. It also translates into better
transfer learning performance to fine-grained datasets, outperforming the current state of the art
with a more efficient network."
47,Hugo Touvron,"[' In this paper, we introduced a full patch-based ConvNet with no pyramidal structure. We used an attention-based pooling on top of the trunk, akin to the attention mechanism in transformers, which offers visualization properties. Our model is only parametrized by its width and depth, and its training does not require a heavy hyper-parameter search. We demonstrated its interest on several computer vision tasks: classification, segmentation, detection. Limitations: There is no perfect metric for measuring the overall performance of a given neural network architecture [14]. We have provided 4 different metrics but there are probably some aspects that are not considered. Deep and wide models have the same behaviour with respect to FLOPs but the wider models have the advantage to be associated with a lower latency [21, 75]. We have mostly experimented with depth rather than width because deep models consume less memory at inference time, which makes them an appealing choice when dealing with higher resolution images [4], as is the case in segmentation and detection. Broader impact: Large scale deep learning models are effective for many different computer vision applications, but the way they reach their decision is still not yet fully understood. When deploying such machine learning-based systems, there would be a benefit to be able to illustrate their choices in critical applications. We hope that our model, by its simplicity, and by its built-in internal visualization mechanism, may foster this direction of interpretability.']",conclusion_chunked," In this paper, we introduced a full patch-based ConvNet with no pyramidal structure. We used an attention-based pooling on top of the trunk, akin to the attention mechanism in transformers, which offers visualization properties. Our model is only parametrized by its width and depth, and its training does not require a heavy hyper-parameter search. We demonstrated its interest on several computer vision tasks: classification, segmentation, detection. Limitations: There is no perfect metric for measuring the overall performance of a given neural network architecture [14]. We have provided 4 different metrics but there are probably some aspects that are not considered. Deep and wide models have the same behaviour with respect to FLOPs but the wider models have the advantage to be associated with a lower latency [21, 75]. We have mostly experimented with depth rather than width because deep models consume less memory at inference time, which makes them an appealing choice when dealing with higher resolution images [4], as is the case in segmentation and detection. Broader impact: Large scale deep learning models are effective for many different computer vision applications, but the way they reach their decision is still not yet fully understood. When deploying such machine learning-based systems, there would be a benefit to be able to illustrate their choices in critical applications. We hope that our model, by its simplicity, and by its built-in internal visualization mechanism, may foster this direction of interpretability."
48,Hugo Touvron,"[' In this paper, we have introduced DeiT, which are image transformers that\ndo not require very large amount of data to be trained, thanks to improved training and in particular a novel distillation procedure. Convolutional neural networks have optimized, both in terms of architecture and optimization\nduring almost a decade, including through extensive architecture search that\nis prone to overfiting, as it is the case for instance for EfficientNets [51]. For\nDeiT we have started the existing data augmentation and regularization strategies pre-existing for convnets, not introducing any significant architectural beyond our novel distillation token. Therefore it is likely that research on dataaugmentation more adapted or learned for transformers will bring further gains. Therefore, considering our results, where image transformers are on par\nwith convnets already, we believe that they will rapidly become a method of\nchoice considering their lower memory footprint for a given accuracy. We provide an open-source implementation of our method. It is available\nat https://github.com/facebookresearch/deit.']",conclusion_chunked," In this paper, we have introduced DeiT, which are image transformers that
do not require very large amount of data to be trained, thanks to improved training and in particular a novel distillation procedure. Convolutional neural networks have optimized, both in terms of architecture and optimization
during almost a decade, including through extensive architecture search that
is prone to overfiting, as it is the case for instance for EfficientNets [51]. For
DeiT we have started the existing data augmentation and regularization strategies pre-existing for convnets, not introducing any significant architectural beyond our novel distillation token. Therefore it is likely that research on dataaugmentation more adapted or learned for transformers will bring further gains. Therefore, considering our results, where image transformers are on par
with convnets already, we believe that they will rapidly become a method of
choice considering their lower memory footprint for a given accuracy. We provide an open-source implementation of our method. It is available
at https://github.com/facebookresearch/deit."
49,Hugo Touvron,"[' In this paper we have shown that a simple residual architecture, whose residual blocks consist of a one-hidden layer feed-forward network and a linear patch interaction layer, achieves an unexpectedly high performance on ImageNet classification benchmarks, provided that we adopt a modern training strategy such as those recently introduced for transformer-based architectures. Thanks to their simple structure, with linear layers as the main mean of communication between patches, we can vizualize the filters learned by this simple MLP. While some of the layers are similar to convolutional filters, we also observe sparse long-range interactions as early as the second layer of the network. We hope that our model free of spatial priors will contribute to further understanding of what networks with less priors learn, and potentially guide the design choices of future networks without the pyramidal design prior adopted by most convolutional neural networks.']",conclusion_chunked," In this paper we have shown that a simple residual architecture, whose residual blocks consist of a one-hidden layer feed-forward network and a linear patch interaction layer, achieves an unexpectedly high performance on ImageNet classification benchmarks, provided that we adopt a modern training strategy such as those recently introduced for transformer-based architectures. Thanks to their simple structure, with linear layers as the main mean of communication between patches, we can vizualize the filters learned by this simple MLP. While some of the layers are similar to convolutional filters, we also observe sparse long-range interactions as early as the second layer of the network. We hope that our model free of spatial priors will contribute to further understanding of what networks with less priors learn, and potentially guide the design choices of future networks without the pyramidal design prior adopted by most convolutional neural networks."
50,Hugo Touvron,"[' In this paper, we have shown how train deeper transformer-based image classification neural networks when training on Imagenet only. We have also introduced the simple yet effective CaiT architecture designed in the spirit of\nencoder/decoder architectures. Our work further demonstrates that transformer models offer a competitive alternative to the best convolutional neural\nnetworks when considering trade-offs between accuracy and complexity.']",conclusion_chunked," In this paper, we have shown how train deeper transformer-based image classification neural networks when training on Imagenet only. We have also introduced the simple yet effective CaiT architecture designed in the spirit of
encoder/decoder architectures. Our work further demonstrates that transformer models offer a competitive alternative to the best convolutional neural
networks when considering trade-offs between accuracy and complexity."
51,Hugo Touvron,"[' This paper has introduced a procedure to learn a\nneural network that offers a finer granularity than\nthe one provided by the annotation. It improves the\nperformance for fine-grained category retrieval within\na coarsely annotated collection. For on-the-fly kNN\nclassification, Grafit significantly reduces the gap with\na network trained with fine labels. It also translates\ninto better transfer learning to fine-grained datasets,\noutperforming the current state of the art with a more\nefficient network.']",conclusion_chunked," This paper has introduced a procedure to learn a
neural network that offers a finer granularity than
the one provided by the annotation. It improves the
performance for fine-grained category retrieval within
a coarsely annotated collection. For on-the-fly kNN
classification, Grafit significantly reduces the gap with
a network trained with fine labels. It also translates
into better transfer learning to fine-grained datasets,
outperforming the current state of the art with a more
efficient network."
52,Hugo Touvron,"[' Powers of layers consists in iterating a residual block to learns a complex transformation with no direct supervision. On various\ntasks, power of layers gives similar performance to CycleGAN with fewer parameters. The flexibility offered by the common\nembedding space can be used to modulate the strength of a transformation or to compose several transformations. While in most\nexamples the discriminator is only used for training, Powers of layers can also exploit it to adjust the transformation to the input\nimage at inference time.']",conclusion_chunked," Powers of layers consists in iterating a residual block to learns a complex transformation with no direct supervision. On various
tasks, power of layers gives similar performance to CycleGAN with fewer parameters. The flexibility offered by the common
embedding space can be used to modulate the strength of a transformation or to compose several transformations. While in most
examples the discriminator is only used for training, Powers of layers can also exploit it to adjust the transformation to the input
image at inference time."
53,Hugo Touvron,"[' We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network’s pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown\nthat researchers waste resources when both training and testing strong networks at resolution 224 × 224; We introduce\na method that can “fix” these networks post-facto and thus\nimprove their performance. An open-source implementation of our method is available at https://github.com/\nfacebookresearch/FixRes.']",conclusion_chunked," We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network’s pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown
that researchers waste resources when both training and testing strong networks at resolution 224 × 224; We introduce
a method that can “fix” these networks post-facto and thus
improve their performance. An open-source implementation of our method is available at https://github.com/
facebookresearch/FixRes."
54,Hugo Touvron,"[' In this paper, we demonstrate that database effect cannot\nbe properly regressed out if the effect of another confound,\nwhose distribution varies across databases, is not properly\nmodeled. We propose a simple strategy that compensates for\nthe residual variation in position and shape that can appear\nbetween the distributions of data adjusted with the generalized linear model. The benefit of this strategy has been\nhighlighted in the context of a CAD system discriminating\nAD vs healthy subjects. However, the fact that confounds can\nstill be predicted from adjusted data suggests that there is still\nsome room for improvement in the adjustment procedure. The risk of processing data corrupted by several confounding variables is that the adjusted data may still permit the\nprediction of confounds with above chance accuracy. In the\ncontext of a CAD system, confounds that are correlated with\nthe diagnosis may be responsible for ambiguity. To assess the\nreliability of a CAD system, we suggest the following guidelines: (i) test if the confounds are correlated with the target,\n(ii) test if the adjusted data still allow a good prediction of the\nconfounds, (iii) test if the classifier can be misled with new\ntesting data that have not the same distributions of confounds\nthan those of the training set.']",conclusion_chunked," In this paper, we demonstrate that database effect cannot
be properly regressed out if the effect of another confound,
whose distribution varies across databases, is not properly
modeled. We propose a simple strategy that compensates for
the residual variation in position and shape that can appear
between the distributions of data adjusted with the generalized linear model. The benefit of this strategy has been
highlighted in the context of a CAD system discriminating
AD vs healthy subjects. However, the fact that confounds can
still be predicted from adjusted data suggests that there is still
some room for improvement in the adjustment procedure. The risk of processing data corrupted by several confounding variables is that the adjusted data may still permit the
prediction of confounds with above chance accuracy. In the
context of a CAD system, confounds that are correlated with
the diagnosis may be responsible for ambiguity. To assess the
reliability of a CAD system, we suggest the following guidelines: (i) test if the confounds are correlated with the target,
(ii) test if the adjusted data still allow a good prediction of the
confounds, (iii) test if the classifier can be misled with new
testing data that have not the same distributions of confounds
than those of the training set."
55,Hugo Touvron,"[' We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network’s pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown that researchers waste resources when both training and testing strong networks at resolution 224 × 224; We introduce a method that can “fix” these networks post-facto and thus improve their performance. An open-source implementation of our method is available at.']",conclusion_chunked," We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network’s pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown that researchers waste resources when both training and testing strong networks at resolution 224 × 224; We introduce a method that can “fix” these networks post-facto and thus improve their performance. An open-source implementation of our method is available at."
56,Zhiqing Sun,"[' In this paper, we introduce SALMON, a new AI alignment paradigm where a principle-following reward model is trained to effectively and flexibly align language models with human values and intentions. During the RL training stage, by merely adjusting the principles that the reward model follows, we can gain full control over the preferences of the reward model, and subsequently influence the behavior of the RL-trained policy model. This eliminates the traditional reliance on the exhaustive collection of online human preferences. Combined with the SELF-ALIGN technique (Sun et al., 2023b), we build a powerful AI-assistant agent, Dromedary-2, with only six exemplars for in-context learning and 31 human-defined principles. Our self-aligned AI agent significantly surpasses the performance of several state-of-the-art RLHF-trained AI systems in chatbot, reasoning, coding, multilingualism, and truthfulness benchmarks.']",conclusion_chunked," In this paper, we introduce SALMON, a new AI alignment paradigm where a principle-following reward model is trained to effectively and flexibly align language models with human values and intentions. During the RL training stage, by merely adjusting the principles that the reward model follows, we can gain full control over the preferences of the reward model, and subsequently influence the behavior of the RL-trained policy model. This eliminates the traditional reliance on the exhaustive collection of online human preferences. Combined with the SELF-ALIGN technique (Sun et al., 2023b), we build a powerful AI-assistant agent, Dromedary-2, with only six exemplars for in-context learning and 31 human-defined principles. Our self-aligned AI agent significantly surpasses the performance of several state-of-the-art RLHF-trained AI systems in chatbot, reasoning, coding, multilingualism, and truthfulness benchmarks."
57,Zhiqing Sun,"[' We proposed several strategies to tackle the multimodal misalignment problems, particularly for vision language models (VLMs), which often produce text inconsistent with the associated images. First, we enrich GPT-4 generated vision instruction tuning data from LLaVA with existing human-authored image-text pairs. Next, we adopt the Reinforcement Learning from Human Feedback (RLHF) algorithm from the text domain to bridge vision-language gaps, wherein human evaluators discern and mark the more hallucinated output. We train the VLM to optimize against simulated human preferences. Moreover, we introduce the Factually Augmented RLHF, leveraging additional factual information such as image captions to enhance the reward model, countering reward hacking in RLHF, and boosting model performance. For tangible real-world impact assessment, we have devised MMHAL-BENCH, an evaluation benchmark targeting the penalization of hallucination. Remarkably, LLaVA-RLHF, being the first VLM trained with RLHF, shows a notable surge in performance across benchmarks. We opensource our code, and data and hope our findings could help the future development of more reliable and human-aligned LLMs and LMMs.']",conclusion_chunked," We proposed several strategies to tackle the multimodal misalignment problems, particularly for vision language models (VLMs), which often produce text inconsistent with the associated images. First, we enrich GPT-4 generated vision instruction tuning data from LLaVA with existing human-authored image-text pairs. Next, we adopt the Reinforcement Learning from Human Feedback (RLHF) algorithm from the text domain to bridge vision-language gaps, wherein human evaluators discern and mark the more hallucinated output. We train the VLM to optimize against simulated human preferences. Moreover, we introduce the Factually Augmented RLHF, leveraging additional factual information such as image captions to enhance the reward model, countering reward hacking in RLHF, and boosting model performance. For tangible real-world impact assessment, we have devised MMHAL-BENCH, an evaluation benchmark targeting the penalization of hallucination. Remarkably, LLaVA-RLHF, being the first VLM trained with RLHF, shows a notable surge in performance across benchmarks. We opensource our code, and data and hope our findings could help the future development of more reliable and human-aligned LLMs and LMMs."
58,Zhiqing Sun,"[' Models like Alpaca and Vicuna have shown that powerful conversational capabilities can be distilled from existing human-preference-aligned large language models (LLMs), into smaller models. In this paper, we introduce Dromedary , a model for the research community based on principle-driven self-alignment, trained from scratch and requiring very little human annotation. By harnessing the intrinsic knowledge within an LLM, we can define principles that guide how we want an LLM-based AI model to behave, resulting in an AI assistant that not only produces quality interactions but also produces responses that respect the guardrails defined by the model creator. This method represents a distinct direction from RLHF, and it focuses on developing novel alignment techniques for language models from scratch, independent of pre-existing, well-established AI systems. In other words, our approach seeks to explore the potential of aligning AI models in situations where reliance on or access to existing systems may not be feasible or desired. For future work, we propose the following research directions: • Conduct ablation studies on the Dromedary’s 16 self-alignment principles to evaluate the impact of adding or removing specific principles. • Apply Constitutional AI-based self-critique techniques [4] to enhance the performance of Dromedary further. • Perform human evaluations to assess the real-world applicability and effectiveness of SELF-ALIGN. • Investigate better utilization of existing open-source annotation data, such as the 15k original instruction-following data in [10].']",conclusion_chunked," Models like Alpaca and Vicuna have shown that powerful conversational capabilities can be distilled from existing human-preference-aligned large language models (LLMs), into smaller models. In this paper, we introduce Dromedary , a model for the research community based on principle-driven self-alignment, trained from scratch and requiring very little human annotation. By harnessing the intrinsic knowledge within an LLM, we can define principles that guide how we want an LLM-based AI model to behave, resulting in an AI assistant that not only produces quality interactions but also produces responses that respect the guardrails defined by the model creator. This method represents a distinct direction from RLHF, and it focuses on developing novel alignment techniques for language models from scratch, independent of pre-existing, well-established AI systems. In other words, our approach seeks to explore the potential of aligning AI models in situations where reliance on or access to existing systems may not be feasible or desired. For future work, we propose the following research directions: • Conduct ablation studies on the Dromedary’s 16 self-alignment principles to evaluate the impact of adding or removing specific principles. • Apply Constitutional AI-based self-critique techniques [4] to enhance the performance of Dromedary further. • Perform human evaluations to assess the real-world applicability and effectiveness of SELF-ALIGN. • Investigate better utilization of existing open-source annotation data, such as the 15k original instruction-following data in [10]."
59,Zhiqing Sun,"[' We proposed DIFUSCO, a novel graph-based diffusion model for solving NP-complete combinatorial optimization problems. We compared two variants of graph-based diffusion models: one with continuous Gaussian noise and one with discrete Bernoulli noise. We found that the discrete variant performs better than the continuous one. Moreover, we designed a cosine inference schedule that enhances the effectiveness of our model. DIFUSCO achieves state-of-the-art results on TSP and MIS problems, surpassing previous probabilistic NPC solvers in both accuracy and scalability. For future work, we would like to explore the potential of DIFUSCO in solving a broader range of NPC problems, including Mixed Integer Programming (Appendix C). We would also like to explore the use of equivariant graph neural networks (Xu et al., 2021; Hoogeboom et al., 2022) for further improvement of the diffusion models on geometrical NP-complete combinatorial optimization problems such as Euclidean TSP. Finally, we are interested in utilizing (higher-order) accelerated inference techniques for diffusion model based solvers, such as those inspired by the continuous time framework for discrete diffusion (Campbell et al., 2022; Sunet al., 2022).']",conclusion_chunked," We proposed DIFUSCO, a novel graph-based diffusion model for solving NP-complete combinatorial optimization problems. We compared two variants of graph-based diffusion models: one with continuous Gaussian noise and one with discrete Bernoulli noise. We found that the discrete variant performs better than the continuous one. Moreover, we designed a cosine inference schedule that enhances the effectiveness of our model. DIFUSCO achieves state-of-the-art results on TSP and MIS problems, surpassing previous probabilistic NPC solvers in both accuracy and scalability. For future work, we would like to explore the potential of DIFUSCO in solving a broader range of NPC problems, including Mixed Integer Programming (Appendix C). We would also like to explore the use of equivariant graph neural networks (Xu et al., 2021; Hoogeboom et al., 2022) for further improvement of the diffusion models on geometrical NP-complete combinatorial optimization problems such as Euclidean TSP. Finally, we are interested in utilizing (higher-order) accelerated inference techniques for diffusion model based solvers, such as those inspired by the continuous time framework for discrete diffusion (Campbell et al., 2022; Sunet al., 2022)."
60,Zhiqing Sun,"[' In this paper, we propose a novel Temporal Stencil Modeling (TSM) method for solving time-dependent PDEs in conservation form. TSM can be regarded as the temporal generalization of classic finite volume solvers such as WENO (Shu, 2003; Gottlieb et al., 2006) and vanilla neural stencil modeling methods (Kochkov et al., 2021), in that TSM leverages the temporal information from trajectories, instead of only using the latest states, to approximate the (convective) flux more accurately. Our empirical evaluation on 2-D incompressible Navier-Stokes turbulent flow data shows that both the temporal information and its temporal feature encoding scheme are crucial to achieving state-ofthe-art simulation accuracy. We also show that TSM has a strong generalization ability on various out-of-distribution turbulent flows. Finally, we show that the proposed method works well on 1-D Kuramoto–Sivashinsky (KS) equation and 3-D Navier-Stokes. For future work, we plan to evaluate our TSM method with non-periodic boundary conditions. We are also interested in leveraging the Neural Architecture Search (NAS) technique to automatically find better features and neural architectures for solving the Navier-Stokes equation in the TSM framework.']",conclusion_chunked," In this paper, we propose a novel Temporal Stencil Modeling (TSM) method for solving time-dependent PDEs in conservation form. TSM can be regarded as the temporal generalization of classic finite volume solvers such as WENO (Shu, 2003; Gottlieb et al., 2006) and vanilla neural stencil modeling methods (Kochkov et al., 2021), in that TSM leverages the temporal information from trajectories, instead of only using the latest states, to approximate the (convective) flux more accurately. Our empirical evaluation on 2-D incompressible Navier-Stokes turbulent flow data shows that both the temporal information and its temporal feature encoding scheme are crucial to achieving state-ofthe-art simulation accuracy. We also show that TSM has a strong generalization ability on various out-of-distribution turbulent flows. Finally, we show that the proposed method works well on 1-D Kuramoto–Sivashinsky (KS) equation and 3-D Navier-Stokes. For future work, we plan to evaluate our TSM method with non-periodic boundary conditions. We are also interested in leveraging the Neural Architecture Search (NAS) technique to automatically find better features and neural architectures for solving the Navier-Stokes equation in the TSM framework."
61,Zhiqing Sun,"[' n this paper, we propose a novel recitation-augmented generation framework to improve language models’ performance in the closed-book question-answering setting. We hypothesize that for knowledge-intensive NLP tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. In addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. We show promising results over three large language models and across three different closed-book QA datasets, demonstrating the effectiveness of our proposed recite-and-answer approach. One limitation of our method is that updating time-sensitive knowledge for a pure LLM-based method requires training or fine-tuning the LLMs on the new corpus, which can be costly. For future work, we plan to further validate the effectiveness of recitation-augmented generation for other knowledge intensive NLP tasks in the closed-book setting, such as fact checking.']",conclusion_chunked," n this paper, we propose a novel recitation-augmented generation framework to improve language models’ performance in the closed-book question-answering setting. We hypothesize that for knowledge-intensive NLP tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. In addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. We show promising results over three large language models and across three different closed-book QA datasets, demonstrating the effectiveness of our proposed recite-and-answer approach. One limitation of our method is that updating time-sensitive knowledge for a pure LLM-based method requires training or fine-tuning the LLMs on the new corpus, which can be costly. For future work, we plan to further validate the effectiveness of recitation-augmented generation for other knowledge intensive NLP tasks in the closed-book setting, such as fact checking."
62,Zhiqing Sun,"[' In this paper, we address the limitations of ANN-based sparse attention methods and propose the Learning-to-Hash Attention (LHA) as our new solution. Specifically, LHA leverages separate learnable hash functions for queries and keys, respectively, and utilizes kernelized techniques for efficient approximation of attention utilities. The experiments on language modeling, natural language understanding, text classification, and image classification demonstrated the effectiveness of LHA. For future work, we would like to validate the effectiveness of LHA model on much larger language modeling datasets, such as PG-19 (Rae et al., 2019).']",conclusion_chunked," In this paper, we address the limitations of ANN-based sparse attention methods and propose the Learning-to-Hash Attention (LHA) as our new solution. Specifically, LHA leverages separate learnable hash functions for queries and keys, respectively, and utilizes kernelized techniques for efficient approximation of attention utilities. The experiments on language modeling, natural language understanding, text classification, and image classification demonstrated the effectiveness of LHA. For future work, we would like to validate the effectiveness of LHA model on much larger language modeling datasets, such as PG-19 (Rae et al., 2019)."
63,Zhiqing Sun,"[' Aiming to accelerate the training convergence of DETR as well as to improve prediction power in object detection, we present an investigation on the causes of its slow convergence through extensive experiments, and propose two novel solutions, namely TSP-FCOS and TSP-RCNN, which require much less training time and achieve the state-of-the-art detection performance. For future work, we would like to investigate the successful use of sparse attention mechanism [5, 6, 36] for directly modeling the relationship among multi-level features.']",conclusion_chunked," Aiming to accelerate the training convergence of DETR as well as to improve prediction power in object detection, we present an investigation on the causes of its slow convergence through extensive experiments, and propose two novel solutions, namely TSP-FCOS and TSP-RCNN, which require much less training time and achieve the state-of-the-art detection performance. For future work, we would like to investigate the successful use of sparse attention mechanism [5, 6, 36] for directly modeling the relationship among multi-level features."
64,Zhiqing Sun,"[' We have presented MobileBERT which is a taskagnostic compact variant of BERT. Empirical results on popular NLP benchmarks show that MobileBERT is comparable with BERTBASE while being much smaller and faster. MobileBERT can enable various NLP applications7 to be easily deployed on mobile devices. In this paper, we show that 1) it is crucial to keep MobileBERT deep and thin, 2) bottleneck/invertedbottleneck structures enable effective layer-wise knowledge transfer, and 3) progressive knowledge transfer can efficiently train MobileBERT. We believe our findings are generic and can be applied to other model compression problems.']",conclusion_chunked," We have presented MobileBERT which is a taskagnostic compact variant of BERT. Empirical results on popular NLP benchmarks show that MobileBERT is comparable with BERTBASE while being much smaller and faster. MobileBERT can enable various NLP applications7 to be easily deployed on mobile devices. In this paper, we show that 1) it is crucial to keep MobileBERT deep and thin, 2) bottleneck/invertedbottleneck structures enable effective layer-wise knowledge transfer, and 3) progressive knowledge transfer can efficiently train MobileBERT. We believe our findings are generic and can be applied to other model compression problems."
65,Zhiqing Sun,"[' In this paper, we performed an extensive re-examination study of recent neural network based KGC techniques. We find that many such models have issues with their score functions. Combined with inappropriate evaluation protocol, such methods reported inflated performance. Based on our observations, we propose RANDOM evaluation protocol that can clearly distinguish between these affected methods from others. We also strongly encourage the research community to follow the RANDOM evaluation protocol for all KGC evaluation purposes.']",conclusion_chunked," In this paper, we performed an extensive re-examination study of recent neural network based KGC techniques. We find that many such models have issues with their score functions. Combined with inappropriate evaluation protocol, such methods reported inflated performance. Based on our observations, we propose RANDOM evaluation protocol that can clearly distinguish between these affected methods from others. We also strongly encourage the research community to follow the RANDOM evaluation protocol for all KGC evaluation purposes."
66,Zhiqing Sun,"[' This paper proposes a novel EM approach to non-autoregressive conditional sequence generation, which efectively addresses the multi-modality issue in NAR training by iterative optimizing both the teacher AR model and the student NAR model. We also developed a principled plug-and-play decoding method for efficiently removing word duplication in the model’s output. Experimental results on three tasks prove the effectiveness of our approach. For future work, we plan to examine the effectiveness of ourmethod in a broader range of applications, such as text summarization.']",conclusion_chunked," This paper proposes a novel EM approach to non-autoregressive conditional sequence generation, which efectively addresses the multi-modality issue in NAR training by iterative optimizing both the teacher AR model and the student NAR model. We also developed a principled plug-and-play decoding method for efficiently removing word duplication in the model’s output. Experimental results on three tasks prove the effectiveness of our approach. For future work, we plan to examine the effectiveness of ourmethod in a broader range of applications, such as text summarization."
67,Zhiqing Sun,"[' Non-autoregressive sequence models have achieved impressive inference speedup but suffer from decoding inconsistency problem, and thus performs poorly compared to autoregressive sequence models. In this paper, we propose a novel framework to bridge the performance gap between non-autoregressive and autoregressive sequence models. Specifically, we use linear-chain Conditional Random Fields (CRF) to model the co-occurrence relationship between adjacent words during the decoding. We design two effective approximation methods to tackle the issue of the large vocabulary size, and further propose a dynamic transition technique to model positional contexts in the CRF. The results significantly outperform previous non-autoregressive baselines on WMT14 En-De and De-En datasets and achieve comparable performance to the autoregressive counterparts. In the future, we plan to utilize other existing techniques for our NART-CRF models to further bridge the gap between non-autoregressive and autoregressive sequence models. Besides, although the rescoring process is also parallelized, it severely increases the inference latency, as can be seen in Table 2. An additional module that can accurately predict the target length might be useful. As our major contribution in this paper is to model richer structural dependency in the non-autoregressive decoder, we leave this for future work.']",conclusion_chunked," Non-autoregressive sequence models have achieved impressive inference speedup but suffer from decoding inconsistency problem, and thus performs poorly compared to autoregressive sequence models. In this paper, we propose a novel framework to bridge the performance gap between non-autoregressive and autoregressive sequence models. Specifically, we use linear-chain Conditional Random Fields (CRF) to model the co-occurrence relationship between adjacent words during the decoding. We design two effective approximation methods to tackle the issue of the large vocabulary size, and further propose a dynamic transition technique to model positional contexts in the CRF. The results significantly outperform previous non-autoregressive baselines on WMT14 En-De and De-En datasets and achieve comparable performance to the autoregressive counterparts. In the future, we plan to utilize other existing techniques for our NART-CRF models to further bridge the gap between non-autoregressive and autoregressive sequence models. Besides, although the rescoring process is also parallelized, it severely increases the inference latency, as can be seen in Table 2. An additional module that can accurately predict the target length might be useful. As our major contribution in this paper is to model richer structural dependency in the non-autoregressive decoder, we leave this for future work."
68,Zhiqing Sun,"[' In this paper, we propose an end-to-end method called DivGraphPointer for extracting diverse keyphrases. It formulates documents as graphs and applies graph convolutional networks for encoding the graphs, which efficiently capture the document-level wordsalience by modeling both the short- and long-range dependency between words in documents and aggregate the information of multiple appearances of identical words. To avoid extracting similar keyphrases, a diversified pointer network is proposed to generate diverse keyphrases from the nodes of the graphs. Experiments on five benchmark data sets show that our proposed DivGraphPointer model achieves state-of-the-art performance, significantly outperforming existing state-of-the-art supervised and unsuper-vised methods. Our research can be extended in many directions. To begin with, currently our diversified pointer network decoders extract keyphrase in an auto-regressive fashion. We could further leverage reinforcement learning to address the exposure bias as well as consequent error propagation problem in the sequential generation process. Moreover, our graph convolutional network encoder aggregates the word relation information through manually designed edge weights based on proximity at present. We would like to further explore utilizing graph attention networks (GATs) [41 ] to dynamically capture correlation between words in word graph. Finally, utilizing linguistic information when constructing edges in word graphs to ease keyphrase extraction is also an interesting future direction.']",conclusion_chunked," In this paper, we propose an end-to-end method called DivGraphPointer for extracting diverse keyphrases. It formulates documents as graphs and applies graph convolutional networks for encoding the graphs, which efficiently capture the document-level wordsalience by modeling both the short- and long-range dependency between words in documents and aggregate the information of multiple appearances of identical words. To avoid extracting similar keyphrases, a diversified pointer network is proposed to generate diverse keyphrases from the nodes of the graphs. Experiments on five benchmark data sets show that our proposed DivGraphPointer model achieves state-of-the-art performance, significantly outperforming existing state-of-the-art supervised and unsuper-vised methods. Our research can be extended in many directions. To begin with, currently our diversified pointer network decoders extract keyphrase in an auto-regressive fashion. We could further leverage reinforcement learning to address the exposure bias as well as consequent error propagation problem in the sequential generation process. Moreover, our graph convolutional network encoder aggregates the word relation information through manually designed edge weights based on proximity at present. We would like to further explore utilizing graph attention networks (GATs) [41 ] to dynamically capture correlation between words in word graph. Finally, utilizing linguistic information when constructing edges in word graphs to ease keyphrase extraction is also an interesting future direction."
69,Zhiqing Sun,"[' We have proposed a new knowledge graph embedding method called RotatE, which represents entities as complex vectors and relations as rotations in complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Our experimental results show that the RotatE model outperforms all existing state-of-the-art models on four large-scale benchmarks. Moreover, RotatE also achieves state-of-the-art results on a benchmark that is explicitly designed for composition pattern inference and modeling. A deep investigation into RotatE relation embeddings shows that the three relation patterns are implicitly represented in the relation embeddings. In the future, we plan to evaluate the RotatE model on more datasets and leverage a probabilistic framework to model the uncertainties of entities and relations.']",conclusion_chunked," We have proposed a new knowledge graph embedding method called RotatE, which represents entities as complex vectors and relations as rotations in complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Our experimental results show that the RotatE model outperforms all existing state-of-the-art models on four large-scale benchmarks. Moreover, RotatE also achieves state-of-the-art results on a benchmark that is explicitly designed for composition pattern inference and modeling. A deep investigation into RotatE relation embeddings shows that the three relation patterns are implicitly represented in the relation embeddings. In the future, we plan to evaluate the RotatE model on more datasets and leverage a probabilistic framework to model the uncertainties of entities and relations."
70,Zhiqing Sun,"[' In this paper, we proposed a neural generative model for fully unsupervised Chinese word seg-mentation (CWS). To the best of knowledge, this is the first neural model for CWS. Our segmental language model is an intuitive generalization of vanilla neural language models that directly modeling the segmental nature of Chinese. Experimental results show that our models achieve competitive performance to the previous state-of-the-art statistical models on four datasets from SIGHAN 2005. We also show the improvement of incorporating ad hoc guidelines into our segmental language models. Our future work may include the following two directions. In this work, we only consider the sequential segmental language modeling. In the future, we are interested in build a hierarchical neural language model like the Pitman-Yor process. Like vanilla language models, the segmental language models can also provide useful information for semi-supervised learning tasks. It would also be interesting to explore our models in the semi-supervised schemes.']",conclusion_chunked," In this paper, we proposed a neural generative model for fully unsupervised Chinese word seg-mentation (CWS). To the best of knowledge, this is the first neural model for CWS. Our segmental language model is an intuitive generalization of vanilla neural language models that directly modeling the segmental nature of Chinese. Experimental results show that our models achieve competitive performance to the previous state-of-the-art statistical models on four datasets from SIGHAN 2005. We also show the improvement of incorporating ad hoc guidelines into our segmental language models. Our future work may include the following two directions. In this work, we only consider the sequential segmental language modeling. In the future, we are interested in build a hierarchical neural language model like the Pitman-Yor process. Like vanilla language models, the segmental language models can also provide useful information for semi-supervised learning tasks. It would also be interesting to explore our models in the semi-supervised schemes."
71,Timo Schick,"[' We have shown that providing task descriptions to pretrained language models can be combined with standard supervised training. Our proposed method, PET, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks. We finetune models for all pattern-verbalizer pairs and use them to create large annotated datasets on which standard classifiers can be trained. When the initial amount of training data is limited, PET gives large improvements over standard supervised training and strong semi-supervised approaches.']",conclusion_chunked," We have shown that providing task descriptions to pretrained language models can be combined with standard supervised training. Our proposed method, PET, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks. We finetune models for all pattern-verbalizer pairs and use them to create large annotated datasets on which standard classifiers can be trained. When the initial amount of training data is limited, PET gives large improvements over standard supervised training and strong semi-supervised approaches."
72,Timo Schick,"[' We have proposed a simple yet effective modification of PET, enabling us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of PET combined with ALBERT: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates, and the underlying LM itself. We have shown that using PET, it is possible to achieve few-shot text classification performance similar to GPT-3 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This not only lowers financial cost, but above all reduces environmental impact immensely and leads to a much smaller carbon footprint. We see this as an important contribution to achieving the goal of an environmentally more friendly NLP. To enable comparisons with our work, we make our code, models and datasets publicly available. For future work, it would be interesting to see whether PET also works for generative tasks when combined with generative LMs and whether further improvements are possible in multi-task settings.']",conclusion_chunked," We have proposed a simple yet effective modification of PET, enabling us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of PET combined with ALBERT: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates, and the underlying LM itself. We have shown that using PET, it is possible to achieve few-shot text classification performance similar to GPT-3 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This not only lowers financial cost, but above all reduces environmental impact immensely and leads to a much smaller carbon footprint. We see this as an important contribution to achieving the goal of an environmentally more friendly NLP. To enable comparisons with our work, we make our code, models and datasets publicly available. For future work, it would be interesting to see whether PET also works for generative tasks when combined with generative LMs and whether further improvements are possible in multi-task settings."
73,Timo Schick,"[' We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they reduce perplexity on future tokens. Toolformer considerably improves zero-shot performance of a 6.7B parameter GPT-J model, enabling it to even outperform a much larger GPT-3 model on a range of different downstream tasks.']",conclusion_chunked," We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they reduce perplexity on future tokens. Toolformer considerably improves zero-shot performance of a 6.7B parameter GPT-J model, enabling it to even outperform a much larger GPT-3 model on a range of different downstream tasks."
74,Timo Schick,"[' In this paper, we have shown that large language models are capable of performing self-diagnosis, i.e., of investigating their own outputs with regards to the presence of undesirable attributes using only their internal knowledge and textual descriptions. Based on this finding, we have proposed a decoding algorithm that reduces the probability of a model generating biased text by comparing the original probability of a token with its probability if undesired behavior is explicitly encouraged. As our evaluation is limited to two English datasets covering only a small portion of potentially undesired behaviors in an imperfect fashion, it is important to extend our analysis to other kinds of behaviors and biases, languages, benchmarks and models. It is clear that self-diagnosis and self-debiasing only reduce and do not eliminate corpus-based bias. For this reason, they are not a viable path towards bias-free models if used in isolation. However, we hope that future work can leverage our proposals, e.g., by combining them with complementary models or by extending them to build stronger debiasing solutions.']",conclusion_chunked," In this paper, we have shown that large language models are capable of performing self-diagnosis, i.e., of investigating their own outputs with regards to the presence of undesirable attributes using only their internal knowledge and textual descriptions. Based on this finding, we have proposed a decoding algorithm that reduces the probability of a model generating biased text by comparing the original probability of a token with its probability if undesired behavior is explicitly encouraged. As our evaluation is limited to two English datasets covering only a small portion of potentially undesired behaviors in an imperfect fashion, it is important to extend our analysis to other kinds of behaviors and biases, languages, benchmarks and models. It is clear that self-diagnosis and self-debiasing only reduce and do not eliminate corpus-based bias. For this reason, they are not a viable path towards bias-free models if used in isolation. However, we hope that future work can leverage our proposals, e.g., by combining them with complementary models or by extending them to build stronger debiasing solutions."
75,Timo Schick,"[' We have devised PETAL, a simple approach that enriches PET with the ability to automatically map labels to words. Qualitative and quantitative analysis shows that our approach is able to identify words that are suitable to represent labels with as little as 50 examples and almost matches the performance of hand-crafted mappings for some tasks. For future work, it would be interesting to see whether the patterns required by PET can similarly be obtained in an automated fashion.']",conclusion_chunked," We have devised PETAL, a simple approach that enriches PET with the ability to automatically map labels to words. Qualitative and quantitative analysis shows that our approach is able to identify words that are suitable to represent labels with as little as 50 examples and almost matches the performance of hand-crafted mappings for some tasks. For future work, it would be interesting to see whether the patterns required by PET can similarly be obtained in an automated fashion."
76,Timo Schick,"[' We have introduced DINO, a method for using large PLMs to generate entire datasets of labeled sentence pairs from scratch, requiring no labeled data and no parameter updates. This is achieved by providing instructions in natural language, combined with the self-debiasing method of Schick et al. (2021). With appropriate measures for handling noisy data, models trained on datasets generated with DINO achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with DINO can further be reduced, e.g., by using different sets of instructions (Jiang et al., 2020; Schick and Schütze, 2021a) or by supplementing our pipeline with some additional filtering steps.']",conclusion_chunked," We have introduced DINO, a method for using large PLMs to generate entire datasets of labeled sentence pairs from scratch, requiring no labeled data and no parameter updates. This is achieved by providing instructions in natural language, combined with the self-debiasing method of Schick et al. (2021). With appropriate measures for handling noisy data, models trained on datasets generated with DINO achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with DINO can further be reduced, e.g., by using different sets of instructions (Jiang et al., 2020; Schick and Schütze, 2021a) or by supplementing our pipeline with some additional filtering steps."
77,Timo Schick,"[' We have introduced WNLaMPro, a new dataset that allows us to explicitly investigate the ability of language models to understand rare words. Using this dataset, we have shown that BERT struggles with words if they are too rare. To address this problem, we proposed to apply Attentive Mimicking (AM). For AM to work, we introduced one-token approximation (OTA), an effective method to obtain “single-token” embeddings for multi-token words. Using this method, we showed that AM is able to substantially improve BERT’s understanding of rare words. Future work might investigate whether more complex architectures than AM can bring further benefit to deep language models; it would also be interesting to see whether training AM on a larger corpus – such as the one used for training BERT by Devlin et al. (2019) – is beneficial. Furthermore, it would be interesting to see the impact of integrating AM on downstream tasks.']",conclusion_chunked," We have introduced WNLaMPro, a new dataset that allows us to explicitly investigate the ability of language models to understand rare words. Using this dataset, we have shown that BERT struggles with words if they are too rare. To address this problem, we proposed to apply Attentive Mimicking (AM). For AM to work, we introduced one-token approximation (OTA), an effective method to obtain “single-token” embeddings for multi-token words. Using this method, we showed that AM is able to substantially improve BERT’s understanding of rare words. Future work might investigate whether more complex architectures than AM can bring further benefit to deep language models; it would also be interesting to see whether training AM on a larger corpus – such as the one used for training BERT by Devlin et al. (2019) – is beneficial. Furthermore, it would be interesting to see the impact of integrating AM on downstream tasks."
78,Timo Schick,"[' We have shown how Pattern-Exploiting Training (PET) can be transferred to text generation tasks by (i) introducing the concept of decoder prefixes and (ii) combining patterns through knowledge distillation where target sequences are generated from randomly chosen patterns. With these modifications, a pretrained PEGASUS model finetuned with PET clearly outperforms regular finetuning in few-shot settings. For future work, it would be interesting to see whether it is also possible to make pretrained language models understand more complex task descriptions than the simple prompts we have used, especially as some concurrent work (Weller et al., 2020; Efrat and Levy, 2020) suggests this might not be the case.']",conclusion_chunked," We have shown how Pattern-Exploiting Training (PET) can be transferred to text generation tasks by (i) introducing the concept of decoder prefixes and (ii) combining patterns through knowledge distillation where target sequences are generated from randomly chosen patterns. With these modifications, a pretrained PEGASUS model finetuned with PET clearly outperforms regular finetuning in few-shot settings. For future work, it would be interesting to see whether it is also possible to make pretrained language models understand more complex task descriptions than the simple prompts we have used, especially as some concurrent work (Weller et al., 2020; Efrat and Levy, 2020) suggests this might not be the case."
79,Timo Schick,"[' We investigated the ability of pretrained language models to make use of simple instructions with the aim of enabling more data-efficient text generation. We identified three major challenges: enabling language models to make good use of the instructions provided, ensuring that the instructions are useful and preventing overfitting. We tackle these in our proposed approach, GENPET, by (i) introducing the concept of decoder prefixes, (ii) combining instructions through knowledge distillation where target sequences are generated with probabilistically sampled instructions and (iii) making use of unsupervised scoring and joint training. A pretrained PEGASUS model finetuned with GENPET clearly outperforms regular finetuning in few-shot settings.']",conclusion_chunked," We investigated the ability of pretrained language models to make use of simple instructions with the aim of enabling more data-efficient text generation. We identified three major challenges: enabling language models to make good use of the instructions provided, ensuring that the instructions are useful and preventing overfitting. We tackle these in our proposed approach, GENPET, by (i) introducing the concept of decoder prefixes, (ii) combining instructions through knowledge distillation where target sequences are generated with probabilistically sampled instructions and (iii) making use of unsupervised scoring and joint training. A pretrained PEGASUS model finetuned with GENPET clearly outperforms regular finetuning in few-shot settings."
80,Timo Schick,"[' We have introduced PEER, a language model that can act as a writing assistant by following plans to perform a variety of different textual edits, ranging from syntactic and stylistic edits to changing the meaning of a text by removing, updating or adding information. Through extensive experiments, we have shown that training variants of PEER capable of infilling various parts of the editing process enables it to perform edits in different domains, makes it better at following instructions and improves its ability to cite and quote from relevant documents.']",conclusion_chunked," We have introduced PEER, a language model that can act as a writing assistant by following plans to perform a variety of different textual edits, ranging from syntactic and stylistic edits to changing the meaning of a text by removing, updating or adding information. Through extensive experiments, we have shown that training variants of PEER capable of infilling various parts of the editing process enables it to perform edits in different domains, makes it better at following instructions and improves its ability to cite and quote from relevant documents."
81,Timo Schick,"[' We have introduced attentive mimicking (AM) and showed that attending to informative and reliable contexts improves representations of rare and medium-frequency words for a diverse set of evaluations. In future work, one might investigate whether attention mechanisms on the word level (cf. Ling et al., 2015) can further improve the model’s performance. Furthermore, it would be interesting to investigate whether the proposed architecture is also beneficial for languages typologically different from English, e.g., morphologically rich languages.']",conclusion_chunked," We have introduced attentive mimicking (AM) and showed that attending to informative and reliable contexts improves representations of rare and medium-frequency words for a diverse set of evaluations. In future work, one might investigate whether attention mechanisms on the word level (cf. Ling et al., 2015) can further improve the model’s performance. Furthermore, it would be interesting to investigate whether the proposed architecture is also beneficial for languages typologically different from English, e.g., morphologically rich languages."
82,Timo Schick,"[' We have introduced BERTRAM, a novel architecture for inducing high-quality representations for rare words in BERT’s and RoBERTa’s embedding spaces. This is achieved by employing a powerful pretrained language model and deeply integrating surface-form and context information. By replacing important words with rare synonyms, we created downstream task datasets that are more challenging and support the evaluation of NLP models on the task of understanding rare words, a capability that human speakers have. On all of these datasets, BERTRAM improves over standard BERT and RoBERTa, demonstrating the usefulness of our method. Our analysis showed that BERTRAM is beneficial not only for rare words (our main target in this paper), but also for frequent words. In future work, we want to investigate BERTRAM’s potential benefits for such frequent words. Furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level CNN similar to the one of Kim et al. (2016) – to balance out the potency of BERTRAM’s form and context parts.']",conclusion_chunked," We have introduced BERTRAM, a novel architecture for inducing high-quality representations for rare words in BERT’s and RoBERTa’s embedding spaces. This is achieved by employing a powerful pretrained language model and deeply integrating surface-form and context information. By replacing important words with rare synonyms, we created downstream task datasets that are more challenging and support the evaluation of NLP models on the task of understanding rare words, a capability that human speakers have. On all of these datasets, BERTRAM improves over standard BERT and RoBERTa, demonstrating the usefulness of our method. Our analysis showed that BERTRAM is beneficial not only for rare words (our main target in this paper), but also for frequent words. In future work, we want to investigate BERTRAM’s potential benefits for such frequent words. Furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level CNN similar to the one of Kim et al. (2016) – to balance out the potency of BERTRAM’s form and context parts."
83,Timo Schick,"[' In light of recent work casting doubt on the performance of prompt-based approaches in true few-shot settings (Perez et al., 2021), we have conducted an extensive study of PET. In a controlled environment, we found that manually designed instructions consistently outperform null prompts, with Q&Astyle prompts performing best (Q1, Q2). Across different tasks, models and training set sizes, PET consistently outperforms even the best individual prompt (Q1, Q2). We have also shown that PET is robust to uninformative prompts and to different choices of hyperparameters (Q3, Q5), that as little as four prompts are sufficient to reach good performance (Q4), and that synthetic examples can be used to replace large amounts of unlabeled data (Q6). On the basis of these insights, we applied PET to a benchmark of real-world tasks, where it achieves near-human performance for 7 out of 11 tasks without any tuning on a development set, demonstrating the power of instruction-based approaches in true few-shot settings.']",conclusion_chunked," In light of recent work casting doubt on the performance of prompt-based approaches in true few-shot settings (Perez et al., 2021), we have conducted an extensive study of PET. In a controlled environment, we found that manually designed instructions consistently outperform null prompts, with Q&Astyle prompts performing best (Q1, Q2). Across different tasks, models and training set sizes, PET consistently outperforms even the best individual prompt (Q1, Q2). We have also shown that PET is robust to uninformative prompts and to different choices of hyperparameters (Q3, Q5), that as little as four prompts are sufficient to reach good performance (Q4), and that synthetic examples can be used to replace large amounts of unlabeled data (Q6). On the basis of these insights, we applied PET to a benchmark of real-world tasks, where it achieves near-human performance for 7 out of 11 tasks without any tuning on a development set, demonstrating the power of instruction-based approaches in true few-shot settings."
84,Timo Schick,"[' We have presented a model that is capable of inferring\nhigh-quality representations for novel words by processing\nboth the word’s internal structure and words in its context. This is done by intelligently combining an embedding based\non n-grams with an embedding obtained from averaging\nover all context words. Our algorithm can be trained from\nand combined with any preexisting word embedding model. On both the Definitional Nonce dataset and the Contextual Rare Words dataset, our model outperforms all previous ap-\nproaches to learning embeddings of rare words by a large\nmargin, even beating the embedding algorithm it was trained\nfrom on the latter dataset. Careful analysis of our combined\nmodel showed that in many cases, it is able to effectively\nbalance out the influences of both embeddings it is com-\nposed of, allowing it to greatly improve upon representations\nthat are either purely surface-form-based or purely context-\nbased. By providing a development set that complements the\nCRW dataset, we hope to further spur research in the area of\n“few-shot learning” for word embeddings. While we showed that a context-dependent combination\nof surface-form and context embeddings substantially im-\nproves the model’s performance on the Definitional Nonce\ntask, results on the Contextual Rare Words dataset indicate\nthat there is still room for further enhancement.', 'This could\npotentially be achieved by incorporating the number and in-\nformativeness of the available contexts into the composition\nfunction; i.e., the gate would not only be conditioned on\nthe embeddings, but on richer information about the context\nsentences. It would also be interesting to investigate whether\nour model profits from using more complex ways than aver-\naging to obtain surface-form and context embeddings, re-\nspectively. For example, one might introduce weights for\nn-grams and words depending on their contexts (i.e. the\nn-grams or words surrounding them). For scenarios in which\nnot just one, but multiple contexts are available to infer a\nword’s embedding, a promising extension of our model is\nto weight the influence of each context based on its “defi-\nnitional quality”; a similar modification was also proposed\nby Herbelot and Baroni (2017) for their Nonce2Vec model. Yet another interesting approach would be to integrate rela-\ntive position information into our model. This could be done\nsimilar to Shaw, Uszkoreit, and Vaswani (2018) by addition-\nally learning position embeddings and weighting the influ-\nence of context words based on those embeddings.']",conclusion_chunked," We have presented a model that is capable of inferring
high-quality representations for novel words by processing
both the word’s internal structure and words in its context. This is done by intelligently combining an embedding based
on n-grams with an embedding obtained from averaging
over all context words. Our algorithm can be trained from
and combined with any preexisting word embedding model. On both the Definitional Nonce dataset and the Contextual Rare Words dataset, our model outperforms all previous ap-
proaches to learning embeddings of rare words by a large
margin, even beating the embedding algorithm it was trained
from on the latter dataset. Careful analysis of our combined
model showed that in many cases, it is able to effectively
balance out the influences of both embeddings it is com-
posed of, allowing it to greatly improve upon representations
that are either purely surface-form-based or purely context-
based. By providing a development set that complements the
CRW dataset, we hope to further spur research in the area of
“few-shot learning” for word embeddings. While we showed that a context-dependent combination
of surface-form and context embeddings substantially im-
proves the model’s performance on the Definitional Nonce
task, results on the Contextual Rare Words dataset indicate
that there is still room for further enhancement."
85,Timo Schick,"[' We have presented a model that is capable of inferring\nhigh-quality representations for novel words by processing\nboth the word’s internal structure and words in its context. This is done by intelligently combining an embedding based\non n-grams with an embedding obtained from averaging\nover all context words. Our algorithm can be trained from\nand combined with any preexisting word embedding model. On both the Definitional Nonce dataset and the Contextual Rare Words dataset, our model outperforms all previous ap-\nproaches to learning embeddings of rare words by a large\nmargin, even beating the embedding algorithm it was trained\nfrom on the latter dataset. Careful analysis of our combined\nmodel showed that in many cases, it is able to effectively\nbalance out the influences of both embeddings it is com-\nposed of, allowing it to greatly improve upon representations\nthat are either purely surface-form-based or purely context-\nbased. By providing a development set that complements the\nCRW dataset, we hope to further spur research in the area of\n“few-shot learning” for word embeddings. While we showed that a context-dependent combination\nof surface-form and context embeddings substantially im-\nproves the model’s performance on the Definitional Nonce\ntask, results on the Contextual Rare Words dataset indicate\nthat there is still room for further enhancement.', 'This could\npotentially be achieved by incorporating the number and in-\nformativeness of the available contexts into the composition\nfunction; i.e., the gate would not only be conditioned on\nthe embeddings, but on richer information about the context\nsentences. It would also be interesting to investigate whether\nour model profits from using more complex ways than aver-\naging to obtain surface-form and context embeddings, re-\nspectively. For example, one might introduce weights for\nn-grams and words depending on their contexts (i.e. the\nn-grams or words surrounding them). For scenarios in which\nnot just one, but multiple contexts are available to infer a\nword’s embedding, a promising extension of our model is\nto weight the influence of each context based on its “defi-\nnitional quality”; a similar modification was also proposed\nby Herbelot and Baroni (2017) for their Nonce2Vec model. Yet another interesting approach would be to integrate rela-\ntive position information into our model. This could be done\nsimilar to Shaw, Uszkoreit, and Vaswani (2018) by addition-\nally learning position embeddings and weighting the influ-\nence of context words based on those embeddings.']",conclusion_chunked,"This could
potentially be achieved by incorporating the number and in-
formativeness of the available contexts into the composition
function; i.e., the gate would not only be conditioned on
the embeddings, but on richer information about the context
sentences. It would also be interesting to investigate whether
our model profits from using more complex ways than aver-
aging to obtain surface-form and context embeddings, re-
spectively. For example, one might introduce weights for
n-grams and words depending on their contexts (i.e. the
n-grams or words surrounding them). For scenarios in which
not just one, but multiple contexts are available to infer a
word’s embedding, a promising extension of our model is
to weight the influence of each context based on its “defi-
nitional quality”; a similar modification was also proposed
by Herbelot and Baroni (2017) for their Nonce2Vec model. Yet another interesting approach would be to integrate rela-
tive position information into our model. This could be done
similar to Shaw, Uszkoreit, and Vaswani (2018) by addition-
ally learning position embeddings and weighting the influ-
ence of context words based on those embeddings."
86,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked," We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences
can easily be inferred through application of the yield function. We chose the principle
component of our approach to be the transition system SAMR, whose set of transitions
TAMR defines how the transformation from AMR graphs to suitable trees can be per-
formed. Some transitions contained within this set, such as Merge, Swap and Delete,
have an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and
defined the score of a transition sequence to be a linear combination of the probabilities
of all its transitions and the probability assigned to the resulting sentence by a language
model. We approximated these probabilities using maximum entropy models that were
trained with a set of gold transitions extracted from a large corpus of AMR graphs and
corresponding realizations."
87,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked,"As an exhaustive search for the highest-scoring transition
sequence given some input would be far too time-consuming, we developed an algorithm
that approximates this sequence in two phases: In a first phase, only transitions from
a subset Trestr of TAMR are greedily applied without taking the language model into
consideration; in a second phase, the output of this first phase is processed bottom-
up, considering multiple partial transition sequences at each step and factoring in the
language model. Through parametrized pruning, we restricted the number of sequences
to be considered, allowing us to find a good balance between required time and quality
of the generated sentences. We introduced the concepts of syntactic annotations and
default realizations to help our system decide which transition to apply next. To further
improve our results, we defined some postprocessing steps – such as the insertion of
punctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we
obtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,
the second best result reported so far and the best without using parsed sentences
from an external source such as Gigaword (LDC2011T07) as additional training data."
88,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked,"This result strongly suggests that our transition-based transformation of AMR graphs
into ordered tree structures is indeed quite a promising approach for the AMR-to-text
generation task. Throughout this work, we have highlighted a number of ways in which the results
obtained by our system may further be improved upon. As outlined in Section 6, one
promising way that could easily be implemented, but would require access to Gigaword,
would be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences
with AMR graphs using a parser to augment the number of available training data; as
pointed out in Section 6, it is reasonable to assume that implementing this idea would
have a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of
Merge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be
merged. In this context, one may also investigate whether the generator could further
be tweaked by revising other classes of transitions."
89,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked,"Of course, such a revision does not
have to be limited to the formal definitions of the transitions themselves, but may also
be extended to the extraction of gold transitions from a training corpus as done by the
oracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training
of our maximum entropy models, one could of course also try to improve our generator’s
output by adding new features extracted from the given contexts. In addition, it should
be investigated whether the conditional probability P (t | c) of a transition t given a
configuration c and the various conditional probabilities of syntactic annotations can
be predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network
architectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic
neural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps
introduced in Section 4.4."
90,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked,"For instance, the assignment of punctuation marks could be
refined – or even be integrated into the actual transition system – as the current output
of punctuation marks by our generator shows some room for improvement, especially
with respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the
current implementation in order to make it more resource-friendly and time-efficient;
as outlined in Section 6, the latter could be achieved through parallelization. A time-
optimized implementation may also lead to better results in terms of Bleu score, as it
would allow us to both drop some of the transition constraints introduced in Section 5.1
and increase the maximum values allowed for performance-relevant hyperparameters
used by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed
in Section 1, in fact transferable to other languages. As indicated in Section 4.1, this
would require us to revise the concept of syntactic annotations to properly reflect the
linguistic peculiarities of the considered language. Unfortunately, however, such an
investigation is not feasible at present, as no sufficiently large AMR corpus is available
for any other language than English."
