,Author,Abstract,GPT_prompt\r\n0,Aman Madaan,"Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%.2","Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%.2 - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n1,Aman Madaan,"Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by \xe2\x88\xbc20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.","Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by \xe2\x88\xbc20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n2,Aman Madaan,"Machine learning systems typically apply the\nsame model to both easy and tough cases. This is\nin stark contrast with humans, who tend to evoke\neither fast (instinctive) or slow (analytical) thinking process, depending on the difficulty of the\nproblem\xe2\x80\x94a property called the dual-process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual-process\ntheory of mind. Depending on the difficulty of\ngraph completion at the current step, the system either calls a FAST (weaker) module or a\nSLOW (stronger) module for the task. These modules have identical architectures, but vary in the\nnumber of parameters and consequently differ\nin generative power. Experiments on real-world\ngraphs show that FLOWGEN can successfully generate graphs similar to those generated by a single\nlarge model, while being up to 2x faster.\n","Machine learning systems typically apply the\nsame model to both easy and tough cases. This is\nin stark contrast with humans, who tend to evoke\neither fast (instinctive) or slow (analytical) thinking process, depending on the difficulty of the\nproblem\xe2\x80\x94a property called the dual-process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual-process\ntheory of mind. Depending on the difficulty of\ngraph completion at the current step, the system either calls a FAST (weaker) module or a\nSLOW (stronger) module for the task. These modules have identical architectures, but vary in the\nnumber of parameters and consequently differ\nin generative power. Experiments on real-world\ngraphs show that FLOWGEN can successfully generate graphs similar to those generated by a single\nlarge model, while being up to 2x faster.\n - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n3,Aman Madaan,"We study a novel task of numerical relation extraction with\nthe goal of extracting relations where one of the arguments\nis a number or a quantity (e.g., atomic number(Aluminium,\n13), inflation rate(India, 10.9%)). This task presents peculiar\nchallenges not found in standard Information Extraction (IE),\nsuch as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: (1) NumberRule, a rule based extractor, and (2) NumberTron, a probabilistic graphical model. We find that both\nsystems dramatically outperform MultiR, a state-of-the-art\nnon-numerical IE model, obtaining up to 25 points F-score\nimprovement.","We study a novel task of numerical relation extraction with\nthe goal of extracting relations where one of the arguments\nis a number or a quantity (e.g., atomic number(Aluminium,\n13), inflation rate(India, 10.9%)). This task presents peculiar\nchallenges not found in standard Information Extraction (IE),\nsuch as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: (1) NumberRule, a rule based extractor, and (2) NumberTron, a probabilistic graphical model. We find that both\nsystems dramatically outperform MultiR, a state-of-the-art\nnon-numerical IE model, obtaining up to 25 points F-score\nimprovement. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n4,Hugo Touvron,"Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers\nhas been little studied so far. In this work, we build and optimize deeper\ntransformer networks for image classification. In particular, we investigate\nthe interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models\nwhose performance does not saturate early with more depth, for instance\nwe obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters.\nMoreover, our best model establishes the new state of the art on Imagenet\nwith Reassessed labels and Imagenet-V2 / match frequency, in the setting\nwith no additional training data. We share our code and models.","Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers\nhas been little studied so far. In this work, we build and optimize deeper\ntransformer networks for image classification. In particular, we investigate\nthe interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models\nwhose performance does not saturate early with more depth, for instance\nwe obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters.\nMoreover, our best model establishes the new state of the art on Imagenet\nwith Reassessed labels and Imagenet-V2 / match frequency, in the setting\nwith no additional training data. We share our code and models. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n5,Hugo Touvron,"We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising,\ndeblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a\nresidual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training\nschedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number\nof weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with\nthe number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance.\nExperimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance\nof our model is comparable or better than CycleGAN with significantly fewer parameters.","We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising,\ndeblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a\nresidual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training\nschedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number\nof weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with\nthe number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance.\nExperimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance\nof our model is comparable or better than CycleGAN with significantly fewer parameters. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n6,Hugo Touvron,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n7,Hugo Touvron,"The growing availability of large neuroimaging databases\noffers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these\ndatabases may be prone to several sources of variability (e.g.,\nage, gender, acquisition parameters,...). These nuisance variables can hamper the performance of a classification method\nand can even lead to misinterpret its behavior. We focus in\nthis paper on how to account for data coming from different\ndatabases. First, we present experiments on simulated data\nthat illustrate how interactions with other confounds such as\nage can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust\ndata and evaluate them in the real scenario of a ComputerAided Diagnosis (CAD) system that discriminates healthy\nfrom Alzheimer\xe2\x80\x99s Disease (AD) subjects based on volumetric\ncharacteristics derived from structural MRI.","The growing availability of large neuroimaging databases\noffers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these\ndatabases may be prone to several sources of variability (e.g.,\nage, gender, acquisition parameters,...). These nuisance variables can hamper the performance of a classification method\nand can even lead to misinterpret its behavior. We focus in\nthis paper on how to account for data coming from different\ndatabases. First, we present experiments on simulated data\nthat illustrate how interactions with other confounds such as\nage can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust\ndata and evaluate them in the real scenario of a ComputerAided Diagnosis (CAD) system that discriminates healthy\nfrom Alzheimer\xe2\x80\x99s Disease (AD) subjects based on volumetric\ncharacteristics derived from structural MRI. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n8,Zhiqing Sun,"Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.","Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n9,Zhiqing Sun,"Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8\xe2\x88\xbc14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models.","Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8\xe2\x88\xbc14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n10,Zhiqing Sun,"Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principlefOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.","Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principlefOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n11,Zhiqing Sun,"We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.","We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n12,Timo Schick,"Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today\xe2\x80\x99s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER\xe2\x80\x99s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.","Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today\xe2\x80\x99s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER\xe2\x80\x99s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n13,Timo Schick,"Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch\xc2\xa8utze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form  and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency  words on both a rare word probing task and three downstream tasks.","Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch\xc2\xa8utze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form  and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency  words on both a rare word probing task and three downstream tasks. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n14,Timo Schick,"Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \xe2\x80\x9ctask descriptions\xe2\x80\x9d in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.","Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \xe2\x80\x9ctask descriptions\xe2\x80\x9d in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n15,Timo Schick,"Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for novel words on the fly, i.e., for words that did  not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: (i) learning an embedding from the novel word\xe2\x80\x99s surface-form (e.g., subword n-grams) and (ii) learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information \xe2\x80\x93 surface-form and context \xe2\x80\x93 and show that it results in large increases in embedding quality. Our  architecture obtains state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words.","Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for novel words on the fly, i.e., for words that did  not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: (i) learning an embedding from the novel word\xe2\x80\x99s surface-form (e.g., subword n-grams) and (ii) learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information \xe2\x80\x93 surface-form and context \xe2\x80\x93 and show that it results in large increases in embedding quality. Our  architecture obtains state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words. - can you write me an abstract, introduction and conclusion for the paper that is summarized above"\r\n