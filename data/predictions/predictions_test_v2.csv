,Unnamed: 0,Author,Pub,Type,Chunk,flesch_score_v2,word_count,sent_count,sybl_count,sent_score,re_text,POS_string,lexical_diversity,avg_word_per_sentence,avg_word_length,ID,prediction_SVC,prediction_SVC_pca,prediction_RF,prediction_RF_pca,prediction_LR,prediction_LR_pca
246,160,Zhiqing Sun,"[' Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013).', 'There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets.']",intro_chunked,"There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets.",40.22668269230772,195.0,8.0,327.0,0.28886228799819946," There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n gram language models in these approaches by neural language models. There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Propname Propname Propname, Propname Propname Propname and Propname Propname Propname Propname. However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Propname Propname Propname, a neural generative model that explicitly focuses on the segmental nature of Propname: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from Propname 0000 bakeoff, namely Propname, Propname, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state of the art statistical models on four different datasets.", PRON VERB DET ADJ NOUN PART VERB DET ADJ NOUN NOUN PUNCT SCONJ PRON AUX ADV VERB DET NOUN NOUN NOUN NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT PRON AUX VERB ADJ ADV ADJ ADJ ADJ NOUN PUNCT CCONJ PRON AUX PART DET NOUN ADP DET NOUN PUNCT ADP ADJ NOUN PUNCT ADJ ADJ NOUN VERB PROPN PROPN PROPN PUNCT PROPN PROPN PROPN CCONJ PROPN PROPN PROPN PROPN PUNCT ADV PUNCT NOUN ADP PRON AUX AUX ADV VERB ADP DET ADJ NOUN PUNCT ADV PUNCT ADJ ADJ NOUN ADP NOUN NOUN AUX VERB PART AUX VERB PUNCT ADP DET NOUN PUNCT PRON VERB DET PROPN PROPN PROPN PUNCT DET ADJ ADJ NOUN PRON ADV VERB ADP DET ADJ NOUN ADP PROPN PUNCT NOUN AUX ADV VERB VERB NOUN CCONJ VERB DET VERB ADJ NOUN PUNCT PRON VERB PRON NOUN ADP NUM ADJ NOUN NOUN ADP PROPN NUM NOUN PUNCT ADV PROPN PUNCT PROPN PUNCT NOUN CCONJ NOUN ADP PRON NOUN PUNCT PRON AUX DET ADJ PART VERB DET ADJ NOUN ADP ADJ ADJ NOUN NOUN CCONJ VERB ADJ NOUN ADP DET NOUN ADP DET NOUN ADJ NOUN ADP NUM ADJ NOUN PUNCT,0.517948717948718,24.375,5.256410256410256,246,Zhiqing Sun,Aman Madaan,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
287,201,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.",41.24826839826841,154.0,6.0,254.0,0.5364612936973572," We proceed as follows: After giving a succinct overview of previous work on Propname to text generation and related tasks in Section 0, we discuss basic notation and other preliminaries such as the Propname formalism, transition systems and maximum entropy models in Section 0. We introduce our generator in Section 0, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 0, we discuss our Propname based implementation of the generator. Results obtained with this implementation are reported in Section 0; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Propname 0 of Section 0. We conclude with a concise summary of our work and an outlook on future research topics in Section 0.", PRON VERB SCONJ VERB PUNCT ADP VERB DET ADJ NOUN ADP ADJ NOUN ADP PROPN PART VERB NOUN CCONJ ADJ NOUN ADP NOUN NUM PUNCT PRON VERB ADJ NOUN CCONJ ADJ NOUN ADJ ADP DET PROPN NOUN PUNCT NOUN NOUN CCONJ ADJ NOUN NOUN ADP NOUN NUM PUNCT PRON VERB PRON NOUN ADP NOUN NUM PUNCT PRON VERB DET NOUN ADP DET NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP DET VERB NOUN ADV ADV ADP DET ADJ NOUN ADP PRON NOUN NOUN CCONJ DET NOUN ADP DET VERB NOUN NOUN PUNCT ADP NOUN NUM PUNCT PRON VERB PRON PROPN VERB NOUN ADP DET NOUN PUNCT NOUN VERB ADP DET NOUN AUX VERB ADP NOUN NUM PUNCT ADP DET ADJ NOUN ADP DET NOUN ADP PRON NOUN CCONJ DET NOUN ADP DET ADJ ADV VERB NOUN PUNCT PRON VERB ADP PROPN NUM ADP NOUN NUM PUNCT PRON VERB ADP DET ADJ NOUN ADP PRON NOUN CCONJ DET NOUN ADP ADJ NOUN NOUN ADP NOUN NUM PUNCT,0.5209580838323353,27.833333333333332,4.790419161676646,287,Hugo Touvron,Aman Madaan,Hugo Touvron,Hugo Touvron,Hugo Touvron,Aman Madaan
149,63,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked,"Additionally, unlike previous systems, the outputs
of the intermediate steps in our system are fully
realized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target
style, our model won’t add any stylistic markers
and thus would allow the input to flow as is. We evaluate our model on politeness transfer as
well as 5 additional tasks described in prior work
(Shen et al., 2017; Prabhumoye et al., 2018; Li
et al., 2018) on content preservation, fluency and
style transfer accuracy. Both automatic and human
evaluations show that our model beats the stateof-the-art methods in content preservation, while
either matching or improving the transfer accuracy
across six different style transfer tasks(§5). The
results show that our technique is effective across a
broad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves
upon several of its limitations as described in (§2). Our main contribution is the design of politeness
transfer task. To this end, we provide a large dataset
of nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/
politeness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)
which are annotated as requests. To the best of our knowledge, we are the first to undertake politeness
as a style transfer task. In the process, we highlight an important class of problems wherein the
transfer involves going from a neutral style to the
target style. Finally, we design a “tag and generate”
pipeline that is particularly well suited for tasks like
politeness, while being general enough to match
or beat the performance of the existing systems on
popular style transfer tasks.",56.56274725274727,280.0,13.0,425.0,0.6511604189872742," Additionally, unlike previous systems, the outputs of the intermediate steps in our system are fully realized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target style, our model wo nt add any stylistic markers and thus would allow the input to flow as is. We evaluate our model on politeness transfer as well as 0 additional tasks described in prior work Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000 on content preservation, fluency and style transfer accuracy. Both automatic and human evaluations show that our model beats the stateof the art methods in content preservation, while either matching or improving the transfer accuracy across six different style transfer tasks. The results show that our technique is effective across a broad spectrum of style transfer tasks. Our methodology is inspired by Propname Propname Propname. and improves upon several of its limitations as described in. Our main contribution is the design of politeness transfer task. To this end, we provide a large dataset of nearly 0.00 million sentences labeled for politeness. Additionally, we hand curate a test set of 000 samples which are annotated as requests. To the best of our knowledge, we are the first to undertake politeness as a style transfer task. In the process, we highlight an important class of problems wherein the transfer involves going from a neutral style to the target style. Finally, we design a tag and generate pipeline that is particularly well suited for tasks like politeness, while being general enough to match or beat the performance of the existing systems on popular style transfer tasks.", ADV PUNCT ADP ADJ NOUN PUNCT DET NOUN ADP DET ADJ NOUN ADP PRON NOUN AUX ADV VERB PUNCT VERB DET ADJ NOUN ADJ PUNCT ADV PUNCT SCONJ DET NOUN NOUN AUX ADV ADP DET NOUN NOUN PUNCT PRON NOUN AUX PART VERB DET ADJ NOUN CCONJ ADV AUX VERB DET NOUN PART VERB SCONJ AUX PUNCT PRON VERB PRON NOUN ADP NOUN NOUN ADV ADV ADP NUM ADJ NOUN VERB ADP ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM ADP NOUN NOUN PUNCT NOUN CCONJ NOUN NOUN NOUN PUNCT CCONJ ADJ CCONJ ADJ NOUN VERB SCONJ PRON NOUN VERB DET ADJ DET NOUN NOUN ADP NOUN NOUN PUNCT SCONJ PRON VERB CCONJ VERB DET NOUN NOUN ADP NUM ADJ NOUN NOUN NOUN PUNCT DET NOUN VERB SCONJ PRON NOUN AUX ADJ ADP DET ADJ NOUN ADP NOUN NOUN NOUN PUNCT PRON NOUN AUX VERB ADP PROPN PROPN PROPN PUNCT CCONJ VERB SCONJ ADJ ADP PRON NOUN SCONJ VERB ADP PUNCT PRON ADJ NOUN AUX DET NOUN ADP NOUN NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP ADV NUM NUM NOUN VERB ADP NOUN PUNCT ADV PUNCT PRON AUX VERB DET NOUN NOUN ADP NUM NOUN PRON AUX VERB ADP NOUN PUNCT ADP DET ADJ ADP PRON NOUN PUNCT PRON AUX DET ADJ PART VERB NOUN ADP DET NOUN NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP NOUN SCONJ DET NOUN VERB VERB ADP DET ADJ NOUN ADP DET NOUN NOUN PUNCT ADV PUNCT PRON VERB DET NOUN CCONJ VERB NOUN PRON AUX ADV ADV ADJ ADP NOUN ADP NOUN PUNCT SCONJ AUX ADJ ADV PART VERB CCONJ VERB DET NOUN ADP DET VERB NOUN ADP ADJ NOUN NOUN NOUN PUNCT,0.49508196721311476,23.46153846153846,4.79344262295082,149,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Hugo Touvron
42,42,Hugo Touvron,"[' We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly available datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community.']",abstract_chunked," We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B
parameters. We train our models on trillions
of tokens, and show that it is possible to train
state-of-the-art models using publicly available datasets exclusively, without resorting
to proprietary and inaccessible datasets. In
particular, LLaMA-13B outperforms GPT-3
(175B) on most benchmarks, and LLaMA65B is competitive with the best models,
Chinchilla-70B and PaLM-540B. We release
all our models to the research community.",51.54791666666668,81.0,4.0,129.0,0.7741825580596924," We introduce Propname, a collection of foundation language models ranging from 0B to 00B parameters. We train our models on trillions of tokens, and show that it is possible to train state of the art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, Propname 00B outperforms Propname 0 on most benchmarks, and Propname is competitive with the best models, Chinchilla 00B and Propname Propname We release all our models to the research community.", PRON VERB PROPN PUNCT DET NOUN ADP NOUN NOUN NOUN VERB ADP NOUN ADP NUM NOUN PUNCT PRON VERB PRON NOUN ADP NOUN ADP NOUN PUNCT CCONJ VERB SCONJ PRON AUX ADJ PART VERB NOUN ADP DET NOUN NOUN VERB ADV ADJ NOUN ADV PUNCT ADP VERB ADP ADJ CCONJ ADJ NOUN PUNCT ADP ADJ PUNCT PROPN NOUN VERB PROPN NUM ADP ADJ NOUN PUNCT CCONJ PROPN AUX ADJ ADP DET ADJ NOUN PUNCT NOUN NUM CCONJ PROPN PROPN PRON VERB DET PRON NOUN ADP DET NOUN NOUN PUNCT,0.6067415730337079,29.666666666666668,5.0,42,Hugo Touvron,Hugo Touvron,Aman Madaan,Hugo Touvron,Hugo Touvron,Timo Schick
159,73,Hugo Touvron,"[' Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties first appeared when scaling models to a\nsufficient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that\nmore parameters will lead to better performance. However, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale. In this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of performance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al.', '(2022) recommends\ntraining a 10B model on 200B tokens, we find\nthat the performance of a 7B model continues to\nimprove even after 1T tokens. The focus of this work is to train a series of\nlanguage models that achieve the best possible performance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA, ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10× smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work compatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview\nof the modifications we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.']",intro_chunked," Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a
few examples (Brown et al., 2020). These few-shot
properties first appeared when scaling models to a
sufficient size (Kaplan et al., 2020), resulting in a
line of work that focuses on further scaling these
models (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that
more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022)
shows that, for a given compute budget, the best
performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. (2022) is to determine how to best
scale the dataset and model sizes for a particular
training compute budget. However, this objective
disregards the inference budget, which becomes
critical when serving a language model at scale. In this context, given a target level of performance,
the preferred model is not the fastest to train but the
fastest at inference, and although it may be cheaper
to train a large model to reach a certain level of performance, a smaller one trained longer will
ultimately be cheaper at inference. For instance,
although Hoffmann et al.",56.11045454545456,220.0,7.0,309.0,0.28232625126838684," Large Propname Propname trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examples. These few shot properties first appeared when scaling models to a sufficient size, resulting in a line of work that focuses on further scaling these models. These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from Propname Propname Propname. shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Propname Propname Propname. is to determine how to best scale the dataset and model sizes for a particular training compute budget. However, this objective disregards the inference budget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, although Propname Propname Propname.", ADJ PROPN PROPN VERB ADP ADJ NOUN ADP NOUN AUX VERB PRON NOUN PART VERB ADJ NOUN ADP ADJ NOUN CCONJ ADP DET ADJ NOUN PUNCT DET ADJ NOUN NOUN ADV VERB SCONJ VERB NOUN ADP DET ADJ NOUN PUNCT VERB ADP DET NOUN ADP NOUN PRON VERB ADP ADV VERB DET NOUN PUNCT DET NOUN AUX VERB ADP DET NOUN SCONJ ADJ NOUN AUX VERB ADP ADJ NOUN PUNCT ADV PUNCT ADJ NOUN ADP PROPN PROPN PROPN PUNCT VERB SCONJ PUNCT ADP DET VERB NOUN NOUN PUNCT DET ADJ NOUN AUX PART VERB ADP DET ADJ NOUN PUNCT CCONJ ADP ADJ NOUN VERB ADP ADJ NOUN PUNCT DET NOUN ADP DET NOUN NOUN ADP PROPN PROPN PROPN PUNCT AUX PART VERB SCONJ PART ADV VERB DET NOUN CCONJ NOUN NOUN ADP DET ADJ NOUN NOUN NOUN PUNCT ADV PUNCT DET ADJ VERB DET NOUN NOUN PUNCT PRON VERB ADJ SCONJ VERB DET NOUN NOUN ADP NOUN PUNCT ADP DET NOUN PUNCT VERB DET NOUN NOUN ADP NOUN PUNCT DET ADJ NOUN AUX PART DET ADJ PART VERB CCONJ DET ADJ ADP NOUN PUNCT CCONJ SCONJ PRON AUX AUX ADJ PART VERB DET ADJ NOUN PART VERB DET ADJ NOUN ADP NOUN PUNCT DET ADJ NOUN VERB ADV AUX ADV AUX ADJ ADP NOUN PUNCT ADP NOUN PUNCT SCONJ PROPN PROPN PROPN PUNCT,0.5022421524663677,22.3,4.695067264573991,159,Timo Schick,Hugo Touvron,Timo Schick,Hugo Touvron,Timo Schick,Hugo Touvron
130,44,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked,"A model that first predicts the number of
sub-emotions can be more precise and avoid overgeneration, a significant challenge with language
generation models (Welleck et al., 2020; Fu et al.,
2021). We efficiently sample such informative orders from the combinatorial space of all possible
orders and jointly model cardinality by leveraging
the auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly
modeling the cardinality and augmenting the
training data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show
that our method serves as a better proposal
distribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with
no additional annotations or architecture
changes.",33.49289041095892,146.0,5.0,248.0,0.6020856499671936," A model that first predicts the number of sub emotions can be more precise and avoid overgeneration, a significant challenge with language generation models Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. We efficiently sample such informative orders from the combinatorial space of all possible orders and jointly model cardinality by leveraging the auto regressive nature of Propname models. We show an efficient way to model sequenceto set prediction as a Propname task by jointly modeling the cardinality and augmenting the training data with informative sequences using our novel Propname data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show that our method serves as a better proposal distribution in a variational inference framework. With our approach, Propname models of different sizes achieve a 00 relative improvement on four real world tasks, with no additional annotations or architecture changes.", DET NOUN PRON ADV VERB DET NOUN ADP NOUN NOUN AUX AUX ADV ADJ CCONJ VERB NOUN PUNCT DET ADJ NOUN ADP NOUN NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PRON ADV VERB ADJ ADJ NOUN ADP DET ADJ NOUN ADP DET ADJ NOUN CCONJ ADV NOUN NOUN ADP VERB DET NOUN ADJ NOUN ADP PROPN NOUN PUNCT PRON VERB DET ADJ NOUN PART VERB NOUN VERB NOUN ADP DET PROPN NOUN ADP ADV VERB DET NOUN CCONJ VERB DET NOUN NOUN ADP ADJ NOUN VERB PRON ADJ PROPN NOUN NOUN NOUN PUNCT PRON ADV VERB PRON NOUN PUNCT VERB DET NOUN ADP DET NOUN NOUN PUNCT PRON VERB SCONJ PRON NOUN VERB ADP DET ADJ NOUN NOUN ADP DET ADJ NOUN NOUN PUNCT ADP PRON NOUN PUNCT PROPN NOUN ADP ADJ NOUN VERB DET NUM ADJ NOUN ADP NUM ADJ NOUN NOUN PUNCT ADP DET ADJ NOUN CCONJ NOUN NOUN PUNCT,0.6335403726708074,32.2,5.366459627329193,130,Zhiqing Sun,Aman Madaan,Zhiqing Sun,Aman Madaan,Zhiqing Sun,Zhiqing Sun
115,29,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked,"Figure 1 illustrates the
high-level idea, that SELF-REFINE uses the same underlying language model to generate feedback
and refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural
language and source-code generation. We show that SELF-REFINE outperforms direct generation
from strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang
et al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,
SELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code
models such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which
is easily extensible to other LLMs. In essence, our results show that even when an LLM cannot
generate an optimal output on its first try, the LLM can often provide useful feedback and improve
its own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs
from a single model without any additional training, via iterative (self-)feedback and refinement",49.01500000000003,180.0,6.0,271.0,0.6473260521888733," Figure 0 illustrates the high level idea, that Propname Propname uses the same underlying language model to generate feedback and refine its outputs. We evaluate Propname Propname on 0 generation tasks that span diverse domains, including natural language and source code generation. We show that Propname REFINE outperforms direct generation from strong LLMs like Propname 0.0 text davinci Propname and gpt 0.0 turbo; Propname; Propname Propname Propname Propname, 0000 and Propname 0 by 0 00 absolute improvement. In code generation tasks, Propname Propname improves the initial generation by up to absolute 00 when applied to strong code models such as Propname. We release all of our code, which is easily extensible to other LLMs. In essence, our results show that even when an Propname can not generate an optimal output on its first try, the Propname can often provide useful feedback and improve its own output accordingly. In turn, Propname Propname provides an effective way to obtain better outputs from a single model without any additional training, via iterative feedback and refinement", NOUN NUM VERB DET ADJ NOUN NOUN PUNCT SCONJ PROPN PROPN VERB DET ADJ ADJ NOUN NOUN PART VERB NOUN CCONJ VERB PRON NOUN PUNCT PRON VERB PROPN PROPN ADP NUM NOUN NOUN PRON VERB ADJ NOUN PUNCT VERB ADJ NOUN CCONJ NOUN NOUN NOUN PUNCT PRON VERB SCONJ PROPN VERB NOUN ADJ NOUN ADP ADJ NOUN ADP PROPN NUM NOUN NOUN PROPN CCONJ VERB NUM NOUN PUNCT PROPN PUNCT PROPN PROPN PROPN PROPN PUNCT NUM CCONJ PROPN NUM ADP NUM NUM ADJ NOUN PUNCT ADP NOUN NOUN NOUN PUNCT PROPN PROPN VERB DET ADJ NOUN ADP ADP PART VERB NUM SCONJ VERB ADP ADJ NOUN NOUN ADJ ADP PROPN PUNCT PRON VERB PRON ADP PRON NOUN PUNCT PRON AUX ADV ADJ ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB SCONJ ADV SCONJ DET PROPN AUX PART VERB DET ADJ NOUN ADP PRON ADJ NOUN PUNCT DET PROPN AUX ADV VERB ADJ NOUN CCONJ VERB PRON ADJ NOUN ADV PUNCT ADP NOUN PUNCT PROPN PROPN VERB DET ADJ NOUN PART VERB ADJ NOUN ADP DET ADJ NOUN ADP DET ADJ NOUN PUNCT ADP ADJ NOUN CCONJ NOUN,0.544973544973545,27.0,4.915343915343915,115,GPT-3.5,Aman Madaan,Timo Schick,Zhiqing Sun,GPT-3.5,Timo Schick
275,189,Timo Schick,"[' As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations\nhave been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.', 'It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.', 'Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.']",intro_chunked,"Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.",51.98186466165416,266.0,10.0,402.0,0.5890332460403442," Assessing the effectiveness of methods like Propname in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words, these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task relevant frequent words with rare synonyms obtained using semantic resources such as Propname. We rarify three common text classification datasets: Propname, AGs Propname and Propname. Propname outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on Propname. In summary, our contributions are as follows: We introduce Propname, a model that integrates Propname into Propname Propname, enabling a deep integration of surface form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding Propname to Propname achieves a new state of the art on WNLaM Propname and beats all baselines on rarified AGs Propname, Propname and Propname, resulting in an absolute improvement of up to 00 over Propname.", VERB DET NOUN ADP NOUN ADP PROPN ADP DET VERB NOUN AUX VERB PUNCT SCONJ ADV ADJ NOUN ADP ADJ NOUN AUX VERB ADP NOUN ADV VERB ADP ADJ NOUN PUNCT DET NOUN AUX VERB ADP ADJ NOUN CCONJ ADV PART ADJ ADP VERB PRON NOUN PUNCT ADV PUNCT ADJ NOUN AUX PART ADV VERB ADP ADV VERB ADJ NOUN NOUN PUNCT PRON ADV VERB NOUN PUNCT DET NOUN PART ADV VERB NOUN NOUN ADP NOUN ADP PRON ADJ NOUN AUX VERB PART AUX ADJ PUNCT PRON AUX VERB ADP VERB NOUN ADJ ADJ NOUN ADP ADJ NOUN VERB VERB ADJ NOUN ADJ ADP PROPN PUNCT PRON VERB NUM ADJ NOUN NOUN NOUN PUNCT PROPN PUNCT ADJ PROPN CCONJ PROPN PUNCT PROPN VERB ADJ NOUN ADP NUM ADJ NOUN ADP DET ADJ NOUN PUNCT ADP DET NUM VERB NOUN CCONJ ADP PROPN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB PROPN PUNCT DET NOUN PRON VERB PROPN ADP PROPN PROPN PUNCT VERB DET ADJ NOUN ADP NOUN NOUN CCONJ NOUN CCONJ ADV ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB NOUN PUNCT DET NOUN PRON VERB NOUN NOUN ADP NOUN ADP PRON ADJ NOUN AUX VERB PART AUX ADJ PUNCT PRON VERB SCONJ VERB PROPN ADP PROPN VERB DET ADJ NOUN ADP DET NOUN ADP ADJ PROPN CCONJ VERB DET NOUN ADP ADJ ADJ PROPN PUNCT PROPN CCONJ PROPN PUNCT VERB ADP DET ADJ NOUN ADP ADP PART NUM ADP PROPN PUNCT,0.5040650406504065,27.333333333333332,5.154471544715447,275,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
270,184,Timo Schick,"[' Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;\nRaffel et al., 2020; Brown et al., 2020; Rae et al.,\n2021; Zhang et al., 2022; Chowdhery et al., 2022,\ni.a.). However, the way these models operate—\nproducing outputs in a single pass from left to\nright—differs strongly from the iterative process\nby which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond\nthat, they are hard to control (Korbak et al., 2022)\nand verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the\nability to explain their intentions. All of this makes\nit very difficult for humans to collaborate with such\nmodels for writing coherent, factual texts. To address these shortcomings of existing LMs,\nwe propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process.', 'As illustrated in Figure 1, PEER operates in several steps\nthat aim to mirror the human writing process: For\na given text, either a user or the model itself can\nplan an action to be applied, for example by means\nof a natural language instruction. This plan is then\nrealized by an edit, which the model can explain\nboth in form of a textual comment and by pointing\nto references used; this is enabled by augmenting\neach input text with retrieved passages containing\npotentially relevant background information. We\nrepeat these steps until the text is in a satisfactory\nstate that does not require any further updates. This\niterative approach does not only enable the model\nto decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it\nalso allows humans to intervene at any time and\nsteer the model in the right direction, either by providing it with their own plans and comments or by\nmaking edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),\nwe use Wikipedia as our main source of edits and\nassociated comments, which we use as proxies for\nplans and explanations.', 'In contrast to this prior\nwork, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:\nIt should be capable of following human-written\ninstructions for updating texts in any domain. To\nachieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in\nsequential order, but also to infill various parts; for\nexample, given an edited text and a set of relevant\ndocuments, we teach it to produce the original version of this text before it was edited. This enables\nus to use self-training techniques (e.g., Yarowsky,\n1995; Sennrich et al., 2016; He et al., 2020a; Schick\nand Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We\nshow that this substantially improves PEER along\nseveral axes, including its ability to edit texts in any\ndomain, to understand human-written instructions,\nand to explain its actions. In summary, our contributions are as follows:\n• We introduce PEER, a collaborative language\nmodel trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities\nessential for collaborative writing. • For different tasks related to editing texts, we\nshow that PEER clearly outperforms various\nbaselines and analyze factors leading to its\nstrong performance. • To facilitate further research on collaborative\nLMs, we release a variety of PEER models as\nwell as the data and code used to train them.']",intro_chunked,"In contrast to this prior
work, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:
It should be capable of following human-written
instructions for updating texts in any domain. To
achieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in
sequential order, but also to infill various parts; for
example, given an edited text and a set of relevant
documents, we teach it to produce the original version of this text before it was edited. This enables
us to use self-training techniques (e.g., Yarowsky,
1995; Sennrich et al., 2016; He et al., 2020a; Schick
and Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We
show that this substantially improves PEER along
several axes, including its ability to edit texts in any
domain, to understand human-written instructions,
and to explain its actions. In summary, our contributions are as follows:
• We introduce PEER, a collaborative language
model trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities
essential for collaborative writing. • For different tasks related to editing texts, we
show that PEER clearly outperforms various
baselines and analyze factors leading to its
strong performance. • To facilitate further research on collaborative
LMs, we release a variety of PEER models as
well as the data and code used to train them.",43.266366279069786,258.0,8.0,399.0,0.7796499133110046," In contrast to this prior work, however, our goal is to obtain a collaborative model that is useful beyond just Propname: It should be capable of following human written instructions for updating texts in any domain. To achieve this goal, we train PEER not only to perform the writing process illustrated in Figure 0 in sequential order, but also to infill various parts; for example, given an edited text and a set of relevant documents, we teach it to produce the original version of this text before it was edited. This enables us to use self training techniques Propname, Propname, 0000; Propname Propname Propname Propname, 0000; He Propname Propname Propname, 0000a; Propname and Propname, 0000a for training PEER with synthetic plans, edits, explanations and documents. We show that this substantially improves PEER along several axes, including its ability to edit texts in any domain, to understand human written instructions, and to explain its actions. In summary, our contributions are as follows: We introduce Propname, a collaborative language model trained primarily on Propname edit histories. By training Propname to infill parts of the writing process and leveraging self training techniques, we make it applicable in any domain and enhance several of its core capabilities essential for collaborative writing. For different tasks related to editing texts, we show that Propname clearly outperforms various baselines and analyze factors leading to its strong performance. To facilitate further research on collaborative LMs, we release a variety of PEER models as well as the data and code used to train them.", ADP NOUN ADP DET ADJ NOUN PUNCT ADV PUNCT PRON NOUN AUX PART VERB DET ADJ NOUN PRON AUX ADJ ADP ADV PROPN PUNCT PRON AUX AUX ADJ ADP VERB ADJ VERB NOUN ADP VERB NOUN ADP DET NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB ADJ PART ADV PART VERB DET NOUN NOUN VERB ADP NOUN NUM ADP ADJ NOUN PUNCT CCONJ ADV PART VERB ADJ NOUN PUNCT ADP NOUN PUNCT VERB DET ADJ NOUN CCONJ DET NOUN ADP ADJ NOUN PUNCT PRON VERB PRON PART VERB DET ADJ NOUN ADP DET NOUN SCONJ PRON AUX VERB PUNCT PRON VERB PRON PART VERB NOUN NOUN NOUN PROPN PUNCT PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PRON PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN PUNCT NUM ADP VERB ADJ ADP ADJ NOUN PUNCT NOUN PUNCT NOUN CCONJ NOUN PUNCT PRON VERB SCONJ PRON ADV VERB ADJ ADP ADJ NOUN PUNCT VERB PRON NOUN PART VERB NOUN ADP DET NOUN PUNCT PART VERB ADJ VERB NOUN PUNCT CCONJ PART VERB PRON NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN VERB ADV ADP PROPN NOUN NOUN PUNCT ADP VERB PROPN PART VERB NOUN ADP DET NOUN NOUN CCONJ VERB NOUN NOUN NOUN PUNCT PRON VERB PRON ADJ ADP DET NOUN CCONJ VERB ADJ ADP PRON NOUN NOUN ADJ ADP ADJ NOUN PUNCT ADP ADJ NOUN VERB ADP VERB NOUN PUNCT PRON VERB SCONJ PROPN ADV VERB ADJ NOUN CCONJ VERB NOUN VERB ADP PRON ADJ NOUN PUNCT PART VERB ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET NOUN ADP ADJ NOUN ADV ADV ADP DET NOUN CCONJ NOUN VERB PART VERB PRON PUNCT,0.5103448275862069,36.25,4.73448275862069,270,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
38,38,Aman Madaan,"[' This paper introduces a new task of politeness\ntransfer which involves converting non-polite\nsentences to polite sentences while preserving\nthe meaning. We also provide a dataset of\nmore than 1.39 million instances automatically\nlabeled for politeness to encourage benchmark\nevaluations on this new task. We design a tag\nand generate pipeline that identifies stylistic attributes and subsequently generates a sentence\nin the target style while preserving most of the\nsource content. For politeness as well as five\nother transfer tasks, our model outperforms the\nstate-of-the-art methods on automatic metrics\nfor content preservation, with a comparable\nor better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer\naccuracy across all the six style transfer tasks. The data and code is located at https://\ngithub.com/tag-and-generate/']",abstract_chunked," This paper introduces a new task of politeness
transfer which involves converting non-polite
sentences to polite sentences while preserving
the meaning. We also provide a dataset of
more than 1.39 million instances automatically
labeled for politeness to encourage benchmark
evaluations on this new task. We design a tag
and generate pipeline that identifies stylistic attributes and subsequently generates a sentence
in the target style while preserving most of the
source content. For politeness as well as five
other transfer tasks, our model outperforms the
state-of-the-art methods on automatic metrics
for content preservation, with a comparable
or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer
accuracy across all the six style transfer tasks. The data and code is located at https://
github.com/tag-and-generate/",39.935952380952386,140.0,6.0,237.0,0.6314231157302856," This paper introduces a new task of politeness transfer which involves converting non polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 0.00 million instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state of the art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at weblink", DET NOUN VERB DET ADJ NOUN ADP NOUN NOUN PRON VERB VERB ADJ ADJ NOUN ADP ADJ NOUN SCONJ VERB DET NOUN PUNCT PRON ADV VERB DET NOUN ADP ADJ ADP NUM NUM NOUN ADV VERB ADP NOUN PART VERB ADJ NOUN ADP DET ADJ NOUN PUNCT PRON VERB DET NOUN CCONJ VERB NOUN PRON VERB ADJ NOUN CCONJ ADV VERB DET NOUN ADP DET NOUN NOUN SCONJ VERB ADJ ADP DET NOUN NOUN PUNCT ADP NOUN ADV ADV ADP NUM ADJ NOUN NOUN PUNCT PRON NOUN VERB DET NOUN ADP DET NOUN NOUN ADP ADJ NOUN ADP NOUN NOUN PUNCT ADP DET ADJ CCONJ ADJ NOUN ADP NOUN NOUN NOUN PUNCT ADV PUNCT PRON NOUN VERB VERB NOUN ADP ADJ NOUN ADP NOUN PUNCT VERB NOUN CCONJ NOUN NOUN ADP DET DET NUM NOUN NOUN NOUN PUNCT DET NOUN CCONJ NOUN AUX VERB ADP NOUN,0.636986301369863,24.333333333333332,5.3493150684931505,38,GPT-3.5,Aman Madaan,GPT-3.5,Aman Madaan,GPT-3.5,GPT-3.5
180,94,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked,"• Interestingly, with our distillation, image transformers learn more from a
convnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to
different downstream tasks such as fine-grained classification, on several
popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,
Stanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,
and focus on transformers for image classification in Section 3. We introduce
our distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent
transformers, as well as a comparative evaluation of our transformer-specific
distillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the
key ingredients involved in DeiT. We conclude in Section 7.",39.702742346938805,147.0,8.0,258.0,0.6479602456092834," Interestingly, with our distillation, image transformers learn more from a convnet than from another transformer with comparable performance. Our models pre learned on Propname are competitive when transferred to different downstream tasks such as fine grained classification, on several popular public benchmarks: Propname 00, Propname 000, Propname 000 flowers, Propname Propname and Propname 0000. This paper is organized as follows: we review related works in Section 0, and focus on transformers for image classification in Section 0. We introduce our distillation strategy for transformers in Section 0. The experimental section 0 provides analysis and comparisons against both convnets and recent transformers, as well as a comparative evaluation of our transformer specific distillation. Section 0 details our training scheme. It includes an extensive ablation of our data efficient training choices, which gives some insight on the key ingredients involved in DeiT. We conclude in Section 0.", ADV PUNCT ADP PRON NOUN PUNCT NOUN NOUN VERB ADJ ADP DET NOUN ADP ADP DET NOUN ADP ADJ NOUN PUNCT PRON NOUN VERB VERB ADP PROPN AUX ADJ SCONJ VERB ADP ADJ ADJ NOUN ADJ ADP ADJ ADJ NOUN PUNCT ADP ADJ ADJ ADJ NOUN PUNCT PROPN NUM PUNCT PROPN NUM PUNCT PROPN NUM NOUN PUNCT PROPN PROPN CCONJ PROPN NUM PUNCT DET NOUN AUX VERB SCONJ VERB PUNCT PRON VERB ADJ NOUN ADP NOUN NUM PUNCT CCONJ VERB ADP NOUN ADP NOUN NOUN ADP NOUN NUM PUNCT PRON VERB PRON NOUN NOUN ADP NOUN ADP NOUN NUM PUNCT DET ADJ NOUN NUM VERB NOUN CCONJ NOUN ADP DET NOUN CCONJ ADJ NOUN PUNCT ADV ADV ADP DET ADJ NOUN ADP PRON ADJ ADJ NOUN PUNCT NOUN NUM NOUN PRON NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP PRON NOUN ADJ NOUN NOUN PUNCT PRON VERB DET NOUN ADP DET ADJ NOUN VERB ADP NOUN PRON VERB ADP NOUN NUM PUNCT,0.6097560975609756,20.5,5.219512195121951,180,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,GPT-3.5
10,10,GPT-3.5,"[' In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever-expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as AlexNet, ResNet, and GPT, coupled with well-crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Transformer architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks (convnets). Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi-layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in-depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the ImageNet dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness.']",abstract_chunked," In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever-expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as AlexNet, ResNet, and GPT, coupled with well-crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Transformer architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks (convnets). Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi-layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in-depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the ImageNet dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness.",13.855677710843395,249.0,12.0,506.0,0.5303924679756165," In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as Propname, ResNet, and Propname, coupled with well crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Propname architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks. Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the Propname dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness.", ADP DET ADJ NOUN PUNCT DET NOUN ADP NOUN NOUN PUNCT ADV ADJ NOUN PUNCT AUX ADV VERB PUNCT VERB ADV ADP ADJ ADJ NOUN PUNCT ADV PUNCT PRON NOUN AUX VERB ADP ADJ NOUN NOUN PUNCT VERB ADP NOUN ADP NOUN NOUN NOUN CCONJ NOUN NOUN PUNCT ADV PUNCT ADP NOUN NOUN PUNCT ADJ NOUN VERB DET ADJ NOUN ADP VERB NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN ADP ADJ NOUN PUNCT DET NOUN ADP ADJ NOUN AUX ADV VERB ADP ADJ NOUN ADJ ADP PROPN PUNCT NOUN PUNCT CCONJ PROPN PUNCT VERB ADP ADV VERB NOUN NOUN PUNCT DET NOUN VERB ADP DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT VERB ADV ADP DET NOUN ADP PROPN VERB ADP ADJ NOUN PUNCT ADV PUNCT NOUN NOUN ADP NOUN AUX ADV ADJ VERB ADP PRON ADP ADJ NOUN PUNCT VERB SCONJ NOUN AUX ADJ ADP VERB DET ADJ ADJ NOUN ADP NOUN PUNCT DET NOUN NOUN ADP ADP VERB NOUN ADJ ADP VERB ADJ NOUN ADP NOUN CCONJ ADV ADJ NOUN VERB ADJ NOUN NOUN PUNCT DET NOUN NOUN ADP VERB DET NOUN ADP VERB ADP ADJ NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT ADV PUNCT ADJ NOUN ADP NOUN NOUN AUX VERB PUNCT VERB DET ADP NOUN NOUN ADP PRON NOUN PUNCT NOUN PUNCT NOUN PUNCT CCONJ ADJ NOUN NOUN PUNCT DET NOUN VERB ADP DET NOUN ADP DET ADJ NOUN ADP NOUN CCONJ DET NOUN NOUN PUNCT DET VERB NOUN AUX ADV VERB ADP NOUN NOUN ADP DET PROPN NOUN CCONJ VERB NOUN PUNCT ADV PUNCT DET NOUN AUX VERB ADP ADJ NOUN ADJ ADP ADJ NOUN PUNCT VERB DET ADJ NOUN ADP PRON NOUN PUNCT,0.5653710247349824,23.583333333333332,5.992932862190813,10,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5
283,197,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked," Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.",37.41541760089689,223.0,8.0,372.0,0.6673001646995544," Semantic representations of natural language are of great interest for various aspects of natural language processing. For example, semantic representations may be useful for challenging tasks such as information extraction, question answering, natural language generation and machine translation. To provide a coherent framework for semantic representations, Propname Propname Propname. introduced Propname Propname Propname, a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so called concepts and edges encode relations between them. As Propname abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary Propname graph can be seen in Propname Propname; several sentences corresponding to this graph are listed in Propname 0b. For Propname to be useful in solving the above mentioned tasks, one must of course be able to convert sentences into Propname graphs and vice versa. Therefore, two important domain specific problems are parsing, the task of finding the graph corresponding to a given natural language sentence, and generation, the inverse task of finding a good natural language realization for a given Propname graph.", ADJ NOUN ADP ADJ NOUN AUX ADP ADJ NOUN ADP ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADP NOUN PUNCT ADJ NOUN AUX AUX ADJ ADP VERB NOUN ADJ ADP NOUN NOUN PUNCT NOUN VERB PUNCT ADJ NOUN NOUN CCONJ NOUN NOUN PUNCT PART VERB DET ADJ NOUN ADP ADJ NOUN PUNCT PROPN PROPN PROPN PUNCT VERB PROPN PROPN PROPN PUNCT DET ADJ NOUN NOUN PRON VERB DET NOUN ADP ADJ NOUN NOUN ADP VERB ADJ NOUN ADP NOUN VERB ADP DET NOUN CCONJ NOUN PUNCT ADP DET NOUN PUNCT NOUN VERB ADV VERB NOUN CCONJ VERB ADJ NOUN ADP PRON PUNCT SCONJ PROPN VERB ADV ADJ NOUN ADP NOUN PUNCT DET NOUN ADV VERB ADP PART ADV NUM PUNCT CCONJ DET NOUN ADP ADJ NOUN PUNCT DET ADJ PROPN NOUN AUX AUX VERB ADP PROPN PROPN PUNCT ADJ NOUN VERB ADP DET NOUN AUX VERB ADP PROPN NOUN PUNCT SCONJ PROPN PART AUX ADJ ADP VERB DET ADJ VERB NOUN PUNCT PRON AUX ADP NOUN AUX ADJ PART VERB NOUN ADP PROPN NOUN CCONJ NOUN ADV PUNCT ADV PUNCT NUM ADJ NOUN ADJ NOUN AUX VERB PUNCT DET NOUN ADP VERB DET NOUN VERB ADP DET VERB ADJ NOUN NOUN PUNCT CCONJ NOUN PUNCT DET ADJ NOUN ADP VERB DET ADJ ADJ NOUN NOUN ADP DET VERB PROPN NOUN PUNCT,0.5158371040723982,24.555555555555557,5.334841628959276,283,Timo Schick,Aman Madaan,Timo Schick,GPT-3.5,Timo Schick,GPT-3.5
100,14,GPT-3.5,"[' Supervised Fine-Tuning (SFT) in conjunction with Reinforcement Learning from Human Feedback (RLHF) has proven effective for aligning AI agents based on large language models (LLMs). However, the necessity for high-quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in-distribution preferences is non-trivial. This paper presents SALMON (Self-ALignMent with principlefOllowiNg reward models), a pioneering approach to aligning base language models with minimal human supervision. Central to SALMON is a principle-following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human-defined principles. By adjusting these principles during RL training, we achieve precise control over preferences, influencing the behavior of RL-trained policies, and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method results in the creation of Dromedary-2, an AI assistant outperforming state-of-the-art systems with only 6 exemplars for in-context learning and 31 human-defined principles.']",intro_chunked," Supervised Fine-Tuning (SFT) in conjunction with Reinforcement Learning from Human Feedback (RLHF) has proven effective for aligning AI agents based on large language models (LLMs). However, the necessity for high-quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in-distribution preferences is non-trivial. This paper presents SALMON (Self-ALignMent with principlefOllowiNg reward models), a pioneering approach to aligning base language models with minimal human supervision. Central to SALMON is a principle-following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human-defined principles. By adjusting these principles during RL training, we achieve precise control over preferences, influencing the behavior of RL-trained policies, and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method results in the creation of Dromedary-2, an AI assistant outperforming state-of-the-art systems with only 6 exemplars for in-context learning and 31 human-defined principles.",30.365000000000038,168.0,6.0,294.0,0.5361313223838806," Supervised Fine Tuning in conjunction with Propname Propname from Propname Propname has proven effective for aligning Propname agents based on large language models. However, the necessity for high quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in distribution preferences is non trivial. This paper presents Propname, a pioneering approach to aligning base language models with minimal human supervision. Central to Propname is a principle following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human defined principles. By adjusting these principles during Propname training, we achieve precise control over preferences, influencing the behavior of Propname trained policies, and eliminating the need for online human preferences. Applied to the Propname 0 00b base language model, our method results in the creation of Propname 0, an Propname assistant outperforming state of the art systems with only 0 exemplars for in context learning and 00 human defined principles.", ADJ ADJ NOUN ADP NOUN ADP PROPN PROPN ADP PROPN PROPN AUX VERB ADJ ADP VERB PROPN NOUN VERB ADP ADJ NOUN NOUN PUNCT ADV PUNCT DET NOUN ADP ADJ NOUN ADJ NOUN VERB DET NOUN PUNCT ADV ADP ADJ NOUN SCONJ VERB ADJ NOUN NOUN CCONJ ADP NOUN NOUN AUX ADV ADJ PUNCT DET NOUN VERB PROPN PUNCT DET VERB NOUN ADP VERB NOUN NOUN NOUN ADP ADJ ADJ NOUN PUNCT ADJ ADP PROPN AUX DET ADJ VERB NOUN NOUN VERB ADP ADJ NOUN NOUN PUNCT ADJ ADP VERB NOUN NOUN VERB ADP ADJ ADJ VERB NOUN PUNCT ADP VERB DET NOUN ADP PROPN NOUN PUNCT PRON VERB ADJ NOUN ADP NOUN PUNCT VERB DET NOUN ADP PROPN VERB NOUN PUNCT CCONJ VERB DET NOUN ADP ADJ ADJ NOUN PUNCT VERB ADP DET PROPN NUM NOUN NOUN NOUN NOUN PUNCT PRON NOUN VERB ADP DET NOUN ADP PROPN NUM PUNCT DET PROPN NOUN VERB NOUN ADP DET NOUN NOUN ADP ADV NUM NOUN ADP ADP NOUN NOUN CCONJ NUM ADJ VERB NOUN PUNCT,0.5977011494252874,29.0,5.528735632183908,100,Zhiqing Sun,GPT-3.5,Zhiqing Sun,GPT-3.5,Zhiqing Sun,GPT-3.5
70,70,Zhiqing Sun,"[' Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.']",abstract_chunked," Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.",35.900645161290356,155.0,7.0,272.0,0.4423273801803589," Previous traditional approaches to unsupervised Chinese word segmentation can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non trivial. In this paper, we propose the segmental language models for Propname. Our approach explicitly focuses on the segmental nature of Propname, as well as preserves several properties of language models. In Propname, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised Propname and achieve competitive performance to the state of the art statistical models on four different datasets from Propname 0000 bakeoff.", ADJ ADJ NOUN PART VERB ADJ NOUN NOUN AUX AUX ADV VERB ADP ADJ CCONJ ADJ NOUN PUNCT DET ADJ VERB DET ADV VERB NOUN NOUN ADP NOUN NOUN PUNCT SCONJ DET ADJ VERB ADP VERB DET ADJ NOUN ADP DET ADJ ADJ NOUN PUNCT ADV PUNCT SCONJ PRON VERB DET ADJ NOUN PART VERB DET ADJ NOUN ADP ADJ NOUN ADP VERB ADJ NOUN NOUN PUNCT PRON ADP ADJ NOUN AUX ADV ADJ PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN NOUN ADP PROPN PUNCT PRON NOUN ADV VERB ADP DET ADJ NOUN ADP PROPN PUNCT ADV ADV ADP VERB ADJ NOUN ADP NOUN NOUN PUNCT ADP PROPN PUNCT DET NOUN NOUN VERB DET ADJ NOUN CCONJ DET NOUN NOUN VERB DET NOUN ADV PUNCT ADV ADV SCONJ PRON VERB PUNCT PRON AUX DET ADJ PART VERB DET ADJ NOUN ADP ADJ PROPN CCONJ VERB ADJ NOUN ADP DET NOUN ADP DET NOUN ADJ NOUN ADP NUM ADJ NOUN ADP PROPN NUM NOUN PUNCT,0.5748502994011976,23.857142857142858,5.323353293413174,70,Zhiqing Sun,GPT-3.5,Zhiqing Sun,Aman Madaan,Zhiqing Sun,GPT-3.5
63,63,Zhiqing Sun,"[' DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly out-perform DETR and other baselines in terms of detection accuracy. Code is released at.']",abstract_chunked," DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly out-perform DETR and other baselines in terms of detection accuracy. Code is released at.",50.7568859649123,133.0,6.0,210.0,0.29406243562698364," Propname is a recently proposed Propname based method which views object detection as a set prediction problem and achieves state of the art performance but demands extra long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of Propname. Our examinations reveal several factors contributing to the slow convergence of Propname, primarily the issues with the Hungarian loss and the Propname cross attention mechanism. To overcome these issues we propose two solutions, namely, Propname Propname and Propname Propname. Experimental results show that the proposed methods not only converge much faster than the original Propname, but also significantly out perform Propname and other baselines in terms of detection accuracy. Propname is released at.", PROPN AUX DET ADV VERB PROPN VERB NOUN PRON VERB NOUN NOUN ADP DET VERB NOUN NOUN CCONJ VERB NOUN ADP DET NOUN NOUN CCONJ VERB ADJ ADJ NOUN NOUN PART VERB PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN NOUN ADP DET NOUN ADP PROPN PUNCT PRON NOUN VERB ADJ NOUN VERB ADP DET ADJ NOUN ADP PROPN PUNCT ADV DET NOUN ADP DET ADJ NOUN CCONJ DET PROPN VERB NOUN NOUN PUNCT PART VERB DET NOUN PRON VERB NUM NOUN PUNCT ADV PUNCT PROPN PROPN CCONJ PROPN PROPN PUNCT ADJ NOUN VERB SCONJ DET VERB NOUN PART ADV VERB ADV ADV ADP DET ADJ PROPN PUNCT CCONJ ADV ADV ADV VERB PROPN CCONJ ADJ NOUN ADP NOUN ADP NOUN NOUN PUNCT PROPN AUX VERB ADP PUNCT,0.6439393939393939,22.0,5.25,63,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Hugo Touvron
90,4,GPT-3.5,"[' In the realm of natural language processing, this paper pioneers the task of politeness transfer – a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 1.39 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag-and-generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state-of-the-art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open-sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research.']",intro_chunked," In the realm of natural language processing, this paper pioneers the task of politeness transfer – a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 1.39 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag-and-generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state-of-the-art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open-sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research.",26.42147058823531,170.0,7.0,313.0,0.7302294969558716," In the realm of natural language processing, this paper pioneers the task of politeness transfer a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 0.00 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag and generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state of the art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research.", ADP DET NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN VERB DET NOUN ADP NOUN NOUN DET NOUN VERB ADP VERB ADJ NOUN ADP ADJ NOUN SCONJ VERB ADJ NOUN PUNCT ADP DET NOUN ADP VERB NOUN ADP DET NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADV VERB ADP NOUN PUNCT VERB NUM NUM NOUN PUNCT DET NOUN VERB ADP DET ADJ NOUN ADP VERB CCONJ VERB NOUN NOUN NOUN PUNCT PRON NOUN VERB DET NOUN CCONJ VERB NOUN PUNCT VERB DET ADJ NOUN PRON VERB CCONJ VERB ADJ NOUN PART VERB ADJ NOUN NOUN PUNCT PRON VERB PRON NOUN ADP NOUN NOUN PART VERB NUM ADJ NOUN NOUN PUNCT ADV VERB ADJ NOUN ADP DET NOUN NOUN ADP ADJ NOUN VERB ADP ADJ NOUN PUNCT ADV PUNCT PRON NOUN VERB VERB NOUN ADP ADJ NOUN PUNCT VERB ADP NOUN PUNCT VERB NOUN PUNCT CCONJ ADJ NOUN NOUN ADP DET NUM NOUN NOUN NOUN PUNCT PART VERB ADJ NOUN CCONJ NOUN PUNCT PRON AUX ADJ VERB CCONJ PRON NOUN CCONJ NOUN PUNCT VERB DET NOUN PART VERB ADP DET NOUN ADP NOUN NOUN CCONJ ADJ NOUN PUNCT,0.6702127659574468,26.857142857142858,5.76595744680851,90,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5
164,78,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"Our contributions can be summarized as follows:
• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and
fine-tuning it at resolution 448, we obtain 87.4% top-1
accuracy on Imagenet-val. • We provide an efficient implementation to subsample
models on the fly. It is a simple yet effective variation
of stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models
by themselves even with significant trimming, similar
to LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures
(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from
scratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the
DeiT repository.",47.565,162.0,9.0,270.0,0.7339382767677307," Our contributions can be summarized as follows: We introduce a novel training approach for deep neural networks: We co train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre training Propname Propname on Propname 00k and fine tuning it at resolution 000, we obtain 00.0 top 0 accuracy on Propname val. We provide an efficient implementation to subsample models on the fly. It is a simple yet effective variation of stochastic depth to drop residual blocks. We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models by themselves even with significant trimming, similar to Propname in natural language processing. We validate our approach on multiple architectures, both for image classification trained from scratch or with transfer, and semantic segmentation. We will share Propname for reproducibility in the Propname repository.", PRON NOUN AUX AUX VERB SCONJ VERB PUNCT PRON VERB DET ADJ NOUN NOUN ADP ADJ ADJ NOUN PUNCT PRON VERB NOUN NOUN PUNCT PRON ADV VERB DET NOUN ADP ADJ NOUN PUNCT VERB DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT ADP NOUN VERB PROPN PROPN ADP PROPN NOUN CCONJ ADJ VERB PRON ADP NOUN NUM PUNCT PRON VERB NUM ADJ NUM NOUN ADP PROPN NOUN PUNCT PRON VERB DET ADJ NOUN ADP ADJ NOUN ADP DET NOUN PUNCT PRON AUX DET ADJ ADV ADJ NOUN ADP ADJ NOUN PART VERB ADJ NOUN PUNCT PRON VERB ADJ NOUN CCONJ NOUN PUNCT ADV PUNCT PRON VERB SCONJ PRON NOUN AUX ADJ NOUN ADP PRON ADV ADP ADJ NOUN PUNCT ADJ ADP PROPN ADP ADJ NOUN NOUN PUNCT PRON VERB PRON NOUN ADP ADJ NOUN PUNCT CCONJ ADP NOUN NOUN VERB ADP NOUN CCONJ ADP NOUN PUNCT CCONJ ADJ NOUN PUNCT PRON AUX VERB PROPN ADP NOUN ADP DET PROPN NOUN PUNCT,0.6566265060240963,18.444444444444443,5.0,164,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
192,106,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.', 'This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]']",intro_chunked," Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].",53.91708010335918,215.0,9.0,327.0,0.2545716166496277," Propname Propname Propname are used extensively in computer vision tasks such as image classification, object detection, inpainting, style transfer and even image compression. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Propname of Propname, is then resized to obtain a crop of a fixed size that is fed to the Propname. At test time, the Propname is instead set to a square covering the central part of the image, which results in the extraction of a so called center crop. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different Propname, which skews the distribution of data seen by the Propname. Over the years, training and testing pre processing procedures have evolved to improve the performance of Propname, but so far they have been optimized separately.", PROPN PROPN PROPN AUX VERB ADV ADP NOUN NOUN NOUN ADJ ADP NOUN NOUN PUNCT NOUN NOUN PUNCT VERB PUNCT NOUN NOUN CCONJ ADV NOUN NOUN PUNCT ADP NOUN PART VERB DET ADJ ADJ NOUN ADP DET NOUN PUNCT DET NOUN CCONJ NOUN NOUN NOUN AUX VERB PUNCT ADV PUNCT ADV NOUN X NOUN NOUN AUX ADJ ADP NOUN CCONJ NOUN PUNCT ADP NOUN PUNCT ADP NOUN NOUN DET ADJ ADJ NOUN NOUN AUX PART VERB DET NOUN ADP ADJ NOUN ADP DET NOUN PUNCT PRON ADV VERB DET NOUN ADP NOUN NOUN PUNCT DET NOUN PUNCT PRON PRON VERB DET PROPN ADP PROPN PUNCT AUX ADV VERB PART VERB DET NOUN ADP DET VERB NOUN PRON AUX VERB ADP DET PROPN PUNCT ADP NOUN NOUN PUNCT DET PROPN AUX ADV VERB ADP DET NOUN VERB DET ADJ NOUN ADP DET NOUN PUNCT PRON VERB ADP DET NOUN ADP DET ADV VERB NOUN NOUN PUNCT PRON VERB DET NOUN ADP NOUN PRON VERB ADJ ADJ ADJ NOUN PUNCT ADV PUNCT SCONJ DET NOUN VERB ADP NOUN CCONJ NOUN NOUN VERB DET ADJ NOUN PUNCT PRON VERB ADP ADJ PROPN PUNCT PRON VERB DET NOUN ADP NOUN VERB ADP DET PROPN PUNCT ADP DET NOUN PUNCT NOUN CCONJ NOUN NOUN NOUN NOUN AUX VERB PART VERB DET NOUN ADP PROPN PUNCT CCONJ ADV ADV PRON AUX AUX VERB ADV PUNCT,0.5152838427947598,25.444444444444443,4.724890829694323,192,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
199,113,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs.', 'For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].']",intro_chunked,"For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].",49.32167605633805,142.0,5.0,216.0,0.6264891028404236," For instance, for a target test resolution of 000000, training at resolution 000000 provides better results than the standard practice of training at resolution 000000, while being more efficient. In addition we can adapt a ResNet 00 train at resolution 000000 for the test resolution 000000 and thus obtain top 0 accuracy of 00.0 on Propname. Alternatively, we leverage the improved efficiency to train high accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top 0 accuracy of 00.0 on Propname with a Propname 000 00x00d pre trained in weakly supervised fashion on 000 million public images. Finally, our method makes it possible to save Propname memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance.", ADP NOUN PUNCT ADP DET NOUN NOUN NOUN ADP NUM PUNCT NOUN ADP NOUN NUM VERB ADJ NOUN ADP DET ADJ NOUN ADP NOUN ADP NOUN NUM PUNCT SCONJ AUX ADV ADJ PUNCT ADP NOUN PRON AUX VERB DET NOUN NUM NOUN ADP NOUN NUM ADP DET NOUN NOUN NUM CCONJ ADV VERB ADJ NUM NOUN ADP NUM ADP PROPN PUNCT ADV PUNCT PRON VERB DET VERB NOUN PART VERB ADJ NOUN NOUN PRON VERB ADP ADV ADJ NOUN ADP NOUN NOUN SCONJ ADV VERB ADV PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NUM NOUN ADP NUM ADP PROPN ADP DET PROPN NUM X VERB VERB ADP ADJ ADJ NOUN ADP NUM NUM ADJ NOUN PUNCT ADV PUNCT PRON NOUN VERB PRON ADJ PART VERB PROPN NOUN PUNCT PRON AUX ADP NOUN AUX VERB ADP NOUN PUNCT VERB ADJ NOUN NOUN ADV VERB ADP DET ADJ ADJ NOUN PUNCT,0.62,30.0,4.786666666666667,199,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Timo Schick
224,138,Zhiqing Sun,"[' The Transformer architecture (Vaswani et al., 2017) has been successfully applied to various tasks, including natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2020), and time series forecasting (Zhou et al., 2020; Wu et al., 2021). Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) (Kitaev et al., 2020; Daras et al., 2020) and mini-batch spherical k-means (Roy et al., 2021; Wang et al., 2020a). The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.', 'The effectiveness of those ANN approaches rely on the assumption that the (transformed) query and key vectors should lie in the same space, which could be sub-optimal in dealing with different sparsity patterns, as analyzed in this paper (Section 3.2). Besides, the hash functions in LSH are randomized and data-agnostic, which cannot fully utilize the rich information in real-world data distributions. In this paper, we address the above limitations of existing ANN-based methods for attention sparsification. Firstly, we analyze two imbalance issues in LSH-produced sparse attention patterns, i.e., unbalanced hash bucket sizes and unbalanced query-key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that ANN-derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning-to-Hash Attention (LHA), for dynamic attention sparsification with enhanced model expressiveness. LHA directly optimizes our newly defined\nattention utility metric in an end-to-end manner via separate learnable hash functions for queries and\nkeys, respectively. As for reducing the computational complexity in the training phase, LHA uses\nunbiased kernelized attention techniques (Choromanski et al., 2020; Peng et al., 2021) to efficiently\napproximate the attention utilities. Similar to other sparse attention models (Kitaev et al., 2020;\nRoy et al., 2021), LHA reduces the overall complexity of self-attention from O(N2\n) to O(N1.5\n)\nfor sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for\nlanguage modeling, natural language understanding, and Long-Range-Arena show that LHA achieves\nbetter performance compared to strong transformer baselines.']",intro_chunked," The Transformer architecture (Vaswani et al., 2017) has been successfully applied to various tasks, including natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2020), and time series forecasting (Zhou et al., 2020; Wu et al., 2021). Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) (Kitaev et al., 2020; Daras et al., 2020) and mini-batch spherical k-means (Roy et al., 2021; Wang et al., 2020a). The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.",45.33486434108528,215.0,6.0,318.0,0.4301791191101074," The Propname architecture has been successfully applied to various tasks, including natural language processing, computer vision, and time series forecasting. Such a success is mainly due to the self attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Propname Propname Propname approaches, including Propname Propname Propname and mini batch Propname Propname means. The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.", DET PROPN NOUN AUX AUX ADV VERB ADP ADJ NOUN PUNCT VERB ADJ NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ NOUN NOUN NOUN PUNCT DET DET NOUN AUX ADV ADJ ADP DET NOUN NOUN NOUN PUNCT PRON VERB DET ADJ PART ADV VERB ADP DET ADJ NOUN ADP DET ADJ NOUN PUNCT CCONJ NOUN NOUN VERB DET ADJ NOUN CCONJ NOUN NOUN ADP NOUN ADP DET NOUN NOUN CCONJ ADV AUX PART VERB ADV ADP ADJ NOUN ADP DET NOUN PUNCT PART VERB DET NOUN NOUN PUNCT NUM ADP DET NOUN AUX PART VERB DET ADJ NOUN NOUN ADP DET ADJ NOUN PUNCT SCONJ DET NOUN NOUN AUX VERB ADP DET ADJ NOUN PUNCT DET ADJ NOUN VERB ADP ADJ NOUN ADP ADJ NOUN NOUN ADP PROPN PROPN PROPN NOUN PUNCT VERB PROPN PROPN PROPN CCONJ ADJ NOUN PROPN PROPN VERB PUNCT DET NOUN CCONJ NOUN AUX VERB CCONJ VERB ADP ADJ NOUN PUNCT VERB SCONJ DET NOUN CCONJ NOUN ADP DET ADJ NOUN AUX ADJ ADP DET ADJ NOUN PUNCT,0.6257309941520468,28.5,5.023391812865497,224,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
25,25,Aman Madaan,"[' Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.']",abstract_chunked," Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.",45.35529411764708,187.0,7.0,297.0,0.6079076528549194," Like humans, large language models do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Propname Propname, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an Propname; then, the same Propname provides feedback for its output and uses it to refine itself, iteratively. Propname Propname does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single Propname as the generator, refiner and the feedback provider. We evaluate Propname Propname across 0 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state of the art LLMs. Across all evaluated tasks, outputs generated with Propname Propname are preferred by humans and automatic metrics over those generated with the same Propname using conventional one step generation, improving by 00 absolute on average in task performance. Our work demonstrates that even state of the art LLMs like Propname 0 can be further improved at test time using our simple, standalone approach.", ADP NOUN PUNCT ADJ NOUN NOUN AUX PART ADV VERB DET ADJ NOUN ADP PRON ADJ NOUN PUNCT VERB ADP SCONJ NOUN VERB PRON VERB NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET NOUN ADP VERB ADJ NOUN ADP NOUN ADP ADJ NOUN CCONJ NOUN PUNCT DET ADJ NOUN AUX PART VERB DET ADJ NOUN VERB DET PROPN PUNCT ADV PUNCT DET ADJ PROPN VERB NOUN ADP PRON NOUN CCONJ VERB PRON PART VERB PRON PUNCT ADV PUNCT PROPN PROPN AUX PART VERB DET ADJ NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN NOUN PUNCT CCONJ ADV VERB DET ADJ PROPN ADP DET NOUN PUNCT NOUN CCONJ DET NOUN NOUN PUNCT PRON VERB PROPN PROPN ADP NUM ADJ NOUN PUNCT VERB ADP NOUN NOUN NOUN ADP ADJ NOUN PUNCT VERB NOUN ADP DET NOUN NOUN PUNCT ADP DET VERB NOUN PUNCT NOUN VERB ADP PROPN PROPN AUX VERB ADP NOUN CCONJ ADJ NOUN ADP PRON VERB ADP DET ADJ PROPN VERB ADJ NUM NOUN NOUN PUNCT VERB ADP NUM ADJ ADP ADJ ADP NOUN NOUN PUNCT PRON NOUN VERB SCONJ ADV NOUN ADP DET NOUN NOUN ADP PROPN NUM AUX AUX ADV VERB ADP NOUN NOUN VERB PRON ADJ PUNCT ADJ NOUN PUNCT,0.6059113300492611,29.0,4.975369458128079,25,GPT-3.5,Hugo Touvron,GPT-3.5,Hugo Touvron,GPT-3.5,Aman Madaan
254,168,Timo Schick,"[' Pretraining neural networks using a language modeling objective leads to large improvements across\na variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019). With model sizes continually increasing\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are\ntypically based on crawls from the internet that are\nonly filtered with some basic rules (Radford et al.,\n2019; Raffel et al., 2020). As a consequence, they\ncontain non-negligible amounts of text exhibiting\nbiases that are undesirable or outright harmful for\nmany potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such\ndata pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;\nBasta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned\nwords (Raffel et al., 2020) fall short of mitigating\nthis problem for at least two reasons. First, they do\nnot reliably keep language models from generating\nbiased text: Examples in Figure 1 show that biased text can easily be generated by using only words\nthat are, by themselves, completely unproblematic.', 'As many such words are important words of the\nEnglish vocabulary and thus needed for meaningful\ntext generation, they should not be included in a list\nof banned words. Secondly, banning words also\nprevents language models from gaining knowledge\nof topics related to the banned words, which may\nbe necessary for some applications.2\nIt is therefore inherently difficult to ban words without doing\nharm to a model’s capabilities. Building training datasets with more care and\ndeliberation, an alternative solution discussed by\nBender et al. (2021), is important, especially for\nimproving linguistic and cultural diversity in online\nand other forms of communication. However, for\nlarge language models that are available for common global languages, it is desirable to also have\nother mechanisms to address bias because dataset\ncuration and documentation is extremely resource\nintensive, given the amount of data required. It\ncan also necessitate building different training sets\nand, accordingly, training different models for each\ndesired behavior, which can result in high environmental impact (Strubell et al., 2019).', 'In this paper, we therefore propose an approach\nthat, instead of trusting that a model will implicitly learn desired behaviors from the training data,\nmakes explicit how we expect it to behave at test\ntime: If the model is told which biases are undesired – and it is able to discern their presence –,\nit should be able to avoid them even if they are\npresent in some of the texts it has been trained on. As it is a necessary condition for this approach, we\nfirst explore whether language models are able to\ndetect when their own outputs exhibit undesirable\nattributes, based only on their internal knowledge –\na process to which we refer as self-diagnosis. We\nthen investigate whether this ability can be used\nto perform self-debiasing, i.e., whether language\nmodels can use this knowledge to discard undesired\nbehaviors in a fully unsupervised fashion. To this\nend, we propose a decoding algorithm that reduces\nthe probability of a model producing biased text,\nrequiring nothing more than a textual description\nof the undesired behavior, which can be as simple\nas a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in\nparticular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find\nthat their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in\nNLP']",intro_chunked," Pretraining neural networks using a language modeling objective leads to large improvements across
a variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,
2019). With model sizes continually increasing
(Radford et al., 2019; Raffel et al., 2020; Brown
et al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are
typically based on crawls from the internet that are
only filtered with some basic rules (Radford et al.,
2019; Raffel et al., 2020). As a consequence, they
contain non-negligible amounts of text exhibiting
biases that are undesirable or outright harmful for
many potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such
data pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;
Basta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned
words (Raffel et al., 2020) fall short of mitigating
this problem for at least two reasons. First, they do
not reliably keep language models from generating
biased text: Examples in Figure 1 show that biased text can easily be generated by using only words
that are, by themselves, completely unproblematic.",55.4298173515982,219.0,9.0,328.0,0.10151366144418716," Pretraining neural networks using a language modeling objective leads to large improvements across a variety of natural language processing tasks Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. With model sizes continually increasing Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, ever larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are typically based on crawls from the internet that are only filtered with some basic rules Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. As a consequence, they contain non negligible amounts of text exhibiting biases that are undesirable or outright harmful for many potential applications. Unsurprisingly, language models trained on such data pick up, reproduce or even amplify these biases Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, Propname Propname Simple solutions such as using a list of banned words fall short of mitigating this problem for at least two reasons. First, they do not reliably keep language models from generating biased text: Examples in Figure 0 show that biased text can easily be generated by using only words that are, by themselves, completely unproblematic.", VERB ADJ NOUN VERB DET NOUN NOUN ADJ VERB ADP ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP NOUN NOUN ADV VERB PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADV ADJ VERB NOUN AUX ADJ PRON PART VERB NOUN CCONJ PART VERB NOUN ADP ADV ADJ NOUN NOUN ADP ADJ PUNCT ADV PUNCT ADJ ADJ NOUN AUX ADV VERB ADP NOUN ADP DET NOUN PRON AUX ADV VERB ADP DET ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET NOUN PUNCT PRON VERB ADJ ADJ NOUN ADP NOUN VERB NOUN PRON AUX ADJ CCONJ ADJ ADJ ADP ADJ ADJ NOUN PUNCT ADV PUNCT NOUN NOUN VERB ADP ADJ NOUN VERB ADP PUNCT VERB CCONJ ADV VERB DET NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN ADJ NOUN ADJ ADP VERB DET NOUN ADP VERB NOUN VERB ADJ ADP VERB DET NOUN ADP ADV ADV NUM NOUN PUNCT ADV PUNCT PRON AUX PART ADV VERB NOUN NOUN ADP VERB ADJ NOUN PUNCT NOUN ADP NOUN NUM VERB SCONJ ADJ NOUN AUX ADV AUX VERB ADP VERB ADV NOUN PRON AUX PUNCT ADP PRON PUNCT ADV ADJ PUNCT,0.4714828897338403,43.833333333333336,5.269961977186312,254,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
228,142,Zhiqing Sun,"[' The NLP community has witnessed a revolution of\npre-training self-supervised models. These models\nusually have hundreds of millions of parameters\n(Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)\n∗This work was done when the first author was an intern\nat Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP,\nBERT suffers from the heavy model size and high\nlatency, making it impractical for resource-limited\nmobile devices to deploy the power of BERT in\nmobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models\n(Turc et al., 2019; Tang et al., 2019; Sun et al.,\n2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a\nmodel that can be generically fine-tuned on different downstream NLP tasks as the original BERT\ndoes. In this paper, we propose MobileBERT to\nfill this gap. In practice, task-agnostic compression\nof BERT is desirable. Task-specific compression\nneeds to first fine-tune the original large BERT\nmodel into a task-specific teacher and then distill.', 'Such a process is much more complicated (Wu\net al., 2019) and costly than directly fine-tuning a\ntask-agnostic compact model. At first glance, it may seem straightforward to\nobtain a task-agnostic compact BERT. For example,\none may just take a narrower or shallower version\nof BERT, and train it until convergence by minimizing a convex combination of the prediction loss\nand distillation loss (Turc et al., 2019; Sun et al.,\n2019). Unfortunately, empirical results show that\nsuch a straightforward approach results in significant accuracy loss (Turc et al., 2019). This may not\nbe that surprising. It is well-known that shallow\nnetworks usually do not have enough representation power while narrow and deep networks are\ndifficult to train. Our MobileBERT is designed to be as deep as\nBERTLARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self-attentions and feed-forward networks (Figure 1). To train MobileBERT, a deep\nand thin model, we first train a specially designed\nteacher model, an inverted-bottleneck incorporated\nBERTLARGE model (IB-BERT). Then, we conduct\nknowledge transfer from IB-BERT to MobileBERT. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations1\nshow that MobileBERT\nis 4.3× smaller and 5.5× faster than BERTBASE,\nwhile it can still achieve competitive results on\nwell-known NLP benchmarks. On the natural language inference tasks of GLUE, MobileBERT can\nachieve a GLUE score of 77.7, which is only 0.6\nlower than BERTBASE, with a latency of 62 ms on\na Pixel 4 phone. On the SQuAD v1.1/v2.0 question\nanswering task, MobileBER obtains a dev F1 score\nof 90.3/80.2, which is even 1.5/2.1 higher than\nBERTBASE.']",intro_chunked," The NLP community has witnessed a revolution of
pre-training self-supervised models. These models
usually have hundreds of millions of parameters
(Peters et al., 2018; Radford et al., 2018; Devlin
et al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)
∗This work was done when the first author was an intern
at Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP,
BERT suffers from the heavy model size and high
latency, making it impractical for resource-limited
mobile devices to deploy the power of BERT in
mobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models
(Turc et al., 2019; Tang et al., 2019; Sun et al.,
2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a
model that can be generically fine-tuned on different downstream NLP tasks as the original BERT
does. In this paper, we propose MobileBERT to
fill this gap. In practice, task-agnostic compression
of BERT is desirable. Task-specific compression
needs to first fine-tune the original large BERT
model into a task-specific teacher and then distill.",66.83409090909092,220.0,10.0,306.0,0.3202774226665497," The Propname community has witnessed a revolution of pre training self supervised models. These models usually have hundreds of millions of parameters Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. Among these models, Propname This work was done when the first author was an intern at Propname Propname. shows substantial accuracy improvements. However, as one of the largest models ever in Propname, Propname suffers from the heavy model size and high latency, making it impractical for resource limited mobile devices to deploy the power of Propname in mobile based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill Propname into compact models Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre trained model, that is, a model that can be generically fine tuned on different downstream Propname tasks as the original Propname does. In this paper, we propose MobileBERT to fill this gap. In practice, task agnostic compression of Propname is desirable. Task specific compression needs to first fine tune the original large Propname model into a task specific teacher and then distill.", DET PROPN NOUN AUX VERB DET NOUN ADP ADJ NOUN NOUN VERB NOUN PUNCT DET NOUN ADV VERB NOUN ADP NOUN ADP NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET NOUN PUNCT PROPN DET NOUN AUX VERB SCONJ DET ADJ NOUN AUX DET NOUN ADP PROPN PROPN PUNCT VERB ADJ NOUN NOUN PUNCT ADV PUNCT ADP NUM ADP DET ADJ NOUN ADV ADP PROPN PUNCT PROPN VERB ADP DET ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT VERB PRON ADJ ADP NOUN VERB ADJ NOUN PART VERB DET NOUN ADP PROPN ADP ADJ VERB NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ DET ADJ PUNCT PRON AUX AUX DET NOUN PRON ADV VERB PROPN ADP ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET ADJ ADP PRON NOUN PUNCT PRON VERB PART ADV DET NOUN ADP VERB DET ADJ ADJ ADJ VERB NOUN PUNCT ADV ADV PUNCT DET NOUN PRON AUX AUX ADV ADJ VERB ADP ADJ ADJ PROPN NOUN SCONJ DET ADJ PROPN VERB PUNCT ADP DET NOUN PUNCT PRON VERB NOUN PART VERB DET NOUN PUNCT ADP NOUN PUNCT NOUN ADJ NOUN ADP PROPN AUX ADJ PUNCT NOUN ADJ NOUN VERB PART VERB ADJ NOUN DET ADJ ADJ PROPN NOUN ADP DET NOUN ADJ NOUN CCONJ ADV VERB PUNCT,0.4732824427480916,26.2,5.015267175572519,228,Zhiqing Sun,Aman Madaan,Aman Madaan,Aman Madaan,Zhiqing Sun,Timo Schick
178,92,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked," Convolutional neural networks have been the main design paradigm for image
understanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,
namely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest
in architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled
image dataset (JFT-300M [46], 300 millions images). The paper concluded that
transformers “do not generalize well when trained on insufficient amounts of data”,
and the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two
to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)
that is competitive with convnets having a similar number of parameters and
efficiency. It uses Imagenet as the sole training set.",40.842406554019476,217.0,9.0,363.0,0.6757087707519531," Convolutional neural networks have been the main design paradigm for image understanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set, namely Propname. Motivated by the success of attention based models in Propname Propname Propname, there has been increasing interest in architectures leveraging attention mechanisms within convnets. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks. The vision transformer introduced by Propname Propname Propname. is an architecture directly inherited from Propname Propname Propname, but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled image dataset. The paper concluded that transformers do not generalize well when trained on insufficient amounts of data, and the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 0 Propname node in two to three days that is competitive with convnets having a similar number of parameters and efficiency. It uses Propname as the sole training set.", ADJ ADJ NOUN AUX AUX DET ADJ NOUN NOUN ADP NOUN NOUN NOUN PUNCT SCONJ ADV VERB ADP NOUN NOUN NOUN PUNCT NUM ADP DET NOUN ADP PRON NOUN AUX DET NOUN ADP DET ADJ NOUN NOUN PUNCT ADV PROPN PUNCT VERB ADP DET NOUN ADP NOUN VERB NOUN ADP PROPN PROPN PROPN PUNCT PRON AUX AUX VERB NOUN ADP NOUN VERB NOUN NOUN ADP NOUN PUNCT ADV ADV ADJ NOUN AUX VERB ADJ NOUN VERB NOUN NOUN ADP NOUN PART VERB NOUN NOUN PUNCT DET NOUN NOUN VERB ADP PROPN PROPN PROPN PUNCT AUX DET NOUN ADV VERB ADP PROPN PROPN PROPN PUNCT CCONJ VERB ADP NOUN NOUN ADP ADJ NOUN NOUN ADP NOUN PUNCT PRON NOUN VERB ADJ NOUN ADP NOUN VERB ADP DET ADJ ADJ VERB NOUN NOUN PUNCT DET NOUN VERB SCONJ NOUN AUX PART VERB ADV SCONJ VERB ADP ADJ NOUN ADP NOUN PUNCT CCONJ DET NOUN ADP DET NOUN VERB ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN NOUN ADP DET ADJ NUM PROPN NOUN ADP NUM PART NUM NOUN PRON AUX ADJ ADP NOUN VERB DET ADJ NOUN ADP NOUN CCONJ NOUN PUNCT PRON VERB PROPN ADP DET ADJ NOUN NOUN PUNCT,0.5862068965517241,20.3,5.482758620689655,178,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,GPT-3.5
109,23,GPT-3.5,"[' The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text-related tasks and improving few-shot learning outcomes. This paper introduces GENPET, a method rooted in pattern-exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few-shot text generation.']",intro_chunked," The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text-related tasks and improving few-shot learning outcomes. This paper introduces GENPET, a method rooted in pattern-exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few-shot text generation.",18.27928571428575,140.0,5.0,265.0,0.7787601351737976," The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text related tasks and improving few shot learning outcomes. This paper introduces GENPET, a method rooted in pattern exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few shot text generation.", DET NOUN ADP VERB NOUN NOUN ADP ADJ NOUN NOUN VERB DET ADJ NOUN ADP VERB ADJ NOUN VERB NOUN CCONJ VERB ADJ NOUN VERB NOUN PUNCT DET NOUN VERB NOUN PUNCT DET NOUN VERB ADP NOUN VERB NOUN PUNCT DET NOUN ADV VERB ADP VERB ADJ NOUN ADP ADJ NOUN ADP NOUN NOUN PUNCT DET ADJ NOUN AUX PART VERB DET NOUN ADP DET NOUN PART NOUN NOUN NOUN PUNCT ADP DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT VERB DET ADJ NOUN VERB ADP ADJ NOUN NOUN PUNCT ADJ NOUN ADP VERB NOUN PUNCT CCONJ DET NOUN ADP NOUN PUNCT NOUN VERB PART VERB DET NOUN ADP NOUN NOUN ADP ADJ NOUN PUNCT ADP ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET ADJ NOUN VERB ADP NOUN ADP ADJ NOUN PUNCT VERB PRON NOUN ADP DET ADJ CCONJ ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN NOUN PUNCT,0.6470588235294118,30.6,5.823529411764706,109,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5
217,131,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked,"The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.",36.3953300561798,267.0,8.0,431.0,0.43164122104644775," The main difference between such image based diffusion solver and our graph based diffusion solver is that the latter can explicitly model the Propname selection process via the corresponding random variables, which is a natural design choice for formulating Propname problems, while the former does not support such a desirable formalism. Although graph based modeling has been employed with both constructive and improvement heuristics solvers, how to use graph based diffusion models for solving Propname problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the Propname framework: continuous diffusion with Propname noise and discrete diffusion with Propname noise. These two types of diffusion models have been applied to image processing but not to Propname problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin. We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Propname Propname Propname Propname, can be used as the backbone network for two different Propname complete combinatorial optimization problems: Traveling Propname Propname and Propname Propname Propname. Our experimental results show that DIFUSCO outperforms previous probabilistic Propname solvers on benchmark datasets of Propname and Propname problems with various sizes.", DET ADJ NOUN ADP ADJ NOUN VERB NOUN VERB CCONJ PRON NOUN VERB NOUN NOUN AUX SCONJ DET ADJ AUX ADV VERB DET PROPN NOUN NOUN ADP DET ADJ ADJ NOUN PUNCT PRON AUX DET ADJ NOUN NOUN ADP VERB PROPN NOUN PUNCT SCONJ DET ADJ AUX PART VERB DET DET ADJ NOUN PUNCT SCONJ NOUN VERB NOUN AUX AUX VERB ADP PRON ADJ CCONJ NOUN NOUN NOUN PUNCT SCONJ PART VERB NOUN VERB NOUN NOUN ADP VERB PROPN NOUN AUX PART AUX VERB ADV PUNCT ADP DET ADJ ADP PRON NOUN PUNCT PRON VERB NUM NOUN ADP ADJ NOUN ADP DET PROPN NOUN PUNCT ADJ NOUN ADP PROPN NOUN CCONJ ADJ NOUN ADP PROPN NOUN PUNCT DET NUM NOUN ADP NOUN NOUN AUX AUX VERB ADP NOUN NOUN CCONJ PART ADP PROPN NOUN PUNCT PRON ADV VERB DET NUM NOUN ADP VERB CCONJ VERB SCONJ ADJ NOUN VERB ADJ ADP ADJ NOUN ADP DET ADJ NOUN PUNCT PRON ADV VERB DET ADJ NOUN NOUN PART VERB DET NOUN NOUN ADP DET ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB SCONJ DET ADJ NOUN ADJ NOUN NOUN PUNCT ADV DET PROPN PROPN PROPN PROPN PUNCT AUX AUX VERB ADP DET NOUN NOUN ADP NUM ADJ PROPN ADJ ADJ NOUN NOUN PUNCT VERB PROPN PROPN CCONJ PROPN PROPN PROPN PUNCT PRON ADJ NOUN VERB SCONJ NOUN VERB ADJ ADJ PROPN NOUN ADP ADJ NOUN ADP PROPN CCONJ PROPN NOUN ADP ADJ NOUN PUNCT,0.5206611570247934,30.25,5.586776859504132,217,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
131,45,Aman Madaan,"[' Language models are now better than ever before at\ngenerating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense\nis in misunderstanding a user’s intent. The typical\nremedy of retraining with more data is prohibitive\ndue to the cost and infrastructure requirements. In\nsuch cases, even if users repeatedly observe the\nmodel making a mistake, there are no avenues to\nprovide feedback to the model to make it more\naccurate and personalized over time. Our goal is to allow users to correct such errors\ndirectly through interaction, and without retraining by injecting the knowledge required to correct the\nmodel’s misunderstanding. Building upon the recent success of injecting commonsense in the input\n(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in\nthe input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with\na growing memory of cases where the model misunderstood user’s intent and was provided with\ncorrective feedback. This feedback is question dependent, and thus the prompt for each sample is\nedited to adapt to the input. In this sense, our\nwork can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing\nthe prompts.', 'Our work adds interactivity to prompt\nengineering as it involves dynamically updating the\nprompt for every instance. Figure 1 presents a sample interaction between a\nuser and GPT-3 that our setup enables. The model\nwas asked for a similar word. However, the model’s\n(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task\ninstruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,\nnote that such instructional correction is feasible\neven if the user does not know the correct answer to\ntheir question, as they are critiquing the model’s understanding of their intent, rather than the answers\nthemselves. Thus, our setup does not require the\nusers to be experts at tasks being solved, another\nadvantage of our approach. Further, it is desirable to have a system that can\nleverage past feedback on new, unseen examples\nfor prompt-editing. We maintain a memory M of\nsuch feedback as a set of key-value pairs, where the\nkey is a misunderstood question, and the value is\nthe user’s feedback to correct that misunderstanding. Given a new question, we check if the model\nhas made a mistake on a similar question earlier,\nby querying the memory for a similar question.', 'If\nfound, append the corresponding feedback to the\nquestion prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive\nreminding in psychology (Jacoby and Wahlheim,\n2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for\nthe system and provides representative implementations for each component. We then demonstrate\nthe system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure\n1), (2) word scrambling (e.g., anagrams), (3) ethical\nreasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)\nethics reasoning with user feedback being natural\nlanguage. We find that in all cases, GPT-3’s accuracy significantly increases with time, without\nretraining, as our approach enables it to use corrective feedback from earlier examples to avoid\nsimilar misunderstandings on future examples. In\nsummary, our contributions are:• We show that a large model like GPT-3 can be\nimproved after deployment, without retraining,\nthrough a memory-assisted architecture. • Our implementation, MemPrompt, is the first\ndemonstration that this is possible - this is an important step forward for real use of LMs, and the\npaper sets out a general architecture that others can\nbuild on, a specific implementation, and detailed\nevaluation on multiple tasks.']",intro_chunked," Language models are now better than ever before at
generating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense
is in misunderstanding a user’s intent. The typical
remedy of retraining with more data is prohibitive
due to the cost and infrastructure requirements. In
such cases, even if users repeatedly observe the
model making a mistake, there are no avenues to
provide feedback to the model to make it more
accurate and personalized over time. Our goal is to allow users to correct such errors
directly through interaction, and without retraining by injecting the knowledge required to correct the
model’s misunderstanding. Building upon the recent success of injecting commonsense in the input
(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in
the input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with
a growing memory of cases where the model misunderstood user’s intent and was provided with
corrective feedback. This feedback is question dependent, and thus the prompt for each sample is
edited to adapt to the input. In this sense, our
work can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing
the prompts.",54.70405684754522,215.0,9.0,325.0,0.16483958065509796," Language models are now better than ever before at generating realistic content, but still lack commonsense. One failure mode due to a lack of commonsense is in misunderstanding a users intent. The typical remedy of retraining with more data is prohibitive due to the cost and infrastructure requirements. In such cases, even if users repeatedly observe the model making a mistake, there are no avenues to provide feedback to the model to make it more accurate and personalized over time. Our goal is to allow users to correct such errors directly through interaction, and without retraining by injecting the knowledge required to correct the models misunderstanding. Building upon the recent success of injecting commonsense in the input, we propose a novel approach of injecting knowledge in the input via interactive feedback from an end user. Our approach, Propname, pairs Propname 0 with a growing memory of cases where the model misunderstood users intent and was provided with corrective feedback. This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input. In this sense, our work can be seen as an instance of prompt engineering which involves editing the prompts.", NOUN NOUN AUX ADV ADJ ADP ADV ADV ADP VERB ADJ NOUN PUNCT CCONJ ADV VERB NOUN PUNCT NUM NOUN NOUN ADP ADP DET NOUN ADP NOUN AUX ADP VERB DET NOUN NOUN PUNCT DET ADJ NOUN ADP VERB ADP ADJ NOUN AUX ADJ ADJ ADP DET NOUN CCONJ NOUN NOUN PUNCT ADP ADJ NOUN PUNCT ADV SCONJ NOUN ADV VERB DET NOUN VERB DET NOUN PUNCT PRON VERB DET NOUN PART VERB NOUN ADP DET NOUN PART VERB PRON ADV ADJ CCONJ VERB ADP NOUN PUNCT PRON NOUN AUX PART VERB NOUN PART VERB ADJ NOUN ADV ADP NOUN PUNCT CCONJ ADP VERB ADP VERB DET NOUN VERB PART VERB DET NOUN VERB PUNCT VERB SCONJ DET ADJ NOUN ADP VERB NOUN ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP VERB NOUN ADP DET NOUN ADP ADJ NOUN ADP DET NOUN NOUN PUNCT PRON NOUN PUNCT PROPN PUNCT VERB PROPN NUM ADP DET VERB NOUN ADP NOUN SCONJ DET NOUN NOUN NOUN ADJ CCONJ AUX VERB ADP ADJ NOUN PUNCT DET NOUN AUX NOUN ADJ PUNCT CCONJ ADV DET NOUN ADP DET NOUN AUX VERB PART VERB ADP DET NOUN PUNCT ADP DET NOUN PUNCT PRON NOUN AUX AUX VERB ADP DET NOUN ADP ADJ NOUN PRON VERB VERB DET NOUN PUNCT,0.586046511627907,23.88888888888889,4.758139534883721,131,Aman Madaan,Aman Madaan,Aman Madaan,Zhiqing Sun,Aman Madaan,Hugo Touvron
122,36,Aman Madaan,"[' The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.', '(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),\ndeliberately sketched as controlled studies. First, we identify key components of an example in few-shot\nprompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping\nall but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).', 'Finally, we elicit\nmeaningful findings via conducting a systematic and qualitative analysis of the performance divergence\nbetween different prompt queries. Our experiments on four diverse reasoning tasks and across three large\nlanguage models—PaLM, GPT-3, and CODEX, reveal several surprising findings:\n- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In\naddition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the\ncorrectness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute\nchiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain\ncorrect outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays\na vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense\nknowledge), and patterns help reinforce task understanding, enabling the language model to generate text\nthat helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this\ninterplay between text and patterns—COT helps a language model in imitating the prompt and generating\nthe right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated\nby applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key\nrole in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a\nset of key design principles is an important challenge. To this end, we distill our findings to create concise\nprompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without\nnegative repercussions on the task solve rate.']",intro_chunked,"(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),
deliberately sketched as controlled studies. First, we identify key components of an example in few-shot
prompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping
all but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).",51.54055456171736,215.0,13.0,352.0,0.27030858397483826," proposed chain of thought prompting, showing that the few shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks. Despite its wide range usage, the rationale behind the success of Propname remains unclear. Recent work draws parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind Propname. Contributions and findings. We construct a series of tailored counterfactual prompts, deliberately sketched as controlled studies. First, we identify key components of an example in few shot prompting as follows: Propname, Propname, and Propname. Next, we perform counterfactual promptingkeeping all but one component fixed with Greek alphabets.", VERB NOUN ADP NOUN VERB PUNCT VERB SCONJ DET ADJ NOUN NOUN ADP NOUN ADV VERB ADP ADJ ADJ NOUN NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT SCONJ PRON ADJ NOUN NOUN PUNCT DET NOUN ADP DET NOUN ADP PROPN VERB ADJ PUNCT ADJ NOUN VERB NOUN ADP ADJ NOUN PUNCT NOUN ADV VERB ADP DET NOUN ADP VERB DET NOUN PUNCT ADJ ADP DET NOUN PUNCT PRON AUX VERB SCONJ NOUN AUX ADV AUX ADJ PART VERB DET ADJ NOUN PUNCT SCONJ ADJ PUNCT ADJ ADJ ADJ NOUN VERB ADJ ADP VERB SCONJ PUNCT SCONJ PUNCT CCONJ SCONJ DET NOUN VERB PUNCT ADV PUNCT NOUN AUX VERB PART VERB DET ADJ ADJ NOUN ADP DET VERB NOUN PUNCT ADV PUNCT PRON VERB ADV DET ADJ NOUN ADP PRON NOUN CCONJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB ADJ NOUN ADP VERB DET NOUN ADP PROPN PUNCT NOUN CCONJ NOUN PUNCT PRON VERB DET NOUN ADP VERB ADJ NOUN PUNCT ADV VERB ADP VERB NOUN PUNCT ADV PUNCT PRON VERB ADJ NOUN ADP DET NOUN ADP ADJ NOUN VERB SCONJ VERB PUNCT PROPN PUNCT PROPN PUNCT CCONJ PROPN PUNCT ADV PUNCT PRON VERB ADJ NOUN PRON CCONJ NUM NOUN VERB ADP ADJ NOUN PUNCT,0.6666666666666666,15.923076923076923,5.091787439613527,122,Aman Madaan,Aman Madaan,Timo Schick,Timo Schick,Aman Madaan,Hugo Touvron
260,174,Timo Schick,"[' Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.', 'The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.', 'However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.']",intro_chunked," Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.",59.14833582461388,223.0,9.0,323.0,0.2773232161998749," Distributed representations of words are a key component of natural language processing systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective have led to large performance gains for a variety of Propname tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine tune entire language models for specific tasks. Taking up this idea, Propname Propname Propname. introduced Propname, a bidirectional language model based on the Propname that has achieved a new state of the art for several Propname tasks. As demonstrated by Propname Propname Propname., it is possible for language models to solve a diverse set of tasks to some extent without any form of task specific fine tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can ask it for properties of that word using natural language. For example, a language model that understands the concept of guilt should be able to correctly complete the sentence Propname is the opposite of. with the word innocence.", VERB NOUN ADP NOUN AUX DET ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT ADP ADJ PUNCT ADJ ADJ NOUN VERB VERB DET ADJ NOUN NOUN NOUN AUX VERB ADP ADJ NOUN NOUN ADP DET NOUN ADP PROPN NOUN PUNCT ADV PUNCT ADJ NOUN AUX VERB PART PART ADV VERB NOUN NOUN ADP NOUN NOUN PUNCT CCONJ ADP ADJ NOUN ADJ NOUN NOUN ADP ADJ NOUN PUNCT VERB ADP DET NOUN PUNCT PROPN PROPN PROPN PUNCT VERB PROPN PUNCT DET ADJ NOUN NOUN VERB ADP DET PROPN PRON AUX VERB DET ADJ NOUN ADP DET NOUN ADP ADJ PROPN NOUN PUNCT SCONJ VERB ADP PROPN PROPN PROPN PUNCT PUNCT PRON AUX ADJ SCONJ NOUN NOUN PART VERB DET ADJ NOUN ADP NOUN ADP DET NOUN ADP DET NOUN ADP NOUN ADJ ADJ NOUN PUNCT PRON AUX AUX VERB ADP ADV VERB DET NOUN ADP NOUN ADP ADJ NOUN NOUN PRON AUX PART AUX VERB ADP DET NOUN PUNCT DET ADV ADJ NOUN AUX ADV AUX VERB PART VERB SCONJ ADV DET NOUN NOUN VERB DET VERB NOUN PUNCT PRON AUX VERB PRON ADP NOUN ADP DET NOUN VERB ADJ NOUN PUNCT ADP NOUN PUNCT DET NOUN NOUN PRON VERB DET NOUN ADP NOUN AUX AUX ADJ PART ADV VERB DET NOUN PROPN AUX DET NOUN ADP PUNCT ADP DET NOUN NOUN PUNCT,0.5294117647058824,22.1,4.8054298642533935,260,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
240,154,Zhiqing Sun,"[' Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.', 'Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.', 'Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.']",intro_chunked,"Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.",40.303570376914024,283.0,12.0,477.0,0.724690854549408," Therefore, we are seeking an approach that can have the advantage of modeling document level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end to end fashion. In this paper, we propose an end to end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short and long range dependency between the words in the document. Afterwards, the graph convolutional neural network is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism to address the over generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 0 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back propagation in an end to end fashion. Experimental results show that our proposed DivGraphPointer achieves state of the art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.", ADV PUNCT PRON AUX VERB DET NOUN PRON AUX VERB DET NOUN ADP VERB NOUN NOUN NOUN NOUN PUNCT VERB ADJ NOUN PUNCT CCONJ ADV AUX ADV VERB ADP DET NOUN PART VERB NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP NOUN NOUN VERB NOUN ADP VERB ADJ NOUN ADP NOUN PUNCT ADV PUNCT VERB DET NOUN NOUN PUNCT PRON ADV VERB DET NOUN NOUN ADP PRON PUNCT PRON VERB ADJ NOUN ADP NUM NOUN CCONJ VERB CCONJ DET ADJ CCONJ ADJ NOUN NOUN ADP DET NOUN ADP DET NOUN PUNCT ADV PUNCT DET NOUN ADJ ADJ NOUN AUX VERB ADP DET NOUN NOUN PART VERB DET NOUN ADP DET NOUN PUNCT PRON ADV VERB DET NOUN NOUN PUNCT PART VERB ADJ NOUN ADP NOUN PUNCT PRON VERB DET ADJ NOUN NOUN NOUN ADP DET NOUN NOUN PUNCT PRON ADV VERB NOUN ADP DET NOUN NOUN PART VERB DET NOUN PUNCT NUM NOUN NOUN AUX VERB PART VERB DET NOUN ADP DET ADJ NOUN PUNCT ADV PUNCT PRON VERB DET NOUN NOUN NOUN PART VERB DET ADP NOUN NOUN ADP NOUN NOUN ADP ADJ NOUN CCONJ DET ADJ NOUN NOUN PART ADV VERB DET ADJ NOUN NOUN ADP ADJ NOUN PUNCT NOUN NUM VERB PRON NOUN ADV PUNCT DET ADJ NOUN AUX AUX ADV CCONJ ADV VERB ADP ADJ NOUN ADP DET NOUN PART VERB NOUN PUNCT ADJ NOUN VERB SCONJ PRON VERB NOUN VERB NOUN ADP DET NOUN NOUN ADP NOUN NOUN ADP NUM NOUN CCONJ ADV VERB DET VERB VERB CCONJ VERB NOUN NOUN NOUN PUNCT DET NOUN ADP DET NOUN AUX ADJ PUNCT PRON VERB DET NOUN ADJ NOUN NOUN ADP NOUN NOUN PRON AUX ADV VERB NOUN NOUN NOUN NOUN PUNCT PRON VERB NUM ADJ NOUN NOUN PRON VERB DET NOUN NOUN NOUN PART VERB ADJ NOUN PUNCT,0.4721311475409836,25.416666666666668,5.488524590163935,240,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
236,150,Zhiqing Sun,"[' Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently.', 'Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.', 'However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts.']",intro_chunked,"Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.",45.6006363636364,198.0,15.0,346.0,0.7370893359184265," Thus the decoding consistency can not be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non autoregressive models can not model the highly multimodal distribution of target sequences properly. For example, an English sentence Thank you. can have many correct German translations like Propname., Propname schon., or Propname Propname.. In practice, this will lead to inconsistent outputs such as Propname Propname. or Propname schon.. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation as a sequence labeling problem and propose to use linear chain Propname Propname Propname to model richer structural dependencies. By modeling the co occurrence relationship between adjacent words, the Propname based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 0, the probability of the target sentence is globally normalized: where Propname is the pairwise potential for yi0 and Propname. Such a probability form could better model the multiple modes in target translations.", ADV DET VERB NOUN AUX PART AUX VERB ADP DET NOUN NOUN PUNCT DET ADJ NOUN PRON AUX AUX VERB AUX DET NOUN NOUN PUNCT DET ADJ ADJ NOUN AUX PART VERB DET ADV ADJ NOUN ADP NOUN NOUN ADV PUNCT ADP NOUN PUNCT DET ADJ NOUN VERB PRON PUNCT AUX VERB ADJ ADJ ADJ NOUN ADP PROPN PUNCT PUNCT PROPN NOUN PUNCT PUNCT CCONJ PROPN PROPN PUNCT ADP NOUN PUNCT PRON AUX VERB ADP ADJ NOUN ADJ ADP PROPN PROPN PUNCT CCONJ PROPN NOUN PUNCT PART VERB DET NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PART VERB DET ADJ NOUN NOUN ADP DET ADJ ADJ NOUN PART ADV VERB DET ADJ NOUN ADP NOUN NOUN PUNCT ADV PUNCT PRON VERB NOUN NOUN ADP DET NOUN NOUN NOUN CCONJ VERB PART VERB ADJ NOUN PROPN PROPN PROPN PART VERB ADJ ADJ NOUN PUNCT ADP VERB DET NOUN NOUN NOUN ADP ADJ NOUN PUNCT DET PROPN VERB ADJ NOUN NOUN AUX ADV VERB VERB NOUN ADP DET NOUN PUNCT ADJ ADP DET NOUN NOUN NOUN ADP NOUN NUM PUNCT DET NOUN ADP DET NOUN NOUN AUX ADV VERB PUNCT SCONJ PROPN AUX DET NOUN NOUN ADP NOUN CCONJ PROPN PUNCT DET DET NOUN NOUN AUX ADV VERB DET ADJ NOUN ADP NOUN NOUN PUNCT,0.5518867924528302,21.2,5.320754716981132,236,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Hugo Touvron
169,83,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"• Our training procedure for ImageNet-1k allow us to train a billion-parameter
ViT-H (52 layers) without any hyper-parameter adaptation, just using the
same stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,
i.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of
GPUs required and the training time for ViT-H, making it effectively possible
to train such models without a reduced amount of resources. This is thanks
to our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with
BerT-like self-supervised approaches [2, 19] with their default setting and
when using the same level of annotations and less epochs, both for the tasks
of image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance
trade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better
to another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.",51.34369477911646,249.0,9.0,375.0,0.649741530418396," Our training procedure for Propname 0k allow us to train a billion parameter ViT Propname without any hyper parameter adaptation, just using the same stochastic depth drop rate as for the ViT Propname It attains 00.0 at 000000, ie, 0.0 higher than the corresponding ViT Propname trained in the same setting. Without sacrificing performance, we divide by more than 0 the number of GPUs required and the training time for Propname Propname, making it effectively possible to train such models without a reduced amount of resources. This is thanks to our pre training at lower resolution, which reduces the peak memory. For ViT Propname and Propname Propname models, our supervised training approach is on par with Propname like self supervised approaches with their default setting and when using the same level of annotations and less epochs, both for the tasks of image classification and of semantic segmentation. With this improved training procedure, a vanilla ViT closes the gap with recent state of the art architectures, often offering better computeperformance trade offs. Our models are also comparatively better on the additional test set Propname Propname, which indicates that our trained models generalize better to another validation set than most prior works. An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target datasettask.", PRON NOUN NOUN ADP PROPN NOUN VERB PRON PART VERB DET NUM NOUN NOUN PROPN ADP DET ADJ NOUN NOUN PUNCT ADV VERB DET ADJ ADJ NOUN NOUN NOUN ADP ADP DET NOUN PROPN PRON VERB NUM ADP NUM PUNCT ADV PUNCT NUM ADJ ADP DET VERB NOUN PROPN VERB ADP DET ADJ NOUN PUNCT ADP VERB NOUN PUNCT PRON VERB ADP ADJ ADP NUM DET NOUN ADP NOUN VERB CCONJ DET NOUN NOUN ADP PROPN PROPN PUNCT VERB PRON ADV ADJ PART VERB ADJ NOUN ADP DET VERB NOUN ADP NOUN PUNCT PRON AUX NOUN ADP PRON ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET NOUN NOUN PUNCT ADP NOUN PROPN CCONJ PROPN PROPN NOUN PUNCT PRON ADJ NOUN NOUN AUX ADP NOUN ADP PROPN SCONJ NOUN VERB NOUN ADP PRON NOUN NOUN CCONJ SCONJ VERB DET ADJ NOUN ADP NOUN CCONJ ADJ NOUN PUNCT PRON ADP DET NOUN ADP NOUN NOUN CCONJ ADP ADJ NOUN PUNCT ADP DET ADJ NOUN NOUN PUNCT DET NOUN NOUN VERB DET NOUN ADP ADJ NOUN ADP DET NOUN NOUN PUNCT ADV VERB ADJ NOUN NOUN NOUN PUNCT PRON NOUN AUX ADV ADV ADV ADP DET ADJ NOUN VERB PROPN PROPN PUNCT PRON VERB SCONJ PRON VERB NOUN VERB ADV ADP DET NOUN VERB ADP ADV ADJ NOUN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN NOUN VERB ADP NOUN VERB NOUN NOUN PUNCT PRON VERB SCONJ PRON VERB DET ADJ NOUN ADP DET NOUN CCONJ SCONJ DET ADJ NOUN VERB DET NOUN ADP DET NOUN NOUN PUNCT,0.5758754863813229,32.125,4.898832684824903,169,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Timo Schick
121,35,Aman Madaan,"[' The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.', '(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),\ndeliberately sketched as controlled studies. First, we identify key components of an example in few-shot\nprompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping\nall but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).', 'Finally, we elicit\nmeaningful findings via conducting a systematic and qualitative analysis of the performance divergence\nbetween different prompt queries. Our experiments on four diverse reasoning tasks and across three large\nlanguage models—PaLM, GPT-3, and CODEX, reveal several surprising findings:\n- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In\naddition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the\ncorrectness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute\nchiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain\ncorrect outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays\na vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense\nknowledge), and patterns help reinforce task understanding, enabling the language model to generate text\nthat helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this\ninterplay between text and patterns—COT helps a language model in imitating the prompt and generating\nthe right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated\nby applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key\nrole in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a\nset of key design principles is an important challenge. To this end, we distill our findings to create concise\nprompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without\nnegative repercussions on the task solve rate.']",intro_chunked," The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.",54.8607263814617,187.0,6.0,266.0,0.5116338133811951," The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence. This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few shot learning. Few shot learning has shown promising applications for a wide range of tasks. While beneficial, this setting requires meticulous design of prompts. Ling Propname Propname. pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Propname Propname Propname.", DET NOUN PART VERB DET ADV ADJ NOUN ADP VERB DET ADJ NOUN AUX NUM ADP DET NOUN ADP ADJ NOUN PUNCT PRON AUX ADP ADJ NOUN ADP ADJ ADJ NOUN NOUN PUNCT PRON ADV VERB ADP DET ADJ VERB NOUN ADP NOUN PUNCT ADV PUNCT ADJ NOUN NOUN AUX VERB ADJ NOUN ADP VERB DET NOUN PART VERB DET NOUN PUNCT ADP DET ADJ NOUN PUNCT ADV VERB ADP ADJ NOUN NOUN PUNCT ADJ NOUN NOUN AUX VERB ADJ NOUN ADP DET ADJ NOUN ADP NOUN PUNCT SCONJ ADJ PUNCT DET NOUN VERB ADJ NOUN ADP NOUN PUNCT VERB PROPN PROPN PUNCT VERB DET NOUN ADP VERB ADJ NOUN NOUN ADP DET ADJ NOUN ADP NOUN PART VERB NOUN NOUN ADP ADJ NOUN PUNCT ADV PUNCT PROPN PROPN PROPN PUNCT,0.6564885496183206,16.375,5.106870229007634,121,Aman Madaan,Aman Madaan,Timo Schick,Timo Schick,Aman Madaan,Hugo Touvron
85,85,Timo Schick,"[' This work addresses the task of generating English sentences from Abstract Meaning Representation (AMR) graphs. To cope with this task, we transform each input AMR graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an algorithm that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Bleu score of 27.4 on the LDC2014T12 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data.']",abstract_chunked," This work addresses the task of generating English sentences from Abstract Meaning Representation (AMR) graphs. To cope with this task, we transform each input AMR graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an algorithm that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Bleu score of 27.4 on the LDC2014T12 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data.",38.7494375,128.0,5.0,215.0,0.7665859460830688," This work addresses the task of generating English sentences from Propname Propname Propname graphs. To cope with this task, we transform each input Propname graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an Propname that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Propname score of 00.0 on the LDC0000T00 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data.", DET NOUN VERB DET NOUN ADP VERB ADJ NOUN ADP PROPN PROPN PROPN NOUN PUNCT PART VERB ADP DET NOUN PUNCT PRON VERB DET NOUN PROPN NOUN ADP DET NOUN ADJ ADP DET NOUN NOUN CCONJ VERB PRON ADP ADJ NOUN ADP VERB ADJ VERB NOUN ADP PRON PUNCT ADV PUNCT DET NOUN AUX VERB ADP DET NOUN NOUN ADP VERB PRON NOUN ADP DET ADJ NOUN PUNCT PRON VERB ADJ NOUN NOUN PART VERB DET NOUN ADP DET ADJ NOUN CCONJ VERB DET PROPN PRON ADV VERB DET ADJ NOUN ADP NOUN PART AUX VERB PUNCT VERB DET ADJ NOUN NOUN PUNCT PRON NOUN VERB DET PROPN NOUN ADP NUM ADP DET NOUN NOUN NOUN PUNCT DET ADJ NOUN VERB ADV ADV ADP VERB ADJ ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT,0.7058823529411765,27.2,5.110294117647059,85,Timo Schick,Aman Madaan,GPT-3.5,Aman Madaan,Timo Schick,Aman Madaan
250,164,Timo Schick,"[' Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018; Devlin et al., 2019; Liuet al., 2019; Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model’s output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is __”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Schütze, 2021). Recently, Brown et al. (2020) introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as LM problems, GPT-3 achieves near state-of-the-art results for some SuperGLUE (Wang et al., 2019) tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed.', 'While being straightforward to use, this method has two major drawbacks: It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern-exploiting training (PET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way. In this work, we adapt PET for tasks that require predicting multiple tokens. We then show that incombination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET’s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET’s “green” properties, we see ourwork as an important contribution to an environmentally sound NLP.']",intro_chunked,"While being straightforward to use, this method has two major drawbacks: It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern-exploiting training (PET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way. In this work, we adapt PET for tasks that require predicting multiple tokens. We then show that incombination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET’s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET’s “green” properties, we see ourwork as an important contribution to an environmentally sound NLP.",42.69071201413428,283.0,10.0,453.0,0.5704786777496338," While being straightforward to use, this method has two major drawbacks: It requires a gigantic Propname to work well, making it unusable in many real world scenarios and resulting in a large carbon footprint. It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern exploiting training, which combines the idea of reformulating tasks as cloze questions with regular gradient based finetuning. While Propname additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real world applications. Crucially, Propname only works when the answers to be predicted by the Propname correspond to a single token in its vocabulary; this is a severe limitation as many tasks can not easily be worded that way. In this work, we adapt Propname for tasks that require predicting multiple tokens. We then show that incombination with Propname, Propname and its iterative variant both outperform Propname 0 on SuperGLUE with 00 training examples, while requiring only 0.0 of its parameters. Moreover, training with Propname can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to Propname strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying Propname. Given PETs green properties, we see ourwork as an important contribution to an environmentally sound Propname.", SCONJ AUX ADJ PART VERB PUNCT DET NOUN VERB NUM ADJ NOUN PUNCT PRON VERB DET ADJ PROPN PART VERB ADV PUNCT VERB PRON ADJ ADP ADJ ADJ NOUN NOUN CCONJ VERB ADP DET ADJ NOUN NOUN PUNCT PRON AUX PART VERB ADP ADJ ADP DET ADJ NOUN ADP DET NOUN NOUN ADP ADJ NOUN AUX VERB ADP DET ADJ NUM NOUN PUNCT DET NOUN ADP NOUN AUX NOUN VERB NOUN PUNCT PRON VERB DET NOUN ADP VERB NOUN ADP VERB NOUN ADP ADJ NOUN VERB NOUN PUNCT SCONJ PROPN ADV VERB ADJ NOUN PUNCT ADJ NOUN AUX ADV ADJ PART VERB ADP VERB NOUN ADP ADJ ADJ NOUN NOUN PUNCT ADV PUNCT PROPN ADV VERB SCONJ DET NOUN PART AUX VERB ADP DET PROPN VERB ADP DET ADJ NOUN ADP PRON NOUN PUNCT PRON AUX DET ADJ NOUN SCONJ ADJ NOUN AUX PART ADV AUX VERB DET NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN ADP NOUN PRON VERB VERB ADJ NOUN PUNCT PRON ADV VERB DET NOUN ADP PROPN PUNCT PROPN CCONJ PRON ADJ NOUN PRON ADJ PROPN NUM ADP NUM ADP NUM NOUN NOUN PUNCT SCONJ VERB ADV NUM ADP PRON NOUN PUNCT ADV PUNCT NOUN ADP PROPN AUX AUX VERB ADP ADJ NOUN ADP DET ADJ NOUN ADP VERB ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB SCONJ ADJ NOUN AUX ADV AUX VERB ADP ADJ NOUN CCONJ VERB DET ADJ NOUN ADP DET NOUN VERB ADP PROPN ADJ NOUN PUNCT PRON NOUN PART VERB ADJ NOUN NOUN PUNCT PRON NOUN ADP NOUN PRON AUX ADJ PART VERB PUNCT PRON NOUN ADP VERB NOUN PUNCT CCONJ NOUN ADP DET VERB PROPN PUNCT VERB ADJ ADJ NOUN PUNCT PRON VERB NOUN ADP DET ADJ NOUN ADP DET ADV ADJ PROPN PUNCT,0.559322033898305,29.5,4.959322033898305,250,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
255,169,Timo Schick,"[' Pretraining neural networks using a language modeling objective leads to large improvements across\na variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019). With model sizes continually increasing\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are\ntypically based on crawls from the internet that are\nonly filtered with some basic rules (Radford et al.,\n2019; Raffel et al., 2020). As a consequence, they\ncontain non-negligible amounts of text exhibiting\nbiases that are undesirable or outright harmful for\nmany potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such\ndata pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;\nBasta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned\nwords (Raffel et al., 2020) fall short of mitigating\nthis problem for at least two reasons. First, they do\nnot reliably keep language models from generating\nbiased text: Examples in Figure 1 show that biased text can easily be generated by using only words\nthat are, by themselves, completely unproblematic.', 'As many such words are important words of the\nEnglish vocabulary and thus needed for meaningful\ntext generation, they should not be included in a list\nof banned words. Secondly, banning words also\nprevents language models from gaining knowledge\nof topics related to the banned words, which may\nbe necessary for some applications.2\nIt is therefore inherently difficult to ban words without doing\nharm to a model’s capabilities. Building training datasets with more care and\ndeliberation, an alternative solution discussed by\nBender et al. (2021), is important, especially for\nimproving linguistic and cultural diversity in online\nand other forms of communication. However, for\nlarge language models that are available for common global languages, it is desirable to also have\nother mechanisms to address bias because dataset\ncuration and documentation is extremely resource\nintensive, given the amount of data required. It\ncan also necessitate building different training sets\nand, accordingly, training different models for each\ndesired behavior, which can result in high environmental impact (Strubell et al., 2019).', 'In this paper, we therefore propose an approach\nthat, instead of trusting that a model will implicitly learn desired behaviors from the training data,\nmakes explicit how we expect it to behave at test\ntime: If the model is told which biases are undesired – and it is able to discern their presence –,\nit should be able to avoid them even if they are\npresent in some of the texts it has been trained on. As it is a necessary condition for this approach, we\nfirst explore whether language models are able to\ndetect when their own outputs exhibit undesirable\nattributes, based only on their internal knowledge –\na process to which we refer as self-diagnosis. We\nthen investigate whether this ability can be used\nto perform self-debiasing, i.e., whether language\nmodels can use this knowledge to discard undesired\nbehaviors in a fully unsupervised fashion. To this\nend, we propose a decoding algorithm that reduces\nthe probability of a model producing biased text,\nrequiring nothing more than a textual description\nof the undesired behavior, which can be as simple\nas a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in\nparticular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find\nthat their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in\nNLP']",intro_chunked,"As many such words are important words of the
English vocabulary and thus needed for meaningful
text generation, they should not be included in a list
of banned words. Secondly, banning words also
prevents language models from gaining knowledge
of topics related to the banned words, which may
be necessary for some applications.2
It is therefore inherently difficult to ban words without doing
harm to a model’s capabilities. Building training datasets with more care and
deliberation, an alternative solution discussed by
Bender et al. (2021), is important, especially for
improving linguistic and cultural diversity in online
and other forms of communication. However, for
large language models that are available for common global languages, it is desirable to also have
other mechanisms to address bias because dataset
curation and documentation is extremely resource
intensive, given the amount of data required. It
can also necessitate building different training sets
and, accordingly, training different models for each
desired behavior, which can result in high environmental impact (Strubell et al., 2019).",32.60944910179643,167.0,5.0,277.0,0.2345278561115265," As many such words are important words of the English vocabulary and thus needed for meaningful text generation, they should not be included in a list of banned words. Secondly, banning words also prevents language models from gaining knowledge of topics related to the banned words, which may be necessary for someapplications.0 It is therefore inherently difficult to ban words without doing harm to a models capabilities. Building training datasets with more care and deliberation, an alternative solution discussed by Propname Propname Propname., is important, especially for improving linguistic and cultural diversity in online and other forms of communication. However, for large language models that are available for common global languages, it is desirable to also have other mechanisms to address bias because dataset curation and documentation is extremely resource intensive, given the amount of data required. It can also necessitate building different training sets and, accordingly, training different models for each desired behavior, which can result in high environmental impact.", SCONJ ADJ ADJ NOUN AUX ADJ NOUN ADP DET ADJ NOUN CCONJ ADV VERB ADP ADJ NOUN NOUN PUNCT PRON AUX PART AUX VERB ADP DET NOUN ADP VERB NOUN PUNCT ADV PUNCT VERB NOUN ADV VERB NOUN NOUN ADP VERB NOUN ADP NOUN VERB ADP DET VERB NOUN PUNCT PRON AUX AUX ADJ ADP PRON PUNCT PRON AUX ADV ADV ADJ PART VERB NOUN ADP VERB NOUN ADP DET NOUN NOUN PUNCT VERB NOUN NOUN ADP ADJ NOUN CCONJ NOUN PUNCT DET ADJ NOUN VERB ADP PROPN PROPN PROPN PUNCT PUNCT AUX ADJ PUNCT ADV ADP VERB ADJ CCONJ ADJ NOUN ADP ADJ CCONJ ADJ NOUN ADP NOUN PUNCT ADV PUNCT ADP ADJ NOUN NOUN PRON AUX ADJ ADP ADJ ADJ NOUN PUNCT PRON AUX ADJ PART ADV VERB ADJ NOUN PART VERB NOUN SCONJ VERB NOUN CCONJ NOUN AUX ADV NOUN ADJ PUNCT VERB DET NOUN ADP NOUN VERB PUNCT PRON AUX ADV VERB VERB ADJ NOUN NOUN CCONJ PUNCT ADV PUNCT VERB ADJ NOUN ADP DET VERB NOUN PUNCT PRON AUX VERB ADP ADJ ADJ NOUN PUNCT,0.6404494382022472,35.6,5.269662921348314,255,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Zhiqing Sun
203,117,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"Sampled prompts
RM-RLHF
RM-SALMON
RLHF (Ouyang et al., 2022)
SFT
SFT-generated responses
RM-RLAIF
RLAIF (Bai et al., 2022)
SALMON (Ours)
Principle Aggregating
AI-labeled preferences
human-labeled preferences
SFT
SFT
AI-labeled preferences Principle-following reward model
Stand-alone reward model
Stand-alone reward model
Reward Score
Prompt + Response
Principles
Reward Score
Prompt + Response
Reward Score
Prompt + Response
Principles
Principles
Human Annotator
In general, SFT denotes the
Supervised Fine-Tuned model, but it
can also be RLHF-trained in RLAIF. General
Alignment
Safety
Alignment
General
Alignment
Figure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and
SALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give
high scores to generally good responses, while the principle-following reward model in SALMON
is trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1
reward hacking patterns in model outputs, such as self-praising at the
end of the response. Additionally, we found that we are able to emphasize distinct aspects of the
alignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)
by customizing the preference principles.",38.06600000000003,200.0,5.0,303.0,0.7761358022689819," Sampled prompts Propname Propname Propname Propname Propname Propname Propname generated responses Propname Propname Propname Propname Propname Propname Propname labeled preferences human labeled preferences Propname Propname Propname labeled preferences Principle following reward model Propname alone reward model Stand alone reward model Propname Propname Propname Propname Propname Propname Propname Propname Propname Propname Propname Propname Propname Propname Propname Propname Propname In general, Propname denotes the Propname Fine Tuned model, but it can also be Propname trained in Propname. Propname Propname Propname Propname Propname Propname Propname 0: Propname among Propname, Propname, and Propname. The vanilla reward models in Propname Propname are trained to give high scores to generally good responses, while the principle following reward model in Propname is trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed0 reward hacking patterns in model outputs, such as self praising at the end of the response. Additionally, we found that we are able to emphasize distinct aspects of the alignment in the Propname alignment framework by customizing the preference principles.", VERB NOUN PROPN PROPN PROPN PROPN PROPN PROPN PROPN VERB NOUN PROPN PROPN PROPN PROPN PROPN PROPN PROPN VERB NOUN ADJ VERB NOUN PROPN PROPN PROPN VERB NOUN ADJ VERB NOUN NOUN PROPN ADV NOUN NOUN VERB ADV NOUN NOUN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN ADP ADJ PUNCT PROPN VERB DET PROPN ADJ ADJ NOUN PUNCT CCONJ PRON AUX ADV AUX PROPN VERB ADP PROPN PUNCT PROPN PROPN PROPN PROPN PROPN PROPN PROPN NUM PUNCT PROPN ADP PROPN PUNCT PROPN PUNCT CCONJ PROPN PUNCT DET NOUN NOUN NOUN ADP PROPN PROPN AUX VERB PART VERB ADJ NOUN ADP ADV ADJ NOUN PUNCT SCONJ DET ADJ VERB NOUN NOUN ADP PROPN AUX VERB PART VERB NOUN NOUN VERB ADP VERB NOUN ADP DET NOUN NOUN PUNCT VERB PART VERB ADJ NOUN VERB NOUN ADP NOUN NOUN PUNCT ADJ ADP NOUN VERB ADP DET NOUN ADP DET NOUN PUNCT ADV PUNCT PRON VERB SCONJ PRON AUX ADJ PART VERB ADJ NOUN ADP DET NOUN ADP DET PROPN NOUN NOUN ADP VERB DET NOUN NOUN PUNCT,0.42702702702702705,37.0,5.902702702702703,203,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
18,18,GPT-3.5,"["" Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present PEER, a collaborative language model trained to imitate the entire writing process. PEER can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of PEER are trained to infill various parts of the writing process, leveraging self-training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate PEER's strong performance across diverse domains and editing tasks.""]",abstract_chunked," Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present PEER, a collaborative language model trained to imitate the entire writing process. PEER can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of PEER are trained to infill various parts of the writing process, leveraging self-training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate PEER's strong performance across diverse domains and editing tasks.",30.831865671641793,134.0,7.0,248.0,0.6524875164031982," Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present Propname, a collaborative language model trained to imitate the entire writing process. Propname can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of Propname are trained to infill various parts of the writing process, leveraging self training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate Propname strong performance across diverse domains and editing tasks.", ADJ NOUN VERB DET ADJ NOUN ADP NOUN PUNCT VERB NOUN PUNCT CCONJ ADJ NOUN PUNCT ADJ NOUN NOUN PUNCT ADV PUNCT AUX VERB ADV PART VERB ADJ NOUN PUNCT VERB ADJ NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN VERB PART VERB DET ADJ NOUN NOUN PUNCT PROPN AUX VERB NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN ADP PRON NOUN PUNCT VERB NOUN ADP VERB VERB NOUN PUNCT NOUN PUNCT CCONJ ADJ NOUN PUNCT ADV PUNCT ADJ NOUN ADP PROPN AUX VERB PART VERB ADJ NOUN ADP DET NOUN NOUN PUNCT VERB NOUN NOUN NOUN PART VERB DET NOUN PUNCT NOUN PUNCT CCONJ NOUN ADP NOUN NOUN PUNCT DET NOUN VERB ADJ PART VERB ADP NOUN ADP NOUN NOUN CCONJ VERB PRON NOUN PART VERB NOUN PUNCT VERB ADJ NOUN PUNCT CCONJ VERB PRON NOUN PUNCT ADJ NOUN VERB PROPN ADJ NOUN ADP ADJ NOUN CCONJ NOUN NOUN PUNCT,0.610062893081761,22.714285714285715,5.484276729559748,18,Timo Schick,GPT-3.5,GPT-3.5,GPT-3.5,Timo Schick,GPT-3.5
31,31,Aman Madaan,"[' Conditional set generation learns a mapping\nfrom an input sequence of tokens to a set. Several NLP tasks, such as entity typing and\ndialogue emotion tagging, are instances of\nset generation. SEQ2SEQ models, a popular choice for set generation, treat a set as\na sequence and do not fully leverage its key\nproperties, namely order-invariance and cardinality. We propose a novel algorithm for\neffectively sampling informative orders over\nthe combinatorial space of label orders. We\njointly model the set cardinality and output\nby prepending the set size and taking advantage of the autoregressive factorization used\nby SEQ2SEQ models. Our method is a modelindependent data augmentation approach that\nendows any SEQ2SEQ model with the signals\nof order-invariance and cardinality. Training a\nSEQ2SEQ model on this augmented data (without any additional annotations) gets an average\nrelative improvement of 20% on four benchmark datasets across various models: BARTbase, T5-11B, and GPT3-175B.']",abstract_chunked," Conditional set generation learns a mapping
from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and
dialogue emotion tagging, are instances of
set generation. SEQ2SEQ models, a popular choice for set generation, treat a set as
a sequence and do not fully leverage its key
properties, namely order-invariance and cardinality. We propose a novel algorithm for
effectively sampling informative orders over
the combinatorial space of label orders. We
jointly model the set cardinality and output
by prepending the set size and taking advantage of the autoregressive factorization used
by SEQ2SEQ models. Our method is a modelindependent data augmentation approach that
endows any SEQ2SEQ model with the signals
of order-invariance and cardinality. Training a
SEQ2SEQ model on this augmented data (without any additional annotations) gets an average
relative improvement of 20% on four benchmark datasets across various models: BARTbase, T5-11B, and GPT3-175B.",41.7542105263158,152.0,7.0,257.0,0.5514452457427979," Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Propname models, a popular choice for set generation, treat a set as a sequence and do not fully leverage its key properties, namely order invariance and cardinality. We propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders. We jointly model the set cardinality and output by prepending the set size and taking advantage of the autoregressive factorization used by Propname models. Our method is a modelindependent data augmentation approach that endows any Propname model with the signals of order invariance and cardinality. Training a Propname model on this augmented data gets an average relative improvement of 00 on four benchmark datasets across various models: Propname, Propname 00B, and Propname Propname", ADJ NOUN NOUN VERB DET NOUN ADP DET NOUN NOUN ADP NOUN ADP DET NOUN PUNCT ADJ NOUN NOUN PUNCT ADJ ADP NOUN NOUN CCONJ NOUN NOUN NOUN PUNCT AUX NOUN ADP ADJ NOUN PUNCT PROPN NOUN PUNCT DET ADJ NOUN ADP ADJ NOUN PUNCT VERB DET NOUN ADP DET NOUN CCONJ AUX PART ADV VERB PRON ADJ NOUN PUNCT ADV VERB NOUN CCONJ NOUN PUNCT PRON VERB DET ADJ NOUN ADP ADV VERB ADJ NOUN ADP DET ADJ NOUN ADP NOUN NOUN PUNCT PRON ADV VERB DET VERB NOUN CCONJ NOUN ADP VERB DET VERB NOUN CCONJ VERB NOUN ADP DET ADJ NOUN VERB ADP PROPN NOUN PUNCT PRON NOUN AUX DET ADJ NOUN NOUN NOUN PRON VERB DET PROPN NOUN ADP DET NOUN ADP NOUN NOUN CCONJ NOUN PUNCT VERB DET PROPN NOUN ADP DET VERB NOUN VERB DET ADJ ADJ NOUN ADP NUM ADP NUM ADJ NOUN ADP ADJ NOUN PUNCT PROPN PUNCT PROPN NUM PUNCT CCONJ PROPN PROPN,0.5987654320987654,23.142857142857142,5.055555555555555,31,Aman Madaan,Aman Madaan,GPT-3.5,Aman Madaan,Aman Madaan,Aman Madaan
234,148,Zhiqing Sun,"[' State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences.', 'Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model.', 'We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.']",intro_chunked,"We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.",49.66403089187844,223.0,9.0,348.0,0.3895777463912964," We argue that such a single pass knowledge distillation process may not be sufficient for optimizing the Propname model as sequence Propname predicted by the Propname model can not be perfect. More importantly, it is not necessarily the best choice for alleviating the multi modality problem in the Propname model. In other words, without knowing how the choice of Propname by the Propname model would effect the training process in the Propname model, the current knowledge distillation approach is unavoidably sub optimal. To address this fundamental limitation, we propose a novel Propname Propname approach to the training of NAR models, where both the teacher and the student are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between Propname and Propname models, and is the first Propname approach to Propname models, to our knowledge. Propname. Propname illustrates our new framework. In addition, we develop a principled plug and play decoding module for effective removal of word duplication in the models output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.", PRON VERB SCONJ DET DET ADJ NOUN NOUN NOUN NOUN AUX PART AUX ADJ ADP VERB DET PROPN NOUN SCONJ NOUN PROPN VERB ADP DET PROPN NOUN AUX PART AUX ADJ PUNCT ADV ADV PUNCT PRON AUX PART ADV DET ADJ NOUN ADP VERB DET ADJ NOUN NOUN ADP DET PROPN NOUN PUNCT ADP ADJ NOUN PUNCT ADP VERB SCONJ DET NOUN ADP PROPN ADP DET PROPN NOUN AUX VERB DET NOUN NOUN ADP DET PROPN NOUN PUNCT DET ADJ NOUN NOUN NOUN AUX ADV VERB ADJ PUNCT PART VERB DET ADJ NOUN PUNCT PRON VERB DET ADJ PROPN PROPN NOUN ADP DET NOUN ADP NOUN NOUN PUNCT SCONJ CCONJ DET NOUN CCONJ DET NOUN AUX VERB DET ADJ ADP DET ADJ NOUN PUNCT CCONJ DET ADJ NOUN ADP DET NOUN AUX VERB PART VERB ADP DET ADJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN PART VERB NOUN ADP PROPN CCONJ PROPN NOUN PUNCT CCONJ AUX DET ADJ PROPN NOUN ADP PROPN NOUN PUNCT ADP PRON NOUN PUNCT PROPN PUNCT PROPN VERB PRON ADJ NOUN PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NOUN CCONJ VERB VERB NOUN ADP ADJ NOUN ADP NOUN NOUN ADP DET NOUN NOUN PUNCT PRON NOUN ADP NUM NOUN NOUN NOUN NOUN VERB SCONJ DET VERB NOUN VERB ADJ NOUN ADP PRON ADP DET ADJ NOUN NOUN ADP NOUN ADP NOUN NOUN PUNCT CCONJ VERB DET NOUN NOUN ADV PUNCT,0.5254237288135594,26.22222222222222,5.029661016949152,234,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
26,26,Aman Madaan,"[' The waning of Moore’s Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program’s performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, we use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI’s CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5ˆ for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10ˆ smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.']",abstract_chunked," The waning of Moore’s Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program’s performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, we use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI’s CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5ˆ for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10ˆ smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.",49.5869186046512,258.0,12.0,413.0,0.5869742035865784," The waning of Propname Propname has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large scale dataset of Performance Improving Propname, Propname. Propname contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the programs performance. We use Propname to evaluate and improve the capacity of large language models. Specifically, we use examples from Propname to fine tune multiple variants of Propname, a billion scale Propname decoder model. Additionally, we use examples from Propname to prompt OpenAIs Propname using a few shot prompting. By leveraging Propname, we find that both Propname and Propname can generate performance improving edits, with speedups of more than 0.0 for over 00 of the programs, for Propname and Propname, even after the Propname programs were compiled using the Propname optimization level. Crucially, we show that Propname allows Propname, an open sourced and 00 smaller model than Propname, to match the performance of Propname on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.", DET NOUN ADP PROPN PROPN AUX VERB DET NOUN ADP DET NOUN NOUN ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT SCONJ VERB NOUN AUX DET ADJ NOUN PART VERB VERB NOUN NOUN PUNCT NOUN VERB PART VERB ADJ NOUN ADP VERB CCONJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP ADJ NOUN NOUN PART VERB ADV ADJ PUNCT NOUN VERB NOUN NOUN PUNCT PRON VERB SCONJ NOUN NOUN AUX VERB ADJ NOUN ADP NOUN PRON AUX AUX ADJ ADP ADJ NOUN ADV PUNCT PRON VERB DET NOUN ADP VERB DET ADJ NOUN NOUN ADP NOUN VERB PROPN PUNCT PROPN PUNCT PROPN VERB NOUN ADP NOUN PUNCT SCONJ DET NOUN VERB ADP DET ADJ PUNCT ADJ NOUN CCONJ ADV VERB NOUN PART VERB DET NOUN NOUN PUNCT PRON VERB PROPN PART VERB CCONJ VERB DET NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP PROPN ADP ADJ NOUN ADJ NOUN ADP PROPN PUNCT DET NUM NOUN PROPN NOUN NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP PROPN PART VERB NOUN PROPN VERB DET ADJ NOUN NOUN PUNCT ADP VERB PROPN PUNCT PRON VERB SCONJ PRON PROPN CCONJ PROPN AUX VERB NOUN VERB NOUN PUNCT ADP NOUN ADP ADJ ADP NUM ADP ADP NUM ADP DET NOUN PUNCT ADP PROPN CCONJ PROPN PUNCT ADV SCONJ DET PROPN NOUN AUX VERB VERB DET PROPN NOUN NOUN PUNCT ADV PUNCT PRON VERB SCONJ PROPN VERB PROPN PUNCT DET ADJ VERB CCONJ NUM ADJ NOUN ADP PROPN PUNCT PART VERB DET NOUN ADP PROPN ADP DET ADJ NOUN PUNCT ADV PUNCT DET NOUN VERB ADJ NOUN ADP VERB NOUN CCONJ NOUN PRON AUX VERB NOUN VERB ADJ NOUN PUNCT,0.5174825174825175,23.833333333333332,5.206293706293707,26,Aman Madaan,Hugo Touvron,Aman Madaan,Hugo Touvron,Aman Madaan,Hugo Touvron
213,127,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.",19.2897967479675,328.0,6.0,512.0,0.6958320736885071," In short, by harnessing the intrinsic knowledge within an Propname and combining the power of humanunderstandable principles that specify how we want an Propname to behave, Propname Propname allows us to train a well behaved Propname agent whose generated responses follow the guardrails defined 0 Propname Propname Propname Propname Propname Propname Propname Propname 00k prompts and human preferences Propname fine tuning 00k user prompts from customers Propname Propname Propname Instruct 000 seed prompts w 0 rules for new instruction generation 000k synthetic prompts Principle Propname Propname Propname 00 principles for Propname assistant to follow w 0 in context learning demonstrations Propname Propname Propname tuning the original model after pruning principles and demonstrations 000k self aligned responses to synthetic prompts Supervised Propname Tuning 00k prompts and human annotations 000k self aligned verbose responses to synthetic prompts Propname Propname Propname the model to produce indepth and detailed responses 00k total human annotations 000 lines of human annotations Propname Propname 0: Side by side comparison: on the left is a typical Propname Propname alignment pipeline, and on the right are the four stages in our SELF Propname procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the Propname Propname method as open source to promote collaboration and innovation within the research community. The base model of Propname is the Propname 00b language model, which is accessible for research only, noncommercial purposes. By investigating different strategies from that in Propname, our work seeks to broaden the scope of Propname alignment techniques, and promote a deeper understanding of how to improve the capabilities of Propname systems, not only in terms of being more powerful, but also more responsible and well aligned with human values.", ADP ADJ PUNCT ADP VERB DET ADJ NOUN ADP DET PROPN CCONJ VERB DET NOUN ADP ADJ NOUN PRON VERB SCONJ PRON VERB DET PROPN PART VERB PUNCT PROPN PROPN VERB PRON PART VERB DET ADV VERB PROPN NOUN DET VERB NOUN VERB DET NOUN VERB NUM PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN NOUN CCONJ ADJ NOUN PROPN ADJ NOUN NOUN NOUN NOUN ADP NOUN PROPN PROPN PROPN VERB NUM NOUN NOUN NOUN NUM NOUN ADP ADJ NOUN NOUN X ADJ NOUN ADJ PROPN PROPN PROPN NUM NOUN ADP PROPN NOUN PART VERB NOUN NUM ADP NOUN NOUN NOUN PROPN PROPN PROPN VERB DET ADJ NOUN ADP VERB NOUN CCONJ NOUN VERB NOUN VERB NOUN ADP ADJ NOUN VERB PROPN NOUN NOUN NOUN CCONJ ADJ NOUN VERB NOUN VERB ADJ NOUN ADP ADJ NOUN PROPN PROPN PROPN DET NOUN PART VERB ADJ CCONJ ADJ NOUN NOUN VERB ADJ NOUN NUM NOUN ADP ADJ NOUN PROPN PROPN NUM PUNCT NOUN ADP NOUN NOUN PUNCT ADP DET NOUN AUX DET ADJ PROPN PROPN NOUN NOUN PUNCT CCONJ ADP DET NOUN AUX DET NUM NOUN ADP PRON NOUN PROPN NOUN PUNCT ADP DET NOUN NOUN PUNCT CCONJ ADV ADV PUNCT DET ADJ NOUN NOUN VERB DET VERB NOUN ADP ADJ NOUN ADP ADJ NOUN ADP NOUN PUNCT VERB ADP ADJ VERB NOUN PUNCT PRON AUX VERB DET NOUN ADP DET PROPN PROPN NOUN SCONJ ADJ NOUN PART VERB NOUN CCONJ NOUN ADP DET NOUN NOUN PUNCT DET ADJ NOUN ADP PROPN AUX DET PROPN NOUN NOUN NOUN PUNCT PRON AUX ADJ ADP NOUN ADV PUNCT ADJ NOUN PUNCT ADP VERB ADJ NOUN ADP PRON ADP PROPN PUNCT PRON NOUN VERB PART VERB DET NOUN ADP PROPN NOUN NOUN PUNCT CCONJ VERB DET ADJ NOUN ADP SCONJ PART VERB DET NOUN ADP PROPN NOUN PUNCT PART ADV ADP NOUN ADP AUX ADV ADJ PUNCT CCONJ ADV ADV ADJ CCONJ ADV VERB ADP ADJ NOUN PUNCT,0.49074074074074076,54.0,5.432098765432099,213,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Timo Schick
189,103,Hugo Touvron,"[' Image classification now achieves a performance\nthat meets many application needs [27, 37, 54]. In\npractice however, the dataset and labels available at\ntraining time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for\nfine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to\nfind enough images of rare classes, and annotating\nthem precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states\nthat: “Manually labeling a large number of images with\nthe presence or absence of 19,794 different classes is not\nfeasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases\ndue to the assisting algorithm. Being able to get strong\nclassification and image retrieval performance on fine\nconcepts using only coarse labels at training time can\ncircumvents the issue, liberating the data collection\nprocess from the quirks of a rigid fine-grained taxonomy.', 'In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:\nGiven a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their\nfine-grained semantic similarity to a new query image\noutside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we\nuse a non-parametric kNN classifier [61] for on-the-fly\nclassification, i.e. without training on the fine-grained\nlabels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build\nupon recent works [3, 62] that exploits two losses to\naddress both image classification and instance recognition, leveraging the “free” annotations provided by\nmultiple data augmentations of a same instance, in the\nspirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly\ninfer coarse labels even when classifying for a finer\ngranularity. For this purpose, we propose a simple\nmethod that exploits both a coarse classifier and image\nembeddings to improve fine-grained category-level\nretrieval.', 'This strategy outperforms existing works\nthat exploit coarse labels at training time but do not\nexplicitly rely on them when retrieving finer-grained\nconcepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following\ncontributions:\n• We propose a method that learns a representation\nat a finer granularity than the one offered by the\nannotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is\nstill +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the\ncoarse level. A byproduct of our study is a very\nstrong kNN-classifier on Imagenet: Grafit with\nResNet-50 trunk reaches 79.6% top-1 accuracy at\nresolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates\nbetter at a finer granularity. Everything being\nequal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the\nart on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],\nFood101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method\nin Section 3. Section 4 compares our approach against\nbaselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one\nlearned by a vanilla cross-entropy loss. Appendix B\ncomplements our experimental section 4 with more\ndetailed results. Appendix C provides visual results\nassociated with different levels of training/testing\ngranularities.']",intro_chunked,"This strategy outperforms existing works
that exploit coarse labels at training time but do not
explicitly rely on them when retrieving finer-grained
concepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following
contributions:
• We propose a method that learns a representation
at a finer granularity than the one offered by the
annotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly
classification on ImageNet. This improvement is
still +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the
coarse level. A byproduct of our study is a very
strong kNN-classifier on Imagenet: Grafit with
ResNet-50 trunk reaches 79.6% top-1 accuracy at
resolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates
better at a finer granularity. Everything being
equal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the
art on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],
Food101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method
in Section 3. Section 4 compares our approach against
baselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one
learned by a vanilla cross-entropy loss. Appendix B
complements our experimental section 4 with more
detailed results. Appendix C provides visual results
associated with different levels of training/testing
granularities.",54.76686842105266,304.0,20.0,491.0,0.671516478061676," This strategy outperforms existing works that exploit coarse labels at training time but do not explicitly rely on them when retrieving finer grained concepts. In summary, in this context of coarse to fine representation learning, our paper makes the following contributions: We propose a method that learns a representation at a finer granularity than the one offered by the annotation at training time. It exhibits a significant accuracy improvement on all the coarse tofine tasks that we consider. For instance, we improve by 00.0 the top 0 accuracy for on the fly classification on Propname. This improvement is still 0.0 w.r.t. our own stronger baseline, everything being equal otherwise. Our approach performs similarly or better at the coarse level. A byproduct of our study is a very strong kNN classifier on Propname: Propname with ResNet 00 trunk reaches 00.0 top 0 accuracy at resolution 000000. Propname improves transfer learning: our experiments show that our representation discriminates better at a finer granularity. Everything being equal otherwise, fine tuning our model for finegrained benchmarks significantly improves the accuracy. As a result we establish the new state of the art on five public benchmarks for transfer learning: Propname Propname 000, Propname Propname, Propname, Propname 0000 0000. This paper is organized as follows. After reviewing related works in Section 0, we present our method in Section 0. Section 0 compares our approach against baselines on various datasets, and presents an extensive ablation. Section 0 concludes the paper. In the supplemental material, Propname Propname summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one learned by a vanilla cross Propname loss. Propname Propname complements our experimental section 0 with more detailed results. Propname Propname provides visual results associated with different levels of trainingtesting granularities.", DET NOUN VERB VERB NOUN PRON VERB ADJ NOUN ADP NOUN NOUN CCONJ AUX PART ADV VERB ADP PRON SCONJ VERB ADJ VERB NOUN PUNCT ADP NOUN PUNCT ADP DET NOUN ADP NOUN ADP ADJ NOUN NOUN PUNCT PRON NOUN VERB DET ADJ NOUN PUNCT PRON VERB DET NOUN PRON VERB DET NOUN ADP DET ADJ NOUN ADP DET NOUN VERB ADP DET NOUN ADP NOUN NOUN PUNCT PRON VERB DET ADJ NOUN NOUN ADP DET DET ADJ NOUN NOUN PRON PRON VERB PUNCT ADP NOUN PUNCT PRON VERB ADP NUM DET ADJ NUM NOUN ADP ADP DET NOUN NOUN ADP PROPN PUNCT DET NOUN AUX ADV NUM NOUN PUNCT PRON ADJ ADJ NOUN PUNCT PRON AUX ADJ ADV PUNCT PRON NOUN VERB ADV CCONJ ADJ ADP DET ADJ NOUN PUNCT DET NOUN ADP PRON NOUN AUX DET ADV ADJ ADJ NOUN ADP PROPN PUNCT PROPN ADP NOUN NUM NOUN VERB NUM ADJ NUM NOUN ADP NOUN NUM PUNCT PROPN VERB NOUN VERB PUNCT PRON NOUN VERB SCONJ PRON NOUN VERB ADV ADP DET ADJ NOUN PUNCT PRON AUX ADJ ADV PUNCT ADJ VERB PRON NOUN ADP VERB NOUN ADV VERB DET NOUN PUNCT ADP DET NOUN PRON VERB DET ADJ NOUN ADP DET NOUN ADP NUM ADJ NOUN ADP NOUN NOUN PUNCT PROPN PROPN NUM PUNCT PROPN PROPN PUNCT PROPN PUNCT PROPN NUM NUM PUNCT DET NOUN AUX VERB SCONJ VERB PUNCT ADP VERB ADJ NOUN ADP NOUN NUM PUNCT PRON VERB PRON NOUN ADP NOUN NUM PUNCT NOUN NUM VERB PRON NOUN ADP NOUN ADP ADJ NOUN PUNCT CCONJ VERB DET ADJ NOUN PUNCT NOUN NUM VERB DET NOUN PUNCT ADP DET ADJ NOUN PUNCT PROPN PROPN VERB NUM NOUN PRON VERB SCONJ DET ADJ NOUN VERB DET NOUN ADP DET NOUN VERB ADP DET NOUN NOUN PROPN NOUN PUNCT PROPN PROPN VERB PRON ADJ NOUN NUM ADP ADV ADJ NOUN PUNCT PROPN PROPN VERB ADJ NOUN VERB ADP ADJ NOUN ADP ADJ NOUN PUNCT,0.5198776758409785,18.166666666666668,5.113149847094801,189,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
145,59,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked,"We then encode
the graph in each training pair as a string in the
graph representation format DOT, transforming the
text-to-graph mapping into sequence-to-sequence
mapping. We fine-tune GPT-2 on this dataset of
document-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document
and the generated graph by our system. In summary, our main contributions are:
1. We present the first investigation on using
large pre-trained language models for contextualized temporal event graph generation by
proposing a new formulation of the problem
as a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large
collection of human-annotated graphs, which
is crucial for effective fine-tuning of pretrained models, by automatically producing a
collection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different
models) and a hand-labeled, out-of-domain
dataset (TimeBank-Dense), show the advantage of our proposed approach over strong
baselines. Further, we show that our approach
can help in generating plausible answers for
open ended-temporal questions in a reading
comprehension dataset, Torque (Ning et al.,
2020).",50.400785714285746,224.0,10.0,354.0,0.7263843417167664," We then encode the graph in each training pair as a string in the graph representation format Propname, transforming the text to graph mapping into sequence to sequence mapping. We fine tune Propname 0 on this dataset of document graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms Propname on TimeBankDense on multiple metrics. Figure 0 shows an example of the input document and the generated graph by our system. In summary, our main contributions are: 0. We present the first investigation on using large pre trained language models for contextualized temporal event graph generation by proposing a new formulation of the problem as a sequence to sequence mapping task.0. We address the difficulty of obtaining a large collection of human annotated graphs, which is crucial for effective fine tuning of pretrained models, by automatically producing a collection of 00,000 document graph pairs.0. Our experimental results on both the systemgenerated test set which allows us to compare the relative performance of different models and a hand labeled, out of domain dataset, show the advantage of our proposed approach over strong baselines. Further, we show that our approach can help in generating plausible answers for open ended temporal questions in a reading comprehension dataset, Propname Propname Propname Propname Propname, 0000.", PRON ADV VERB DET NOUN ADP DET NOUN NOUN ADP DET NOUN ADP DET NOUN NOUN NOUN PROPN PUNCT VERB DET NOUN PART VERB NOUN ADP NOUN PART VERB NOUN PUNCT PRON ADJ NOUN PROPN NUM ADP DET NOUN ADP NOUN NOUN NOUN PUNCT PRON VERB ADJ NOUN NOUN ADP ADJ NOUN ADP NOUN VERB NOUN VERB CCONJ VERB PROPN ADP NOUN ADP ADJ NOUN PUNCT NOUN NUM VERB DET NOUN ADP DET NOUN NOUN CCONJ DET VERB NOUN ADP PRON NOUN PUNCT ADP NOUN PUNCT PRON ADJ NOUN AUX PUNCT NUM PUNCT PRON VERB DET ADJ NOUN ADP VERB ADJ ADJ VERB NOUN NOUN ADP VERB ADJ NOUN NOUN NOUN ADP VERB DET ADJ NOUN ADP DET NOUN ADP DET NOUN PART VERB NOUN NOUN PUNCT PUNCT PUNCT PRON VERB DET NOUN ADP VERB DET ADJ NOUN ADP ADJ ADJ NOUN PUNCT PRON AUX ADJ ADP ADJ ADJ NOUN ADP VERB NOUN PUNCT ADP ADV VERB DET NOUN ADP NUM NOUN NOUN NOUN PUNCT PUNCT PUNCT PRON ADJ NOUN ADP CCONJ DET ADJ NOUN NOUN PRON VERB PRON PART VERB DET ADJ NOUN ADP ADJ NOUN CCONJ DET NOUN VERB PUNCT ADP ADP NOUN NOUN PUNCT VERB DET NOUN ADP PRON VERB NOUN ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB SCONJ PRON NOUN AUX VERB ADP VERB ADJ NOUN ADP ADJ VERB ADJ NOUN ADP DET NOUN NOUN NOUN PUNCT PROPN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT,0.5423728813559322,29.5,5.0423728813559325,145,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Timo Schick
125,39,Aman Madaan,"[' Graphs provide a rich abstraction for a wide range of tasks\nincluding molecular design (De Cao & Kipf, 2018; Samanta\net al., 2019; Lim et al., 2020), temporal and commonsense\nreasoning (Madaan & Yang, 2021; Madaan et al., 2021;\nSakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout\ndesign (Mi et al., 2021). Developing generative models of\ngraphs is is therefore an important classical problem, which\nhas seen renewed interest with the success of deep learning\nmodels. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit\nmodels, implicit generative models do not explicitly model\nthe distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and\nhave recently shown state of the art results for generative\nmodeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of\ngraphs currently use identical model complexity and computational strength while generating graphs. However, since\nthese models are constructive by design (i.e., they build a\ngraph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of\nreasoning. For example, generating a 2-hop neighborhood\nfrequently seen during training might be easier than generating a novel 4-hop neighborhood.', 'Indeed, it has long been posited (Posner & Snyder, 1975;\nShiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;\nKahneman, 2003; Frankish, 2010) that humans frequently\nuse differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)\n203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different\nlevel of difficulty for a human solver. The answer to 2*2\nwill almost instinctively come to most, while solving 19*3\nwill require more careful thinking. Specifically, Stanovich\n(2000) propose to divide mental processing as being done\nby two metaphorical systems referred by them as System\n1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for\nSystems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a\ncombination of fast and slow reasoning systems in diverse\nareas of Machine Learning (Anthony et al., 2017; Mujika\net al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model\nthat is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into\nthe problem of learning to generate walks.', 'Generating\nwalks provides a setting where identifying the easy and\nchallenging portions is easier: starting from a given node,\nthe model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating\nsuch walks then gradually increases for two reasons. First,\nconditioning on increasingly longer contexts is required\nfor generating longer walks. Second, as the length of the\nwalks exceeds the length seen during training, a model is\nforced to create neighborhoods not seen during the training:\na task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty\nby dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1\nprovides an overview of our approach. FLOWGEN method\nachieves the same results as using the SLOW method alone\non three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer\nmodel, similar to the architectures used by the popular GPT2\nmodels. Using transformers allows us to easily instantiate\nfast and slow versions of the same model by varying the\nnumber of layers. In contrast to the state-of-the-art methods\nfor generative modeling of graphs that use either an implicit\nmodel (e.g., GANs as done by Bojchevski et al. (2018)),\nexplicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and\nleverage graph-aware decoding methods (You et al., 2018),\nour method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. ).']",intro_chunked,"Indeed, it has long been posited (Posner & Snyder, 1975;
Shiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;
Kahneman, 2003; Frankish, 2010) that humans frequently
use differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)
203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different
level of difficulty for a human solver. The answer to 2*2
will almost instinctively come to most, while solving 19*3
will require more careful thinking. Specifically, Stanovich
(2000) propose to divide mental processing as being done
by two metaphorical systems referred by them as System
1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for
Systems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a
combination of fast and slow reasoning systems in diverse
areas of Machine Learning (Anthony et al., 2017; Mujika
et al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model
that is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into
the problem of learning to generate walks.",56.96877883310722,201.0,11.0,312.0,0.40067657828330994," Indeed, it has long been posited Propname Propname, 0000; Propname Propname, 0000; Propname, 0000; Propname, 0000; Propname, 0000; Propname, 0000 that humans frequently use differential reasoning based on the problem difficulty. For example, consider two problems: i0 0?, and Propname 000 000? Both these problems involve multiplication between two integers. Yet, they pose a very different level of difficulty for a human solver. The answer to 00 will almost instinctively come to most, while solving 000 will require more careful thinking. Specifically, Propname propose to divide mental processing as being done by two metaphorical systems referred by them as System 0 and Propname 0. The terms FAST and SLOW for Propname 0 and 0 were subsequently popularized by Propname. There is now a growing interest in utilizing a combination of fast and slow reasoning systems in diverse areas of Propname Propname Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000b. This paper introduces Propname, a generative graph model that is inspired by the dual process theory of mind. Propname decomposes the problem of generating a graph into the problem of learning to generate walks.", ADV PUNCT PRON AUX ADV AUX VERB PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PUNCT NUM PUNCT PROPN PUNCT NUM PUNCT PROPN PUNCT NUM PUNCT PROPN PUNCT NUM PUNCT PROPN PUNCT NUM SCONJ NOUN ADV VERB ADJ NOUN VERB ADP DET NOUN NOUN PUNCT ADP NOUN PUNCT VERB NUM NOUN PUNCT PRON PUNCT NUM PUNCT PUNCT CCONJ PROPN NUM NUM PUNCT CCONJ DET NOUN VERB NOUN ADP NUM NOUN PUNCT ADV PUNCT PRON VERB DET ADV ADJ NOUN ADP NOUN ADP DET ADJ NOUN PUNCT DET NOUN ADP NUM AUX ADV ADV VERB ADP ADJ PUNCT SCONJ VERB NUM AUX VERB ADV ADJ NOUN PUNCT ADV PUNCT PROPN VERB PART VERB ADJ NOUN ADP AUX VERB ADP NUM ADJ NOUN VERB ADP PRON ADP NOUN NUM CCONJ PROPN NUM PUNCT DET NOUN VERB CCONJ NOUN ADP PROPN NUM CCONJ NUM AUX ADV VERB ADP PROPN PUNCT PRON VERB ADV DET VERB NOUN ADP VERB DET NOUN ADP ADJ CCONJ ADJ NOUN NOUN ADP ADJ NOUN ADP PROPN PROPN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN NOUN PRON AUX VERB ADP DET ADJ NOUN NOUN ADP NOUN PUNCT PROPN VERB DET NOUN ADP VERB DET NOUN ADP DET NOUN ADP VERB PART VERB NOUN PUNCT,0.5111111111111111,22.5,4.711111111111111,125,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan
162,76,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"Data-augmentation strategies, including those
mixing different images like Mixup [61] and CutMix [60],
have proven to provide a complementary data-driven form
of regularization. More recently, multiple works propose
to resort to self-supervised pre-training. These approaches
rely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)
auto-encoders [5, 22, 16], which were popular in the early
deep learning literature [7, 19, 27]. Similarly, contrastive
approaches [23, 9] provide a richer supervision less prone to
a supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,
possibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over
learning from the data directly. In contrast to traditional
distillation, co-distillation does not require pre-training a
(strong) teacher. Instead, a pool of models supervise each
other. Practically, it faces several limitations, including the
difficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.",46.963711385701714,206.0,11.0,343.0,0.39266178011894226," Data augmentation strategies, including those mixing different images like Propname and Propname, have proven to provide a complementary data driven form of regularization. More recently, multiple works propose to resort to self supervised pre training. These approaches rely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in auto encoders, which were popular in the early deep learning literature. Similarly, contrastive approaches provide a richer supervision less prone to a supervision collapse. Overall, self supervised learning makes it possible to learn larger models with less data, possibly reducing the need of a pre training stage. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model, allowing the student to improve over learning from the data directly. In contrast to traditional distillation, Propname distillation does not require pre training a teacher. Instead, a pool of models supervise each other. Practically, it faces several limitations, including the difficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.", NOUN NOUN NOUN PUNCT VERB PRON VERB ADJ NOUN ADP PROPN CCONJ PROPN PUNCT AUX VERB PART VERB DET ADJ NOUN VERB NOUN ADP NOUN PUNCT ADV ADV PUNCT ADJ NOUN VERB PART VERB ADP NOUN VERB ADJ NOUN PUNCT DET NOUN VERB ADP DET NOUN NOUN PRON ADV VERB ADJ NOUN NOUN ADP DET NOUN ADJ ADP NOUN PUNCT ADP NOUN PUNCT ADV PRON AUX AUX VERB NOUN ADP NOUN NOUN PUNCT PRON AUX ADJ ADP DET ADJ ADJ VERB NOUN PUNCT ADV PUNCT ADJ NOUN VERB DET ADJ NOUN ADV ADJ ADP DET NOUN NOUN PUNCT ADV PUNCT NOUN ADJ NOUN VERB PRON ADJ PART VERB ADJ NOUN ADP ADJ NOUN PUNCT ADV VERB DET NOUN ADP DET ADJ NOUN NOUN PUNCT NOUN AUX DET ADJ NOUN PART VERB NOUN PUNCT NOUN NOUN AUX ADV VERB PART VERB NOUN ADP DET NOUN NOUN ADP DET NOUN NOUN PUNCT VERB DET NOUN PART VERB ADP VERB ADP DET NOUN ADV PUNCT ADP NOUN ADP ADJ NOUN PUNCT PROPN NOUN AUX PART VERB ADJ VERB DET NOUN PUNCT ADV PUNCT DET NOUN ADP NOUN VERB DET ADJ PUNCT ADV PUNCT PRON VERB ADJ NOUN PUNCT VERB DET NOUN ADP ADV VERB ADJ ADP NUM NOUN ADP NOUN NOUN PUNCT SCONJ PRON VERB VERB DET NOUN PUNCT,0.6018518518518519,19.636363636363637,5.282407407407407,162,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
114,28,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked,"For example, when drafting an email to request a document from a colleague, an individual may initially write a direct
request such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the
potential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data
at your earliest convenience?"". When writing code, a programmer may implement an initial “quick
and dirty” implementation, and then, upon reflection, refactor their code to a solution that is more
efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement
without additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get
feedback. Then, the feedback is passed back to the same model to refine the previously-generated
draft. This process is repeated either for a specified number of iterations or until M determines that
no further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to
both generate feedback and incorporate the feedback into an improved draft.",55.25856265356268,222.0,11.0,344.0,0.45218583941459656," For example, when drafting an email to request a document from a colleague, an individual may initially write a direct request such as Send me the data ASAP. Upon reflection, however, the writer recognizes the potential impoliteness of the phrasing and revises it to Propname Propname, could you please send me the data at your earliest convenience?. When writing code, a programmer may implement an initial quick and dirty implementation, and then, upon reflection, refactor their code to a solution that is more efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self refinement without additional training, leading to higher quality outputs on a wide range of tasks. We present Propname Propname: an iterative self refinement Propname that alternates between two generative stepsFEEDBACK and Propname. These steps work in tandem to generate high quality outputs. Given an initial output generated by a model Propname, we pass it back to the same model Propname to get feedback. Then, the feedback is passed back to the same model to refine the previously generated draft. This process is repeated either for a specified number of iterations or until Propname determines that no further refinement is necessary. We use few shot prompting to guide Propname to both generate feedback and incorporate the feedback into an improved draft.", ADP NOUN PUNCT SCONJ VERB DET NOUN PART VERB DET NOUN ADP DET NOUN PUNCT DET NOUN AUX ADV VERB DET ADJ NOUN ADJ ADP VERB PRON DET NOUN ADV PUNCT SCONJ NOUN PUNCT ADV PUNCT DET NOUN VERB DET ADJ NOUN ADP DET NOUN CCONJ VERB PRON ADP PROPN PROPN PUNCT AUX PRON INTJ VERB PRON DET NOUN ADP PRON ADJ NOUN PUNCT PUNCT SCONJ VERB NOUN PUNCT DET NOUN AUX VERB DET ADJ ADJ CCONJ ADJ NOUN PUNCT CCONJ ADV PUNCT SCONJ NOUN PUNCT VERB PRON NOUN ADP DET NOUN PRON AUX ADV ADJ CCONJ ADJ PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ NOUN AUX VERB ADJ NOUN NOUN ADP ADJ NOUN PUNCT VERB ADP ADJ NOUN NOUN ADP DET ADJ NOUN ADP NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET ADJ NOUN NOUN PROPN SCONJ VERB ADP NUM ADJ NOUN CCONJ PROPN PUNCT DET NOUN VERB ADP NOUN PART VERB ADJ NOUN NOUN PUNCT VERB DET ADJ NOUN VERB ADP DET NOUN PROPN PUNCT PRON VERB PRON ADV ADP DET ADJ NOUN PROPN PART VERB NOUN PUNCT ADV PUNCT DET NOUN AUX VERB ADV ADP DET ADJ NOUN PART VERB DET ADV VERB NOUN PUNCT DET NOUN AUX VERB CCONJ ADP DET ADJ NOUN ADP NOUN CCONJ SCONJ PROPN VERB SCONJ DET ADJ NOUN AUX ADJ PUNCT PRON VERB ADJ NOUN VERB PART VERB PROPN PART PRON VERB NOUN CCONJ VERB DET NOUN ADP DET ADJ NOUN PUNCT,0.5867768595041323,24.2,4.7727272727272725,114,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Hugo Touvron
57,57,Zhiqing Sun,"[' Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in “hallucination”, generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at.']",abstract_chunked," Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in “hallucination”, generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at.",36.75313559322035,236.0,7.0,379.0,0.666664183139801," Large Propname Propname are built across modalities and the misalignment between two modalities can result in hallucination, generating textual outputs that are not grounded by the multimodal information in context. To address the Propname misalignment issue, we adapt the Propname Propname from Propname Propname from the text domain to the task of vision language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Propname Propname that augments the reward model with additional factual information such as image captions and ground truth multi choice options, which alleviates the reward hacking phenomenon in Propname and further improves the performance. We also enhance the Propname 0 generated training data with previously available human written image text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real world scenarios, we develop a new evaluation benchmark Propname Propname with a special focus on penalizing hallucinations. As the first Propname trained with Propname, our approach achieves remarkable improvement on the Propname Propname dataset with the 00 performance level of the text only Propname 0, and an improvement by 00 on Propname Propname over other baselines. We opensource our code, model, data at.", ADJ PROPN PROPN AUX VERB ADP NOUN CCONJ DET NOUN ADP NUM NOUN AUX VERB ADP NOUN PUNCT VERB ADJ NOUN PRON AUX PART VERB ADP DET ADJ NOUN ADP NOUN PUNCT PART VERB DET PROPN NOUN NOUN PUNCT PRON VERB DET PROPN PROPN ADP PROPN PROPN ADP DET NOUN NOUN ADP DET NOUN ADP NOUN NOUN NOUN PUNCT SCONJ ADJ NOUN AUX VERB PART VERB NUM NOUN CCONJ VERB DET ADV VERB NUM PUNCT CCONJ DET NOUN NOUN NOUN AUX VERB PART VERB DET ADJ ADJ NOUN PUNCT PRON VERB DET ADJ NOUN NOUN VERB ADV PROPN PROPN PRON VERB DET NOUN NOUN ADP ADJ ADJ NOUN ADJ ADP NOUN NOUN CCONJ NOUN NOUN NOUN NOUN NOUN PUNCT PRON VERB DET NOUN VERB NOUN ADP PROPN CCONJ ADV VERB DET NOUN PUNCT PRON ADV VERB DET PROPN NUM VERB NOUN NOUN ADP ADV ADJ ADJ VERB NOUN NOUN VERB PART VERB DET ADJ NOUN ADP PRON NOUN PUNCT PART VERB DET VERB NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB DET ADJ NOUN NOUN PROPN PROPN ADP DET ADJ NOUN ADP VERB NOUN PUNCT SCONJ DET ADJ PROPN VERB ADP PROPN PUNCT PRON NOUN VERB ADJ NOUN ADP DET PROPN PROPN VERB ADP DET NUM NOUN NOUN ADP DET NOUN ADV PROPN NUM PUNCT CCONJ DET NOUN ADP NUM ADP PROPN PROPN ADP ADJ NOUN PUNCT PRON VERB PRON NOUN PUNCT NOUN PUNCT NOUN ADP PUNCT,0.5443037974683544,33.857142857142854,5.286919831223629,57,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
20,20,GPT-3.5,"[' While some natural language processing (NLP) tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Pattern-Exploiting Training (PET), a novel semi-supervised training approach that combines unsupervised and supervised learning. PET reformulates input examples as cloze-style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, PET demonstrates superior performance compared to both supervised training and strong semi-supervised approaches, particularly in low-resource settings.']",abstract_chunked," While some natural language processing (NLP) tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Pattern-Exploiting Training (PET), a novel semi-supervised training approach that combines unsupervised and supervised learning. PET reformulates input examples as cloze-style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, PET demonstrates superior performance compared to both supervised training and strong semi-supervised approaches, particularly in low-resource settings.",37.44186639118459,121.0,6.0,213.0,0.5727147459983826," While some natural language processing tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Propname Propname Propname, a novel semi supervised training approach that combines unsupervised and supervised learning. Propname reformulates input examples as cloze style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, Propname demonstrates superior performance compared to both supervised training and strong semi supervised approaches, particularly in low resource settings.", SCONJ DET ADJ NOUN NOUN NOUN AUX AUX VERB ADP DET ADV ADJ NOUN ADP VERB VERB NOUN NOUN ADP NOUN NOUN PUNCT DET NOUN VERB PART VERB VERB ADP VERB NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN PROPN PROPN PUNCT DET ADJ ADJ ADJ NOUN NOUN PRON VERB ADJ CCONJ ADJ NOUN PUNCT PROPN VERB NOUN NOUN SCONJ VERB NOUN NOUN PART VERB NOUN NOUN ADP VERB DET ADJ NOUN PUNCT DET NOUN AUX ADV VERB PART VERB ADJ NOUN ADP DET ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT ADJ ADJ NOUN AUX VERB ADP DET VERB NOUN NOUN PUNCT ADP ADJ NOUN CCONJ NOUN PUNCT PROPN VERB ADJ NOUN VERB ADP DET ADJ NOUN CCONJ ADJ ADJ ADJ NOUN PUNCT ADV ADP ADJ NOUN NOUN PUNCT,0.648854961832061,21.833333333333332,5.717557251908397,20,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5
176,90,Hugo Touvron,"[' Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches.', 'Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.', 'This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 .']",intro_chunked,"Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.",48.20207070707073,220.0,9.0,348.0,0.464266836643219," Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Propname 0 and 0, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:0000.00000v000 Dec 0000 In Figure 0, we show the attention maps extracted from ViT by using a visualization procedure inspired by Propname Propname Propname.. It involves some post processing as there are multiple layers and heads providing patch weights. Then we show a ResNet 00 augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low resolution attention map with artefacts: We need an architecture producing a higher resolution feature maps in order to better leverage the proposed attention based pooling. For this purpose we introduce a simple patch based convolutional architecture0 that keeps the input resolution constant throughout the network.", VERB ADP NOUN PUNCT ADP PRON DET NOUN AUX VERB ADP ADJ NOUN CCONJ NOUN PUNCT PRON NOUN VERB DET ADJ NOUN ADP NOUN PUNCT CCONJ ADV DET ADJ NOUN PART VERB DET NOUN NOUN PUNCT PRON AUX DET ADJ NOUN ADP DET NOUN ADP DET ADJ NOUN VERB DET NOUN PUNCT DET NOUN VERB DET NOUN PART VERB ADP ADJ NOUN ADV CCONJ ADV PUNCT SCONJ PRON VERB NUM ADJ ADP DET NOUN ADV ADP DET ADJ NOUN PUNCT SCONJ VERB ADP PROPN NUM CCONJ NUM PUNCT ADV PRON VERB DET NOUN NOUN ADP NOUN ADP DET ADJ NOUN PUNCT ADP PRON ADJ NOUN PRON ADV VERB ADP DET ADJ NOUN NOUN PUNCT PRON AUX ADV ADV VERB ADP DET NOUN NOUN PUNCT NOUN PUNCT ADJ NUM ADP NOUN NUM PUNCT PRON VERB DET NOUN NOUN VERB ADP NOUN ADP VERB DET NOUN NOUN VERB ADP PROPN PROPN PROPN PUNCT PUNCT PRON VERB DET NOUN NOUN SCONJ PRON VERB ADJ NOUN CCONJ NOUN VERB NOUN NOUN PUNCT ADV PRON VERB DET NOUN NUM VERB ADP VERB PRON VERB NOUN NOUN PUNCT PRON ADJ NOUN VERB ADP DET ADJ NOUN NOUN NOUN ADP NOUN PUNCT PRON VERB DET NOUN VERB DET ADJ NOUN NOUN NOUN ADP NOUN PART VERB VERB DET VERB NOUN VERB NOUN PUNCT ADP DET NOUN PRON VERB DET ADJ NOUN VERB ADJ NOUN PRON VERB DET NOUN NOUN ADJ ADP DET NOUN PUNCT,0.5914893617021276,29.375,4.8936170212765955,176,Hugo Touvron,Aman Madaan,Hugo Touvron,Hugo Touvron,Hugo Touvron,Timo Schick
183,97,Hugo Touvron,"[' Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.', 'We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation.', 'To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.']",intro_chunked,"To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.",28.900500000000022,150.0,4.0,248.0,0.5251596570014954," To do so, we develop a sequence to sequence version of Propname, where both encoder and decoders are based on Propname with across attention between the encoder and decoder. This model is similar to the original Propname Propname with Propname layers instead of Propname layers. Despite not being originally designed for this task, we observe that Propname is competitive with Transformers on the challenging Propname benchmarks. In summary, in this paper, we make the following observations: despite its simplicity, Propname reaches surprisingly good accuracycomplexity trade offs with Propname Propname training only0, without requiring normalization based on batch or channel statistics; these models benefit significantly from distillation methods; they are also compatible with modern self supervised learning methods based on data augmentation, such as Propname; A Propname Propname achieves competitive performances compared to a Propname Propname on the Propname benchmark for Propname Propname.", PART VERB ADV PUNCT PRON VERB DET NOUN PART VERB NOUN ADP PROPN PUNCT SCONJ PRON NOUN CCONJ NOUN AUX VERB ADP PROPN ADP ADP NOUN ADP DET NOUN CCONJ NOUN PUNCT DET NOUN AUX ADJ ADP DET ADJ PROPN PROPN ADP PROPN NOUN ADV ADP PROPN NOUN PUNCT SCONJ PART AUX ADV VERB ADP DET NOUN PUNCT PRON VERB SCONJ PROPN AUX ADJ ADP NOUN ADP DET ADJ PROPN NOUN PUNCT ADP NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET VERB NOUN PUNCT SCONJ PRON NOUN PUNCT PROPN VERB ADV ADJ NOUN NOUN NOUN ADP PROPN PROPN NOUN NOUN PUNCT ADP VERB NOUN VERB ADP NOUN CCONJ NOUN NOUN PUNCT DET NOUN VERB ADV ADP NOUN NOUN PUNCT PRON AUX ADV ADJ ADP ADJ NOUN VERB NOUN NOUN VERB ADP NOUN NOUN PUNCT ADJ ADP PROPN PUNCT DET PROPN PROPN VERB ADJ NOUN VERB ADP DET PROPN PROPN ADP DET PROPN NOUN ADP PROPN PROPN PUNCT,0.620253164556962,39.5,5.455696202531645,183,Hugo Touvron,Zhiqing Sun,Hugo Touvron,Hugo Touvron,Hugo Touvron,Zhiqing Sun
271,185,Timo Schick,"[' Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavi- cencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to finetune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g.', 'Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019) recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a word’s contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn – by way of self-attention – to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking (AM). Our contributions are as follows: (i) We introduce the attentive mimicking model. It produces high-quality embeddings for rare and medium-frequency words by attending to the most informative contexts. (ii) We propose a novel evaluation method based on VecMap (Artetxe et al., 2018) that allows us to easily evaluate the embedding quality of low- and medium-frequency words. (iii) We show that attentive mimicking improves word\nembeddings on various datasets.']",intro_chunked," Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavi- cencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to finetune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g.",49.360942408376985,191.0,7.0,293.0,0.3970867991447449," Word embeddings have led to large performance gains in natural language processing. However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface form information into learning. This can either be done directly, or a two step process is employed: first, an embedding model is trained on the word level and then, surface form information is used either to finetune embeddings or to completely recompute them. The latter can be achieved using a model trained to reproduce the original embeddings. However, these methods only work if a words meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task Propname", NOUN NOUN AUX VERB ADP ADJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT VERB NOUN ADV VERB ADJ NOUN ADP DET NOUN PART VERB DET ADJ NOUN ADP PRON PUNCT NUM NOUN PART VERB DET NOUN CCONJ VERB NOUN ADP ADJ NOUN AUX PART VERB NOUN NOUN NOUN ADP NOUN PUNCT PRON AUX CCONJ AUX VERB ADV PUNCT CCONJ DET NUM NOUN NOUN AUX VERB PUNCT ADV PUNCT DET VERB NOUN AUX VERB ADP DET NOUN NOUN CCONJ ADV PUNCT NOUN NOUN NOUN AUX VERB CCONJ PART NOUN NOUN CCONJ PART ADV VERB PRON PUNCT DET ADJ AUX AUX VERB VERB DET NOUN VERB PART VERB DET ADJ NOUN PUNCT ADV PUNCT DET NOUN ADV VERB SCONJ DET NOUN VERB AUX ADP ADJ ADV AUX VERB ADP PRON NOUN PUNCT DET ADV VERB NOUN ADP NOUN AUX VERB VERB ADP ADJ NOUN PUNCT SCONJ DET NOUN AUX PART VERB NOUN ADP ADV ADJ NOUN ADP ADP ADJ DET NOUN ADP NOUN PUNCT SCONJ ADJ ADJ NOUN ADV VERB NOUN NOUN ADP DET NOUN PROPN,0.6136363636363636,22.0,4.903409090909091,271,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Hugo Touvron
245,159,Zhiqing Sun,"[' Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013).', 'There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets.']",intro_chunked," Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013).",31.759336734693903,196.0,8.0,348.0,0.6367156505584717," Unlike Propname and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Propname Propname is a crucial step for many Propname Propname Propname Propname tasks such as syntactic parsing, information retrieval and word representation learning. Recently, neural approaches for supervised Propname are attracting huge interest. A great quantities of neural models, Propname, tensor neural network, recursive neural network, long short term memory and convolutionalneural network, have been proposed and given competitive results to the best statistical models. However, the neural approaches for unsupervised Propname have not been investigated. Previous unsupervised approaches to Propname can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Propname and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Propname Propname, normalized Propname of Propname Propname and Propname Propname Propname.", ADP PROPN CCONJ ADJ ADJ NOUN PUNCT ADJ NOUN VERB DET ADJ NOUN NOUN PUNCT ADV PUNCT ADJ PROPN PROPN AUX DET ADJ NOUN ADP ADJ PROPN PROPN PROPN PROPN NOUN ADJ ADP ADJ NOUN PUNCT NOUN NOUN CCONJ NOUN NOUN NOUN PUNCT ADV PUNCT ADJ NOUN ADP ADJ PROPN AUX VERB ADJ NOUN PUNCT DET ADJ NOUN ADP ADJ NOUN PUNCT PROPN PUNCT NOUN ADJ NOUN PUNCT ADJ ADJ NOUN PUNCT ADJ ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT AUX AUX VERB CCONJ VERB ADJ NOUN ADP DET ADJ ADJ NOUN PUNCT ADV PUNCT DET ADJ NOUN ADP ADJ PROPN AUX PART AUX VERB PUNCT ADJ VERB NOUN ADP PROPN AUX AUX ADV VERB ADP ADJ CCONJ ADJ NOUN PUNCT DET ADJ VERB ADV VERB NOUN NOUN ADP NOUN NOUN PUNCT SCONJ DET ADJ VERB ADP VERB ADJ NOUN ADP PROPN CCONJ VERB DET ADJ NOUN ADP DET ADJ ADJ NOUN PUNCT ADJ NOUN NOUN ADP ADJ NOUN VERB PROPN PROPN PUNCT VERB PROPN ADP PROPN PROPN CCONJ PROPN PROPN PROPN PUNCT,0.5375722543352601,21.625,5.901734104046243,245,Zhiqing Sun,GPT-3.5,Zhiqing Sun,GPT-3.5,Zhiqing Sun,GPT-3.5
220,134,Zhiqing Sun,"[' Complex physical systems described by non-linear partial\ndifferential equations (PDEs) are ubiquitous throughout the\n1Carnegie Mellon University, Pittsburgh, PA 15213, USA\n2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems\nin aeronautics (Rhie & Chow, 1983), medicine (Sallam &\nHwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations\n(Courant et al., 1967). Solving most equations of importance\nis usually computationally intractable with direct numerical\nsimulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE\nsolvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov\net al. 2021; Brandstetter et al. 2021, inter alia) have shown\nthat end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike\nclassical finite differences, finite volumes, finite elements,\nor pseudo-spectral methods that require a smooth variation\non the high-resolution meshes for guaranteed convergence,\nneural solvers do not rely on such conditions and are able\nto model the underlying physics with under-resolved low\nresolutions and produce high-quality simulations with significantly reduced computational cost.', 'The power of learnable PDE solvers is usually believed to\ncome from the super-resolution ability of neural networks,\nwhich means that the machine learning model is capable of\nrecovering the missing details based on the coarse features\n(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly\ntraining a super-resolution model, and then find that since\nlow-resolution down-sampling of the field can lead to some\ninformation loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the\ntrajectories and the temporal feature encoding scheme are\ncrucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of\ntwo worlds: stencil learning (i.e., Learned Interpolation in\nKochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO\n(Gu et al., 2020) as a state-of-the-art time series sequence\nmodel. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux\nwithin a finite volume method framework. As illustrated in\nFig.', '1, TSM can be regarded as a temporal generalization\nof classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned\ninterpolation solvers (Kochkov et al., 2021), both of which\nadaptively weight or interpolate the stencils based on the\nlatest states only. On the other hand, in TSM we use the\nHiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity\non each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation\ncoefficients, while the stencil learning framework ensures\nthat the neural system’s prediction exactly conserves the\nConservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize\nthe temporal bundling technique (Brandstetter et al., 2021)\nto avoid over-fitting and improve the prediction latency for\nTSM. Following the precedent work in the field (Li et al., 2020c;\nKochkov et al., 2021; Brandstetter et al., 2021), we evaluate\nthe proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing\nequation for turbulent flows with the conservation of mass\nand momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows\ncan achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and\ndifferent Reynolds numbers.']",intro_chunked,"1, TSM can be regarded as a temporal generalization
of classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned
interpolation solvers (Kochkov et al., 2021), both of which
adaptively weight or interpolate the stencils based on the
latest states only. On the other hand, in TSM we use the
HiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity
on each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation
coefficients, while the stencil learning framework ensures
that the neural system’s prediction exactly conserves the
Conservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize
the temporal bundling technique (Brandstetter et al., 2021)
to avoid over-fitting and improve the prediction latency for
TSM. Following the precedent work in the field (Li et al., 2020c;
Kochkov et al., 2021; Brandstetter et al., 2021), we evaluate
the proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing
equation for turbulent flows with the conservation of mass
and momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows
can achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and
different Reynolds numbers.",32.72000000000003,235.0,7.0,389.0,0.7506037354469299," 0, Propname can be regarded as a temporal generalization of classic finite volume methods such as Propname, and recently proposed learned interpolation solvers, both of which adaptively weight or interpolate the stencils based on the latest states only. On the other hand, in Propname we use the Propname based temporal features to calculate the interpolation coefficients for approximating the integrated velocity on each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation coefficients, while the stencil learning framework ensures that the neural systems prediction exactly conserves the Propname Propname and the incompressibility of the fluid. With the abundant temporal information, we further utilize the temporal bundling technique to avoid over fitting and improve the prediction latency for Propname. Following the precedent work in the field Propname Propname Propname Propname, 0000c; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, we evaluate the proposed Propname neural Propname solver on the 0 D incompressible Propname Propname equation, which is the governing equation for turbulent flows with the conservation of mass and momentum in a Newtonian fluid. Our empirical evaluation shows that Propname achieves both state of the art simulation accuracy and inference speed. We also show that Propname trained with steady state flows can achieve strong generalization performance on out of distribution turbulent flows, including different forcings and different Propname numbers.", X PUNCT PROPN AUX AUX VERB ADP DET ADJ NOUN ADP ADJ ADJ NOUN NOUN ADJ ADP PROPN PUNCT CCONJ ADV VERB VERB NOUN NOUN PUNCT PRON ADP PRON ADV NOUN CCONJ VERB DET NOUN VERB ADP DET ADJ NOUN ADV PUNCT ADP DET ADJ NOUN PUNCT ADP PROPN PRON VERB DET PROPN VERB ADJ NOUN PART VERB DET NOUN NOUN ADP VERB DET VERB NOUN ADP DET NOUN NOUN PUNCT DET ADJ ADJ NOUN VERB DET ADJ NOUN ADP VERB DET NOUN NOUN PUNCT SCONJ DET NOUN VERB NOUN VERB SCONJ DET ADJ NOUN NOUN ADV VERB DET PROPN PROPN CCONJ DET NOUN ADP DET NOUN PUNCT ADP DET ADJ ADJ NOUN PUNCT PRON ADV VERB DET ADJ NOUN NOUN PART VERB ADP ADJ CCONJ VERB DET NOUN NOUN ADP PROPN PUNCT VERB DET NOUN NOUN ADP DET NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PRON VERB DET VERB PROPN ADJ PROPN VERB ADP DET NUM ADJ ADJ PROPN PROPN NOUN PUNCT PRON AUX DET VERB NOUN ADP ADJ NOUN ADP DET NOUN ADP NOUN CCONJ NOUN ADP DET ADJ NOUN PUNCT PRON ADJ NOUN VERB SCONJ PROPN VERB DET NOUN ADP DET NOUN NOUN NOUN CCONJ NOUN NOUN PUNCT PRON ADV VERB SCONJ PROPN VERB ADP ADJ NOUN NOUN AUX VERB ADJ NOUN NOUN ADP ADP ADP NOUN ADJ NOUN PUNCT VERB ADJ NOUN CCONJ ADJ PROPN NOUN PUNCT,0.5284552845528455,35.142857142857146,5.540650406504065,220,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
14,14,GPT-3.5,"[' Supervised Fine-Tuning (SFT) on response demonstrations coupled with Reinforcement Learning from Human Feedback (RLHF) offers a robust paradigm for aligning AI agents based on large language models (LLMs). However, the reliance on high-quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in-distribution preferences is challenging. This paper introduces SALMON (Self-ALignMent with principlefOllowiNg reward models), a novel approach to align base language models with minimal human supervision. SALMON utilizes a principle-following reward model, trained on synthetic preference data, to generate reward scores based on human-defined principles. By adjusting these principles during RL training, we gain precise control over preferences, influencing RL-trained policies and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method produces the AI assistant Dromedary-2, surpassing state-of-the-art AI systems with only 6 exemplars for in-context learning and 31 human-defined principles. We open-source the code and model weights, encouraging further research in aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.']",abstract_chunked," Supervised Fine-Tuning (SFT) on response demonstrations coupled with Reinforcement Learning from Human Feedback (RLHF) offers a robust paradigm for aligning AI agents based on large language models (LLMs). However, the reliance on high-quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in-distribution preferences is challenging. This paper introduces SALMON (Self-ALignMent with principlefOllowiNg reward models), a novel approach to align base language models with minimal human supervision. SALMON utilizes a principle-following reward model, trained on synthetic preference data, to generate reward scores based on human-defined principles. By adjusting these principles during RL training, we gain precise control over preferences, influencing RL-trained policies and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method produces the AI assistant Dromedary-2, surpassing state-of-the-art AI systems with only 6 exemplars for in-context learning and 31 human-defined principles. We open-source the code and model weights, encouraging further research in aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",31.330695187165787,187.0,7.0,328.0,0.614621102809906," Supervised Fine Tuning on response demonstrations coupled with Propname Propname from Propname Propname offers a robust paradigm for aligning Propname agents based on large language models. However, the reliance on high quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in distribution preferences is challenging. This paper introduces Propname, a novel approach to align base language models with minimal human supervision. Propname utilizes a principle following reward model, trained on synthetic preference data, to generate reward scores based on human defined principles. By adjusting these principles during Propname training, we gain precise control over preferences, influencing Propname trained policies and eliminating the need for online human preferences. Applied to the Propname 0 00b base language model, our method produces the Propname assistant Propname 0, surpassing state of the art Propname systems with only 0 exemplars for in context learning and 00 human defined principles. We open source the code and model weights, encouraging further research in aligning Propname based Propname agents with enhanced supervision efficiency, improved controllability, and scalable oversight.", ADJ NOUN NOUN ADP NOUN NOUN VERB ADP PROPN PROPN ADP PROPN PROPN VERB DET ADJ NOUN ADP VERB PROPN NOUN VERB ADP ADJ NOUN NOUN PUNCT ADV PUNCT DET NOUN ADP ADJ NOUN ADJ NOUN VERB DET ADJ NOUN PUNCT ADV ADP ADJ NOUN SCONJ VERB ADJ NOUN NOUN CCONJ ADP NOUN NOUN AUX VERB PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN ADP ADJ NOUN NOUN NOUN ADP ADJ ADJ NOUN PUNCT PROPN VERB DET NOUN VERB NOUN NOUN PUNCT VERB ADP ADJ NOUN NOUN PUNCT PART VERB NOUN NOUN VERB ADP ADJ VERB NOUN PUNCT ADP VERB DET NOUN ADP PROPN NOUN PUNCT PRON VERB ADJ NOUN ADP NOUN PUNCT VERB PROPN VERB NOUN CCONJ VERB DET NOUN ADP ADJ ADJ NOUN PUNCT VERB ADP DET PROPN NUM NOUN NOUN NOUN NOUN PUNCT PRON NOUN VERB DET PROPN NOUN PROPN NUM PUNCT VERB NOUN ADP DET NOUN PROPN NOUN ADP ADV NUM NOUN ADP ADP NOUN NOUN CCONJ NUM ADJ VERB NOUN PUNCT PRON VERB NOUN DET NOUN CCONJ NOUN NOUN PUNCT VERB ADJ NOUN ADP VERB PROPN VERB PROPN NOUN ADP ADJ NOUN NOUN PUNCT VERB NOUN PUNCT CCONJ ADJ NOUN PUNCT,0.5786802030456852,28.142857142857142,5.66497461928934,14,Zhiqing Sun,GPT-3.5,GPT-3.5,Zhiqing Sun,Zhiqing Sun,GPT-3.5
103,17,GPT-3.5,"[' Natural Language Processing (NLP) has made remarkable strides, leveraging massive pre-trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces MobileBERT, a solution that addresses these challenges by compressing and accelerating the popular BERT model. Maintaining the versatility of the original BERT, MobileBERT is designed to be task-agnostic, allowing seamless application to diverse NLP tasks with minimal fine-tuning. The architecture of MobileBERT draws inspiration from BERTLARGE but incorporates bottleneck structures and a finely tuned balance between self-attentions and feed-forward networks. The training strategy involves a specially crafted teacher model, an inverted-bottleneck incorporated BERTLARGE, from which knowledge is transferred to MobileBERT. Empirical results demonstrate the efficiency of MobileBERT, achieving a 4.3× reduction in size and a 5.5× acceleration compared to BERTBASE, while still maintaining competitive performance on established benchmarks.']",intro_chunked," Natural Language Processing (NLP) has made remarkable strides, leveraging massive pre-trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces MobileBERT, a solution that addresses these challenges by compressing and accelerating the popular BERT model. Maintaining the versatility of the original BERT, MobileBERT is designed to be task-agnostic, allowing seamless application to diverse NLP tasks with minimal fine-tuning. The architecture of MobileBERT draws inspiration from BERTLARGE but incorporates bottleneck structures and a finely tuned balance between self-attentions and feed-forward networks. The training strategy involves a specially crafted teacher model, an inverted-bottleneck incorporated BERTLARGE, from which knowledge is transferred to MobileBERT. Empirical results demonstrate the efficiency of MobileBERT, achieving a 4.3× reduction in size and a 5.5× acceleration compared to BERTBASE, while still maintaining competitive performance on established benchmarks.",31.360201342281925,149.0,7.0,271.0,0.6360482573509216," Propname Propname Propname has made remarkable strides, leveraging massive pre trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces Propname, a solution that addresses these challenges by compressing and accelerating the popular Propname model. Maintaining the versatility of the original Propname, Propname is designed to be task agnostic, allowing seamless application to diverse Propname tasks with minimal fine tuning. The architecture of MobileBERT draws inspiration from Propname but incorporates bottleneck structures and a finely tuned balance between self attentions and feed forward networks. The training strategy involves a specially crafted teacher model, an inverted bottleneck incorporated BERTLARGE, from which knowledge is transferred to Propname. Empirical results demonstrate the efficiency of Propname, achieving a 0.0 reduction in size and a 0.0 acceleration compared to Propname, while still maintaining competitive performance on established benchmarks.", PROPN PROPN PROPN AUX VERB ADJ NOUN PUNCT VERB ADJ ADJ VERB NOUN ADP ADJ NOUN PUNCT ADV PUNCT DET NOUN ADP DET NOUN ADP ADJ NOUN VERB NOUN ADP ADP PRON NOUN CCONJ NOUN PUNCT DET NOUN VERB PROPN PUNCT DET NOUN PRON VERB DET NOUN ADP VERB CCONJ VERB DET ADJ PROPN NOUN PUNCT VERB DET NOUN ADP DET ADJ PROPN PUNCT PROPN AUX VERB PART AUX NOUN ADJ PUNCT VERB ADJ NOUN PART VERB PROPN NOUN ADP ADJ ADJ NOUN PUNCT DET NOUN ADP NUM VERB NOUN ADP PROPN CCONJ VERB NOUN NOUN CCONJ DET ADV VERB NOUN ADP NOUN NOUN CCONJ VERB ADJ NOUN PUNCT DET NOUN NOUN VERB DET ADV VERB NOUN NOUN PUNCT DET VERB NOUN VERB NOUN PUNCT ADP PRON NOUN AUX VERB ADP PROPN PUNCT ADJ NOUN VERB DET NOUN ADP PROPN PUNCT VERB DET NUM NOUN ADP NOUN CCONJ DET NUM NOUN VERB ADP PROPN PUNCT SCONJ ADV VERB ADJ NOUN ADP VERB NOUN PUNCT,0.6646341463414634,23.428571428571427,5.762195121951219,103,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5
116,30,Aman Madaan,"[' Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited\nor the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing\ncompiler, which iteratively applies optimizations as an intermediate step in generating machine code (Aho et al.,\n2007). Despite the impressive progress in optimizing compilers, programmers are still generally responsible for\nnumerous performance considerations. Experienced programmers can often shave off microseconds from an\nalready optimized code. However, such improvements typically come after laborious consideration of factors\nsuch as algorithm selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models (LLMs) have shown great potential in a variety of software-related tasks ranging from\ncompetitive programming (Li et al., 2022), code completion/editing (Fried et al., 2022), to clone detection,\ndefect detection, and much more (Lu et al., 2021; Xu et al., 2022; Zhou et al., 2022; Austin et al., 2021). Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the\nefficiency of programs while operating at the level of high-level programming languages like Python or C++? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or\nvery non-trivial to expect from an optimizing compiler or search procedure.', 'If so, large language models may\nhave an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating\na dataset of Performance-Improving Edits, PIE. We collect trajectories of programs written by the same user,\nwhere we track a single programmer’s submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us\nwith pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize\nimpact on the program’s runtime. We show that PIE allows us to improve the efficacy of large language models for code optimization. Specifically,\nwe investigate the effects of using PIE for few-shot prompting on CODEX and fine-tuning models like CODEGEN. We see noticeable improvements in all experiments using PIE. Ultimately, these models can successfully\nperform substantial (up to 2.5ˆ) code optimization for Python and C++ for many examples (up to 50% of the\ntest set). We also demonstrate that CODEGEN and CODEGEN trained on PIE can, in some cases, match the perfromance of CODEX while being up to 10 X smaller in size than CODEX.']",intro_chunked," Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited
or the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing
compiler, which iteratively applies optimizations as an intermediate step in generating machine code (Aho et al.,
2007). Despite the impressive progress in optimizing compilers, programmers are still generally responsible for
numerous performance considerations. Experienced programmers can often shave off microseconds from an
already optimized code. However, such improvements typically come after laborious consideration of factors
such as algorithm selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models (LLMs) have shown great potential in a variety of software-related tasks ranging from
competitive programming (Li et al., 2022), code completion/editing (Fried et al., 2022), to clone detection,
defect detection, and much more (Lu et al., 2021; Xu et al., 2022; Zhou et al., 2022; Austin et al., 2021). Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the
efficiency of programs while operating at the level of high-level programming languages like Python or C++? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or
very non-trivial to expect from an optimizing compiler or search procedure.",31.545521313364077,217.0,8.0,379.0,0.4534202218055725," Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited or the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing compiler, which iteratively applies optimizations as an intermediate step in generating machine code Propname Propname Propname Propname, 0000. Despite the impressive progress in optimizing compilers, programmers are still generally responsible for numerous performance considerations. Experienced programmers can often shave off microseconds from an already optimized code. However, such improvements typically come after laborious consideration of factors such as Propname selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models have shown great potential in a variety of software related tasks ranging from competitive programming, code completionediting, to clone detection, defect detection, and much more. Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the efficiency of programs while operating at the level of high level programming languages like Propname or C? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or very non trivial to expect from an optimizing compiler or search procedure.", VERB NOUN AUX DET ADJ NOUN ADP NOUN PUNCT ADV SCONJ ADJ NOUN AUX ADJ CCONJ DET NOUN AUX VERB ADP DET ADJ NOUN PUNCT DET ADJ NOUN ADP VERB NOUN NOUN AUX DET VERB NOUN PUNCT PRON ADV VERB NOUN ADP DET ADJ NOUN ADP VERB NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SCONJ DET ADJ NOUN ADP VERB NOUN PUNCT NOUN AUX ADV ADV ADJ ADP ADJ NOUN NOUN PUNCT VERB NOUN AUX ADV VERB ADP NOUN ADP DET ADV VERB NOUN PUNCT ADV PUNCT ADJ NOUN ADV VERB ADP ADJ NOUN ADP NOUN ADJ ADP PROPN NOUN PUNCT NOUN NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ VERB NOUN NOUN PUNCT ADJ NOUN NOUN AUX VERB ADJ NOUN ADP DET NOUN ADP NOUN VERB NOUN VERB ADP ADJ NOUN PUNCT NOUN NOUN PUNCT PART VERB NOUN PUNCT VERB NOUN PUNCT CCONJ ADV ADJ PUNCT VERB ADP DET NOUN PUNCT PRON VERB PART VERB CCONJ VERB DET ADJ NOUN PUNCT AUX NOUN VERB DET NOUN ADP NOUN SCONJ VERB ADP DET NOUN ADP ADJ NOUN NOUN NOUN ADP PROPN CCONJ NOUN PUNCT PRON VERB SCONJ ADJ NOUN NOUN VERB DET NOUN PART VERB NOUN PRON AUX AUX ADJ CCONJ ADV X ADJ PART VERB ADP DET VERB NOUN CCONJ NOUN NOUN PUNCT,0.6511627906976745,26.875,5.52093023255814,116,Aman Madaan,Aman Madaan,Aman Madaan,GPT-3.5,Aman Madaan,GPT-3.5
22,22,GPT-3.5,"[' This paper introduces an innovative approach to obtaining high-quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models (PLMs). Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high-performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique.']",abstract_chunked," This paper introduces an innovative approach to obtaining high-quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models (PLMs). Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high-performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique.",23.48578378378383,111.0,5.0,211.0,0.5419113636016846," This paper introduces an innovative approach to obtaining high quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models. Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique.", DET NOUN VERB DET ADJ NOUN ADP VERB ADJ NOUN NOUN NOUN ADP DET NOUN ADP VERB NOUN PUNCT VERB PUNCT CCONJ NOUN ADP VERB NOUN ADP VERB NOUN NOUN PUNCT ADJ NOUN CCONJ NOUN NOUN ADP ADJ VERB NOUN CCONJ ADJ NOUN ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT VERB ADJ ADJ NOUN PART VERB ADJ NOUN PUNCT PRON VERB NOUN VERB DET ADJ NOUN ADP ADJ CCONJ ADJ VERB NOUN PART VERB ADJ NOUN ADP VERB NOUN NOUN ADP NOUN PUNCT DET NOUN AUX ADV VERB ADP VERB ADJ CCONJ ADV ADJ NOUN PUNCT PRON ADV ADJ NOUN VERB ADJ NOUN ADP ADJ ADJ ADJ NOUN NOUN PUNCT VERB DET NOUN ADP DET NOUN NOUN PUNCT,0.6974789915966386,23.8,6.050420168067227,22,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5
227,141,Zhiqing Sun,"[' Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state-of-the-art neural detectors [21, 24, 19, 25, 38, 26, 11] are developed in a detect-and-merge fashion that is, instead of directly optimizing the predicted set in an end-to-end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post-processing step (e.g., “non-maximum suppression” or NMS) for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end-to-end and arguably sub-optimal.DEtection TRansformer (DETR) [2] is recently proposed as the first fully end-to-end object detector. It usesTransformer [32] to directly output a final set of predictions without further post-processing. However, it takes extra-long training time to converge. For example, the popular Faster RCNN model [26] only requires about 30 epochs to convergence, but DETR needs 500 epochs, which takes at least 10 days on 8 V100 GPUs.', 'Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like Transformer-based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETR’s optimization difficulty we conduct extensive experiments and find that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder-only version of DETR by removing the cross-attention module. We find that the encoder-only DETR yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETR’s Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only DETR with feature pyramids [18]. Specifically, we present TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN), which are inspired by a classic one-stage detector FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respectively. A novel Feature of Interest (FoI) selection mechanism is developed in TSP-FCOS to help Transformer encoder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20] the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.']",intro_chunked,"Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like Transformer-based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETR’s optimization difficulty we conduct extensive experiments and find that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder-only version of DETR by removing the cross-attention module. We find that the encoder-only DETR yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETR’s Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only DETR with feature pyramids [18]. Specifically, we present TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN), which are inspired by a classic one-stage detector FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respectively. A novel Feature of Interest (FoI) selection mechanism is developed in TSP-FCOS to help Transformer encoder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20] the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.",38.43322884012542,319.0,11.0,524.0,0.5780467987060547," Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for Propname like Propname based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETRs optimization difficulty we conduct extensive experiments and find that the cross attention module, by which the Propname decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder only version of Propname by removing the cross attention module. We find that the encoder only Propname yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETRs Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Propname based set prediction methods, both of which can be regarded as improved versions of encoder only Propname with feature pyramids. Specifically, we present Propname Propname and Propname Propname, which are inspired by a classic one stage detector Propname and a classic two stage detector Propname Propname, respectively. A novel Feature of Propname selection mechanism is developed in Propname Propname to help Propname encoder handle multi level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the Propname 0000 detection benchmark the proposed methods not only converge much faster than the original Propname, but also significantly outperform Propname and other baselines in terms of detection accuracy.", ADJ ADJ NOUN NOUN AUX AUX ADV ADJ ADP ADJ NOUN PUNCT ADV PUNCT ADP DET NOUN AUX PRON VERB DET NOUN NOUN ADP ADJ NOUN ADP PROPN ADP PROPN VERB NOUN AUX DET ADJ NOUN NOUN CCONJ AUX DET ADJ NOUN ADP DET NOUN PUNCT ADP VERB DET NOUN ADP NOUN NOUN NOUN PRON VERB ADJ NOUN CCONJ VERB SCONJ DET NOUN NOUN NOUN PUNCT ADP PRON DET PROPN NOUN VERB ADJ NOUN ADP NOUN PUNCT AUX ADV ADJ ADP DET ADJ NOUN PUNCT ADP NOUN ADP ADJ NOUN PUNCT PRON ADV VERB DET NOUN ADV NOUN ADP PROPN ADP VERB DET NOUN NOUN NOUN PUNCT PRON VERB SCONJ DET NOUN ADV PROPN VERB DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN ADP ADJ CCONJ ADJ NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB SCONJ DET NOUN ADP DET ADJ NOUN ADP NOUN ADJ NOUN ADV VERB ADP DET ADJ NOUN PUNCT VERB ADP DET ADJ NOUN PRON VERB NUM NOUN ADP ADV VERB DET NOUN NOUN ADP PROPN VERB VERB NOUN NOUN PUNCT PRON ADP PRON AUX AUX VERB ADP VERB NOUN ADP NOUN ADV PROPN ADP NOUN NOUN PUNCT ADV PUNCT PRON VERB PROPN PROPN CCONJ PROPN PROPN PUNCT PRON AUX VERB ADP DET ADJ NUM NOUN NOUN PROPN CCONJ DET ADJ NUM NOUN NOUN PROPN PROPN PUNCT ADV PUNCT DET ADJ NOUN ADP PROPN NOUN NOUN AUX VERB ADP PROPN PROPN PART VERB PROPN NOUN VERB ADJ NOUN NOUN PUNCT PART VERB DET NOUN ADP DET ADJ NOUN ADP DET ADJ NOUN PUNCT PRON ADV VERB DET ADJ ADJ NOUN NOUN ADP PRON ADP PRON NUM NOUN ADP VERB DET NOUN ADP NOUN PUNCT ADP PRON NOUN ADP DET PROPN NUM NOUN NOUN DET VERB NOUN PART ADV VERB ADV ADV ADP DET ADJ PROPN PUNCT CCONJ ADV ADV VERB PROPN CCONJ ADJ NOUN ADP NOUN ADP NOUN NOUN PUNCT,0.4968553459119497,28.90909090909091,5.3081761006289305,227,Zhiqing Sun,Zhiqing Sun,Hugo Touvron,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
104,18,GPT-3.5,"["" Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce PEER, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of PEER to cover various parts of the writing process, leveraging self-training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEER's applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEER's robust performance across a range of domains and editing tasks.""]",intro_chunked," Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce PEER, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of PEER to cover various parts of the writing process, leveraging self-training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEER's applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEER's robust performance across a range of domains and editing tasks.",23.1496153846154,156.0,7.0,297.0,0.6495703458786011," Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce Propname, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of Propname to cover various parts of the writing process, leveraging self training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEERs applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEERs robust performance across a range of domains and editing tasks.", SCONJ DET ADJ NOUN ADP NOUN NOUN VERB NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN PUNCT ADJ NOUN NOUN AUX ADV VERB PART VERB ADJ NOUN PUNCT DET NOUN VERB PRON NOUN ADP ADJ NOUN NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN ADJ ADP VERB DET ADJ NOUN NOUN PUNCT ADJ VERB DET NOUN PART VERB NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN ADP PRON NOUN PUNCT VERB ADJ NOUN ADP ADJ NOUN ADJ ADP VERB VERB NOUN PUNCT NOUN PUNCT CCONJ ADJ NOUN PUNCT DET ADJ NOUN AUX DET NOUN ADP ADJ NOUN ADP PROPN PART VERB ADJ NOUN ADP DET NOUN NOUN PUNCT VERB NOUN NOUN NOUN PART VERB DET NOUN NOUN ADP NOUN ADP NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT DET NOUN VERB VERB NOUN ADP NOUN VERB NOUN NOUN CCONJ VERB PRON NOUN ADP VERB NOUN PUNCT VERB ADJ NOUN PUNCT CCONJ VERB PRON NOUN PUNCT ADP ADJ NOUN PUNCT PRON VERB VERB ADJ NOUN ADP DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT,0.6312849162011173,25.571428571428573,5.553072625698324,104,Timo Schick,GPT-3.5,GPT-3.5,GPT-3.5,Timo Schick,GPT-3.5
207,121,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked,"Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.",51.193218673218695,296.0,11.0,449.0,0.7247234582901001," Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Propname Propname Propname, which calibrates the reward signals by augmenting them with additional information such as image captions or ground truth multi choice option, as illustrated in Propname.0. To improve the general capabilities of Propname during the Propname Propname Tuning stage, we further augment the synthetic vision instruction tuning data with existing high quality human annotated multi modal data in the conversation format. Specifically, we convert Propname Propname and A OKVQA into a multi round QA task, and Flickr00k into a Propname Captioning task, and train the Propname Propname models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of Propname in real world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 00 main object categories in Propname and include 0 different task types, leading to Propname Propname. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti hallucinations. In our experimental evaluation, as the first Propname trained with Propname, Propname Propname delivers impressive outcomes. We observed a notable enhancement on Propname Propname, achieving 00, an improvement by 00 in Propname Propname, and established new performance benchmarks for Propname with a 00.0 score on MMBench and an 00.0 F0 on Propname. We have made our code, model, and data publicly available at.", ADV PUNCT PRON VERB DET ADJ NOUN ADP DET NOUN NOUN ADP VERB DET ADJ NOUN NOUN ADP ADJ NOUN CCONJ DET ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB DET NOUN NOUN VERB PROPN PROPN PROPN PUNCT PRON VERB DET NOUN NOUN ADP VERB PRON ADP ADJ NOUN ADJ ADP NOUN NOUN CCONJ NOUN NOUN NOUN NOUN NOUN PUNCT SCONJ VERB ADP PROPN PUNCT PUNCT PUNCT PART VERB DET ADJ NOUN ADP PROPN ADP DET PROPN PROPN NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN NOUN VERB NOUN ADP VERB ADJ NOUN NOUN VERB ADJ ADJ NOUN ADP DET NOUN NOUN PUNCT ADV PUNCT PRON VERB PROPN PROPN CCONJ DET NOUN ADP DET ADJ ADJ NOUN NOUN PUNCT CCONJ VERB ADP DET PROPN ADJ NOUN PUNCT CCONJ VERB DET PROPN PROPN NOUN VERB ADP DET ADJ NOUN ADP NOUN PUNCT ADV PUNCT PRON VERB ADP VERB DET ADJ NOUN ADP PROPN ADP ADJ NOUN NOUN NOUN PUNCT VERB ADJ NOUN ADP VERB DET NOUN PUNCT PRON VERB DET NOUN ADP ADJ ADJ NOUN PRON VERB DET NUM ADJ NOUN NOUN ADP PROPN CCONJ VERB NUM ADJ NOUN NOUN PUNCT VERB ADP PROPN PROPN PUNCT PRON NOUN VERB SCONJ DET ADJ NOUN NOUN ADV ADP ADJ NOUN PUNCT ADV SCONJ NOUN AUX VERB ADP ADJ NOUN PUNCT ADP PRON ADJ NOUN PUNCT ADP DET ADJ PROPN VERB ADP PROPN PUNCT PROPN PROPN VERB ADJ NOUN PUNCT PRON VERB DET ADJ NOUN ADP PROPN PROPN PUNCT VERB NUM PUNCT DET NOUN ADP NUM ADP PROPN PROPN PUNCT CCONJ VERB ADJ NOUN NOUN ADP PROPN ADP DET NUM NOUN ADP NOUN CCONJ DET NUM NOUN ADP PROPN PUNCT PRON AUX VERB PRON NOUN PUNCT NOUN PUNCT CCONJ NOUN ADV ADJ ADP PUNCT,0.5467128027681661,28.9,5.117647058823529,207,Zhiqing Sun,Hugo Touvron,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Timo Schick
144,58,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked,"As an emerging
area of NLP, large scale pre-trained language models have made strides in addressing challenging
tasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog
generation (Budzianowski and Vulic´, 2019). These
systems typically fine-tune large language models
on a corpus of a task-specific dataset. However,
these techniques have not been investigated for
temporal graph extraction. This paper focuses on the problem of generation
of an event-level temporal graph for each document, and we refer to this task as contextualized
graph generation. We address this open challenge
by proposing a novel reformulation of the task as a
sequence-to-sequence mapping problem (Sutskever
et al., 2014), which enables us to leverage large pretrained models for our task. Further, different from
existing methods, our proposed approach is completely end-to-end and eliminates the need for a
pipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which
is a prerequisite to our main goal: the difficulty of
obtaining a large quantity of training graphs with human-annotated events and temporal relations. To
this end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps
for pruning and noise reduction.",47.04666666666668,216.0,8.0,338.0,0.2858175039291382," As an emerging area of Propname, large scale pre trained language models have made strides in addressing challenging tasks like Propname knowledge graph completion and task oriented dialog generation. These systems typically fine tune large language models on a corpus of a task specific dataset. However, these techniques have not been investigated for temporal graph extraction. This paper focuses on the problem of generation of an event level temporal graph for each document, and we refer to this task as contextualized graph generation. We address this open challenge by proposing a novel reformulation of the task as a sequence to sequence mapping problem Propname Propname Propname Propname, 0000, which enables us to leverage large pretrained models for our task. Further, different from existing methods, our proposed approach is completely end to end and eliminates the need for a pipeline of sub systems commonly used by traditional methods. We also address a related open challenge, which is a prerequisite to our main goal: the difficulty of obtaining a large quantity of training graphs with human annotated events and temporal relations. To this end, we automatically produce a large collection of document graph pairs by using Propname, followed by a few rule based post processing steps for pruning and noise reduction.", ADP DET VERB NOUN ADP PROPN PUNCT ADJ NOUN VERB VERB NOUN NOUN AUX VERB NOUN ADP VERB ADJ NOUN ADP PROPN NOUN NOUN NOUN CCONJ NOUN VERB NOUN NOUN PUNCT DET NOUN ADV ADJ NOUN ADJ NOUN NOUN ADP DET NOUN ADP DET NOUN ADJ NOUN PUNCT ADV PUNCT DET NOUN AUX PART AUX VERB ADP ADJ NOUN NOUN PUNCT DET NOUN VERB ADP DET NOUN ADP NOUN ADP DET NOUN NOUN ADJ NOUN ADP DET NOUN PUNCT CCONJ PRON VERB ADP DET NOUN ADP VERB NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP VERB DET ADJ NOUN ADP DET NOUN ADP DET NOUN PART VERB NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PRON VERB PRON PART VERB ADJ VERB NOUN ADP PRON NOUN PUNCT ADV PUNCT ADJ ADP VERB NOUN PUNCT PRON VERB NOUN AUX ADV ADJ ADP NOUN CCONJ VERB DET NOUN ADP DET NOUN ADP NOUN NOUN ADV VERB ADP ADJ NOUN PUNCT PRON ADV VERB DET ADJ ADJ NOUN PUNCT PRON AUX DET NOUN ADP PRON ADJ NOUN PUNCT DET NOUN ADP VERB DET ADJ NOUN ADP NOUN NOUN ADP ADJ ADJ NOUN CCONJ ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON ADV VERB DET ADJ NOUN ADP NOUN NOUN NOUN ADP VERB PROPN PUNCT VERB ADP DET ADJ NOUN VERB NOUN NOUN NOUN ADP NOUN CCONJ NOUN NOUN PUNCT,0.5701754385964912,28.5,4.951754385964913,144,Aman Madaan,Aman Madaan,Zhiqing Sun,Aman Madaan,Aman Madaan,Timo Schick
51,51,Hugo Touvron,"[' This paper tackles the problem of learning a finer representation than the one provided by training labels. This\nenables fine-grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest-neighbor classifier\nobjective, and an instance loss inspired by self-supervised\nlearning. By jointly leveraging the coarse labels and the underlying fine-grained latent space, it significantly improves\nthe accuracy of category-level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than\nthat available at train time. It also improves the accuracy\nfor transfer learning tasks to fine-grained datasets, thereby\nestablishing the new state of the art on five public benchmarks, like iNaturalist-2018.']",abstract_chunked," This paper tackles the problem of learning a finer representation than the one provided by training labels. This
enables fine-grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest-neighbor classifier
objective, and an instance loss inspired by self-supervised
learning. By jointly leveraging the coarse labels and the underlying fine-grained latent space, it significantly improves
the accuracy of category-level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than
that available at train time. It also improves the accuracy
for transfer learning tasks to fine-grained datasets, thereby
establishing the new state of the art on five public benchmarks, like iNaturalist-2018.",47.35994623655915,124.0,6.0,203.0,0.5622327923774719," This paper tackles the problem of learning a finer representation than the one provided by training labels. This enables fine grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest neighbor classifier objective, and an instance loss inspired by self supervised learning. By jointly leveraging the coarse labels and the underlying fine grained latent space, it significantly improves the accuracy of category level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than that available at train time. It also improves the accuracy for transfer learning tasks to fine grained datasets, thereby establishing the new state of the art on five public benchmarks, like Propname 0000.", DET NOUN VERB DET NOUN ADP VERB DET ADJ NOUN ADP DET NOUN VERB ADP NOUN NOUN PUNCT PRON VERB ADJ VERB NOUN NOUN ADP NOUN ADP DET NOUN VERB ADP ADJ NOUN ADV PUNCT PRON NOUN AUX VERB ADP DET ADJ NOUN NOUN NOUN PUNCT CCONJ DET NOUN NOUN VERB ADP NOUN ADJ NOUN PUNCT ADP ADV VERB DET ADJ NOUN CCONJ DET ADJ NOUN VERB NOUN NOUN PUNCT PRON ADV VERB DET NOUN ADP ADJ NOUN NOUN NOUN PUNCT PRON NOUN VERB DET VERB NOUN ADP VERB CCONJ VERB NOUN ADP DET ADJ NOUN ADP PRON ADJ ADP NOUN NOUN PUNCT PRON ADV VERB DET NOUN ADP NOUN VERB NOUN PART ADJ VERB NOUN PUNCT ADV VERB DET ADJ NOUN ADP DET NOUN ADP NUM ADJ NOUN PUNCT ADP PROPN NUM PUNCT,0.664179104477612,22.333333333333332,5.2164179104477615,51,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
170,84,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked," Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.",39.280320627802695,223.0,10.0,382.0,0.6114025712013245," Since its introduction the Propname architecture has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field of view in a single layer. Along with other attention based architectures, see eg, transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers. As a result, significant improvements have been observed on different computer vision tasks, ranging from Propname Propname 0 Propname 0 Propname 0 Propname 0 Propname0 Propname 0 Propname 0 Propname 0 Propname detection and segmentation and video analysis to image generation. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers.0. Parallel vision transformers.", SCONJ PRON NOUN DET PROPN NOUN AUX VERB DET ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT VERB ADV ADJ NOUN NOUN PUNCT DET NOUN NOUN AUX DET ADJ NOUN ADP NOUN PART NOUN NOUN NOUN ADP NOUN NOUN PUNCT DET NOUN NOUN AUX VERB ADP ADJ NOUN NOUN PUNCT PRON AUX VERB ADP DET NOUN NOUN NOUN PUNCT ADP DET ADJ NOUN NOUN NOUN PUNCT ADP NOUN ADP NOUN VERB ADP ADJ NOUN PUNCT NOUN VERB ADJ NOUN CCONJ DET ADJ NOUN ADP NOUN ADP DET ADJ NOUN PUNCT ADP ADP ADJ NOUN VERB NOUN PUNCT VERB NOUN PUNCT NOUN AUX ADV ADV VERB DET NOUN ADP NOUN NOUN VERB PUNCT ADJ ADJ NOUN ADP NOUN NOUN ADV VERB NOUN ADP PRON NOUN ADP DET NOUN PUNCT CCONJ AUX ADP ADJ VERB ADP DET ADJ NOUN VERB ADP NOUN PUNCT ADP DET NOUN PUNCT ADJ NOUN AUX AUX VERB ADP ADJ NOUN NOUN NOUN PUNCT VERB ADP PROPN PROPN NUM PROPN NUM PROPN NUM PROPN NUM PROPN PUNCT PROPN NUM PROPN NUM PROPN NUM PROPN NOUN CCONJ NOUN CCONJ NOUN NOUN ADP NOUN NOUN PUNCT SCONJ NOUN NOUN AUX VERB ADP ADJ NOUN PUNCT DET NOUN ADP PRON NOUN CCONJ NOUN NOUN AUX ADV AUX VERB ADP DET ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB NUM NOUN ADP VERB NOUN NOUN PUNCT PUNCT PUNCT ADJ NOUN NOUN PUNCT,0.5526315789473685,25.333333333333332,5.495614035087719,170,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Zhiqing Sun
140,54,Aman Madaan,"[' Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.', 'The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1.', 'In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning']",intro_chunked," Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.",52.44166666666669,216.0,9.0,332.0,0.447733074426651," Humans are adept at anticipating and reasoning about events and their causal effects on other events. Consider these questions Would it rain more if we plant more trees?, What would help the water in boiling faster? answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering, process tracking, reasoning about qualitative relationships, and physical commonsense reasoning. authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction, temporal event reasoning, and Propname. However, these systems are primarily extractive they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.", NOUN AUX ADJ ADP VERB CCONJ VERB ADP NOUN CCONJ PRON ADJ NOUN ADP ADJ NOUN PUNCT VERB DET NOUN AUX PRON VERB ADJ SCONJ PRON VERB ADJ NOUN PUNCT PUNCT PRON AUX VERB DET NOUN ADP VERB ADV PUNCT VERB DET NOUN VERB DET NOUN PART VERB DET ADJ NOUN ADP NOUN NOUN CCONJ NOUN NOUN CCONJ DET NOUN PART VERB ADP SCONJ ADJ NOUN VERB DET ADJ ADP DET NOUN PRON AUX ADV ADJ ADP NOUN PUNCT ADV PUNCT VERB ADP NOUN CCONJ NOUN VERB DET ADJ NOUN ADP NOUN PUNCT VERB ADJ NOUN CCONJ VERB PRON NOUN NOUN AUX ADJ ADP NOUN NOUN ADP NOUN NOUN PUNCT NOUN NOUN PUNCT VERB ADP ADJ NOUN PUNCT CCONJ ADJ NOUN NOUN PUNCT NOUN VERB ADV ADP DET NOUN PUNCT ADJ NOUN AUX VERB NOUN NOUN ADP DET NOUN ADP NOUN NOUN PUNCT ADJ NOUN NOUN PUNCT CCONJ PROPN PUNCT ADV PUNCT DET NOUN AUX ADV ADJ PRON NOUN ADP NOUN ADV VERB ADP DET NOUN PUNCT VERB PRON NOUN PART AUX VERB ADP ADJ NOUN PRON VERB ADJ NOUN ADP NOUN PUNCT,0.5792349726775956,22.875,5.3497267759562845,140,Aman Madaan,Hugo Touvron,Aman Madaan,Hugo Touvron,Aman Madaan,Hugo Touvron
72,72,Timo Schick,"[' When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.']",abstract_chunked," When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",42.48235483870968,124.0,5.0,204.0,0.3286168575286865," When scaled to hundreds of billions of parameters, pretrained language models such as Propname 0 achieve remarkable few shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to Propname 0 can be obtained with language models that are much greener in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.", SCONJ VERB ADP NOUN ADP NOUN ADP NOUN PUNCT VERB NOUN NOUN ADJ ADP PROPN NUM VERB ADJ ADJ NOUN NOUN PUNCT ADV PUNCT ADJ NOUN ADP NOUN AUX VERB ADP NOUN CCONJ VERB ADJ ADJ NOUN PUNCT VERB ADP DET ADJ NOUN NOUN CCONJ VERB PRON ADJ SCONJ NOUN CCONJ NOUN PART VERB PRON PUNCT PRON VERB SCONJ NOUN ADJ ADP PROPN NUM AUX AUX VERB ADP NOUN NOUN PRON AUX ADV ADJ SCONJ SCONJ PRON NOUN NOUN AUX ADJ NOUN ADP NOUN ADJ PUNCT PRON AUX VERB ADP VERB ADJ NOUN ADP VERB NOUN PRON VERB DET NOUN NOUN PUNCT VERB ADP NOUN VERB NOUN PUNCT VERB ADJ NOUN VERB ADJ NOUN PUNCT PRON VERB ADJ NOUN VERB ADP ADJ ADJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT,0.7153846153846154,26.0,5.384615384615385,72,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Hugo Touvron
161,75,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked," Although the fundamental ideas of deep trainable neural
networks have been around for decades, only recently have
barriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of
these barriers are related to non-convex optimization in one
way or another, which is central to the success of modern
neural networks. The optimization challenges have been
addressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization
of very deep networks. An exceptionally successful design
principle is using residual connections [24, 25]. Although
this does not change the expressiveness of the functions that
the network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very
deep networks. Another key element to the optimization is
the importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet
dataset [11], and the popularization of transfer learning with
pre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions
of parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address
this issue.",41.33994579945801,205.0,9.0,345.0,0.2637071907520294," Although the fundamental ideas of deep trainable neural networks have been around for decades, only recently have barriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of these barriers are related to non convex optimization in one way or another, which is central to the success of modern neural networks. The optimization challenges have been addressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization of very deep networks. An exceptionally successful design principle is using residual connections. Although this does not change the expressiveness of the functions that the network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very deep networks. Another key element to the optimization is the importance of data, revealed by the step change in visual recognition performance resulting from the Propname dataset, and the popularization of transfer learning with pre training on large datasets. However, even when trained with millions of images, recent deep networks with millions if not billions of parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout, or label smoothing are limited in their ability to address this issue.", SCONJ DET ADJ NOUN ADP ADJ ADJ ADJ NOUN AUX AUX ADV ADP NOUN PUNCT ADV ADV VERB NOUN AUX VERB PART VERB NOUN ADP ADV VERB ADJ ADJ NOUN ADP NOUN PUNCT ADJ ADP DET NOUN AUX VERB ADP ADJ ADJ NOUN ADP NUM NOUN CCONJ PRON PUNCT PRON AUX ADJ ADP DET NOUN ADP ADJ ADJ NOUN PUNCT DET NOUN NOUN AUX AUX VERB ADP ADJ NOUN ADP DET NOUN PUNCT ADV PUNCT ADJ NOUN AUX VERB PART VERB DET NOUN ADP ADV ADJ NOUN PUNCT DET ADV ADJ NOUN NOUN AUX VERB ADJ NOUN PUNCT SCONJ PRON AUX PART VERB DET NOUN ADP DET NOUN PRON DET NOUN AUX VERB PUNCT DET ADJ NOUN NOUN NOUN PUNCT ADP DET NOUN PUNCT DET NOUN ADP VERB ADV ADJ NOUN PUNCT DET ADJ NOUN ADP DET NOUN AUX DET NOUN ADP NOUN PUNCT VERB ADP DET NOUN NOUN ADP ADJ NOUN NOUN VERB ADP DET PROPN NOUN PUNCT CCONJ DET NOUN ADP NOUN VERB ADP ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT ADV SCONJ VERB ADP NOUN ADP NOUN PUNCT ADJ ADJ NOUN ADP NOUN SCONJ PART NOUN ADP NOUN PUNCT AUX ADV ADV VERB PUNCT ADJ NOUN ADP NOUN NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN AUX VERB ADP PRON NOUN PART VERB DET NOUN PUNCT,0.593607305936073,24.333333333333332,5.301369863013699,161,Zhiqing Sun,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Zhiqing Sun
143,57,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked," Temporal reasoning is crucial for analyzing the interactions among complex events and producing
coherent interpretations of text data (Duran et al.,
2007). There is a rich body of research on the
use of temporal information in a variety of important application domains, including topic detection
and tracking (Makkonen et al., 2003), information
extraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://
github.com/madaan/temporal-graph-gen
sis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the
temporal ordering among events, where the nodes
are the individual events, and the edges capture
temporal relationships such as “before”, “after” or
“simultaneous”. Representative work on automated
extraction of such graphs from textual documents
includes the early work by Chambers and Jurafsky
(2009), where the focus is on the construction of
event chains from a collection of documents, and
the more recent CAEVO (Chambers et al., 2014) and
Cogcomptime (Ning et al., 2018), which extract
a graph for each input document instead. These
methods focus on rule-based and statistical submodules to extract verb-centered events and the
temporal relations among them.",33.834666666666664,200.0,6.0,329.0,0.4695475697517395," Temporal reasoning is crucial for analyzing the interactions among complex events and producing coherent interpretations of text data Propname Propname Propname Propname, 0000. There is a rich body of research on the use of temporal information in a variety of important application domains, including topic detection and tracking, information extraction, parsing of clinical records, discourse Propname and pre trained models available at weblink, and question answering. Graphs are a natural choice for representing the temporal ordering among events, where the nodes are the individual events, and the edges capture temporal relationships such as before, after or simultaneous. Representative work on automated extraction of such graphs from textual documents includes the early work by Propname and Propname, where the focus is on the construction of event chains from a collection of documents, and the more recent Propname and Propname, which extract a graph for each input document instead. These methods focus on rule based and statistical submodules to extract verb centered events and the temporal relations among them.", ADJ NOUN AUX ADJ ADP VERB DET NOUN ADP ADJ NOUN CCONJ VERB ADJ NOUN ADP NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PRON VERB DET ADJ NOUN ADP NOUN ADP DET NOUN ADP ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT VERB NOUN NOUN CCONJ NOUN PUNCT NOUN NOUN PUNCT VERB ADP ADJ NOUN PUNCT VERB PROPN CCONJ VERB VERB NOUN ADJ ADP NOUN PUNCT CCONJ NOUN VERB PUNCT NOUN AUX DET ADJ NOUN ADP VERB DET ADJ NOUN ADP NOUN PUNCT SCONJ DET NOUN AUX DET ADJ NOUN PUNCT CCONJ DET NOUN VERB ADJ NOUN ADJ ADP ADV PUNCT ADP CCONJ ADJ PUNCT ADJ NOUN ADP VERB NOUN ADP ADJ NOUN ADP ADJ NOUN VERB DET ADJ NOUN ADP PROPN CCONJ PROPN PUNCT SCONJ DET NOUN AUX ADP DET NOUN ADP NOUN NOUN ADP DET NOUN ADP NOUN PUNCT CCONJ DET ADV ADJ PROPN CCONJ PROPN PUNCT PRON VERB DET NOUN ADP DET NOUN NOUN ADV PUNCT DET NOUN VERB ADP NOUN VERB CCONJ ADJ NOUN PART VERB VERB VERB NOUN CCONJ DET ADJ NOUN ADP PRON PUNCT,0.5760869565217391,36.8,5.233695652173913,143,Aman Madaan,Zhiqing Sun,Aman Madaan,Aman Madaan,Aman Madaan,Zhiqing Sun
113,27,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked," Although large language models (LLMs) can generate coherent outputs, they often fall short in
addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such
as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program
readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may
benefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved
one—to ensure that the desired quality is achieved. Iterative refinement typically involves training
a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models
require large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),
which may not always be feasible to obtain. These limitations underscore the need for an effective
refinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;
Flower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating
an initial draft and subsequently refining it based on self-provided feedback.",33.177901119402975,201.0,8.0,352.0,0.5293113589286804," Although large language models can generate coherent outputs, they often fall short in addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such as dialogue response generation, or tasks with hard to define goals, such as enhancing program readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may benefit from further iterative refinementie, iteratively mapping a candidate output to an improved oneto ensure that the desired quality is achieved. Propname refinement typically involves training a refinement model that relies on domain specific data; Propname Propname Propname.; Propname Propname Propname.. Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations, which may not always be feasible to obtain. These limitations underscore the need for an effective refinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self refinement is a fundamental characteristic of human problem solving Propname, 0000; Propname and Propname, 0000; Propname, 0000. Iterative self refinement is a process that involves creating an initial draft and subsequently refining it based on self provided feedback.", SCONJ ADJ NOUN NOUN AUX VERB ADJ NOUN PUNCT PRON ADV VERB ADJ ADP VERB ADJ NOUN PUNCT PRON ADV VERB NOUN ADP ADJ NOUN PUNCT ADJ ADP NOUN NOUN NOUN PUNCT CCONJ NOUN ADP NOUN PART VERB NOUN PUNCT ADJ ADP VERB NOUN NOUN PUNCT ADP DET NOUN PUNCT ADJ NOUN AUX VERB DET ADJ ADJ NOUN PUNCT ADV AUX VERB ADP ADJ ADJ NOUN PUNCT ADV VERB DET NOUN NOUN ADP DET ADJ NOUN VERB SCONJ DET VERB NOUN AUX VERB PUNCT PROPN NOUN ADV VERB VERB DET ADJ NOUN PRON VERB ADP NOUN ADJ NOUN PUNCT PROPN PROPN PROPN PUNCT PUNCT PROPN PROPN PROPN PUNCT PUNCT ADJ NOUN PRON VERB ADP ADJ NOUN CCONJ NOUN NOUN VERB ADJ NOUN NOUN CCONJ ADJ ADJ NOUN PUNCT PRON AUX PART ADV AUX ADJ PART VERB PUNCT DET NOUN VERB DET NOUN ADP DET ADJ ADJ NOUN PRON AUX AUX VERB ADP ADJ NOUN ADP VERB ADJ NOUN PUNCT ADJ NOUN NOUN AUX DET ADJ NOUN ADP ADJ NOUN VERB PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PUNCT NUM PUNCT ADJ NOUN NOUN AUX DET NOUN PRON VERB VERB DET ADJ NOUN CCONJ ADV VERB PRON VERB ADP NOUN VERB NOUN PUNCT,0.624390243902439,25.625,5.4487804878048784,113,Aman Madaan,Aman Madaan,Timo Schick,Timo Schick,Aman Madaan,GPT-3.5
231,145,Zhiqing Sun,"[' Real-world knowledge bases are usually expressed as multi-relational graphs, which are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. However, real-word knowledge bases are usually incomplete (Dong et al., 2014), which motivates the research of automatically predicting missing links. A popular approach for Knowledge Graph Completion (KGC) is to embed entities and relations into continuous vector or matrix space, and use a well-designed score function f (h, r, t) to measure the plausibility of the triplet (h, r, t). Most of the previous methods use translation distance based (Bordes et al., 2013; Wang et al., 2014; Xiao et al., 2016; Sun et al., 2019) and semantic matching based (Nickel and Tresp, 2013; Yang et al., 2014; Nickel et al., 2016; Trouillon et al., 2016; scoring functions which are easy to analyze. However, recently, a vast number of neural network-based methods have been proposed. They have complex score functions which utilize black-box neural networks including Convolutional Neural Networks (CNNs) (Dettmers et al., 2018; Nguyen et al., 2018), Recurrent Neural Networks (RNNs) (Lin et al., 2015; Wang et al., 2018), Graph Neural Networks (GNNs) (Schlichtkrull et al., 2017; Shang et al., 2019), and Capsule Networks (Nguyen et al., 2019).', 'While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re-examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.']",intro_chunked,"While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re-examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.",46.60869165023013,169.0,9.0,282.0,0.3430982232093811," While some of them report state of the art performance on several benchmark datasets that are competitive to previous embedding based approaches, a considerable portion of recent neural network based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.", SCONJ PRON ADP PRON VERB NOUN ADP DET NOUN NOUN ADP ADJ ADJ NOUN PRON AUX ADJ ADP ADJ VERB VERB NOUN PUNCT DET ADJ NOUN ADP ADJ ADJ NOUN VERB NOUN VERB ADV ADJ NOUN NOUN PRON AUX PART ADJ ADP ADJ NOUN PUNCT ADV PUNCT ADJ ADP DET ADJ NOUN AUX PART ADV ADV VERB PUNCT DET DET NOUN AUX VERB ADJ CCONJ AUX VERB DET ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN CCONJ VERB SCONJ PRON AUX VERB ADP DET ADJ NOUN NOUN VERB ADP DET NOUN PUNCT PRON VERB SCONJ PRON NOUN NOUN VERB DET ADJ NOUN ADP DET NOUN PRON ADV VERB DET ADJ NOUN ADP DET NOUN PUNCT PRON AUX VERB ADP ADJ NOUN ADP NOUN ADP ADJ NOUN PUNCT ADP PRON PUNCT PRON VERB DET ADJ NOUN NOUN PRON VERB DET ADJ NOUN NOUN ADP DET NOUN ADP NOUN NOUN PUNCT PRON VERB ADJ NOUN PART AUX VERB DET ADJ NOUN CCONJ ADV VERB PRON ADP VERB NOUN PUNCT DET NOUN NOUN ADP DET NOUN AUX AUX ADV ADJ ADP PUNCT,0.6098901098901099,20.22222222222222,5.06043956043956,231,Zhiqing Sun,Hugo Touvron,Zhiqing Sun,Timo Schick,Zhiqing Sun,Hugo Touvron
73,73,Timo Schick,"[' Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.']",abstract_chunked," Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",50.70055555555555,162.0,7.0,254.0,0.7291654944419861," Language models exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Propname, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self supervised way, requiring nothing more than a handful of demonstrations for each Propname. We incorporate a range of tools, including a calculator, a QA system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", NOUN NOUN VERB ADJ NOUN PART VERB ADJ NOUN ADP ADV DET ADJ NOUN CCONJ ADJ NOUN PUNCT ADV ADP NOUN PUNCT PRON ADV PUNCT ADV PUNCT VERB ADP ADJ NOUN PUNCT ADJ ADP ADJ CCONJ ADJ NOUN PUNCT SCONJ ADV ADJ CCONJ ADJ NOUN VERB PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ NOUN AUX VERB PRON PART VERB ADJ NOUN ADP ADJ NOUN CCONJ VERB DET ADJ ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN VERB PART VERB DET NOUN PART VERB PUNCT SCONJ PART VERB PRON PUNCT PRON NOUN PART VERB PUNCT CCONJ SCONJ PART ADV VERB DET NOUN ADP ADJ ADJ NOUN PUNCT PRON AUX VERB ADP DET NOUN VERB NOUN PUNCT VERB PRON ADJ ADP DET NOUN ADP NOUN ADP DET PROPN PUNCT PRON VERB DET NOUN ADP NOUN PUNCT VERB DET NOUN PUNCT DET NOUN NOUN PUNCT DET NOUN NOUN PUNCT DET NOUN NOUN PUNCT CCONJ DET NOUN PUNCT NOUN VERB ADV VERB NUM NOUN NOUN ADP DET NOUN ADP ADJ NOUN PUNCT ADV ADJ ADP ADV ADJ NOUN PUNCT ADP VERB PRON NOUN NOUN VERB NOUN PUNCT,0.6720430107526881,26.571428571428573,4.682795698924731,73,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
55,55,Hugo Touvron,"[' Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top-1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date.']",abstract_chunked," Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top-1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date.",52.85094291907515,173.0,8.0,270.0,0.7947770357131958," Data augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 00.0 top 0 accuracy on Propname with a ResNet 00 trained on 000000 images, and 00.0 with one trained at 000000. A ResNeXt 000 00x00d pre trained with weak supervision on 000 million 000000 images and further optimized with our technique for test resolution 000000 achieves 00.0 top 0 accuracy. To the best of our knowledge this is the highest Propname single crop accuracy to date.", NOUN NOUN AUX ADJ ADP DET NOUN ADP ADJ NOUN ADP NOUN NOUN PUNCT DET NOUN ADV VERB SCONJ VERB NOUN VERB DET ADJ NOUN ADP DET NOUN ADP DET NOUN VERB ADP DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT ADP NOUN PUNCT DET ADJ NOUN NOUN VERB DET NOUN ADP NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN PART VERB DET ADJ NOUN PUNCT PRON VERB ADJ NOUN CCONJ NOUN NOUN PUNCT PRON VERB ADP DET ADV ADJ ADJ NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT PRON VERB VERB ADJ NOUN VERB ADJ NOUN NOUN PUNCT CCONJ ADV ADV VERB DET NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB NUM ADJ NUM NOUN ADP PROPN ADP DET NOUN NUM VERB ADP NUM NOUN PUNCT CCONJ NUM ADP NUM VERB ADP NUM PUNCT DET NOUN NUM X VERB VERB ADP ADJ NOUN ADP NUM NUM NUM NOUN CCONJ ADV VERB ADP PRON NOUN ADP NOUN NOUN NUM VERB NUM ADJ NUM NOUN PUNCT ADP DET ADJ ADP PRON NOUN PRON AUX DET ADJ PROPN ADJ NOUN NOUN ADP NOUN PUNCT,0.5815217391304348,23.0,4.880434782608695,55,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
34,34,Aman Madaan,"[' Defeasible reasoning is a mode of reasoning\nwhere conclusions can be overturned by taking into account new evidence. A commonly\nused method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference\ngraphs through transfer learning from a related\nNLP task that shares the kind of reasoning that\ninference graphs support. Through automated\nmetrics and human evaluation, we find that our\nmethod generates meaningful graphs for the\ndefeasible inference task. Human accuracy on\nthis task improves by 20% by consulting the\ngenerated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning.']",abstract_chunked," Defeasible reasoning is a mode of reasoning
where conclusions can be overturned by taking into account new evidence. A commonly
used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference
graphs through transfer learning from a related
NLP task that shares the kind of reasoning that
inference graphs support. Through automated
metrics and human evaluation, we find that our
method generates meaningful graphs for the
defeasible inference task. Human accuracy on
this task improves by 20% by consulting the
generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning.",42.19357142857143,126.0,7.0,218.0,0.6099236607551575," Defeasible reasoning is a mode of reasoning where conclusions can be overturned by taking into account new evidence. A commonly used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference graphs through transfer learning from a related Propname task that shares the kind of reasoning that inference graphs support. Through automated metrics and human evaluation, we find that our method generates meaningful graphs for the defeasible inference task. Human accuracy on this task improves by 00 by consulting the generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning.", ADJ NOUN AUX DET NOUN ADP NOUN SCONJ NOUN AUX AUX VERB ADP VERB ADP NOUN ADJ NOUN PUNCT DET ADV VERB NOUN ADP ADJ NOUN CCONJ NOUN NOUN AUX PART VERB NOUN VERB NOUN NOUN PUNCT SCONJ NOUN VERB NOUN NOUN ADV ADJ ADP NOUN PUNCT VERB PRON ADP NOUN AUX ADJ PUNCT ADP DET NOUN PUNCT PRON ADV VERB ADJ NOUN NOUN ADP NOUN VERB ADP DET ADJ PROPN NOUN PRON VERB DET NOUN ADP NOUN SCONJ NOUN NOUN NOUN PUNCT ADP VERB NOUN CCONJ ADJ NOUN PUNCT PRON VERB SCONJ PRON NOUN VERB ADJ NOUN ADP DET ADJ NOUN NOUN PUNCT ADJ NOUN ADP DET NOUN VERB ADP NUM ADP VERB DET VERB NOUN PUNCT PRON NOUN VERB ADP ADJ ADJ NOUN NOUN ADP NOUN SCONJ NOUN NOUN AUX VERB ADJ NOUN PUNCT,0.6691176470588235,19.428571428571427,5.338235294117647,34,Aman Madaan,GPT-3.5,Aman Madaan,Aman Madaan,Aman Madaan,GPT-3.5
222,136,Zhiqing Sun,"[' Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.', 'The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.', 'We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).']",intro_chunked,"The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.",64.28733333333335,200.0,12.0,297.0,0.30698326230049133," The motivation of introducing an additional knowledge recitation step comes from our observation that while few shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre training objective. This hinders LLMs from effectively reciting knowledge from their memory. Consider a student taking a closed book exam that contains knowledge intensive questions, for example, what is the tenth decimal of?. They typically can not directly answer this question because in studying stage, it is highly unlikely that they would read the tenth decimal of is 0. However, there can be some sentences like the first Propname digits of are 0.00000 00000... existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite and answer scheme: The first 00 digits of are 0.00000 00000. So the answer is 0. Here, the knowledge recitation step can serve as an intermediate step that mimics the language modeling pre training task, and thus better helps the Propname to generate factual knowledge.", DET NOUN ADP VERB DET ADJ NOUN NOUN NOUN VERB ADP PRON NOUN SCONJ SCONJ ADJ NOUN NOUN AUX VERB NOUN VERB ADJ NOUN NOUN PUNCT DET NOUN AUX ADV PART ADP DET ADJ NOUN ADP DET ADJ ADJ NOUN VERB ADJ NOUN NOUN PUNCT PRON VERB NOUN ADP ADV VERB NOUN ADP PRON NOUN PUNCT VERB DET NOUN VERB DET ADJ NOUN NOUN PRON VERB NOUN ADJ NOUN PUNCT ADP NOUN PUNCT PRON AUX DET ADJ ADJ ADP PUNCT PUNCT PRON ADV AUX PART ADV VERB DET NOUN SCONJ ADP VERB NOUN PUNCT PRON AUX ADV ADJ SCONJ PRON AUX VERB DET ADJ ADJ ADP AUX NUM PUNCT ADV PUNCT PRON AUX AUX DET NOUN ADP DET ADJ PROPN NOUN ADP AUX NUM NUM PUNCT VERB ADP DET NOUN PRON AUX AUX VERB ADP DET NOUN PUNCT ADV PUNCT DET NOUN AUX ADV VERB DET NOUN ADP DET NOUN CCONJ NOUN NOUN PUNCT DET ADJ NUM NOUN ADP AUX NUM NUM PUNCT ADV DET NOUN AUX NUM PUNCT ADV PUNCT DET NOUN NOUN NOUN AUX VERB ADP DET ADJ NOUN PRON VERB DET NOUN VERB ADJ NOUN NOUN PUNCT CCONJ ADV ADV VERB DET PROPN PART VERB ADJ NOUN PUNCT,0.582089552238806,25.125,4.701492537313433,222,Zhiqing Sun,Timo Schick,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Timo Schick
206,120,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked,"One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.",38.52852534562214,217.0,7.0,351.0,0.682161271572113," One of our key contributions is the adaptation of the Propname Propname from Propname Propname, a general and scalable alignment paradigm that shows great success for text based Propname agents, to the multimodal alignment for Propname. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for Propname fine tuning. This approach can improve the multimodal alignment with a relatively low annotation cost, Propname, collecting 00 K human preferences for image based conversations with 0000. To the best of our knowledge, this approach is the first successful adaptation of Propname to multimodal alignment. A potential issue with the current Propname Propname is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work proposed to iteratively collect fresh human feedback, which tends to be costly and can not effectively utilize existing human preference data. In this work, we propose a more data efficient alternative, ie, we try to make the reward model capable of leveraging existing human annotated data and knowledge in larger language models.", NUM ADP PRON ADJ NOUN AUX DET NOUN ADP DET PROPN PROPN ADP PROPN PROPN PUNCT DET ADJ CCONJ ADJ NOUN NOUN PRON VERB ADJ NOUN ADP NOUN VERB PROPN NOUN PUNCT ADP DET ADJ NOUN ADP PROPN PUNCT ADP VERB ADJ NOUN ADP DET NOUN ADP VERB NOUN PUNCT CCONJ VERB DET NOUN ADP NOUN NOUN ADP PROPN ADJ NOUN PUNCT DET NOUN AUX VERB DET ADJ NOUN ADP DET ADV ADJ NOUN NOUN PUNCT PROPN PUNCT VERB NUM NOUN ADJ NOUN ADP NOUN VERB NOUN ADP NUM PUNCT ADP DET ADJ ADP PRON NOUN PUNCT DET NOUN AUX DET ADJ ADJ NOUN ADP PROPN ADP ADJ NOUN PUNCT DET ADJ NOUN ADP DET ADJ PROPN PROPN AUX VERB NOUN NOUN PUNCT PRON VERB VERB ADJ NOUN ADP DET NOUN NOUN AUX PART ADV VERB ADP NOUN ADP ADJ NOUN PUNCT PART VERB NOUN NOUN PUNCT ADJ NOUN VERB PART ADV VERB ADJ ADJ NOUN PUNCT PRON VERB PART AUX ADJ CCONJ AUX PART ADV VERB VERB ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADJ NOUN PUNCT ADV PUNCT PRON VERB PART VERB DET NOUN NOUN ADJ ADP VERB VERB ADJ ADJ NOUN CCONJ NOUN ADP ADJ NOUN NOUN PUNCT,0.5507246376811594,29.571428571428573,5.169082125603865,206,Zhiqing Sun,Zhiqing Sun,Aman Madaan,Hugo Touvron,Zhiqing Sun,Zhiqing Sun
48,48,Hugo Touvron,"[' Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions\nof images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers by\ntraining on Imagenet only. We train them on a single computer in less than\n3 days. Our reference vision transformer (86M parameters) achieves top-1\naccuracy of 83.1% (single-crop) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet\n(where we obtain up to 85.2% accuracy) and when transferring to other\ntasks. We share our code and models.']",abstract_chunked," Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions
of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers by
training on Imagenet only. We train them on a single computer in less than
3 days. Our reference vision transformer (86M parameters) achieves top-1
accuracy of 83.1% (single-crop) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to
transformers. It relies on a distillation token ensuring that the student
learns from the teacher through attention. We show the interest of this
token-based distillation, especially when using a convnet as a teacher. This
leads us to report results competitive with convnets for both Imagenet
(where we obtain up to 85.2% accuracy) and when transferring to other
tasks. We share our code and models.",53.64875000000001,160.0,10.0,259.0,0.6875571608543396," Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution free transformers by training on Propname only. We train them on a single computer in less than 0 days. Our reference vision transformer achieves top 0 accuracy of 00.0 on Propname with no external data. More importantly, we introduce a teacher student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Propname and when transferring to other tasks. We share our code and models.", ADV PUNCT ADJ NOUN ADV VERB ADP NOUN AUX VERB PART VERB NOUN VERB NOUN ADJ ADP NOUN NOUN PUNCT DET NOUN NOUN NOUN AUX VERB VERB ADP NOUN ADP NOUN ADP NOUN VERB DET ADJ NOUN PUNCT ADV VERB PRON NOUN PUNCT ADP DET NOUN PUNCT PRON VERB ADJ NOUN ADJ NOUN ADP NOUN ADP PROPN ADV PUNCT PRON VERB PRON ADP DET ADJ NOUN ADP ADJ ADP NUM NOUN PUNCT PRON NOUN NOUN NOUN VERB ADJ NUM NOUN ADP NUM ADP PROPN ADP DET ADJ NOUN PUNCT ADV ADV PUNCT PRON VERB DET NOUN NOUN NOUN ADJ ADP NOUN PUNCT PRON VERB ADP DET NOUN NOUN VERB SCONJ DET NOUN VERB ADP DET NOUN ADP NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN VERB NOUN PUNCT ADV SCONJ VERB DET NOUN ADP DET NOUN PUNCT PRON VERB PRON PART VERB NOUN ADJ ADP NOUN ADP DET PROPN CCONJ SCONJ VERB ADP ADJ NOUN PUNCT PRON VERB PRON NOUN CCONJ NOUN PUNCT,0.6585365853658537,16.4,5.012195121951219,48,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
128,42,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked,"Prior work has highlighted the importance of
these two properties for set output through loss
functions that encourage order invariance (Ye et al.,
2021), exhaustive search over the label space
for finding an optimal order (Qin et al., 2019;
Rezatofighi et al., 2018; Vinyals et al., 2016), and
post-processing the output (Nag Chowdhury et al.,
2016). Despite the progress, several important gaps
remain. First, exhaustive search does not scale with
large output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is
still not explicitly modeled in the SEQ2SEQ setting
despite being an essential aspect for a set. Finally,
architectural modifications required for specialized
set-generation techniques might not be viable for
modern large-language models. We address these challenges with a novel data
augmentation strategy. Specifically, we take advantage of the auto-regressive factorization used
by SEQ2SEQ models and (i) impose an informative
order over the label space, and (ii) explicitly model
cardinality. First, the label sets are converted to
sequences using informative orders by grouping
labels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the
edges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.",45.350275784753364,223.0,10.0,366.0,0.31369251012802124," Prior work has highlighted the importance of these two properties for set output through loss functions that encourage order invariance Propname et Propname Propname, 0000, exhaustive search over the label space for finding an optimal order Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, and post processing the output Propname Propname Propname Propname Propname, 0000. Despite the progress, several important gaps remain. First, exhaustive search does not scale with large output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is still not explicitly modeled in the Propname setting despite being an essential aspect for a set. Finally, architectural modifications required for specialized set generation techniques might not be viable for modern large language models. We address these challenges with a novel data augmentation strategy. Specifically, we take advantage of the auto regressive factorization used by Propname models and impose an informative order over the label space, and explicitly model cardinality. First, the label sets are converted to sequences using informative orders by grouping labels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the edges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order invariance.", ADJ NOUN AUX VERB DET NOUN ADP DET NUM NOUN ADP ADJ NOUN ADP NOUN NOUN PRON VERB NOUN NOUN PROPN NOUN PROPN PROPN PUNCT NUM PUNCT ADJ NOUN ADP DET NOUN NOUN ADP VERB DET ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT CCONJ NOUN VERB DET NOUN PROPN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SCONJ DET NOUN PUNCT ADJ ADJ NOUN VERB PUNCT ADV PUNCT ADJ NOUN AUX PART VERB ADP ADJ NOUN NOUN ADV VERB ADP NOUN NOUN PUNCT ADV VERB DET NOUN ADP DET ADJ NOUN NOUN ADP DET NOUN PUNCT ADJ PUNCT NOUN AUX ADV PART ADV VERB ADP DET PROPN VERB SCONJ AUX DET ADJ NOUN ADP DET NOUN PUNCT ADV PUNCT ADJ NOUN VERB ADP ADJ ADJ NOUN NOUN AUX PART AUX ADJ ADP ADJ ADJ NOUN NOUN PUNCT PRON VERB DET NOUN ADP DET ADJ NOUN NOUN NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP DET NOUN ADJ NOUN VERB ADP PROPN NOUN CCONJ VERB DET ADJ NOUN ADP DET NOUN NOUN PUNCT CCONJ ADV NOUN NOUN PUNCT ADV PUNCT DET NOUN NOUN AUX VERB ADP NOUN VERB ADJ NOUN ADP VERB NOUN CCONJ VERB PRON NOUN NOUN PUNCT PRON NOUN VERB DET ADJ NOUN NOUN ADP NOUN NOUN SCONJ DET NOUN AUX DET NOUN PUNCT CCONJ DET NOUN VERB DET ADJ NOUN NOUN PUNCT DET NOUN VERB DET ADJ NOUN PART VERB ADJ NOUN SCONJ VERB NOUN NOUN PUNCT,0.531496062992126,25.4,5.307086614173229,128,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan
43,43,Hugo Touvron,"[' We introduce submodel co-training, a regularization\nmethod related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each\nsample we implicitly instantiate two altered networks, “submodels”, with stochastic depth: we activate only a subset\nof the layers. Each network serves as a soft teacher to the\nother, by providing a loss that complements the regular loss\nprovided by the one-hot label. Our approach, dubbed “cosub”, uses a single set of weights, and does not involve a\npre-trained external model or temporal averaging. Experimentally, we show that submodel co-training is\neffective to train backbones for recognition tasks such as\nimage classification and semantic segmentation. Our approach is compatible with multiple architectures, including\nRegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training\nstrategy improves their results in comparable settings. For\ninstance, a ViT-B pretrained with cosub on ImageNet-21k\nobtains 87.4% top-1 acc. @448 on ImageNet-val.']",abstract_chunked," We introduce submodel co-training, a regularization
method related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each
sample we implicitly instantiate two altered networks, “submodels”, with stochastic depth: we activate only a subset
of the layers. Each network serves as a soft teacher to the
other, by providing a loss that complements the regular loss
provided by the one-hot label. Our approach, dubbed “cosub”, uses a single set of weights, and does not involve a
pre-trained external model or temporal averaging. Experimentally, we show that submodel co-training is
effective to train backbones for recognition tasks such as
image classification and semantic segmentation. Our approach is compatible with multiple architectures, including
RegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training
strategy improves their results in comparable settings. For
instance, a ViT-B pretrained with cosub on ImageNet-21k
obtains 87.4% top-1 acc. @448 on ImageNet-val.",49.627992831541235,155.0,9.0,256.0,0.4993394911289215," We introduce submodel co training, a regularization method related to co training, self distillation and stochastic depth. Given a neural network to be trained, for each sample we implicitly instantiate two altered networks, submodels, with stochastic depth: we activate only a subset of the layers. Each network serves as a soft teacher to the other, by providing a loss that complements the regular loss provided by the one hot label. Our approach, dubbed cosub, uses a single set of weights, and does not involve a pre trained external model or temporal averaging. Experimentally, we show that Propname co training is effective to train backbones for recognition tasks such as image classification and semantic segmentation. Our approach is compatible with multiple architectures, including Propname, Propname, Propname, Propname, Propname and Propname. Our training strategy improves their results in comparable settings. For instance, a Propname Propname pretrained with cosub on Propname 00k obtains 00.0 top 0 acc. 000 on Propname val.", PRON VERB NOUN NOUN NOUN PUNCT DET NOUN NOUN VERB ADP NOUN NOUN PUNCT NOUN NOUN CCONJ NOUN NOUN PUNCT VERB DET ADJ NOUN PART AUX VERB PUNCT ADP DET NOUN PRON ADV VERB NUM ADJ NOUN PUNCT NOUN PUNCT ADP NOUN NOUN PUNCT PRON VERB ADV DET NOUN ADP DET NOUN PUNCT DET NOUN VERB ADP DET ADJ NOUN ADP DET ADJ PUNCT ADP VERB DET NOUN PRON VERB DET ADJ NOUN VERB ADP DET NUM ADJ NOUN PUNCT PRON NOUN PUNCT VERB NOUN PUNCT VERB DET ADJ NOUN ADP NOUN PUNCT CCONJ AUX PART VERB DET ADJ VERB ADJ NOUN CCONJ ADJ NOUN PUNCT ADV PUNCT PRON VERB SCONJ PROPN NOUN NOUN AUX ADJ PART VERB NOUN ADP NOUN NOUN ADJ ADP NOUN NOUN CCONJ ADJ NOUN PUNCT PRON NOUN AUX ADJ ADP ADJ NOUN PUNCT VERB PROPN PUNCT PROPN PUNCT PROPN PUNCT PROPN PUNCT PROPN CCONJ PROPN PUNCT PRON NOUN NOUN VERB PRON NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT DET PROPN PROPN VERB SCONJ NOUN ADP PROPN NOUN VERB NUM ADJ NUM NOUN PUNCT NUM ADP PROPN NOUN PUNCT,0.5978260869565217,20.444444444444443,4.809782608695652,43,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
223,137,Zhiqing Sun,"[' Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.', 'The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.', 'We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).']",intro_chunked,"We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).",39.34903883495146,206.0,5.0,306.0,0.667371392250061," We verify the effectiveness of our recitation augmented generation on few shot Propname Propname Propname Propname tasks, as illustrated in Figure 0. CBQA is an attractive open domain QA task in that a fully parameterized Propname can generate answers directly without an external corpus or separate retrieval models. We show that the proposed recite and answer scheme is an effective method for CBQA and compatible with other techniques for boosting few shot performance of Propname. We also show that, in addition to improving the few shot in context learning performance of Propname enhanced Propname, fine tuning the pre trained LLMs on synthetic generated question passage pairs can further improve the recitation performance and lead to a better downstream Propname accuracy. Experiments on four large language models, Propname, Propname, and Propname show that a recite and answer scheme can improve performance on various types of Propname tasks, including Propname basedsingle hop QA, trivia questions, and Propname based Propname hop Propname.", PRON VERB DET NOUN ADP PRON NOUN VERB NOUN ADP ADJ NOUN PROPN PROPN PROPN PROPN NOUN PUNCT SCONJ VERB ADP NOUN NUM PUNCT NOUN AUX DET ADJ ADJ NOUN NOUN NOUN SCONJ PRON DET ADV ADJ PROPN AUX VERB NOUN ADV ADP DET ADJ NOUN CCONJ ADJ NOUN NOUN PUNCT PRON VERB SCONJ DET VERB NOUN CCONJ NOUN NOUN AUX DET ADJ NOUN ADP NOUN CCONJ ADJ ADP ADJ NOUN ADP VERB ADJ NOUN NOUN ADP PROPN PUNCT PRON ADV VERB SCONJ PUNCT ADP NOUN ADP VERB DET ADJ NOUN ADP NOUN NOUN NOUN ADP PROPN VERB PROPN PUNCT ADJ VERB DET ADJ VERB NOUN ADP ADJ VERB NOUN NOUN NOUN AUX ADV VERB DET NOUN NOUN CCONJ VERB ADP DET ADJ ADJ PROPN NOUN PUNCT NOUN ADP NUM ADJ NOUN NOUN PUNCT PROPN PUNCT PROPN PUNCT CCONJ PROPN VERB SCONJ DET NOUN CCONJ NOUN NOUN AUX VERB NOUN ADP ADJ NOUN ADP PROPN NOUN PUNCT VERB PROPN VERB NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ PROPN VERB PROPN NOUN PROPN PUNCT,0.5433526011560693,34.6,5.173410404624278,223,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Aman Madaan,Zhiqing Sun,Zhiqing Sun
137,51,Aman Madaan,"[' Defeasible inference (Rudinger et al., 2020) is a\nmode of reasoning in which given a premise P\n(Rob went for a hike), a hypothesis H (Rob saw\nan elephant, it was pink) may be weakened or\noverturned in light of new evidence i.e., an update U (Rob often has hallucinations). Given the\nnon-monotonic nature of this reasoning, humans\nfind it challenging to master this task (Morgan,\n2004). This problem has been widely studied in\nclassical AI through logic (Israel, 1980; McCarthy,\n1981), and in cognitive science through argumentative models (Pollock, 1987). A prominent approach\nis to support defeasible inference through argumentations by constructing an inference graph (Pollock,\n2009). Despite their prominence (Bentahar et al., 2010),\nargumentative models are not scalable because an\ninference graph needs to be handcrafted for every\nexample. Recently, Rudinger et al.', '(2020) proposed\ntwo auxiliary tasks related to defeasible inference:\n(i) an NLI task to predict whether an update U\nwould weaken or strengthen a hypothesis H, and\n(ii) a generative task to generate an update U given\na premise P and a hypothesis H. However, this\nonly addresses a part of the problem because their\ninference is still not supported by the line of reasoning that a human typically uses to solve this\ntask, namely mediators (e.g., hallucinations can be\ndeceptive) and contextualizers (some elephants can\nhave mutated gene which makes them look different) that are inherently embedded in an inference\ngraph, limiting their utility for humans (figure 1).In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive\nscience and provide a computational model to make\ntheir generation scalable. Training such a model\nwould require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in NLP (Tandon et al., 2019),\nwhere the reasoning is supported by a graph that we\nfind has similarities with the kind of reasoning that\nan inference graph supports. We train a model that\ncan learn from the NLP task and effectively transfer\nit to generate inference graphs. Such transfer learning is made possible due to the powerful seq-to-seq\nneural language models that did not exist before. The contributions of this paper are the answers\nto the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In\n§2, we show that we can effectively construct\nmeaningful graphs using transfer learning. Can our generated graphs help improve human performance? In §3, we show that humans\nleverage generated graphs to improve their performance on a previously reported benchmark.']",intro_chunked,"(2020) proposed
two auxiliary tasks related to defeasible inference:
(i) an NLI task to predict whether an update U
would weaken or strengthen a hypothesis H, and
(ii) a generative task to generate an update U given
a premise P and a hypothesis H. However, this
only addresses a part of the problem because their
inference is still not supported by the line of reasoning that a human typically uses to solve this
task, namely mediators (e.g., hallucinations can be
deceptive) and contextualizers (some elephants can
have mutated gene which makes them look different) that are inherently embedded in an inference
graph, limiting their utility for humans (figure 1).In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive
science and provide a computational model to make
their generation scalable. Training such a model
would require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in NLP (Tandon et al., 2019),
where the reasoning is supported by a graph that we
find has similarities with the kind of reasoning that
an inference graph supports. We train a model that
can learn from the NLP task and effectively transfer
it to generate inference graphs. Such transfer learning is made possible due to the powerful seq-to-seq
neural language models that did not exist before. The contributions of this paper are the answers
to the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In
§2, we show that we can effectively construct
meaningful graphs using transfer learning. Can our generated graphs help improve human performance? In §3, we show that humans
leverage generated graphs to improve their performance on a previously reported benchmark.",46.069616336633686,303.0,12.0,484.0,0.323331356048584," proposed two auxiliary tasks related to defeasible inference: an Propname task to predict whether an update U would weaken or strengthen a hypothesis H, and a generative task to generate an update U given a premise P and a hypothesis Propname However, this only addresses a part of the problem because their inference is still not supported by the line of reasoning that a human typically uses to solve this task, namely mediators Propname, hallucinations can be deceptive and contextualizers some elephants can have mutated gene which makes them look different that are inherently embedded in an inference graph, limiting their utility for humans.In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive science and provide a computational model to make their generation scalable. Training such a model would require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in Propname, where the reasoning is supported by a graph that we find has similarities with the kind of reasoning that an inference graph supports. We train a model that can learn from the Propname task and effectively transfer it to generate inference graphs. Such transfer learning is made possible due to the powerful seq to Propname neural language models that did not exist before. The contributions of this paper are the answers to the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In 0, we show that we can effectively construct meaningful graphs using transfer learning. Can our generated graphs help improve human performance? In 0, we show that humans leverage generated graphs to improve their performance on a previously reported benchmark.", VERB NUM ADJ NOUN VERB ADP ADJ NOUN PUNCT DET PROPN NOUN PART VERB SCONJ DET NOUN PRON AUX VERB CCONJ VERB DET NOUN NOUN PUNCT CCONJ DET ADJ NOUN PART VERB DET NOUN NOUN VERB DET NOUN NOUN CCONJ DET NOUN PROPN ADV PUNCT PRON ADV VERB DET NOUN ADP DET NOUN SCONJ PRON NOUN AUX ADV PART VERB ADP DET NOUN ADP NOUN SCONJ DET NOUN ADV VERB PART VERB DET NOUN PUNCT ADV NOUN PROPN PUNCT NOUN AUX AUX ADJ CCONJ VERB DET NOUN AUX AUX VERB NOUN PRON VERB PRON VERB ADJ PRON AUX ADV VERB ADP DET NOUN NOUN PUNCT VERB PRON NOUN ADP NOUN PUNCT DET NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN NOUN ADP ADJ NOUN ADP ADJ NOUN CCONJ VERB DET ADJ NOUN PART VERB PRON NOUN ADJ PUNCT VERB DET DET NOUN AUX VERB DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT PRON AUX AUX ADV ADJ PART VERB PUNCT ADV PUNCT PRON NOUN AUX PART VERB DET NOUN ADP DET ADJ NOUN NOUN ADP PROPN PUNCT SCONJ DET NOUN AUX VERB ADP DET NOUN PRON PRON VERB VERB NOUN ADP DET NOUN ADP NOUN SCONJ DET NOUN NOUN NOUN PUNCT PRON VERB DET NOUN PRON AUX VERB ADP DET PROPN NOUN CCONJ ADV VERB PRON PART VERB NOUN NOUN PUNCT ADJ NOUN NOUN AUX VERB ADJ ADP ADP DET ADJ NOUN ADP PROPN ADJ NOUN NOUN PRON AUX PART VERB ADV PUNCT DET NOUN ADP DET NOUN AUX DET NOUN ADP DET VERB NUM NOUN NOUN PUNCT AUX PRON VERB DET NOUN ADP DET NOUN VERB NOUN NOUN PUNCT ADP NUM PUNCT PRON VERB SCONJ PRON AUX ADV VERB ADJ NOUN VERB NOUN NOUN PUNCT AUX PRON VERB NOUN VERB VERB ADJ NOUN PUNCT ADP NUM PUNCT PRON VERB SCONJ NOUN VERB VERB NOUN PART VERB PRON NOUN ADP DET ADV VERB NOUN PUNCT,0.5142857142857142,31.5,4.911111111111111,137,Aman Madaan,Timo Schick,Aman Madaan,Timo Schick,Aman Madaan,Timo Schick
267,181,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.', 'For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization.', 'We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.']",intro_chunked,"We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.",44.02898033126297,161.0,6.0,258.0,0.7613859176635742," We introduce GENPET, a novel method based on Propname, that enables finetuning of generative language models using both instructions and labeled examples. We show that Propname is a highly data efficient method that enables us to finetune a pretrained PEGASUS model with as little as 00 or 000 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero shot and few shot settings and show that Propname trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with Propname outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to Propname strong performance and quantify the impact of all its components.", PRON VERB NOUN PUNCT DET ADJ NOUN VERB ADP PROPN PUNCT PRON VERB VERB ADP ADJ NOUN NOUN VERB DET NOUN CCONJ VERB NOUN PUNCT PRON VERB SCONJ PROPN AUX DET ADV NOUN ADJ NOUN PRON VERB PRON PART VERB DET VERB ADJ NOUN ADP ADV ADJ ADP NUM CCONJ NUM NOUN NOUN PUNCT PRON VERB PRON NOUN ADP DET ADJ NOUN ADP NUM ADJ NOUN NOUN CCONJ NOUN NOUN NOUN CCONJ ADP NUM NOUN CCONJ ADJ NOUN NOUN CCONJ VERB SCONJ PROPN VERB ADP NOUN ADV VERB ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB NOUN PUNCT DET VERB NOUN ADP ADJ NOUN NOUN PRON VERB ADJ NOUN NOUN ADP VERB DET ADJ NOUN CCONJ NOUN NOUN PUNCT PRON VERB SCONJ NOUN VERB ADP PROPN NOUN NOUN VERB ADP DET ADJ NOUN ADP NOUN CCONJ NOUN VERB NOUN PUNCT PRON VERB DET NOUN VERB ADP PROPN ADJ NOUN CCONJ VERB DET NOUN ADP DET PRON NOUN PUNCT,0.5548780487804879,27.333333333333332,5.158536585365853,267,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Hugo Touvron
78,78,Timo Schick,"[' Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few-shot results for a wide range of text classification tasks when combined with gradient-based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Pattern-Exploiting Training (PET), a recently proposed few-shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of PET gives consistent improvements over a strong baseline in few-shot settings.']",abstract_chunked," Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few-shot results for a wide range of text classification tasks when combined with gradient-based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Pattern-Exploiting Training (PET), a recently proposed few-shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of PET gives consistent improvements over a strong baseline in few-shot settings.",32.473750000000024,96.0,3.0,161.0,0.5951589941978455," Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few shot results for a wide range of text classification tasks when combined with gradient based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Propname Propname Propname, a recently proposed few shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of Propname gives consistent improvements over a strong baseline in few shot settings.", VERB VERB NOUN NOUN ADP ADJ NOUN NOUN CCONJ NOUN ADP ADJ NOUN NOUN ADJ ADJ NOUN NOUN ADP DET ADJ NOUN ADP NOUN NOUN NOUN SCONJ VERB ADP NOUN VERB VERB ADP NOUN PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ DET ADJ NOUN AUX ADV AUX VERB ADP NOUN NOUN NOUN PUNCT PRON VERB PROPN PROPN PROPN PUNCT DET ADV VERB ADJ NOUN NOUN PUNCT ADP VERB ADJ NOUN NOUN ADP NOUN NOUN NOUN PUNCT ADP ADJ NOUN NOUN CCONJ NOUN NOUN NOUN PUNCT PRON VERB NOUN ADP PROPN VERB ADJ NOUN ADP DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT,0.7184466019417476,34.333333333333336,5.359223300970874,78,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Zhiqing Sun
135,49,Aman Madaan,"[' Defeasible inference is a mode of reasoning where\nadditional information can modify conclusions\n(Koons, 2017). Here we consider the specific formulation and challenge in Rudinger et al. (2020):\nGiven that some premise P plausibly implies a hypothesis H, does new information that the situation\nis S weaken or strengthen the conclusion H? For\nexample, consider the premise “The drinking glass\nfell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow”\nhere weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans\nwith an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan et al.,\n2021), which we use in this paper, draws connections between the P, H, and S through mediating events. This can be seen as a mental model of the\nquestion scenario before answering the question\n(Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario\nwith inference graphs help machines in defeasible\nreasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then,\nuse that graph as an additional input when answering the defeasible reasoning query. Our proposed\nsystem, CURIOUS, comprises a graph generation\nmodule and a graph encoding module to use the\ngenerated graph for the query (Figure 2).', 'To generate inference graphs, we build upon past\nwork that uses a sequence to sequence approach\n(Madaan et al., 2021). However, our analysis revealed that the graphs can often be erroneous, and\nCURIOUS also includes an error correction module\nto generate higher quality inference graphs. This\nwas important because we found that better graphs\nare more helpful in the downstream QA task. The generated inference graph is then used for\nthe QA task on three existing defeasible inference\ndatasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015),\nδ-SOCIAL (reasoning about social norms) (Forbes\net al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way\nthe graph is encoded for input is important. If we\nsimply augment the question with the generated\ngraphs, there are some gains on all datasets. However, the accuracy improves substantially across\nall datasets with a more judicious encoding of the\ngraph-augmented question that accounts for interactions between the graph nodes. To achieve this,\nwe use the mixture of experts approach (Jacobs\net al., 1991) to include a mixture of experts layers\nduring encoding, enabling the ability to attend to\nspecific nodes while capturing their interactions\nselectively. In summary, our contributiion is in drawing on\nthe idea of an inference graph from cognitive science to show benefits in a defeasible inference QA\ntask. Using an error correction module in the graph\ngeneration process, and a judicious encoding of the\ngraph augmented question, CURIOUS achieves a\nnew state-of-the-art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to “think about""\na question before answering can improve performance.']",intro_chunked,"To generate inference graphs, we build upon past
work that uses a sequence to sequence approach
(Madaan et al., 2021). However, our analysis revealed that the graphs can often be erroneous, and
CURIOUS also includes an error correction module
to generate higher quality inference graphs. This
was important because we found that better graphs
are more helpful in the downstream QA task. The generated inference graph is then used for
the QA task on three existing defeasible inference
datasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015),
δ-SOCIAL (reasoning about social norms) (Forbes
et al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way
the graph is encoded for input is important. If we
simply augment the question with the generated
graphs, there are some gains on all datasets. However, the accuracy improves substantially across
all datasets with a more judicious encoding of the
graph-augmented question that accounts for interactions between the graph nodes. To achieve this,
we use the mixture of experts approach (Jacobs
et al., 1991) to include a mixture of experts layers
during encoding, enabling the ability to attend to
specific nodes while capturing their interactions
selectively. In summary, our contributiion is in drawing on
the idea of an inference graph from cognitive science to show benefits in a defeasible inference QA
task. Using an error correction module in the graph
generation process, and a judicious encoding of the
graph augmented question, CURIOUS achieves a
new state-of-the-art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to “think about""
a question before answering can improve performance.",48.59036941856732,283.0,11.0,442.0,0.6046884059906006," To generate inference graphs, we build upon past work that uses a sequence to sequence approach. However, our analysis revealed that the graphs can often be erroneous, and Propname also includes an error correction module to generate higher quality inference graphs. This was important because we found that better graphs are more helpful in the downstream Propname task. The generated inference graph is then used for the Propname task on three existing defeasible inference datasets from diverse domains, Propname Propname, Propname, Propname Propname Propname Propname Propname, 0000, and Propname. We show that the way the graph is encoded for input is important. If we simply augment the question with the generated graphs, there are some gains on all datasets. However, the accuracy improves substantially across all datasets with a more judicious encoding of the graph augmented question that accounts for interactions between the graph Propname. To achieve this, we use the mixture of experts approach Propname Propname Propname Propname, 0000 to include a mixture of experts layers during encoding, enabling the ability to attend to specific nodes while capturing their interactions selectively. In summary, our contributiion is in drawing on the idea of an inference graph from cognitive science to show benefits in a defeasible inference QA task. Using an error correction module in the graph generation process, and a judicious encoding of the graph augmented question, Propname achieves a new state of the art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to think about a question before answering can improve performance.", PART VERB NOUN NOUN PUNCT PRON VERB SCONJ ADJ NOUN PRON VERB DET NOUN PART NOUN NOUN PUNCT ADV PUNCT PRON NOUN VERB SCONJ DET NOUN AUX ADV AUX ADJ PUNCT CCONJ PROPN ADV VERB DET NOUN NOUN NOUN PART VERB ADJ NOUN NOUN NOUN PUNCT PRON AUX ADJ SCONJ PRON VERB SCONJ ADJ NOUN AUX ADV ADJ ADP DET ADJ PROPN NOUN PUNCT DET VERB NOUN NOUN AUX ADV VERB ADP DET PROPN NOUN ADP NUM VERB ADJ NOUN NOUN ADP ADJ NOUN PUNCT PROPN PROPN PUNCT PROPN PUNCT PROPN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT CCONJ PROPN PUNCT PRON VERB SCONJ DET NOUN DET NOUN AUX VERB ADP NOUN AUX ADJ PUNCT SCONJ PRON ADV VERB DET NOUN ADP DET VERB NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN PUNCT ADV PUNCT DET NOUN VERB ADV ADP DET NOUN ADP DET ADV ADJ NOUN ADP DET NOUN VERB NOUN PRON VERB ADP NOUN ADP DET NOUN PROPN PUNCT PART VERB PRON PUNCT PRON VERB DET NOUN ADP NOUN VERB PROPN PROPN PROPN PROPN PUNCT NUM PART VERB DET NOUN ADP NOUN NOUN ADP NOUN PUNCT VERB DET NOUN PART VERB ADP ADJ NOUN SCONJ VERB PRON NOUN ADV PUNCT ADP NOUN PUNCT PRON NOUN AUX ADP VERB ADP DET NOUN ADP DET NOUN NOUN ADP ADJ NOUN PART VERB NOUN ADP DET ADJ NOUN NOUN NOUN PUNCT VERB DET NOUN NOUN NOUN ADP DET NOUN NOUN NOUN PUNCT CCONJ DET ADJ NOUN ADP DET NOUN VERB NOUN PUNCT PROPN VERB DET ADJ NOUN ADP DET NOUN ADP NUM ADJ NOUN PUNCT DET NOUN AUX ADJ ADV SCONJ PRON NOUN VERB SCONJ VERB DET NOUN PART VERB ADP DET NOUN ADP VERB AUX VERB NOUN PUNCT,0.46366782006920415,26.272727272727273,4.996539792387543,135,Aman Madaan,Aman Madaan,Aman Madaan,Zhiqing Sun,Aman Madaan,Aman Madaan
35,35,Aman Madaan,"[' A class of explainable NLP models for reasoning tasks support their decisions by generating free-form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce MERCURIE- an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains.']",abstract_chunked," A class of explainable NLP models for reasoning tasks support their decisions by generating free-form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce MERCURIE- an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains.",37.64433333333335,108.0,5.0,188.0,0.3503601849079132," A class of explainable NLP models for reasoning tasks support their decisions by generating free form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce Propname an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 00 fewer inconsistencies as compared with the off the shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 0.0 points on accuracy on defeasible reasoning across all three domains.", DET NOUN ADP ADJ NOUN NOUN ADP NOUN NOUN VERB PRON NOUN ADP VERB ADJ NOUN CCONJ ADJ NOUN PUNCT CCONJ PRON VERB SCONJ DET VERB NOUN VERB NOUN PUNCT PRON NOUN AUX PART VERB NOUN PART ADV VERB NOUN NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB PROPN DET ADJ NOUN PRON VERB PRON NOUN ADP DET VERB NOUN NOUN ADP VERB ADJ NOUN ADP ADJ NOUN PUNCT PRON NOUN VERB NOUN PRON VERB NUM ADJ NOUN SCONJ VERB ADP DET ADP DET NOUN NOUN PUNCT ADV PUNCT ADV VERB DET VERB NOUN NOUN ADP DET NOUN VERB ADP DET NOUN ADP NUM NOUN ADP NOUN ADP ADJ NOUN ADP DET NUM NOUN PUNCT,0.7652173913043478,23.0,5.417391304347826,35,Aman Madaan,GPT-3.5,Aman Madaan,Timo Schick,Aman Madaan,GPT-3.5
187,101,Hugo Touvron,"[' Image classification now achieves a performance\nthat meets many application needs [27, 37, 54]. In\npractice however, the dataset and labels available at\ntraining time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for\nfine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to\nfind enough images of rare classes, and annotating\nthem precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states\nthat: “Manually labeling a large number of images with\nthe presence or absence of 19,794 different classes is not\nfeasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases\ndue to the assisting algorithm. Being able to get strong\nclassification and image retrieval performance on fine\nconcepts using only coarse labels at training time can\ncircumvents the issue, liberating the data collection\nprocess from the quirks of a rigid fine-grained taxonomy.', 'In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:\nGiven a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their\nfine-grained semantic similarity to a new query image\noutside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we\nuse a non-parametric kNN classifier [61] for on-the-fly\nclassification, i.e. without training on the fine-grained\nlabels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build\nupon recent works [3, 62] that exploits two losses to\naddress both image classification and instance recognition, leveraging the “free” annotations provided by\nmultiple data augmentations of a same instance, in the\nspirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly\ninfer coarse labels even when classifying for a finer\ngranularity. For this purpose, we propose a simple\nmethod that exploits both a coarse classifier and image\nembeddings to improve fine-grained category-level\nretrieval.', 'This strategy outperforms existing works\nthat exploit coarse labels at training time but do not\nexplicitly rely on them when retrieving finer-grained\nconcepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following\ncontributions:\n• We propose a method that learns a representation\nat a finer granularity than the one offered by the\nannotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is\nstill +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the\ncoarse level. A byproduct of our study is a very\nstrong kNN-classifier on Imagenet: Grafit with\nResNet-50 trunk reaches 79.6% top-1 accuracy at\nresolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates\nbetter at a finer granularity. Everything being\nequal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the\nart on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],\nFood101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method\nin Section 3. Section 4 compares our approach against\nbaselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one\nlearned by a vanilla cross-entropy loss. Appendix B\ncomplements our experimental section 4 with more\ndetailed results. Appendix C provides visual results\nassociated with different levels of training/testing\ngranularities.']",intro_chunked," Image classification now achieves a performance
that meets many application needs [27, 37, 54]. In
practice however, the dataset and labels available at
training time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for
fine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to
find enough images of rare classes, and annotating
them precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states
that: “Manually labeling a large number of images with
the presence or absence of 19,794 different classes is not
feasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases
due to the assisting algorithm. Being able to get strong
classification and image retrieval performance on fine
concepts using only coarse labels at training time can
circumvents the issue, liberating the data collection
process from the quirks of a rigid fine-grained taxonomy.",40.20799145299148,208.0,9.0,352.0,0.4129214286804199," Image classification now achieves a performance that meets many application needs. In practice however, the dataset and labels available at training time do not necessarily correspond to those needed in subsequent applications. The granularity of the training time concepts may not suffice for fine grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine grained classification datasets have been developed for specific domains, for instance to distinguish different plants or bird species. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to find enough images of rare classes, and annotating them precisely requires domain specialists with indomain expertise. This is evidenced by the Propname Propname construction annotation protocol that states that: Manually labeling a large number of images with the presence or absence of 00,000 different classes is not feasible. For this reason they resorted to computerassisted annotation, at the risk of introducing biases due to the assisting Propname. Being able to get strong classification and image retrieval performance on fine concepts using only coarse labels at training time can circumvents the issue, liberating the data collection process from the quirks of a rigid fine grained taxonomy.", NOUN NOUN ADV VERB DET NOUN PRON VERB ADJ NOUN NOUN PUNCT ADP NOUN ADV PUNCT DET NOUN CCONJ NOUN ADJ ADP NOUN NOUN AUX PART ADV VERB ADP PRON VERB ADP ADJ NOUN PUNCT DET NOUN ADP DET NOUN NOUN NOUN AUX PART VERB ADP ADJ VERB ADJ NOUN PUNCT PRON AUX VERB DET NOUN ADP ADJ NOUN VERB DET ADV ADJ NOUN PUNCT ADJ VERB NOUN NOUN AUX AUX VERB ADP ADJ NOUN PUNCT SCONJ NOUN PART VERB ADJ NOUN CCONJ NOUN NOUN PUNCT VERB DET ADV ADJ NOUN ADP VERB NOUN AUX ADJ ADP PRON PUNCT SCONJ PRON VERB PART VERB ADJ NOUN ADP ADJ NOUN PUNCT CCONJ VERB PRON ADV VERB NOUN NOUN ADP ADJ NOUN PUNCT PRON AUX VERB ADP DET PROPN PROPN NOUN NOUN NOUN PRON VERB SCONJ PUNCT ADV VERB DET ADJ NOUN ADP NOUN ADP DET NOUN CCONJ NOUN ADP NUM ADJ NOUN AUX PART ADJ PUNCT ADP DET NOUN PRON VERB ADP VERB NOUN PUNCT ADP DET NOUN ADP VERB NOUN ADP ADP DET NOUN PROPN PUNCT AUX ADJ PART VERB ADJ NOUN CCONJ NOUN NOUN NOUN ADP ADJ NOUN VERB ADV ADJ NOUN ADP NOUN NOUN AUX VERB DET NOUN PUNCT VERB DET NOUN NOUN NOUN ADP DET NOUN ADP DET ADJ ADJ ADJ NOUN PUNCT,0.6435185185185185,24.0,5.467592592592593,187,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,GPT-3.5
163,77,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider
a single target model to be trained, and we instantiate two
submodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can
backpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel
serves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the
submodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss
compared to the label loss, and our experiments show that
it significantly increases the final model accuracy. This co-training across different submodels, which we
refer to as cosub, can be regarded as a massive co-training
between 2
L models that share a common set of parameters,
where L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all
models. With a layer drop-rate set to 0.5, for instance for
a ViT-H model, all submodels are equiprobable, and then it
amounts to averaging the weights of 2
2×32 models.",50.00513719512196,205.0,8.0,317.0,0.4942145049571991," In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider a single target model to be trained, and we instantiate two submodels on the fly, simply by layerwise dropout. This gives us two neural networks through which we can backpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel serves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the submodels. Our approach is illustrated in Figure 0: the parameter controls the importance of the co training loss compared to the label loss, and our experiments show that it significantly increases the final model accuracy. This co training across different submodels, which we refer to as cosub, can be regarded as a massive co training between 0 L models that share a common set of parameters, where Propname is the number of layers in the target architecture. The target model can be interpreted as the expectation of all models. With a layer drop rate set to 0.0, for instance for a ViT Propname model, all submodels are equiprobable, and then it amounts to averaging the weights of 0 000 models.", ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN PART VERB VERB ADP DET ADV ADJ NOUN ADP NOUN PUNCT PRON VERB DET ADJ NOUN NOUN PART AUX VERB PUNCT CCONJ PRON VERB NUM NOUN ADP DET NOUN PUNCT ADV ADP NOUN NOUN PUNCT PRON VERB PRON NUM ADJ NOUN ADP PRON PRON AUX VERB ADP DET VERB NOUN ADP DET NOUN NOUN PUNCT ADP NOUN ADP DET ADJ NOUN NOUN PUNCT DET NOUN VERB ADP DET NOUN ADP DET ADJ PUNCT PRON VERB DET ADJ NOUN NOUN VERB DET NOUN ADP DET NOUN PUNCT PRON NOUN AUX VERB ADP NOUN NUM PUNCT DET NOUN VERB DET NOUN ADP DET NOUN NOUN NOUN VERB ADP DET NOUN NOUN PUNCT CCONJ PRON NOUN VERB SCONJ PRON ADV VERB DET ADJ NOUN NOUN PUNCT DET NOUN VERB ADP ADJ NOUN PUNCT PRON PRON VERB ADP ADP NOUN PUNCT AUX AUX VERB ADP DET ADJ NOUN NOUN ADP NUM NOUN NOUN PRON VERB DET ADJ NOUN ADP NOUN PUNCT SCONJ PROPN AUX DET NOUN ADP NOUN ADP DET NOUN NOUN PUNCT DET NOUN NOUN AUX AUX VERB ADP DET NOUN ADP DET NOUN PUNCT ADP DET NOUN NOUN NOUN VERB ADP NUM PUNCT ADP NOUN ADP DET NOUN PROPN NOUN PUNCT DET NOUN AUX ADJ PUNCT CCONJ ADV PRON VERB ADP VERB DET NOUN ADP NUM NUM NOUN PUNCT,0.5398230088495575,28.25,4.464601769911504,163,Hugo Touvron,Aman Madaan,Hugo Touvron,Hugo Touvron,Hugo Touvron,Timo Schick
155,69,Aman Madaan,"[' While there is a long history of relation extraction systems\nin the NLP literature (e.g., (ARPA 1991; Soderland 1999;\nHoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which\nthe arguments are non-numerical. These include real world\nentities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression\ntypes such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,\ni.e., relations involving general numeric arguments such as\npopulation, area, atomic number, inflation rate, or boiling\npoint. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information\nfrom text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present\nseveral peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,\nin which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that\nsentence.', 'The signal from distant supervision becomes much\nweaker for numerical relations since there can be a much\nlarger number of reasons why a certain number is present\nin the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),\na state-of-the-art IE system, obtained an F-score of under\n20, hardly acceptable for real tasks. Secondly, numbers have\nunits and their semantics is important. Thirdly, numbers may\nbe written at different rounding levels necessitating partial\nmatching techniques. Lastly, numerical relations allow for\nsentences which describe the change in the argument value\nfrom the last measurement, instead of the argument value\nitself. In response, we develop two numerical relation extractors\nthat incorporate these observations . Both extractors expect\nminimal human supervision in the form of the unit of the\nrelation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that\nlooks for occurrences of specific numerical relation based\npatterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords\nto learn new keywords and patterns and can also leverage\nany existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries.', 'We\ncompile a knowledge-base using geopolitical data from\nWorld Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a\nslightly higher precision as compared to NumberRule. Both\nsystems massively outperform MultiR model (and its simple\nextensions) obtaining 17–25 point F-score improvements. We release our code1\nand other resources for further research. Overall, we make the following contributions in this\npaper:\n• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this\ntask compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract\na numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns\nwhile also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher\nrecall and F-score than NumberRule, and both systems\noutperform the MultiR model as well as a recall oriented\nbaseline by wide margins.']",intro_chunked,"We
compile a knowledge-base using geopolitical data from
World Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a
slightly higher precision as compared to NumberRule. Both
systems massively outperform MultiR model (and its simple
extensions) obtaining 17–25 point F-score improvements. We release our code1
and other resources for further research. Overall, we make the following contributions in this
paper:
• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this
task compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract
a numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns
while also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher
recall and F-score than NumberRule, and both systems
outperform the MultiR model as well as a recall oriented
baseline by wide margins.",53.26668041237116,194.0,10.0,307.0,0.48828360438346863," We compile a knowledge base using geopolitical data from Propname Propname and learn extractors for ten Propname relations. We find that Propname obtains a much higher recall at a slightly higher precision as compared to Propname. Both systems massively outperform MultiR model and its simple extensions obtaining 0000 point F score improvements. We release our code0 and other resources for further research. Overall, we make the following contributions in this paper: We define and analyze the task of Propname relation extraction. Our analysis highlights stark differences in this task compared to standard Propname. We design Propname, a rule based system that looks for pre defined patterns with specific keywords to extract a numerical relation. We design Propname, an extension of Propname for Propname relation extraction that can learn new patterns while also exploiting other features specific to our task. We compile a knowledge base and a test set of 000 sentences for this task from the geopolitical domain. Our experiments reveal that Propname obtains much higher recall and Propname score than Propname, and both systems outperform the Propname model as well as a recall oriented baseline by wide margins.", PRON VERB DET NOUN NOUN VERB ADJ NOUN ADP PROPN PROPN CCONJ VERB NOUN ADP NUM PROPN NOUN PUNCT PRON VERB SCONJ PROPN VERB DET ADV ADJ NOUN ADP DET ADV ADJ NOUN SCONJ VERB ADP PROPN PUNCT DET NOUN ADV VERB ADJ NOUN CCONJ PRON ADJ NOUN VERB NUM NOUN NOUN NOUN NOUN PUNCT PRON VERB PRON ADJ CCONJ ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB DET VERB NOUN ADP DET NOUN PUNCT PRON VERB CCONJ VERB DET NOUN ADP PROPN NOUN NOUN PUNCT PRON NOUN VERB ADJ NOUN ADP DET NOUN VERB ADP ADJ PROPN PUNCT PRON VERB PROPN PUNCT DET NOUN VERB NOUN PRON VERB ADP ADJ VERB NOUN ADP ADJ NOUN PART VERB DET ADJ NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN ADP PROPN ADP PROPN NOUN NOUN PRON AUX VERB ADJ NOUN SCONJ ADV VERB ADJ NOUN ADJ ADP PRON NOUN PUNCT PRON VERB DET NOUN NOUN CCONJ DET NOUN NOUN ADP NUM NOUN ADP DET NOUN ADP DET ADJ NOUN PUNCT PRON NOUN VERB SCONJ PROPN VERB ADV ADJ NOUN CCONJ PROPN NOUN ADP PROPN PUNCT CCONJ DET NOUN VERB DET PROPN NOUN ADV ADV ADP DET NOUN VERB NOUN ADP ADJ NOUN PUNCT,0.5392156862745098,20.4,5.0588235294117645,155,Aman Madaan,Hugo Touvron,Hugo Touvron,Aman Madaan,Aman Madaan,Hugo Touvron
66,66,Zhiqing Sun,"[' Autoregressive (AR) models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non-autoregressive (NAR) models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi-modality in sequence generation. This paper proposes a new approach that jointly optimizes both AR and NAR models in a unified Expectation-Maximization (EM) framework. In the E-step, an AR model learns to approximate the regularized posterior of the NAR model. In the M-step, the NAR model is updated on the new posterior and selects the training examples for the next AR model. This iterative process can effectively guide the system to remove the multi-modality in the output sequences. To our knowledge, this is the first EM approach to NAR sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.']",abstract_chunked," Autoregressive (AR) models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non-autoregressive (NAR) models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi-modality in sequence generation. This paper proposes a new approach that jointly optimizes both AR and NAR models in a unified Expectation-Maximization (EM) framework. In the E-step, an AR model learns to approximate the regularized posterior of the NAR model. In the M-step, the NAR model is updated on the new posterior and selects the training examples for the next AR model. This iterative process can effectively guide the system to remove the multi-modality in the output sequences. To our knowledge, this is the first EM approach to NAR sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.",44.1014719358534,194.0,9.0,323.0,0.4947231113910675," Autoregressive models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non autoregressive models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi modality in sequence generation. This paper proposes a new approach that jointly optimizes both Propname and Propname models in a unified Propname Propname framework. In the E step, an Propname model learns to approximate the regularized posterior of the Propname model. In the Propname step, the Propname model is updated on the new posterior and selects the training examples for the next Propname model. This iterative process can effectively guide the system to remove the multi modality in the output sequences. To our knowledge, this is the first Propname approach to Propname sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.", ADJ NOUN AUX AUX DET VERB NOUN ADP ADJ NOUN NOUN PUNCT CCONJ AUX VERB ADP DET NOUN ADP ADJ NOUN NOUN PUNCT ADJ ADJ NOUN AUX AUX ADV VERB PART VERB DET NOUN ADP VERB DET NOUN NOUN ADP NOUN CCONJ AUX ADV VERB ADJ NOUN VERB ADP PRON ADJ NOUN PUNCT ADV ADJ ADP DET NOUN ADP VERB ADP DET ADJ NOUN ADP NOUN NOUN PUNCT DET NOUN VERB DET ADJ NOUN PRON ADV VERB CCONJ PROPN CCONJ PROPN NOUN ADP DET ADJ PROPN PROPN NOUN PUNCT ADP DET NOUN NOUN PUNCT DET PROPN NOUN VERB PART VERB DET VERB NOUN ADP DET PROPN NOUN PUNCT ADP DET PROPN NOUN PUNCT DET PROPN NOUN AUX VERB ADP DET ADJ NOUN CCONJ VERB DET NOUN NOUN ADP DET ADJ PROPN NOUN PUNCT DET ADJ NOUN AUX ADV VERB DET NOUN PART VERB DET ADJ NOUN ADP DET NOUN NOUN PUNCT ADP PRON NOUN PUNCT PRON AUX DET ADJ PROPN NOUN ADP PROPN NOUN NOUN PUNCT PRON VERB PRON NOUN ADP DET NOUN ADP NOUN NOUN PUNCT ADJ NOUN ADP ADJ NOUN NOUN VERB SCONJ DET VERB NOUN VERB ADJ PUNCT SCONJ PART ADJ PUNCT NOUN ADP VERB NOUN NOUN CCONJ ADV VERB DET NOUN NOUN PUNCT,0.5507246376811594,23.0,5.251207729468599,66,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun
214,128,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked," Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).",38.48600000000002,200.0,8.0,338.0,0.4463213384151459," Propname Propname problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the Propname Propname class of problems, which are believed to be intractable in polynomial time. Traditionally, Propname solvers rely on integer programming or hand crafted heuristics, which demand signif icant expert efforts to approximate near optimal solutions. Recent development in deep learning has shown new promise in solving Propname problems. Existing neural Propname solvers for Propname problems can be roughly classified into three categories based on how the solutions are generated, ie, the autoregressive constructive solvers, the non autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution. Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems. Methods in the second category rely on non autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical.", PROPN PROPN NOUN AUX ADJ NOUN PRON VERB VERB DET ADJ NOUN ADP DET ADJ NOUN PUNCT PRON AUX ADJ NOUN ADP NOUN NOUN PUNCT ADV DET PROPN PROPN NOUN ADP NOUN PUNCT PRON AUX VERB PART AUX ADJ ADP ADJ NOUN PUNCT ADV PUNCT PROPN NOUN VERB ADP NOUN NOUN CCONJ NOUN VERB NOUN PUNCT PRON VERB VERB ADJ NOUN NOUN PART VERB ADP ADJ NOUN PUNCT ADJ NOUN ADP ADJ NOUN AUX VERB ADJ NOUN ADP VERB PROPN NOUN PUNCT VERB ADJ PROPN NOUN ADP PROPN NOUN AUX AUX ADV VERB ADP NUM NOUN VERB ADP SCONJ DET NOUN AUX VERB PUNCT ADV PUNCT DET ADJ ADJ NOUN PUNCT DET ADJ ADJ ADJ NOUN PUNCT CCONJ DET NOUN NOUN NOUN PUNCT NOUN ADP DET ADJ NOUN VERB ADJ NOUN PART ADV VERB DET ADJ ADJ NOUN PUNCT DET NOUN ADV VERB ADP DET ADJ NOUN ADP PRON ADJ VERB NOUN CCONJ ADV AUX ADJ PART VERB ADP ADP ADJ NOUN PUNCT NOUN ADP DET ADJ NOUN VERB ADP ADJ ADJ NOUN ADP VERB ADP PUNCT ADP DET ADJ NOUN NOUN ADP NOUN ADP ADJ PUNCT,0.5989304812834224,23.375,5.625668449197861,214,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,GPT-3.5
274,188,Timo Schick,"[' As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations\nhave been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.', 'It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.', 'Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.']",intro_chunked,"It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.",43.83207085828346,167.0,6.0,266.0,0.3422289788722992," It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag of words models is a reasonable choice for static embeddings, which are often themselves bag of words, it stands to reason that they are not the best choice to generate input representations for position aware, deep language models. To overcome these limitations, we introduce Propname, a novel architecture for learning rare word representations that combines a pretrained Propname model with Propname. As shown in Figure 0, the learned rare word representations can then be used as an improved input representation for another Propname model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.", PRON VERB NOUN CCONJ NOUN ADP DET ADJ NOUN PUNCT VERB DET NOUN NOUN ADP VERB ADP DET ADJ NOUN PUNCT DET NOUN VERB PART ADV PART VERB PUNCT CCONJ ADP DET ADJ NOUN ADP VERB NOUN ADP ADJ NOUN ADP VERB NOUN CCONJ NOUN PUNCT SCONJ VERB NOUN ADP NOUN NOUN AUX DET ADJ NOUN ADP ADJ NOUN PUNCT PRON AUX ADV PRON NOUN ADP NOUN PUNCT PRON VERB ADP NOUN SCONJ PRON AUX PART DET ADJ NOUN PART VERB NOUN NOUN ADP NOUN ADJ PUNCT ADJ NOUN NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN ADP VERB ADJ NOUN NOUN PRON VERB DET VERB PROPN NOUN ADP PROPN PUNCT SCONJ VERB ADP NOUN NUM PUNCT DET VERB ADJ NOUN NOUN AUX ADV AUX VERB ADP DET VERB NOUN NOUN ADP DET PROPN NOUN PUNCT ADP VERB ADJ NOUN ADP DET NOUN NOUN CCONJ NOUN VERB ADP DET ADJ NOUN PUNCT DET ADJ NOUN ADP DET NOUN NOUN VERB ADJ PUNCT,0.6190476190476191,28.0,4.857142857142857,274,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
179,93,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked,"We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements
included in the timm library [55]. With our Data-efficient image Transformers
(DeiT), we report large improvements over previous results, see Figure 1. Our
ablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce
a token-based strategy, specific to transformers and denoted by DeiT, and
show that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:
• We show that our neural networks that contains no convolutional layer
can achieve competitive results against the state of the art on ImageNet
with no external data. They are learned on a single node with 4 GPUs in
three days1
. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,
which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the
transformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.",53.98827615780448,212.0,11.0,334.0,0.3766777515411377," We build upon the visual transformer architecture from Propname Propname Propname. and improvements included in the timm library. With our Propname efficient image Transformers, we report large improvements over previous results, see Propname 0. Our ablation study details the hyper parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce a token based strategy, specific to transformers and denoted by Propname, and show that it advantageously replaces the usual distillation. In summary, our work makes the following contributions: We show that our neural networks that contains no convolutional layer can achieve competitive results against the state of the art on Propname with no external data. They are learned on a single node with 0 GPUs in three days0. Our two new models Propname Propname and Propname Propname have fewer parameters and can be seen as the counterpart of ResNet00 and ResNet 00. We introduce a new distillation procedure based on a distillation token, which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the transformer through attention. This transformer specific strategy outperforms vanilla distillation by a significant margin.", PRON VERB SCONJ DET ADJ NOUN NOUN ADP PROPN PROPN PROPN PUNCT CCONJ NOUN VERB ADP DET ADJ NOUN PUNCT ADP PRON PROPN ADJ NOUN NOUN PUNCT PRON VERB ADJ NOUN ADP ADJ NOUN PUNCT VERB PROPN NUM PUNCT PRON NOUN NOUN NOUN DET ADJ NOUN CCONJ ADJ NOUN ADP DET ADJ NOUN PUNCT ADJ ADP VERB NOUN PUNCT PRON VERB DET NOUN PUNCT SCONJ PART VERB DET NOUN PUNCT PRON VERB DET ADJ VERB NOUN PUNCT ADJ ADP NOUN CCONJ VERB ADP PROPN PUNCT CCONJ VERB SCONJ PRON ADV VERB DET ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB DET VERB NOUN PUNCT PRON VERB SCONJ PRON ADJ NOUN PRON VERB DET ADJ NOUN AUX VERB ADJ NOUN ADP DET NOUN ADP DET NOUN ADP PROPN ADP DET ADJ NOUN PUNCT PRON AUX VERB ADP DET ADJ NOUN ADP NUM NOUN ADP NUM NOUN PUNCT PRON NUM ADJ NOUN PROPN PROPN CCONJ PROPN PROPN VERB ADJ NOUN CCONJ AUX AUX VERB ADP DET NOUN ADP NOUN PUNCT CCONJ NOUN NUM PUNCT PRON VERB DET ADJ NOUN NOUN VERB ADP DET NOUN NOUN PUNCT PRON VERB DET ADJ NOUN SCONJ DET NOUN VERB PUNCT SCONJ SCONJ PRON VERB ADP VERB DET NOUN VERB ADP DET NOUN PUNCT DET NOUN VERB ADP DET NOUN ADP NOUN PUNCT DET ADJ ADJ NOUN NOUN NOUN NOUN ADP DET ADJ NOUN PUNCT,0.5921052631578947,19.0,5.021929824561403,179,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron,Hugo Touvron
7,7,GPT-3.5,"[' This paper introduces a straightforward architecture designed to tackle unpaired image-to-image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed-weight image autoencoder architecture. We introduce a task-specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof-of-concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters.']",abstract_chunked," This paper introduces a straightforward architecture designed to tackle unpaired image-to-image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed-weight image autoencoder architecture. We introduce a task-specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof-of-concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters.",21.303260869565236,138.0,7.0,270.0,0.6972999572753906," This paper introduces a straightforward architecture designed to tackle unpaired image to image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed weight image autoencoder architecture. We introduce a task specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof of concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters.", DET NOUN VERB DET ADJ NOUN VERB PART VERB ADJ NOUN ADP NOUN NOUN NOUN PUNCT VERB NOUN ADJ ADP NOUN CCONJ NOUN NOUN PUNCT NOUN PUNCT VERB PUNCT VERB PUNCT ADP NOUN PUNCT PRON NOUN VERB SCONJ DET VERB NOUN NOUN NOUN NOUN PUNCT PRON VERB DET NOUN ADJ ADJ NOUN VERB ADP DET ADJ NOUN PUNCT ADV VERB SCONJ DET VERB NOUN AUX VERB PUNCT DET ADV VERB NOUN NOUN NOUN DET NOUN NOUN ADP NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB ADJ NOUN PUNCT VERB ADJ NOUN NOUN CCONJ DET ADJ NOUN VERB DET NOUN ADP NOUN NOUN VERB ADP DET NOUN ADP NOUN PUNCT DET NOUN VERB ADJ PUNCT ADV SCONJ DET NOUN CCONJ NOUN ADP NOUN PART VERB AUX ADJ DET X PUNCT ADJ NOUN VERB DET NOUN ADP PRON NOUN ADP NOUN ADP NOUN NOUN PUNCT VERB ADJ CCONJ ADJ NOUN ADP NOUN ADP ADV ADJ NOUN PUNCT,0.6967741935483871,22.142857142857142,5.806451612903226,7,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5,GPT-3.5
208,122,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked," The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.",33.86250000000001,189.0,6.0,315.0,0.6127406358718872," The problem of aligning large language models to human values and intentions in terms of being comprehensive, respectful, and compliant0 has gained significant attention in research as recent Propname systems have rapidly advanced in their capabilities. Presently, state of the art Propname systems predominantly depend on supervised fine tuning with human instructions and annotations, as well as reinforcement learning from human feedback on their preferences. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self consistence, undesirable biases, etc ., in human provided annotations. To address such issues with intensive human annotations for Propname alignment, we propose a novel approach named Propname Propname. It substantially reduces the efforts on human supervision and renders it virtually annotation free by utilizing a small set of human defined principles to guide the behavior of Propname based Propname agents in generating responses to users queries.", DET NOUN ADP VERB ADJ NOUN NOUN ADP ADJ NOUN CCONJ NOUN ADP NOUN ADP AUX ADJ PUNCT ADJ PUNCT CCONJ NOUN AUX VERB ADJ NOUN ADP NOUN SCONJ ADJ PROPN NOUN AUX ADV VERB ADP PRON NOUN PUNCT ADV PUNCT NOUN ADP DET NOUN PROPN NOUN ADV VERB ADP ADJ ADJ NOUN ADP ADJ NOUN CCONJ NOUN PUNCT ADV ADV ADP NOUN NOUN ADP ADJ NOUN ADP PRON NOUN PUNCT DET NOUN ADP DET NOUN ADV VERB ADP DET NOUN ADP ADJ ADJ NOUN PUNCT PRON AUX PART ADV ADJ PART VERB CCONJ ADV VERB ADJ NOUN ADP DET NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN NOUN PUNCT ADJ NOUN PUNCT X X PUNCT ADP ADJ VERB NOUN PUNCT PART VERB ADJ NOUN ADP ADJ ADJ NOUN ADP PROPN NOUN PUNCT PRON VERB DET ADJ NOUN VERB PROPN PROPN PUNCT PRON ADV VERB DET NOUN ADP ADJ NOUN CCONJ VERB PRON ADV NOUN ADJ ADP VERB DET ADJ NOUN ADP ADJ VERB NOUN PART VERB DET NOUN ADP PROPN VERB PROPN NOUN ADP VERB NOUN ADP NOUN NOUN PUNCT,0.6373626373626373,36.4,5.3791208791208796,208,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Hugo Touvron,Zhiqing Sun,Zhiqing Sun
184,98,Hugo Touvron,"[' Residual architectures are prominent in computer vision since the advent of\nResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function\ng and R define how the network updates the input x at layer l. The function g is typically identity, while\nR is the main building block of the network: many variants in the literature essentially differ on how\none defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and\narchitecture design. As pointed out by He et al. [27], residual networks do not\noffer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]\nthe importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network\nalternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original\narchitecture of Vaswani et al. [66], except the LayerNorm is applied before the\nblock (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et\nal.', '[13] adopt this choice with LayerNorm for training deeper transformers for\nvarious media, including for image generation where they train transformers\nwith 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,\n34, 75]. In Section 2, we revisit this topic for transformer architectures solving\nimage classification problems. Examples of approaches closely related to ours\ninclude Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to\nimprove the training of deeper architecture compared to current methods for\nimage transformers. Formally, we add a learnable diagonal matrix on output of\neach residual block, initialized close to (but not at) 0. Adding this simple layer\nafter each residual block improves the training dynamic, allowing us to train\ndeeper high-capacity image transformers that benefit from depth. We refer to\nthis approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2.', 'It is akin to an encoder/decoder architecture,\nin which we explicitly separate the transformer layers involving self-attention\nbetween patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear\nclassifier. This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54], our model is on par with the state\nof the art (86.5%) while requiring less FLOPs (329B vs 377B) and having\nless parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We\ndiscuss related works along this paper and in the dedicated Section 6, before\nwe conclude in Section 7. The appendices contain some variations we have\ntried during our exploration.']",intro_chunked," Residual architectures are prominent in computer vision since the advent of
ResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function
g and R define how the network updates the input x at layer l. The function g is typically identity, while
R is the main building block of the network: many variants in the literature essentially differ on how
one defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and
architecture design. As pointed out by He et al. [27], residual networks do not
offer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]
the importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network
alternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original
architecture of Vaswani et al. [66], except the LayerNorm is applied before the
block (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et
al.",50.32536036036038,222.0,9.0,345.0,0.44222307205200195," Residual architectures are prominent in computer vision since the advent of ResNet. They are defined as a sequence of functions of the form FORMULA, where the function g and R define how the network updates the input Propname at layer l. The function g is typically identity, while R is the main building block of the network: many variants in the literature essentially differ on how one defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and architecture design. As pointed out by He Propname Propname., residual networks do not offer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He Propname Propname. discussed the importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers instantiate a particular form of residual architecture: after casting the input image into a set Propname of vectors, the network alternates self attention layers with feed forward networks, as FORMULA, where is the Propname operator. This definition follows the original architecture of Propname Propname Propname., except the Propname is applied before the block in the residual branch, as advocated by He Propname Propname.. Child Propname Propname.", ADJ NOUN AUX ADJ ADP NOUN NOUN SCONJ DET NOUN ADP NOUN PUNCT PRON AUX VERB ADP DET NOUN ADP NOUN ADP DET NOUN NOUN PUNCT SCONJ DET NOUN NOUN CCONJ NOUN VERB SCONJ DET NOUN VERB DET NOUN PROPN ADP NOUN NOUN DET NOUN NOUN AUX ADV NOUN PUNCT SCONJ NOUN AUX DET ADJ NOUN NOUN ADP DET NOUN PUNCT ADJ NOUN ADP DET NOUN ADV VERB ADP SCONJ NUM VERB DET ADJ NOUN NOUN AUX VERB CCONJ VERB PUNCT ADJ NOUN VERB DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT SCONJ VERB ADP ADP PRON PROPN PROPN PUNCT PUNCT ADJ NOUN AUX PART VERB DET ADJ ADJ NOUN PUNCT PRON VERB ADJ NOUN SCONJ PRON AUX ADJ PART VERB PUNCT ADV ADP PRON ADJ NOUN PUNCT PRON PROPN PROPN PUNCT VERB DET NOUN ADP VERB DET ADJ NOUN CCONJ ADV CCONJ ADJ PUNCT CCONJ VERB VERB NOUN ADP DET NOUN NOUN PUNCT DET NOUN NOUN VERB DET ADJ NOUN ADP ADJ NOUN PUNCT ADP VERB DET NOUN NOUN ADP DET ADJ PROPN ADP NOUN PUNCT DET NOUN VERB NOUN NOUN NOUN ADP NOUN ADJ NOUN PUNCT ADP NOUN PUNCT SCONJ AUX DET PROPN NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP PROPN PROPN PROPN PUNCT PUNCT SCONJ DET PROPN AUX VERB ADP DET NOUN ADP DET ADJ NOUN PUNCT SCONJ VERB ADP PRON PROPN PROPN PUNCT PUNCT NOUN PROPN PROPN PUNCT,0.5474137931034483,29.0,5.047413793103448,184,Hugo Touvron,Zhiqing Sun,Hugo Touvron,Zhiqing Sun,Hugo Touvron,Zhiqing Sun
253,167,Timo Schick,"[' Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scaling. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022). A simple way to overcome these limitations of\ntoday’s language models is to give them the ability to use external tools such as search engines,\ncalculators, or calendars. However, existing approaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-specific settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.', 'Therefore, we propose Toolformer, a model that\nlearns to use tools in a novel way, which fulfills the\nfollowing desiderata:\n• The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations. This is important not only because of the costs associated\nwith such annotations, but also because what\nhumans find useful may be different from\nwhat a model finds useful. • The LM should not lose any of its generality\nand should be able to decide for itself when\nand how to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspecific tasks. Our approach for achieving these goals is based\non the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Schütze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\nfinetune the LM itself on the API calls that it considers useful.', 'As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a variety of tools, and to choose for themselves which\ntool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the first place. This ensures that the model does not lose any\nof its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) and']",intro_chunked,"As illustrated in Figure 1, through
this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which
tool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset
that was used to pretrain a model in the first place. This ensures that the model does not lose any
of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after
learning to use tools, Toolformer, which is based
on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much
stronger zero-shot results, clearly outperforming a
much larger GPT-3 model (Brown et al., 2020) and",46.45511627906977,129.0,3.0,178.0,0.5203888416290283," As illustrated in Figure 0, through this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which tool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset that was used to pretrain a model in the first place. This ensures that the model does not lose any of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Propname, which is based on a pretrained Propname Propname model with 0.0B parameters, achieves much stronger zero shot results, clearly outperforming a much larger Propname 0 model and", SCONJ VERB ADP NOUN NUM PUNCT ADP DET ADJ NOUN PUNCT NOUN AUX VERB PART VERB DET NOUN ADP NOUN PUNCT CCONJ PART VERB ADP PRON DET NOUN PART VERB SCONJ CCONJ SCONJ PUNCT SCONJ PRON NOUN AUX ADJ ADP DET NOUN AUX VERB PUNCT PRON AUX VERB PRON ADP DET ADJ ADJ NOUN PRON AUX VERB PART VERB DET NOUN ADP DET ADJ NOUN PUNCT PRON VERB SCONJ DET NOUN AUX PART VERB PRON ADP PRON NOUN CCONJ NOUN NOUN NOUN PUNCT PRON VERB NOUN ADP DET NOUN ADP ADJ ADJ NOUN PUNCT VERB SCONJ ADP VERB PART VERB NOUN PUNCT PROPN PUNCT PRON AUX VERB ADP DET VERB PROPN PROPN NOUN ADP NOUN NOUN PUNCT VERB ADV ADJ NUM NOUN NOUN PUNCT ADV VERB DET ADV ADJ PROPN NUM NOUN CCONJ,0.6240601503759399,33.25,4.451127819548872,253,Timo Schick,Timo Schick,Timo Schick,Hugo Touvron,Timo Schick,Timo Schick
277,191,Timo Schick,"[' With pretrained language models (LMs) getting\never larger (Radford et al., 2019; Raffel et al., 2020;\nBrown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method\nfor few-shot text classification (e.g., Jiang et al.,\n2020; Schick and Schütze, 2021a,c; Brown et al.,\n2020; Wei et al., 2021; Sanh et al., 2021). The\nkey idea is to give an LM access to descriptive\nnames for all possible outputs and to short prompts\nexplaining the task to be solved. In settings where\nat most a few dozen examples are available, this\nsimple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;\nGao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong\nfew-shot performance of instruction-based approaches, arguing in particular that the considered\nsettings are often not true few-shot settings (Perez\net al., 2021; Logan IV et al., 2021) mainly for\ntwo reasons: For one, some approaches (e.g., Xie\net al., 2019; Zhang et al., 2020; Chen et al., 2020;\nTam et al., 2021) make use of large development\nsets to optimize hyperparameters. Beyond that, it\nis argued that manually designed instructions require manual tuning on development sets to achieve\nstrong performance (Perez et al., 2021; Logan IV\net al., 2021).', 'Indeed, performance can vary largely\n– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and\nSchütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh\net al., 2021). Even separate from this problem, the\nneed for human involvement is generally seen as a\nhuge drawback of manually designed instructions\n(Shin et al., 2020; Lester et al., 2021). Thus, several\nrecent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao\net al., 2021; Hambardzumyan et al., 2021; Li and\nLiang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when\ncorrectly configured, prompt-based approaches\nachieve strong performance even in true few-shot\nsettings and that there is no problem in using manually designed instructions per se. On the opposite,\nsuch instructions are often relatively easy to specify\nif one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific\nknowledge, and if properly used, they consistently\nimprove model performance in few-shot settings.', 'To provide empirical support for these claims, we\nrevisit PET (Schick and Schütze, 2021a) – a method\nfor combining instructions with example-based\nfinetuning whose key feature is that it allows users\nto specify multiple instructions for a single task\n– and thoroughly examine its performance with\nhuman-made instructions in true few-shot settings. In order to simulate a real-world scenario as best\nas possible, we proceed in two steps: First, we conduct an extensive study of PET using three English\nacademic datasets to analyze its ability to perform\ntrue few-shot learning in a controlled environment\nand to derive best practices regarding the choice of\ninstructions and other hyperparameters. We then\nput our findings to the test and evaluate PET on a\nlarge variety of different real-world tasks from the\nRAFT benchmark (Alex et al., 2021), for which no\nlabeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines\non this dataset and comes surprisingly close to the\nperformance of non-expert humans (see Figure 1),\ndemonstrating that instruction-based learning can\nsuccessfully be applied to real-world tasks in true\nfew-shot settings. In summary, the main contributions of this work\nare as follows:\n• We investigate the performance of PET for\nvarious models, tasks and training set sizes,\nits ability to cope with different instructions\nand its robustness to hyperparameter choices\nin true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant\nfor efficient classification in scenarios with\nmany different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in\ntrue few-shot settings.']",intro_chunked,"Indeed, performance can vary largely
– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and
Schütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh
et al., 2021). Even separate from this problem, the
need for human involvement is generally seen as a
huge drawback of manually designed instructions
(Shin et al., 2020; Lester et al., 2021). Thus, several
recent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao
et al., 2021; Hambardzumyan et al., 2021; Li and
Liang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when
correctly configured, prompt-based approaches
achieve strong performance even in true few-shot
settings and that there is no problem in using manually designed instructions per se. On the opposite,
such instructions are often relatively easy to specify
if one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific
knowledge, and if properly used, they consistently
improve model performance in few-shot settings.",46.63281355932203,177.0,5.0,260.0,0.3480371832847595," Indeed, performance can vary largely and in mostly unpredictable ways across different instructions Propname Propname Propname Propname, 0000; Propname and Propname, 0000a; this issue even persists after finetuning a model on hundreds of instructions Propname Propname Propname Propname, 0000. Even separate from this problem, the need for human involvement is generally seen as a huge drawback of manually designed instructions. Thus, several recent works abandon them in favor of automatically generated prompts Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and Propname, 0000; Propname Propname Propname Propname, 0000. Contrary to this trend, we argue that when correctly configured, prompt based approaches achieve strong performance even in true few shot settings and that there is no problem in using manually designed instructions per se. On the opposite, such instructions are often relatively easy to specify if one is familiar with the task to be solved, they provide an intuitive interface to convey task specific knowledge, and if properly used, they consistently improve model performance in few shot settings.", ADV PUNCT NOUN AUX VERB ADV CCONJ ADP ADV ADJ NOUN ADP ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN PUNCT NUM PUNCT DET NOUN ADV VERB ADP VERB DET NOUN ADP NOUN ADP NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADV ADJ ADP DET NOUN PUNCT DET NOUN ADP ADJ NOUN AUX ADV VERB ADP DET ADJ NOUN ADP ADV VERB NOUN PUNCT ADV PUNCT ADJ ADJ NOUN VERB PRON ADP NOUN ADP ADV VERB NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADJ ADP DET NOUN PUNCT PRON VERB SCONJ SCONJ ADV VERB PUNCT ADJ VERB NOUN VERB ADJ NOUN ADV ADP ADJ ADJ NOUN NOUN CCONJ SCONJ PRON VERB DET NOUN ADP VERB ADV VERB NOUN X X PUNCT ADP DET ADJ PUNCT ADJ NOUN AUX ADV ADV ADJ PART VERB SCONJ NUM AUX ADJ ADP DET NOUN PART AUX VERB PUNCT PRON VERB DET ADJ NOUN PART VERB NOUN ADJ NOUN PUNCT CCONJ SCONJ ADV VERB PUNCT PRON ADV VERB NOUN NOUN ADP ADJ NOUN NOUN PUNCT,0.5270935960591133,40.6,5.1330049261083746,277,Timo Schick,Timo Schick,Aman Madaan,Timo Schick,Timo Schick,Timo Schick
132,46,Aman Madaan,"[' Language models are now better than ever before at\ngenerating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense\nis in misunderstanding a user’s intent. The typical\nremedy of retraining with more data is prohibitive\ndue to the cost and infrastructure requirements. In\nsuch cases, even if users repeatedly observe the\nmodel making a mistake, there are no avenues to\nprovide feedback to the model to make it more\naccurate and personalized over time. Our goal is to allow users to correct such errors\ndirectly through interaction, and without retraining by injecting the knowledge required to correct the\nmodel’s misunderstanding. Building upon the recent success of injecting commonsense in the input\n(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in\nthe input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with\na growing memory of cases where the model misunderstood user’s intent and was provided with\ncorrective feedback. This feedback is question dependent, and thus the prompt for each sample is\nedited to adapt to the input. In this sense, our\nwork can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing\nthe prompts.', 'Our work adds interactivity to prompt\nengineering as it involves dynamically updating the\nprompt for every instance. Figure 1 presents a sample interaction between a\nuser and GPT-3 that our setup enables. The model\nwas asked for a similar word. However, the model’s\n(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task\ninstruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,\nnote that such instructional correction is feasible\neven if the user does not know the correct answer to\ntheir question, as they are critiquing the model’s understanding of their intent, rather than the answers\nthemselves. Thus, our setup does not require the\nusers to be experts at tasks being solved, another\nadvantage of our approach. Further, it is desirable to have a system that can\nleverage past feedback on new, unseen examples\nfor prompt-editing. We maintain a memory M of\nsuch feedback as a set of key-value pairs, where the\nkey is a misunderstood question, and the value is\nthe user’s feedback to correct that misunderstanding. Given a new question, we check if the model\nhas made a mistake on a similar question earlier,\nby querying the memory for a similar question.', 'If\nfound, append the corresponding feedback to the\nquestion prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive\nreminding in psychology (Jacoby and Wahlheim,\n2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for\nthe system and provides representative implementations for each component. We then demonstrate\nthe system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure\n1), (2) word scrambling (e.g., anagrams), (3) ethical\nreasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)\nethics reasoning with user feedback being natural\nlanguage. We find that in all cases, GPT-3’s accuracy significantly increases with time, without\nretraining, as our approach enables it to use corrective feedback from earlier examples to avoid\nsimilar misunderstandings on future examples. In\nsummary, our contributions are:• We show that a large model like GPT-3 can be\nimproved after deployment, without retraining,\nthrough a memory-assisted architecture. • Our implementation, MemPrompt, is the first\ndemonstration that this is possible - this is an important step forward for real use of LMs, and the\npaper sets out a general architecture that others can\nbuild on, a specific implementation, and detailed\nevaluation on multiple tasks.']",intro_chunked,"Our work adds interactivity to prompt
engineering as it involves dynamically updating the
prompt for every instance. Figure 1 presents a sample interaction between a
user and GPT-3 that our setup enables. The model
was asked for a similar word. However, the model’s
(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task
instruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,
note that such instructional correction is feasible
even if the user does not know the correct answer to
their question, as they are critiquing the model’s understanding of their intent, rather than the answers
themselves. Thus, our setup does not require the
users to be experts at tasks being solved, another
advantage of our approach. Further, it is desirable to have a system that can
leverage past feedback on new, unseen examples
for prompt-editing. We maintain a memory M of
such feedback as a set of key-value pairs, where the
key is a misunderstood question, and the value is
the user’s feedback to correct that misunderstanding. Given a new question, we check if the model
has made a mistake on a similar question earlier,
by querying the memory for a similar question.",56.8359090909091,220.0,10.0,332.0,0.21850550174713135," Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance. Figure 0 presents a sample interaction between a user and Propname 0 that our setup enables. The model was asked for a similar word. However, the models task understanding was The homophone of good is. The user can detect such discrepancy between the intended and interpreted task instruction, and can provide feedback as similar to means with a similar meaning, clarifying that they actually wanted a synonym. Crucially, note that such instructional correction is feasible even if the user does not know the correct answer to their question, as they are critiquing the models understanding of their intent, rather than the answers themselves. Thus, our setup does not require the users to be experts at tasks being solved, another advantage of our approach. Further, it is desirable to have a system that can leverage past feedback on new, unseen examples for prompt editing. We maintain a memory Propname of such feedback as a set of key value pairs, where the key is a misunderstood question, and the value is the users feedback to correct that misunderstanding. Given a new question, we check if the model has made a mistake on a similar question earlier, by querying the memory for a similar question.", PRON NOUN VERB NOUN PART VERB NOUN SCONJ PRON VERB ADV VERB DET NOUN ADP DET NOUN PUNCT NOUN NUM VERB DET NOUN NOUN ADP DET NOUN CCONJ PROPN NUM SCONJ PRON NOUN VERB PUNCT DET NOUN AUX VERB ADP DET ADJ NOUN PUNCT ADV PUNCT DET NOUN NOUN NOUN AUX DET NOUN ADP ADJ AUX PUNCT DET NOUN AUX VERB ADJ NOUN ADP DET VERB CCONJ VERB NOUN NOUN PUNCT CCONJ AUX VERB NOUN ADV ADJ ADP NOUN ADP DET ADJ NOUN PUNCT VERB SCONJ PRON ADV VERB DET NOUN PUNCT ADV PUNCT VERB SCONJ ADJ ADJ NOUN AUX ADJ ADV SCONJ DET NOUN AUX PART VERB DET ADJ NOUN ADP PRON NOUN PUNCT SCONJ PRON AUX VERB DET NOUN NOUN ADP PRON NOUN PUNCT ADV ADP DET NOUN PRON PUNCT ADV PUNCT PRON NOUN AUX PART VERB DET NOUN PART AUX NOUN ADP NOUN AUX VERB PUNCT DET NOUN ADP PRON NOUN PUNCT ADV PUNCT PRON AUX ADJ PART VERB DET NOUN PRON AUX VERB ADJ NOUN ADP ADJ PUNCT ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET NOUN PROPN ADP ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT SCONJ DET NOUN AUX DET NOUN NOUN PUNCT CCONJ DET NOUN AUX DET NOUN VERB PART VERB DET NOUN PUNCT VERB DET ADJ NOUN PUNCT PRON VERB SCONJ DET NOUN AUX VERB DET NOUN ADP DET ADJ NOUN ADV PUNCT ADP VERB DET NOUN ADP DET ADJ NOUN PUNCT,0.5185185185185185,24.3,4.5473251028806585,132,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Aman Madaan,Hugo Touvron
58,58,Zhiqing Sun,"[' Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of the AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user’s queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary . With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning), Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings. We have open-sourced the code, LoRA weights of Dromedary, and our synthetic training data to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, reduced biases, and improved controllability']",abstract_chunked," Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of the AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user’s queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary . With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning), Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings. We have open-sourced the code, LoRA weights of Dromedary, and our synthetic training data to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, reduced biases, and improved controllability",16.495833333333366,325.0,6.0,520.0,0.5977640151977539," Recent Propname assistant agents, such as ChatGPT, predominantly rely on supervised fine tuning with human annotations and reinforcement learning from human feedback to align the output of large language models with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of Propname assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self consistency, and undesirable biases. To address these challenges, we propose a novel approach called Propname Propname, which combines principle driven reasoning and the generative power of LLMs for the self alignment of the Propname agents with minimal human supervision. Our approach encompasses four stages: first, we use an Propname to generate synthetic prompts, and a topic guided method to augment the prompt diversity; second, we use a small set of human written principles for Propname models to follow, and guide the Propname through in context learning from demonstrations to produce helpful, ethical, and reliable responses to users queries; third, we fine tune the original Propname with the high quality self aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly brief or indirect responses. Applying SELF Propname to the Propname 00b base language model, we develop an Propname assistant named Propname. With fewer than 000 lines of human annotations, Propname significantly surpasses the performance of several state of the art Propname systems, including Propname Propname 000 and Propname, on benchmark datasets with various settings. We have open sourced the code, Propname weights of Propname, and our synthetic training data to encourage further research into aligning Propname based Propname agents with enhanced supervision efficiency, reduced biases, and improved controllability", ADJ PROPN ADJ NOUN PUNCT ADJ ADP NOUN PUNCT ADV VERB ADP VERB ADJ NOUN ADP ADJ NOUN CCONJ NOUN NOUN ADP ADJ NOUN PART VERB DET NOUN ADP ADJ NOUN NOUN ADP ADJ NOUN PUNCT VERB PRON AUX ADJ PUNCT ADJ PUNCT CCONJ ADJ PUNCT ADV PUNCT DET NOUN AUX ADV VERB DET ADJ NOUN ADP PROPN ADJ NOUN ADP ADP DET ADJ NOUN ADP VERB ADJ NOUN CCONJ DET ADJ NOUN ADP NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN NOUN PUNCT CCONJ ADJ NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB DET ADJ NOUN VERB PROPN PROPN PUNCT PRON VERB ADJ ADJ NOUN CCONJ DET ADJ NOUN ADP NOUN ADP DET NOUN NOUN ADP DET PROPN NOUN ADP ADJ ADJ NOUN PUNCT PRON NOUN VERB NUM NOUN PUNCT ADV PUNCT PRON VERB DET PROPN PART VERB ADJ NOUN PUNCT CCONJ DET NOUN VERB NOUN PART VERB DET ADJ NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADP ADJ VERB NOUN ADP PROPN NOUN PART VERB PUNCT CCONJ VERB DET PROPN ADP ADP NOUN VERB ADP NOUN PART VERB ADJ PUNCT ADJ PUNCT CCONJ ADJ NOUN ADP NOUN NOUN PUNCT ADV PUNCT PRON ADJ NOUN DET ADJ PROPN ADP DET ADJ NOUN NOUN VERB NOUN SCONJ SCONJ DET VERB NOUN AUX VERB ADJ NOUN ADP DET NOUN ADV ADP DET NOUN VERB CCONJ DET NOUN ADV PUNCT CCONJ ADV PUNCT PRON VERB DET ADJ NOUN PART VERB DET NOUN ADP ADV ADJ CCONJ ADJ NOUN PUNCT VERB NOUN PROPN ADP DET PROPN NOUN NOUN NOUN NOUN PUNCT PRON VERB DET PROPN NOUN VERB PROPN PUNCT ADP ADJ ADP NUM NOUN ADP ADJ NOUN PUNCT PROPN ADV VERB DET NOUN ADP ADJ NOUN ADP DET NOUN PROPN NOUN PUNCT VERB PROPN PROPN NUM CCONJ PROPN PUNCT ADP ADJ NOUN ADP ADJ NOUN PUNCT PRON AUX ADJ VERB DET NOUN PUNCT PROPN NOUN ADP PROPN PUNCT CCONJ PRON ADJ NOUN NOUN PART VERB ADJ NOUN ADP VERB PROPN VERB PROPN NOUN ADP ADJ NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ VERB NOUN,0.5160349854227405,49.0,5.186588921282799,58,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,Hugo Touvron,Zhiqing Sun,Zhiqing Sun
252,166,Timo Schick,"[' Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scaling. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022). A simple way to overcome these limitations of\ntoday’s language models is to give them the ability to use external tools such as search engines,\ncalculators, or calendars. However, existing approaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-specific settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.', 'Therefore, we propose Toolformer, a model that\nlearns to use tools in a novel way, which fulfills the\nfollowing desiderata:\n• The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations. This is important not only because of the costs associated\nwith such annotations, but also because what\nhumans find useful may be different from\nwhat a model finds useful. • The LM should not lose any of its generality\nand should be able to decide for itself when\nand how to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspecific tasks. Our approach for achieving these goals is based\non the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Schütze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\nfinetune the LM itself on the API calls that it considers useful.', 'As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a variety of tools, and to choose for themselves which\ntool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the first place. This ensures that the model does not lose any\nof its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) and']",intro_chunked,"Therefore, we propose Toolformer, a model that
learns to use tools in a novel way, which fulfills the
following desiderata:
• The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. This is important not only because of the costs associated
with such annotations, but also because what
humans find useful may be different from
what a model finds useful. • The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. In contrast to
existing approaches, this enables a much more
comprehensive use of tools that is not tied to
specific tasks. Our approach for achieving these goals is based
on the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate
entire datasets from scratch (Schick and Schütze,
2021b; Honovich et al., 2022; Wang et al., 2022):
Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
finetune the LM itself on the API calls that it considers useful.",58.41714932126699,221.0,7.0,304.0,0.5266711711883545," Therefore, we propose Propname, a model that learns to use tools in a novel way, which fulfills the following desiderata: The use of tools should be learned in a self supervised way without requiring large amounts of human annotations. This is important not only because of the costs associated with such annotations, but also because what humans find useful may be different from what a model finds useful. The Propname should not lose any of its generality and should be able to decide for itself when and how to use which tool. In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to specific tasks. Our approach for achieving these goals is based on the recent idea of using large LMs with incontext learning to generate entire datasets from Propname Propname and Propname, 0000b; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000: Given just a handful of human written examples of how an API can be used, we let a Propname annotate a huge language modeling dataset with potential API calls. We then use a self supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, we finetune the Propname itself on the API calls that it considers useful.", ADV PUNCT PRON VERB PROPN PUNCT DET NOUN PRON VERB PART VERB NOUN ADP DET ADJ NOUN PUNCT PRON VERB DET ADJ NOUN PUNCT DET NOUN ADP NOUN AUX AUX VERB ADP DET NOUN VERB NOUN ADP VERB ADJ NOUN ADP ADJ NOUN PUNCT PRON AUX ADJ PART ADV SCONJ ADP DET NOUN VERB ADP ADJ NOUN PUNCT CCONJ ADV SCONJ PRON NOUN VERB ADJ AUX AUX ADJ ADP PRON DET NOUN VERB ADJ PUNCT DET PROPN AUX PART VERB PRON ADP PRON NOUN CCONJ AUX AUX ADJ PART VERB ADP PRON SCONJ CCONJ SCONJ PART VERB DET NOUN PUNCT ADP NOUN ADP VERB NOUN PUNCT PRON VERB DET ADV ADV ADJ NOUN ADP NOUN PRON AUX PART VERB ADP ADJ NOUN PUNCT PRON NOUN ADP VERB DET NOUN AUX VERB ADP DET ADJ NOUN ADP VERB ADJ NOUN ADP NOUN VERB PART VERB ADJ NOUN ADP PROPN PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT VERB ADV DET NOUN ADP ADJ VERB NOUN ADP SCONJ DET NOUN AUX AUX VERB PUNCT PRON VERB DET PROPN VERB DET ADJ NOUN NOUN ADJ ADP ADJ NOUN NOUN PUNCT PRON ADV VERB DET NOUN VERB NOUN PART VERB PRON ADP DET NOUN NOUN ADV VERB DET NOUN ADP VERB ADJ NOUN PUNCT ADV PUNCT PRON VERB DET PROPN PRON ADP DET NOUN VERB SCONJ PRON VERB ADJ PUNCT,0.5378151260504201,34.0,4.5210084033613445,252,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
81,81,Timo Schick,"[' Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word’s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium-frequency range.']",abstract_chunked," Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word’s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium-frequency range.",44.55547887323948,142.0,5.0,224.0,0.428235799074173," Learning high quality embeddings for rare words is a hard problem because of sparse context information. Mimicking has been proposed as a solution: given embeddings learned by a standard Propname, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a words surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium frequency range.", VERB ADJ NOUN NOUN ADP ADJ NOUN AUX DET ADJ NOUN SCONJ ADP ADJ NOUN NOUN PUNCT NOUN AUX AUX VERB ADP DET NOUN PUNCT VERB NOUN VERB ADP DET ADJ PROPN PUNCT DET NOUN AUX ADV VERB PART VERB NOUN ADP ADJ NOUN ADP PRON NOUN NOUN CCONJ ADV VERB PART VERB NOUN ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB ADJ NOUN PUNCT DET ADJ NOUN AUX VERB NOUN PART ADV ADP DET NOUN NOUN NOUN PUNCT CCONJ ADV ADP DET ADJ NOUN CCONJ VERB PART VERB ADP DET ADV ADJ CCONJ ADJ NOUN ADP VERB DET NOUN PUNCT ADP DET NOUN ADP NUM NOUN PUNCT PRON VERB PRON ADJ VERB NOUN ADJ NOUN ADP CCONJ ADJ CCONJ ADJ ADJ NOUN PUNCT ADV PUNCT VERB ADP ADJ NOUN PUNCT ADJ NOUN VERB NOUN ADP DET ADV ADJ NOUN ADP DET NOUN PUNCT VERB DET ADJ ADJ NOUN PUNCT,0.5855263157894737,30.4,4.848684210526316,81,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick,Timo Schick
111,25,Aman Madaan,"[' Human problem-solving inherently follows a multi-step process: generate a solution, verify its\nvalidity, and refine it further based on verification outcomes. The emulation of this self-refinement\nand reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan\net al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,\ndemonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the\nintrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable\ndata) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step\nreasoning), motivate an alternative approach of model switching. Model switching iteratively queries\nover models of disparate sizes and capabilities, verifying feedback at each step and determining\nwhether to accept the output or route to a more capable, albeit computationally intensive, model (Liu\net al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly\nfor each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,\n2022).', 'However, modern LLM often provide access solely through black-box APIs, restricting\ndirect model optimization and adaptability due to the unavailability of fine-tuning capabilities and\nweight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM\nAPIs, circumventing the necessity for separate models or logits access by adopting few-shot learning\nstrategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies\nfor each step of problem-solving: solution generation, verification, and routing, all assuming we only\nhave access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model\nrouting, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified\nearly. This consideration allows AutoMix to judiciously allocate computational resources, preventing\nunwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with\nthe provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan\net al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic\nlife"" would be flagged as inconsistent.', 'However, recognizing that self-verification can sometimes\nbe inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability\nof the initial verification. The meta-verifier acts as a secondary check, providing an additional layer\nof confidence assessment to ensure that the decision to route a task to a larger or smaller model is\nwell-founded. In summary, our contributions are:\n• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating\na solution, verifying the solution, and switching to a larger language model, everything without\naccess to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps\nimprove the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure\nthat quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets\nusing the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large\n(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing\nthe incremental benefit per cost by up to 89%.']",intro_chunked,"However, modern LLM often provide access solely through black-box APIs, restricting
direct model optimization and adaptability due to the unavailability of fine-tuning capabilities and
weight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM
APIs, circumventing the necessity for separate models or logits access by adopting few-shot learning
strategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies
for each step of problem-solving: solution generation, verification, and routing, all assuming we only
have access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model
routing, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified
early. This consideration allows AutoMix to judiciously allocate computational resources, preventing
unwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with
the provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan
et al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic
life"" would be flagged as inconsistent.",36.30230978260872,207.0,8.0,353.0,0.21481259167194366," However, modern Propname often provide access solely through black box APIs, restricting direct model optimization and adaptability due to the unavailability of fine tuning capabilities and weight access. In response to this, we introduce Propname, a method that utilizes black box Propname APIs, circumventing the necessity for separate models or logits access by adopting few shot learning strategies and implementing self verification. Our method proposes strategies for each step of problem solving: solution generation, verification, and routing, all assuming we only have access to black box LLMs. In contrast to existing approaches, which generally classify tasks as Propname or Propname for model routing, Propname integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Propname Propname Propname and should not be routed to larger models if identified early. This consideration allows Propname to judiciously allocate computational resources, preventing unwarranted computational spending on these particularly challenging instances. We use context grounded few shot entailment to evaluate the consistency of generated answers with the provided context, without requiring a large amount of human labeled data Propname, 0000; Propname Propname Propname Propname, 0000. For example, an answer discussing desert animals in a context focused on aquatic life would be flagged as inconsistent.", ADV PUNCT ADJ PROPN ADV VERB NOUN ADV ADP ADJ NOUN NOUN PUNCT VERB ADJ NOUN NOUN CCONJ NOUN ADP ADP DET NOUN ADP ADJ NOUN NOUN CCONJ NOUN NOUN PUNCT ADP NOUN ADP PRON PUNCT PRON VERB PROPN PUNCT DET NOUN PRON VERB ADJ NOUN PROPN NOUN PUNCT VERB DET NOUN ADP ADJ NOUN CCONJ NOUN NOUN ADP VERB ADJ NOUN NOUN NOUN CCONJ VERB NOUN NOUN PUNCT PRON NOUN VERB NOUN ADP DET NOUN ADP NOUN VERB PUNCT NOUN NOUN PUNCT NOUN PUNCT CCONJ VERB PUNCT PRON VERB PRON ADV VERB NOUN ADP ADJ NOUN NOUN PUNCT ADP NOUN ADP VERB NOUN PUNCT PRON ADV VERB NOUN ADP PROPN CCONJ PROPN ADP NOUN NOUN PUNCT PROPN VERB DET ADJ NOUN ADP ADJ NOUN PUNCT DET NOUN AUX ADV ADJ ADV ADP DET PROPN PROPN PROPN CCONJ AUX PART AUX VERB ADP ADJ NOUN SCONJ VERB ADV PUNCT DET NOUN VERB PROPN PART ADV VERB ADJ NOUN PUNCT VERB ADJ ADJ NOUN ADP DET ADV ADJ NOUN PUNCT PRON VERB NOUN VERB ADJ NOUN NOUN PART VERB DET NOUN ADP VERB NOUN ADP DET VERB NOUN PUNCT ADP VERB DET ADJ NOUN ADP ADJ VERB NOUN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP NOUN PUNCT DET NOUN VERB NOUN NOUN ADP DET NOUN VERB ADP ADJ NOUN AUX AUX VERB ADP NOUN PUNCT,0.6244541484716157,28.625,5.393013100436681,111,Aman Madaan,Aman Madaan,Timo Schick,Timo Schick,Aman Madaan,Zhiqing Sun
205,119,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked," Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.",51.8563157894737,209.0,7.0,308.0,0.26378732919692993," Large Propname Propname; Propname Propname Propname.; Propname can delve into the multimodal realm either by further pretraining with image text pairs or by fine tuning them with specialized Propname instruction tuning datasets, leading to the emergence of powerful Large Propname Propname. Yet, developing Propname faces challenges, notably the gap between the volume and quality of multimodal data versus text only datasets. Consider the LLaVA model, which is initialized from a pretrained vision encoder and an instruction tuned language model. It is trained on just 000 K synthetic image based dialogues, which is much less in comparison to the text only models utilizing over 000 Propname examples spanning 0000 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, Propname may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high quality visual instruction tuning data for Propname training, we introduce Propname Propname, a vision language model trained for improved multimodal alignment.", ADJ PROPN PROPN PUNCT PROPN PROPN PROPN PUNCT PUNCT PROPN AUX VERB ADP DET ADJ NOUN CCONJ ADP ADV VERB ADP NOUN NOUN NOUN CCONJ ADP ADJ VERB PRON ADP ADJ PROPN NOUN VERB NOUN PUNCT VERB ADP DET NOUN ADP ADJ ADJ PROPN PROPN PUNCT ADV PUNCT VERB PROPN VERB NOUN PUNCT ADV DET NOUN ADP DET NOUN CCONJ NOUN ADP ADJ NOUN ADP NOUN ADV NOUN PUNCT VERB DET ADJ NOUN PUNCT PRON AUX VERB ADP DET VERB NOUN NOUN CCONJ DET NOUN VERB NOUN NOUN PUNCT PRON AUX VERB ADP ADV NUM NOUN ADJ NOUN VERB NOUN PUNCT PRON AUX ADV ADJ ADP NOUN ADP DET NOUN ADV NOUN VERB ADP NUM PROPN NOUN VERB NUM NOUN PUNCT ADJ NOUN ADP NOUN AUX VERB ADP NOUN ADP DET NOUN CCONJ NOUN NOUN PUNCT ADV PUNCT PROPN AUX VERB VERB NOUN PUNCT PRON AUX PART ADV VERB ADP DET NOUN VERB ADP NOUN PUNCT PART VERB DET NOUN VERB ADP DET NOUN ADP ADJ NOUN ADJ NOUN VERB NOUN ADP PROPN NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET NOUN NOUN NOUN VERB ADP VERB ADJ NOUN PUNCT,0.5631578947368421,23.75,5.21578947368421,205,GPT-3.5,Zhiqing Sun,Zhiqing Sun,Zhiqing Sun,GPT-3.5,Hugo Touvron
