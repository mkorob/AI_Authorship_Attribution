,Unnamed: 0,Author,Pub,Type,Chunk,flesch_score,mean_flesch,ID,sent_score,re_text,text,POS_string,lexical_diversity,avg_word_per_sentence,avg_word_length
0,0,GPT-3.5,"[' In the landscape of cloud-based Large Language Models (LLMs), the array of sizes and configurations available presents a challenge in effectively balancing computational cost and performance optimization. This paper introduces AutoMix, an innovative approach designed to strategically route queries to larger LLMs based on the approximate correctness of outputs from a smaller LM. At its core, AutoMix incorporates a few-shot self-verification mechanism, allowing it to estimate the reliability of its own outputs without the need for additional training. To address the inherent noise in verifications, AutoMix employs a meta verifier, enhancing the accuracy of these assessments. Experimental results using LLAMA2-13/70B on five context-grounded reasoning datasets illustrate that AutoMix outperforms established baselines, yielding an up to 89% improvement in the incremental benefit per cost.']",abstract_chunked," In the landscape of cloud-based Large Language Models (LLMs), the array of sizes and configurations available presents a challenge in effectively balancing computational cost and performance optimization. This paper introduces AutoMix, an innovative approach designed to strategically route queries to larger LLMs based on the approximate correctness of outputs from a smaller LM. At its core, AutoMix incorporates a few-shot self-verification mechanism, allowing it to estimate the reliability of its own outputs without the need for additional training. To address the inherent noise in verifications, AutoMix employs a meta verifier, enhancing the accuracy of these assessments. Experimental results using LLAMA2-13/70B on five context-grounded reasoning datasets illustrate that AutoMix outperforms established baselines, yielding an up to 89% improvement in the incremental benefit per cost.",6.1688000000000045,2.6406588729545177,0,0.5823206305503845," In the landscape of cloud based Propname Propname Propname, the array of sizes and configurations available presents a challenge in effectively balancing computational cost and performance optimization. This paper introduces Propname, an innovative approach designed to strategically route queries to larger LLMs based on the approximate correctness of outputs from a smaller Propname. At its core, Propname incorporates a few shot self verification mechanism, allowing it to estimate the reliability of its own outputs without the need for additional training. To address the inherent noise in verifications, Propname employs a meta verifier, enhancing the accuracy of these assessments. Experimental results using Propname Propname on five context grounded reasoning datasets illustrate that Propname outperforms established baselines, yielding an up to 00 improvement in the incremental benefit per cost."," In the landscape of cloud based Propname Propname Propname, the array of sizes and configurations available presents a challenge in effectively balancing computational cost and performance optimization. This paper introduces Propname, an innovative approach designed to strategically route queries to larger LLMs based on the approximate correctness of outputs from a smaller Propname. At its core, Propname incorporates a few shot self verification mechanism, allowing it to estimate the reliability of its own outputs without the need for additional training. To address the inherent noise in verifications, Propname employs a meta verifier, enhancing the accuracy of these assessments. Experimental results using Propname Propname on five context grounded reasoning datasets illustrate that Propname outperforms established baselines, yielding an up to 00 improvement in the incremental benefit per cost.", ADP DET NOUN ADP NOUN VERB PROPN PROPN PROPN PUNCT DET NOUN ADP NOUN CCONJ NOUN ADJ VERB DET NOUN ADP ADV VERB ADJ NOUN CCONJ NOUN NOUN PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN VERB PART ADV VERB NOUN ADP ADJ NOUN VERB ADP DET ADJ NOUN ADP NOUN ADP DET ADJ PROPN PUNCT ADP PRON NOUN PUNCT PROPN VERB DET ADJ NOUN NOUN NOUN NOUN PUNCT VERB PRON PART VERB DET NOUN ADP PRON ADJ NOUN ADP DET NOUN ADP ADJ NOUN PUNCT PART VERB DET ADJ NOUN ADP NOUN PUNCT PROPN VERB DET ADJ NOUN PUNCT VERB DET NOUN ADP DET NOUN PUNCT ADJ NOUN VERB PROPN PROPN ADP NUM NOUN VERB NOUN NOUN VERB SCONJ PROPN NOUN VERB NOUN PUNCT VERB DET ADP PART NUM NOUN ADP DET ADJ NOUN ADP NOUN PUNCT,0.6762589928057554,27.8,5.618705035971223
1,1,GPT-3.5,"[' Large Language Models (LLMs) often fall short in generating optimal outputs on their initial attempts. Drawing inspiration from the human iterative refinement process, we introduce SELF-REFINE, a novel approach for enhancing LLM outputs through iterative feedback and self-improvement. The core concept involves generating an initial output using an LLM, then utilizing the same model to provide feedback on its output and iteratively refine itself. This method requires no additional supervised training data, extra training, or reinforcement learning, relying solely on a single LLM to serve as the generator, refiner, and feedback provider. We assess SELF-REFINE across a diverse set of 7 tasks, spanning from dialog response generation to mathematical reasoning, employing state-of-the-art LLMs such as GPT-3.5 and GPT-4. The results demonstrate that outputs produced with SELF-REFINE are preferred by both human evaluators and automatic metrics compared to outputs generated using conventional one-step generation. On average, task performance improves by approximately 20% absolute across all evaluated tasks. Our findings underscore the potential for further enhancing even state-of-the-art LLMs, like GPT-4, at test-time through the adoption of our straightforward, standalone approach.']",abstract_chunked," Large Language Models (LLMs) often fall short in generating optimal outputs on their initial attempts. Drawing inspiration from the human iterative refinement process, we introduce SELF-REFINE, a novel approach for enhancing LLM outputs through iterative feedback and self-improvement. The core concept involves generating an initial output using an LLM, then utilizing the same model to provide feedback on its output and iteratively refine itself. This method requires no additional supervised training data, extra training, or reinforcement learning, relying solely on a single LLM to serve as the generator, refiner, and feedback provider. We assess SELF-REFINE across a diverse set of 7 tasks, spanning from dialog response generation to mathematical reasoning, employing state-of-the-art LLMs such as GPT-3.5 and GPT-4. The results demonstrate that outputs produced with SELF-REFINE are preferred by both human evaluators and automatic metrics compared to outputs generated using conventional one-step generation. On average, task performance improves by approximately 20% absolute across all evaluated tasks. Our findings underscore the potential for further enhancing even state-of-the-art LLMs, like GPT-4, at test-time through the adoption of our straightforward, standalone approach.",15.008585164835182,2.6406588729545177,1,0.5678741335868835," Large Propname Propname often fall short in generating optimal outputs on their initial attempts. Drawing inspiration from the human iterative refinement process, we introduce Propname Propname, a novel approach for enhancing Propname outputs through iterative feedback and self improvement. The core concept involves generating an initial output using an Propname, then utilizing the same model to provide feedback on its output and iteratively refine itself. This method requires no additional supervised training data, extra training, or reinforcement learning, relying solely on a single Propname to serve as the generator, refiner, and feedback provider. We assess Propname Propname across a diverse set of 0 tasks, spanning from dialog response generation to mathematical reasoning, employing state of the art LLMs such as Propname 0.0 and Propname 0. The results demonstrate that outputs produced with Propname Propname are preferred by both human evaluators and automatic metrics compared to outputs generated using conventional one step generation. On average, task performance improves by approximately 00 absolute across all evaluated tasks. Our findings underscore the potential for further enhancing even state of the art LLMs, like Propname 0, at test time through the adoption of our straightforward, standalone approach."," Large Propname Propname often fall short in generating optimal outputs on their initial attempts. Drawing inspiration from the human iterative refinement process, we introduce Propname Propname, a novel approach for enhancing Propname outputs through iterative feedback and self improvement. The core concept involves generating an initial output using an Propname, then utilizing the same model to provide feedback on its output and iteratively refine itself. This method requires no additional supervised training data, extra training, or reinforcement learning, relying solely on a single Propname to serve as the generator, refiner, and feedback provider. We assess Propname Propname across a diverse set of 0 tasks, spanning from dialog response generation to mathematical reasoning, employing state of the art LLMs such as Propname 0.0 and Propname 0. The results demonstrate that outputs produced with Propname Propname are preferred by both human evaluators and automatic metrics compared to outputs generated using conventional one step generation. On average, task performance improves by approximately 00 absolute across all evaluated tasks. Our findings underscore the potential for further enhancing even state of the art LLMs, like Propname 0, at test time through the adoption of our straightforward, standalone approach.", ADJ PROPN PROPN ADV VERB ADJ ADP VERB ADJ NOUN ADP PRON ADJ NOUN PUNCT VERB NOUN ADP DET ADJ ADJ ADJ NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET ADJ NOUN ADP VERB PROPN NOUN ADP ADJ NOUN CCONJ NOUN NOUN PUNCT DET NOUN NOUN VERB VERB DET ADJ NOUN VERB DET PROPN PUNCT ADV VERB DET ADJ NOUN PART VERB NOUN ADP PRON NOUN CCONJ ADV VERB PRON PUNCT DET NOUN VERB DET ADJ ADJ NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN NOUN PUNCT VERB ADV ADP DET ADJ PROPN PART VERB ADP DET NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN PUNCT PRON VERB PROPN PROPN ADP DET ADJ NOUN ADP NUM NOUN PUNCT VERB ADP NOUN NOUN NOUN ADP ADJ NOUN PUNCT VERB NOUN ADP DET NOUN NOUN ADJ ADP PROPN NUM CCONJ PROPN NUM PUNCT DET NOUN VERB SCONJ NOUN VERB ADP PROPN PROPN AUX VERB ADP DET ADJ NOUN CCONJ ADJ NOUN VERB ADP NOUN VERB VERB ADJ NUM NOUN NOUN PUNCT ADP ADJ PUNCT NOUN NOUN VERB ADP ADV NUM ADJ ADP DET VERB NOUN PUNCT PRON NOUN VERB DET NOUN ADP ADV VERB ADV NOUN ADP DET NOUN NOUN PUNCT ADP PROPN NUM PUNCT ADP NOUN NOUN ADP DET NOUN ADP PRON NOUN PUNCT ADJ NOUN PUNCT,0.6186046511627907,26.875,5.32093023255814
2,2,GPT-3.5,"["" Machine learning systems often lack the adaptive cognitive mechanisms observed in humans, employing a uniform model for both easy and challenging tasks. In contrast, humans engage in either fast (instinctive) or slow (analytical) thinking processes based on the problem's difficultyâ€”a phenomenon known as the dual-process theory of mind. In this paper, we introduce FLOWGEN, a graph generation model inspired by the dual-process theory. FLOWGEN adapts to the complexity of graph completion by employing either a FAST (weaker) or SLOW (stronger) module. While these modules share architectures, they differ in parameter count and generative power. Through experiments on real-world graphs, FLOWGEN demonstrates its ability to generate graphs comparable to those produced by a single large model, all while achieving up to a 2x speed improvement.""]",abstract_chunked," Machine learning systems often lack the adaptive cognitive mechanisms observed in humans, employing a uniform model for both easy and challenging tasks. In contrast, humans engage in either fast (instinctive) or slow (analytical) thinking processes based on the problem's difficultyâ€”a phenomenon known as the dual-process theory of mind. In this paper, we introduce FLOWGEN, a graph generation model inspired by the dual-process theory. FLOWGEN adapts to the complexity of graph completion by employing either a FAST (weaker) or SLOW (stronger) module. While these modules share architectures, they differ in parameter count and generative power. Through experiments on real-world graphs, FLOWGEN demonstrates its ability to generate graphs comparable to those produced by a single large model, all while achieving up to a 2x speed improvement.",27.475242782152264,2.6406588729545177,2,0.5540898442268372," Machine learning systems often lack the adaptive cognitive mechanisms observed in humans, employing a uniform model for both easy and challenging tasks. In contrast, humans engage in either fast or slow thinking processes based on the problems difficultya phenomenon known as the dual process theory of mind. In this paper, we introduce Propname, a graph generation model inspired by the dual process theory. FLOWGEN adapts to the complexity of graph completion by employing either a FAST or SLOW module. While these modules share architectures, they differ in parameter count and generative power. Through experiments on real world graphs, Propname demonstrates its ability to generate graphs comparable to those produced by a single large model, all while achieving up to a Propname speed improvement."," Machine learning systems often lack the adaptive cognitive mechanisms observed in humans, employing a uniform model for both easy and challenging tasks. In contrast, humans engage in either fast or slow thinking processes based on the problems difficultya phenomenon known as the dual process theory of mind. In this paper, we introduce Propname, a graph generation model inspired by the dual process theory. FLOWGEN adapts to the complexity of graph completion by employing either a FAST or SLOW module. While these modules share architectures, they differ in parameter count and generative power. Through experiments on real world graphs, Propname demonstrates its ability to generate graphs comparable to those produced by a single large model, all while achieving up to a Propname speed improvement.", NOUN NOUN NOUN ADV VERB DET ADJ ADJ NOUN VERB ADP NOUN PUNCT VERB DET ADJ NOUN ADP CCONJ ADJ CCONJ ADJ NOUN PUNCT ADP NOUN PUNCT NOUN VERB ADP CCONJ ADJ CCONJ ADJ NOUN NOUN VERB ADP DET NOUN ADJ NOUN VERB ADP DET ADJ NOUN NOUN ADP NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN NOUN NOUN VERB ADP DET ADJ NOUN NOUN PUNCT VERB NOUN ADP DET NOUN ADP NOUN NOUN ADP VERB CCONJ DET ADJ CCONJ ADJ NOUN PUNCT SCONJ DET NOUN VERB NOUN PUNCT PRON VERB ADP NOUN NOUN CCONJ ADJ NOUN PUNCT ADP NOUN ADP ADJ NOUN NOUN PUNCT PROPN VERB PRON NOUN PART VERB NOUN ADJ ADP PRON VERB ADP DET ADJ ADJ NOUN PUNCT PRON SCONJ VERB ADP ADP DET PROPN NOUN NOUN PUNCT,0.6838235294117647,22.666666666666668,5.007352941176471
3,3,GPT-3.5,"[' In this paper, we present an innovative method for curating high-quality comparable training data tailored for low-resource languages, employing monolingual annotators. Our approach centers around the strategic use of a meticulously chosen set of images, serving as a pivot between source and target languages. We independently collect captions for these images in both languages, thereby establishing a robust and comparable corpus. Human evaluations conducted on the English-Hindi comparable corpora generated through our method reveal a noteworthy 81.1% acceptability rate for translation pairs, with only 2.47% identified as non-translations. To underscore the utility of the dataset, we conduct experiments on two downstream tasksâ€”machine translation and dictionary extraction. The outcomes highlight the potential of our curated dataset. All code and data are openly accessible at https://github.com/madaan/PML4DC-Comparable-Data-Collection.']",abstract_chunked," In this paper, we present an innovative method for curating high-quality comparable training data tailored for low-resource languages, employing monolingual annotators. Our approach centers around the strategic use of a meticulously chosen set of images, serving as a pivot between source and target languages. We independently collect captions for these images in both languages, thereby establishing a robust and comparable corpus. Human evaluations conducted on the English-Hindi comparable corpora generated through our method reveal a noteworthy 81.1% acceptability rate for translation pairs, with only 2.47% identified as non-translations. To underscore the utility of the dataset, we conduct experiments on two downstream tasksâ€”machine translation and dictionary extraction. The outcomes highlight the potential of our curated dataset. All code and data are openly accessible at https://github.com/madaan/PML4DC-Comparable-Data-Collection.",7.895590551181101,2.6406588729545177,3,0.5191590189933777," In this paper, we present an innovative method for curating high quality comparable training data tailored for low resource languages, employing monolingual annotators. Our approach centers around the strategic use of a meticulously chosen set of images, serving as a pivot between source and target languages. We independently collect captions for these images in both languages, thereby establishing a robust and comparable Propname. Human evaluations conducted on the Propname Propname comparable Propname generated through our method reveal a noteworthy 00.0 acceptability rate for translation pairs, with only 0.00 identified as non translations. To underscore the utility of the dataset, we conduct experiments on two downstream tasksmachine translation and dictionary extraction. The outcomes highlight the potential of our curated dataset. All code and data are openly accessible at Propname Propname Propname."," In this paper, we present an innovative method for curating high quality comparable training data tailored for low resource languages, employing monolingual annotators. Our approach centers around the strategic use of a meticulously chosen set of images, serving as a pivot between source and target languages. We independently collect captions for these images in both languages, thereby establishing a robust and comparable Propname. Human evaluations conducted on the Propname Propname comparable Propname generated through our method reveal a noteworthy 00.0 acceptability rate for translation pairs, with only 0.00 identified as non translations. To underscore the utility of the dataset, we conduct experiments on two downstream tasksmachine translation and dictionary extraction. The outcomes highlight the potential of our curated dataset. All code and data are openly accessible at Propname Propname Propname.", ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP VERB ADJ NOUN ADJ NOUN NOUN VERB ADP ADJ NOUN NOUN PUNCT VERB ADJ NOUN PUNCT PRON NOUN NOUN ADP DET ADJ NOUN ADP DET ADV VERB NOUN ADP NOUN PUNCT VERB ADP DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT PRON ADV VERB NOUN ADP DET NOUN ADP DET NOUN PUNCT ADV VERB DET ADJ CCONJ ADJ PROPN PUNCT ADJ NOUN VERB ADP DET PROPN PROPN ADJ PROPN VERB ADP PRON NOUN VERB DET ADJ NUM NOUN NOUN ADP NOUN NOUN PUNCT ADP ADV NUM VERB ADP NOUN NOUN PUNCT PART VERB DET NOUN ADP DET NOUN PUNCT PRON VERB NOUN ADP NUM ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT DET NOUN VERB DET NOUN ADP PRON VERB NOUN PUNCT DET NOUN CCONJ NOUN AUX ADV ADJ ADP PROPN PROPN PROPN PUNCT,0.6783216783216783,20.428571428571427,5.5174825174825175
4,4,GPT-3.5,"[' This paper introduces a novel task in the domain of natural language processing – the task of politeness transfer. The objective is to convert non-polite sentences into polite equivalents while retaining the original meaning. To facilitate research in this area, we present a comprehensive dataset comprising over 1.39 million instances, meticulously labeled for politeness. This dataset serves as a foundation for benchmark evaluations, encouraging advancements in politeness transfer techniques. Our proposed approach involves a tag-and-generate pipeline, which first identifies stylistic attributes and then generates a sentence in the target style while preserving the majority of the source content. Extensive experiments demonstrate that our model outperforms existing state-of-the-art methods on automatic metrics related to content preservation, achieving comparable or superior performance in style transfer accuracy. Moreover, our model excels in human evaluations, demonstrating superior grammaticality, meaning preservation, and transfer accuracy across not only the politeness transfer task but also five other style transfer tasks. The code and dataset are made publicly available, fostering collaboration and further research in this emerging field.']",abstract_chunked," This paper introduces a novel task in the domain of natural language processing – the task of politeness transfer. The objective is to convert non-polite sentences into polite equivalents while retaining the original meaning. To facilitate research in this area, we present a comprehensive dataset comprising over 1.39 million instances, meticulously labeled for politeness. This dataset serves as a foundation for benchmark evaluations, encouraging advancements in politeness transfer techniques. Our proposed approach involves a tag-and-generate pipeline, which first identifies stylistic attributes and then generates a sentence in the target style while preserving the majority of the source content. Extensive experiments demonstrate that our model outperforms existing state-of-the-art methods on automatic metrics related to content preservation, achieving comparable or superior performance in style transfer accuracy. Moreover, our model excels in human evaluations, demonstrating superior grammaticality, meaning preservation, and transfer accuracy across not only the politeness transfer task but also five other style transfer tasks. The code and dataset are made publicly available, fostering collaboration and further research in this emerging field.",1.6344852941176953,2.6406588729545177,4,0.7235382199287415," This paper introduces a novel task in the domain of natural language processing the task of politeness transfer. The objective is to convert non polite sentences into polite equivalents while retaining the original meaning. To facilitate research in this area, we present a comprehensive dataset comprising over 0.00 million instances, meticulously labeled for politeness. This dataset serves as a foundation for benchmark evaluations, encouraging advancements in politeness transfer techniques. Our proposed approach involves a tag and generate pipeline, which first identifies stylistic attributes and then generates a sentence in the target style while preserving the majority of the source content. Extensive experiments demonstrate that our model outperforms existing state of the art methods on automatic metrics related to content preservation, achieving comparable or superior performance in style transfer accuracy. Moreover, our model excels in human evaluations, demonstrating superior grammaticality, meaning preservation, and transfer accuracy across not only the politeness transfer task but also five other style transfer tasks. The code and dataset are made publicly available, fostering collaboration and further research in this emerging field."," This paper introduces a novel task in the domain of natural language processing the task of politeness transfer. The objective is to convert non polite sentences into polite equivalents while retaining the original meaning. To facilitate research in this area, we present a comprehensive dataset comprising over 0.00 million instances, meticulously labeled for politeness. This dataset serves as a foundation for benchmark evaluations, encouraging advancements in politeness transfer techniques. Our proposed approach involves a tag and generate pipeline, which first identifies stylistic attributes and then generates a sentence in the target style while preserving the majority of the source content. Extensive experiments demonstrate that our model outperforms existing state of the art methods on automatic metrics related to content preservation, achieving comparable or superior performance in style transfer accuracy. Moreover, our model excels in human evaluations, demonstrating superior grammaticality, meaning preservation, and transfer accuracy across not only the politeness transfer task but also five other style transfer tasks. The code and dataset are made publicly available, fostering collaboration and further research in this emerging field.", DET NOUN VERB DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN VERB DET NOUN ADP NOUN NOUN PUNCT DET NOUN AUX PART VERB ADJ ADJ NOUN ADP ADJ NOUN SCONJ VERB DET ADJ NOUN PUNCT PART VERB NOUN ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN VERB ADP NUM NUM NOUN PUNCT ADV VERB ADP NOUN PUNCT DET NOUN VERB ADP DET NOUN ADP ADJ NOUN PUNCT VERB NOUN ADP NOUN NOUN NOUN PUNCT PRON VERB NOUN VERB DET NOUN CCONJ VERB NOUN PUNCT PRON ADV VERB ADJ NOUN CCONJ ADV VERB DET NOUN ADP DET NOUN NOUN SCONJ VERB DET NOUN ADP DET NOUN NOUN PUNCT ADJ NOUN VERB SCONJ PRON NOUN VERB VERB NOUN ADP DET NOUN NOUN ADP ADJ NOUN VERB ADP NOUN NOUN PUNCT VERB ADJ CCONJ ADJ NOUN ADP NOUN NOUN NOUN PUNCT ADV PUNCT PRON NOUN NOUN ADP ADJ NOUN PUNCT VERB ADJ NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN ADP PART ADV DET NOUN NOUN NOUN CCONJ ADV NUM ADJ NOUN NOUN NOUN PUNCT DET NOUN CCONJ NOUN AUX VERB ADV ADJ PUNCT VERB NOUN CCONJ ADJ NOUN ADP DET VERB NOUN PUNCT,0.6373056994818653,24.125,5.637305699481865
5,5,GPT-3.5,"[' Understanding processes involves the critical task of reasoning about events and tracking their influences. In this paper, we introduce EIGEN, a novel method harnessing pre-trained language models to generate event influences based on context, the nature of influence, and their position in a reasoning chain. We present a new dataset tailored for researching and evaluating event influence generation methods. EIGEN surpasses robust baselines, demonstrating superior performance in automated evaluation metrics, achieving a remarkable 10 ROUGE points improvement. Human judgments affirm the excellence of EIGEN in terms of both closeness to reference and the relevance of generated content. Additionally, we showcase that the event influences produced by EIGEN significantly enhance performance on a ""what-if"" Question Answering (WIQA) benchmark, exhibiting an impressive over 3% increase in F1 score. This improvement is particularly notable for questions demanding background knowledge and multi-hop reasoning.']",abstract_chunked," Understanding processes involves the critical task of reasoning about events and tracking their influences. In this paper, we introduce EIGEN, a novel method harnessing pre-trained language models to generate event influences based on context, the nature of influence, and their position in a reasoning chain. We present a new dataset tailored for researching and evaluating event influence generation methods. EIGEN surpasses robust baselines, demonstrating superior performance in automated evaluation metrics, achieving a remarkable 10 ROUGE points improvement. Human judgments affirm the excellence of EIGEN in terms of both closeness to reference and the relevance of generated content. Additionally, we showcase that the event influences produced by EIGEN significantly enhance performance on a ""what-if"" Question Answering (WIQA) benchmark, exhibiting an impressive over 3% increase in F1 score. This improvement is particularly notable for questions demanding background knowledge and multi-hop reasoning.",15.390000000000015,2.6406588729545177,5,0.8436278700828552," Understanding processes involves the critical task of reasoning about events and tracking their influences. In this paper, we introduce Propname, a novel method harnessing pre trained language models to generate event influences based on context, the nature of influence, and their position in a reasoning chain. We present a new dataset tailored for researching and evaluating event influence generation methods. EIGEN surpasses robust baselines, demonstrating superior performance in automated evaluation metrics, achieving a remarkable 00 Propname points improvement. Human judgments affirm the excellence of Propname in terms of both closeness to reference and the relevance of generated content. Additionally, we showcase that the event influences produced by Propname significantly enhance performance on a what if Propname Propname benchmark, exhibiting an impressive over 0 increase in Propname score. This improvement is particularly notable for questions demanding background knowledge and multi hop reasoning."," Understanding processes involves the critical task of reasoning about events and tracking their influences. In this paper, we introduce Propname, a novel method harnessing pre trained language models to generate event influences based on context, the nature of influence, and their position in a reasoning chain. We present a new dataset tailored for researching and evaluating event influence generation methods. EIGEN surpasses robust baselines, demonstrating superior performance in automated evaluation metrics, achieving a remarkable 00 Propname points improvement. Human judgments affirm the excellence of Propname in terms of both closeness to reference and the relevance of generated content. Additionally, we showcase that the event influences produced by Propname significantly enhance performance on a what if Propname Propname benchmark, exhibiting an impressive over 0 increase in Propname score. This improvement is particularly notable for questions demanding background knowledge and multi hop reasoning.", NOUN NOUN VERB DET ADJ NOUN ADP VERB ADP NOUN CCONJ VERB PRON NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN VERB ADJ VERB NOUN NOUN PART VERB NOUN NOUN VERB ADP NOUN PUNCT DET NOUN ADP NOUN PUNCT CCONJ PRON NOUN ADP DET NOUN NOUN PUNCT PRON VERB DET ADJ NOUN VERB ADP VERB CCONJ VERB NOUN NOUN NOUN NOUN PUNCT NOUN VERB ADJ NOUN PUNCT VERB ADJ NOUN ADP VERB NOUN NOUN PUNCT VERB DET ADJ NOUN PROPN NOUN NOUN PUNCT ADJ NOUN VERB DET NOUN ADP PROPN ADP NOUN ADP DET NOUN ADP NOUN CCONJ DET NOUN ADP VERB NOUN PUNCT ADV PUNCT PRON VERB SCONJ DET NOUN NOUN VERB ADP PROPN ADV VERB NOUN ADP DET PRON SCONJ PROPN PROPN NOUN PUNCT VERB DET ADJ ADP NUM NOUN ADP PROPN NOUN PUNCT DET NOUN AUX ADV ADJ ADP NOUN VERB NOUN NOUN CCONJ VERB NOUN NOUN PUNCT,0.6666666666666666,22.285714285714285,5.641025641025641
6,6,GPT-3.5,"[' Recent advancements in adapting transformers for large-scale image classification have challenged the longstanding dominance of convolutional neural networks (CNNs). This paper focuses on building and optimizing deeper transformer networks specifically tailored for image classification tasks. Through a thorough exploration of the interplay between architecture and optimization strategies, two key modifications are introduced, resulting in a significant improvement in the accuracy of deep transformers. These architectural enhancements not only prevent early saturation of performance with increased depth but also lead to the development of models achieving a remarkable 86.5% top-1 accuracy on ImageNet without external data. This accomplishment establishes a new state-of-the-art performance, surpassing existing models in terms of both computational efficiency (FLOPs) and model parameters. Furthermore, the proposed model attains superior results on ImageNet with Reassessed labels and ImageNet-V2/match frequency, showcasing its robustness in a setting with no additional training data. The code and models developed in this study are shared with the research community.']",abstract_chunked," Recent advancements in adapting transformers for large-scale image classification have challenged the longstanding dominance of convolutional neural networks (CNNs). This paper focuses on building and optimizing deeper transformer networks specifically tailored for image classification tasks. Through a thorough exploration of the interplay between architecture and optimization strategies, two key modifications are introduced, resulting in a significant improvement in the accuracy of deep transformers. These architectural enhancements not only prevent early saturation of performance with increased depth but also lead to the development of models achieving a remarkable 86.5% top-1 accuracy on ImageNet without external data. This accomplishment establishes a new state-of-the-art performance, surpassing existing models in terms of both computational efficiency (FLOPs) and model parameters. Furthermore, the proposed model attains superior results on ImageNet with Reassessed labels and ImageNet-V2/match frequency, showcasing its robustness in a setting with no additional training data. The code and models developed in this study are shared with the research community.",-0.8028481012657949,2.6406588729545177,6,0.5565051436424255," Recent advancements in adapting transformers for large scale image classification have challenged the longstanding dominance of convolutional neural networks. This paper focuses on building and optimizing deeper transformer networks specifically tailored for image classification tasks. Through a thorough exploration of the interplay between architecture and optimization strategies, two key modifications are introduced, resulting in a significant improvement in the accuracy of deep transformers. These architectural enhancements not only prevent early saturation of performance with increased depth but also lead to the development of models achieving a remarkable 00.0 top 0 accuracy on Propname without external data. This accomplishment establishes a new state of the art performance, surpassing existing models in terms of both computational efficiency and model parameters. Furthermore, the proposed model attains superior results on Propname with Propname labels and Propname V0match frequency, showcasing its robustness in a setting with no additional training data. The code and models developed in this study are shared with the research community."," Recent advancements in adapting transformers for large scale image classification have challenged the longstanding dominance of convolutional neural networks. This paper focuses on building and optimizing deeper transformer networks specifically tailored for image classification tasks. Through a thorough exploration of the interplay between architecture and optimization strategies, two key modifications are introduced, resulting in a significant improvement in the accuracy of deep transformers. These architectural enhancements not only prevent early saturation of performance with increased depth but also lead to the development of models achieving a remarkable 00.0 top 0 accuracy on Propname without external data. This accomplishment establishes a new state of the art performance, surpassing existing models in terms of both computational efficiency and model parameters. Furthermore, the proposed model attains superior results on Propname with Propname labels and Propname V0match frequency, showcasing its robustness in a setting with no additional training data. The code and models developed in this study are shared with the research community.", ADJ NOUN ADP VERB NOUN ADP ADJ NOUN NOUN NOUN AUX VERB DET ADJ NOUN ADP ADJ ADJ NOUN PUNCT DET NOUN VERB ADP VERB CCONJ VERB ADJ NOUN NOUN ADV VERB ADP NOUN NOUN NOUN PUNCT ADP DET ADJ NOUN ADP DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT NUM ADJ NOUN AUX VERB PUNCT VERB ADP DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN PUNCT DET ADJ NOUN PART ADV VERB ADJ NOUN ADP NOUN ADP VERB NOUN CCONJ ADV VERB ADP DET NOUN ADP NOUN VERB DET ADJ NUM ADJ NUM NOUN ADP PROPN ADP ADJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP DET NOUN NOUN PUNCT VERB VERB NOUN ADP NOUN ADP PRON ADJ NOUN CCONJ NOUN NOUN PUNCT ADV PUNCT DET VERB NOUN VERB ADJ NOUN ADP PROPN ADP PROPN NOUN CCONJ PROPN NOUN NOUN PUNCT VERB PRON NOUN ADP DET NOUN ADP DET ADJ NOUN NOUN PUNCT DET NOUN CCONJ NOUN VERB ADP DET NOUN AUX VERB ADP DET NOUN NOUN PUNCT,0.672514619883041,24.428571428571427,5.8654970760233915
7,7,GPT-3.5,"[' This paper introduces a straightforward architecture designed to tackle unpaired image-to-image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed-weight image autoencoder architecture. We introduce a task-specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof-of-concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters.']",abstract_chunked," This paper introduces a straightforward architecture designed to tackle unpaired image-to-image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed-weight image autoencoder architecture. We introduce a task-specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof-of-concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters.",-7.1413636363636215,2.6406588729545177,7,0.6973000764846802," This paper introduces a straightforward architecture designed to tackle unpaired image to image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed weight image autoencoder architecture. We introduce a task specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof of concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters."," This paper introduces a straightforward architecture designed to tackle unpaired image to image translation tasks, encompassing challenges such as style or class transfer, denoising, deblurring, deblocking, among others. Our approach builds upon a fixed weight image autoencoder architecture. We introduce a task specific residual block operating in the latent space, iteratively applied until the desired transformation is achieved. A carefully designed training schedule mitigates the exponentiation effect of iterations. During testing, our method offers several advantages, including limited weight parameters and a compositional design enabling the modulation of transformation strength based on the number of iterations. This flexibility proves valuable, especially when the type or amount of noise to suppress is unknown a priori. Experimental validations demonstrate the efficacy of our approach through proof of concept applications, showcasing comparable or superior performance to CycleGAN with significantly fewer parameters.", DET NOUN VERB DET ADJ NOUN VERB PART VERB ADJ NOUN ADP NOUN NOUN NOUN PUNCT VERB NOUN ADJ ADP NOUN CCONJ NOUN NOUN PUNCT NOUN PUNCT VERB PUNCT VERB PUNCT ADP NOUN PUNCT PRON NOUN VERB SCONJ DET VERB NOUN NOUN NOUN NOUN PUNCT PRON VERB DET NOUN ADJ ADJ NOUN VERB ADP DET ADJ NOUN PUNCT ADV VERB SCONJ DET VERB NOUN AUX VERB PUNCT DET ADV VERB NOUN NOUN NOUN DET NOUN NOUN ADP NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB ADJ NOUN PUNCT VERB ADJ NOUN NOUN CCONJ DET ADJ NOUN VERB DET NOUN ADP NOUN NOUN VERB ADP DET NOUN ADP NOUN PUNCT DET NOUN VERB ADJ PUNCT ADV SCONJ DET NOUN CCONJ NOUN ADP NOUN PART VERB AUX ADJ DET X PUNCT ADJ NOUN VERB DET NOUN ADP PRON NOUN ADP NOUN ADP NOUN NOUN PUNCT VERB ADJ CCONJ ADJ NOUN ADP NOUN ADP ADV ADJ NOUN PUNCT,0.6967741935483871,22.142857142857142,5.806451612903226
8,8,GPT-3.5,"[' In this paper, we introduce Llama 2, a comprehensive collection of pretrained and fine-tuned large language models (LLMs) with parameter scales ranging from 7 billion to 70 billion. Specifically, we present Llama 2-Chat, a set of fine-tuned models optimized for dialogue applications. Through extensive benchmark testing, we demonstrate that our models consistently outperform existing open-source chat models. Human evaluations on helpfulness and safety suggest that Llama 2-Chat may serve as a viable alternative to closed-source models. We provide a detailed account of our fine-tuning methodology and safety enhancements, aiming to empower the community to build upon our work and contribute to the responsible development of large language models.']",abstract_chunked," In this paper, we introduce Llama 2, a comprehensive collection of pretrained and fine-tuned large language models (LLMs) with parameter scales ranging from 7 billion to 70 billion. Specifically, we present Llama 2-Chat, a set of fine-tuned models optimized for dialogue applications. Through extensive benchmark testing, we demonstrate that our models consistently outperform existing open-source chat models. Human evaluations on helpfulness and safety suggest that Llama 2-Chat may serve as a viable alternative to closed-source models. We provide a detailed account of our fine-tuning methodology and safety enhancements, aiming to empower the community to build upon our work and contribute to the responsible development of large language models.",17.61227272727274,2.6406588729545177,8,0.7322891354560852," In this paper, we introduce Propname 0, a comprehensive collection of pretrained and fine tuned large language models with parameter scales ranging from 0 billion to 00 billion. Specifically, we present Propname 0 Propname, a set of fine tuned models optimized for dialogue applications. Through extensive benchmark testing, we demonstrate that our models consistently outperform existing open source chat models. Human evaluations on helpfulness and safety suggest that Propname 0 Chat may serve as a viable alternative to closed source models. We provide a detailed account of our fine tuning methodology and safety enhancements, aiming to empower the community to build upon our work and contribute to the responsible development of large language models."," In this paper, we introduce Propname 0, a comprehensive collection of pretrained and fine tuned large language models with parameter scales ranging from 0 billion to 00 billion. Specifically, we present Propname 0 Propname, a set of fine tuned models optimized for dialogue applications. Through extensive benchmark testing, we demonstrate that our models consistently outperform existing open source chat models. Human evaluations on helpfulness and safety suggest that Propname 0 Chat may serve as a viable alternative to closed source models. We provide a detailed account of our fine tuning methodology and safety enhancements, aiming to empower the community to build upon our work and contribute to the responsible development of large language models.", ADP DET NOUN PUNCT PRON VERB PROPN NUM PUNCT DET ADJ NOUN ADP VERB CCONJ ADJ VERB ADJ NOUN NOUN ADP NOUN NOUN VERB ADP NUM NUM ADP NUM NUM PUNCT ADV PUNCT PRON VERB PROPN NUM PROPN PUNCT DET NOUN ADP ADJ VERB NOUN VERB ADP NOUN NOUN PUNCT ADP ADJ NOUN NOUN PUNCT PRON VERB SCONJ PRON NOUN ADV VERB VERB ADJ NOUN NOUN NOUN PUNCT ADJ NOUN ADP NOUN CCONJ NOUN VERB SCONJ PROPN NUM NOUN AUX VERB ADP DET ADJ NOUN ADP VERB NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP PRON ADJ NOUN NOUN CCONJ NOUN NOUN PUNCT VERB PART VERB DET NOUN PART VERB SCONJ PRON NOUN CCONJ VERB ADP DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT,0.624,25.0,5.16
9,9,GPT-3.5,"["" The surge in availability of large neuroimaging databases presents unprecedented opportunities for training more efficient machine learning algorithms. However, these databases often exhibit variability stemming from diverse sources such as age, gender, and acquisition parameters. These nuisance variables can significantly impact the performance of classification methods, leading to potential misinterpretation of their behavior. This paper focuses on addressing the challenge of integrating data from different databases. Initial experiments on simulated data demonstrate how interactions with confounding variables, such as age, can pose challenges for the adjustment of data from multiple sources. Subsequently, we compare three strategies for data adjustment and evaluate their efficacy in the context of a Computer-Aided Diagnosis (CAD) system. The CAD system discriminates between healthy and Alzheimer's Disease (AD) subjects based on volumetric characteristics derived from structural MRI.""]",abstract_chunked," The surge in availability of large neuroimaging databases presents unprecedented opportunities for training more efficient machine learning algorithms. However, these databases often exhibit variability stemming from diverse sources such as age, gender, and acquisition parameters. These nuisance variables can significantly impact the performance of classification methods, leading to potential misinterpretation of their behavior. This paper focuses on addressing the challenge of integrating data from different databases. Initial experiments on simulated data demonstrate how interactions with confounding variables, such as age, can pose challenges for the adjustment of data from multiple sources. Subsequently, we compare three strategies for data adjustment and evaluate their efficacy in the context of a Computer-Aided Diagnosis (CAD) system. The CAD system discriminates between healthy and Alzheimer's Disease (AD) subjects based on volumetric characteristics derived from structural MRI.",5.078167938931301,2.6406588729545177,9,0.28918057680130005," The surge in availability of large neuroimaging databases presents unprecedented opportunities for training more efficient machine learning algorithms. However, these databases often exhibit variability stemming from diverse sources such as age, gender, and acquisition parameters. These nuisance variables can significantly impact the performance of classification methods, leading to potential misinterpretation of their behavior. This paper focuses on addressing the challenge of integrating data from different databases. Initial experiments on simulated data demonstrate how interactions with confounding variables, such as age, can pose challenges for the adjustment of data from multiple sources. Subsequently, we compare three strategies for data adjustment and evaluate their efficacy in the context of a Propname Propname Propname system. The Propname system discriminates between healthy and Propname Propname subjects based on volumetric characteristics derived from structural Propname."," The surge in availability of large neuroimaging databases presents unprecedented opportunities for training more efficient machine learning algorithms. However, these databases often exhibit variability stemming from diverse sources such as age, gender, and acquisition parameters. These nuisance variables can significantly impact the performance of classification methods, leading to potential misinterpretation of their behavior. This paper focuses on addressing the challenge of integrating data from different databases. Initial experiments on simulated data demonstrate how interactions with confounding variables, such as age, can pose challenges for the adjustment of data from multiple sources. Subsequently, we compare three strategies for data adjustment and evaluate their efficacy in the context of a Propname Propname Propname system. The Propname system discriminates between healthy and Propname Propname subjects based on volumetric characteristics derived from structural Propname.", DET NOUN ADP NOUN ADP ADJ VERB NOUN VERB ADJ NOUN ADP VERB ADV ADJ NOUN NOUN NOUN PUNCT ADV PUNCT DET NOUN ADV VERB NOUN VERB ADP ADJ NOUN ADJ ADP NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN PUNCT DET ADJ NOUN AUX ADV VERB DET NOUN ADP NOUN NOUN PUNCT VERB ADP ADJ NOUN ADP PRON NOUN PUNCT DET NOUN VERB ADP VERB DET NOUN ADP VERB NOUN ADP ADJ NOUN PUNCT ADJ NOUN ADP VERB NOUN VERB SCONJ NOUN ADP VERB NOUN PUNCT ADJ ADP NOUN PUNCT AUX VERB NOUN ADP DET NOUN ADP NOUN ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB NUM NOUN ADP NOUN NOUN CCONJ VERB PRON NOUN ADP DET NOUN ADP DET PROPN PROPN PROPN NOUN PUNCT DET PROPN NOUN VERB ADP ADJ CCONJ PROPN PROPN NOUN VERB ADP NOUN NOUN VERB ADP ADJ PROPN PUNCT,0.6458333333333334,20.571428571428573,6.041666666666667
10,10,GPT-3.5,"[' In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever-expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as AlexNet, ResNet, and GPT, coupled with well-crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Transformer architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks (convnets). Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi-layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in-depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the ImageNet dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness.']",abstract_chunked," In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever-expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as AlexNet, ResNet, and GPT, coupled with well-crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Transformer architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks (convnets). Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi-layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in-depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the ImageNet dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness.",-7.259345238095193,2.6406588729545177,10,0.5303925275802612," In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as Propname, ResNet, and Propname, coupled with well crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Propname architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks. Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the Propname dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness."," In the contemporary landscape, the influence of machine learning, particularly deep learning, is ever expanding, contributing significantly to various societal domains. Notably, its impact is pronounced in natural language processing, exemplified by applications like hate speech detection and document summarization. Similarly, in computer vision, deep learning plays a pivotal role in enhancing image interpretation, medical diagnosis, and advancements in autonomous driving. The success of deep learning is often attributed to iconic architectures such as Propname, ResNet, and Propname, coupled with well crafted optimization procedures. This thesis delves into the intricate relationship between architectures and training procedures, focusing specifically on the application of Propname architectures to visual understanding. Presently, training procedures for transformers are less mature compared to those for convolutional networks. Recognizing that training is instrumental in overcoming the inherent architectural limitations of transformers, this research hones in on developing procedures capable of achieving compelling performance for transformers and even simpler architectures resembling multi layer perceptrons. The exploration commences by investigating the viability of learning with coarse labels through a modification of the training procedure. Subsequently, various architectures for computer vision are scrutinized, encompassing an in depth analysis of their features, advantages, drawbacks, and optimal training methodologies. The research culminates in an examination of the intricate interplay between architecture and the training process. All proposed approaches are rigorously evaluated through image classification on the Propname dataset and transfer learning. Furthermore, the methods are extended to additional tasks such as semantic segmentation, providing a comprehensive assessment of their effectiveness.", ADP DET ADJ NOUN PUNCT DET NOUN ADP NOUN NOUN PUNCT ADV ADJ NOUN PUNCT AUX ADV VERB PUNCT VERB ADV ADP ADJ ADJ NOUN PUNCT ADV PUNCT PRON NOUN AUX VERB ADP ADJ NOUN NOUN PUNCT VERB ADP NOUN ADP NOUN NOUN NOUN CCONJ NOUN NOUN PUNCT ADV PUNCT ADP NOUN NOUN PUNCT ADJ NOUN VERB DET ADJ NOUN ADP VERB NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN ADP ADJ NOUN PUNCT DET NOUN ADP ADJ NOUN AUX ADV VERB ADP ADJ NOUN ADJ ADP PROPN PUNCT NOUN PUNCT CCONJ PROPN PUNCT VERB ADP ADV VERB NOUN NOUN PUNCT DET NOUN VERB ADP DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT VERB ADV ADP DET NOUN ADP PROPN VERB ADP ADJ NOUN PUNCT ADV PUNCT NOUN NOUN ADP NOUN AUX ADV ADJ VERB ADP PRON ADP ADJ NOUN PUNCT VERB SCONJ NOUN AUX ADJ ADP VERB DET ADJ ADJ NOUN ADP NOUN PUNCT DET NOUN NOUN ADP ADP VERB NOUN ADJ ADP VERB ADJ NOUN ADP NOUN CCONJ ADV ADJ NOUN VERB ADJ NOUN NOUN PUNCT DET NOUN NOUN ADP VERB DET NOUN ADP VERB ADP ADJ NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT ADV PUNCT ADJ NOUN ADP NOUN NOUN AUX VERB PUNCT VERB DET ADP NOUN NOUN ADP PRON NOUN PUNCT NOUN PUNCT NOUN PUNCT CCONJ ADJ NOUN NOUN PUNCT DET NOUN VERB ADP DET NOUN ADP DET ADJ NOUN ADP NOUN CCONJ DET NOUN NOUN PUNCT DET VERB NOUN AUX ADV VERB ADP NOUN NOUN ADP DET PROPN NOUN CCONJ VERB NOUN PUNCT ADV PUNCT DET NOUN AUX VERB ADP ADJ NOUN ADJ ADP ADJ NOUN PUNCT VERB DET ADJ NOUN ADP PRON NOUN PUNCT,0.5653710247349824,23.583333333333332,5.992932862190813
11,11,GPT-3.5,"[' This paper introduces ResMLP, a novel architecture for image classification based entirely on multi-layer perceptrons (MLPs). ResMLP is designed as a simple residual network, leveraging alternating layers that enable interaction among image patches and channels. This interaction occurs independently and identically, leading to a streamlined yet effective approach. Through modern training strategies incorporating extensive data augmentation and optional distillation, ResMLP demonstrates remarkable accuracy-to-complexity tradeoffs on the ImageNet dataset. The paper also explores self-supervised training of ResMLP models, further diminishing reliance on labeled datasets. Additionally, by adapting the ResMLP architecture to machine translation tasks, the paper showcases unexpectedly strong performance. Pre-trained models and code, implemented using the Timm library, are shared to facilitate broader exploration and application of ResMLP.']",abstract_chunked," This paper introduces ResMLP, a novel architecture for image classification based entirely on multi-layer perceptrons (MLPs). ResMLP is designed as a simple residual network, leveraging alternating layers that enable interaction among image patches and channels. This interaction occurs independently and identically, leading to a streamlined yet effective approach. Through modern training strategies incorporating extensive data augmentation and optional distillation, ResMLP demonstrates remarkable accuracy-to-complexity tradeoffs on the ImageNet dataset. The paper also explores self-supervised training of ResMLP models, further diminishing reliance on labeled datasets. Additionally, by adapting the ResMLP architecture to machine translation tasks, the paper showcases unexpectedly strong performance. Pre-trained models and code, implemented using the Timm library, are shared to facilitate broader exploration and application of ResMLP.",-6.002118644067764,2.6406588729545177,11,0.8484680652618408," This paper introduces Propname, a novel architecture for image classification based entirely on multi layer perceptrons. Propname is designed as a simple residual network, leveraging alternating layers that enable interaction among image patches and channels. This interaction occurs independently and identically, leading to a streamlined yet effective approach. Through modern training strategies incorporating extensive data augmentation and optional distillation, Propname demonstrates remarkable accuracy to complexity tradeoffs on the Propname dataset. The paper also explores self supervised training of Propname models, further diminishing reliance on labeled datasets. Additionally, by adapting the Propname architecture to machine translation tasks, the paper showcases unexpectedly strong performance. Pre trained models and code, implemented using the Propname library, are shared to facilitate broader exploration and application of Propname."," This paper introduces Propname, a novel architecture for image classification based entirely on multi layer perceptrons. Propname is designed as a simple residual network, leveraging alternating layers that enable interaction among image patches and channels. This interaction occurs independently and identically, leading to a streamlined yet effective approach. Through modern training strategies incorporating extensive data augmentation and optional distillation, Propname demonstrates remarkable accuracy to complexity tradeoffs on the Propname dataset. The paper also explores self supervised training of Propname models, further diminishing reliance on labeled datasets. Additionally, by adapting the Propname architecture to machine translation tasks, the paper showcases unexpectedly strong performance. Pre trained models and code, implemented using the Propname library, are shared to facilitate broader exploration and application of Propname.", DET NOUN VERB PROPN PUNCT DET ADJ NOUN ADP NOUN NOUN VERB ADV ADP ADJ NOUN NOUN PUNCT PROPN AUX VERB ADP DET ADJ ADJ NOUN PUNCT VERB VERB NOUN PRON VERB NOUN ADP NOUN NOUN CCONJ NOUN PUNCT DET NOUN VERB ADV CCONJ ADV PUNCT VERB ADP DET ADJ ADV ADJ NOUN PUNCT ADP ADJ NOUN NOUN VERB ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT PROPN VERB ADJ NOUN ADP NOUN NOUN ADP DET PROPN NOUN PUNCT DET NOUN ADV VERB NOUN ADJ NOUN ADP PROPN NOUN PUNCT ADV VERB NOUN ADP VERB NOUN PUNCT ADV PUNCT ADP VERB DET PROPN NOUN ADP NOUN NOUN NOUN PUNCT DET NOUN VERB ADV ADJ NOUN PUNCT ADJ VERB NOUN CCONJ NOUN PUNCT VERB VERB DET PROPN NOUN PUNCT AUX VERB PART VERB ADJ NOUN CCONJ NOUN ADP PROPN PUNCT,0.6811594202898551,19.714285714285715,6.043478260869565
12,12,GPT-3.5,"[' Knowledge Graph Completion (KGC) endeavors to autonomously predict missing links within expansive knowledge graphs. Recent years have seen a surge in state-of-the-art KGC techniques presented at top conferences across various research domains, including data mining, machine learning, and natural language processing. However, a notable trend reveals that several recent papers boast exceedingly high performance, surpassing established state-of-the-art methods. In this paper, we identify that this phenomenon can be attributed to the adoption of inappropriate evaluation protocols. We propose a straightforward yet effective evaluation protocol to rectify this issue, designed to handle model bias and mitigate its impact on final results. Extensive experiments are conducted, presenting the performance of numerous existing methods using our proposed protocol. We have made the reproducible code publicly available to facilitate transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques.']",abstract_chunked," Knowledge Graph Completion (KGC) endeavors to autonomously predict missing links within expansive knowledge graphs. Recent years have seen a surge in state-of-the-art KGC techniques presented at top conferences across various research domains, including data mining, machine learning, and natural language processing. However, a notable trend reveals that several recent papers boast exceedingly high performance, surpassing established state-of-the-art methods. In this paper, we identify that this phenomenon can be attributed to the adoption of inappropriate evaluation protocols. We propose a straightforward yet effective evaluation protocol to rectify this issue, designed to handle model bias and mitigate its impact on final results. Extensive experiments are conducted, presenting the performance of numerous existing methods using our proposed protocol. We have made the reproducible code publicly available to facilitate transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques.",4.22970588235296,2.6406588729545177,12,0.37581294775009155," Propname Propname Propname endeavors to autonomously predict missing links within expansive knowledge graphs. Recent years have seen a surge in state of the art Propname techniques presented at top conferences across various research domains, including data mining, machine learning, and natural language processing. However, a notable trend reveals that several recent papers boast exceedingly high performance, surpassing established state of the art methods. In this paper, we identify that this phenomenon can be attributed to the adoption of inappropriate evaluation protocols. We propose a straightforward yet effective evaluation protocol to rectify this issue, designed to handle model bias and mitigate its impact on final results. Extensive experiments are conducted, presenting the performance of numerous existing methods using our proposed protocol. We have made the reproducible code publicly available to facilitate transparency and reproducibility in the evaluation of Propname Propname Propname techniques."," Propname Propname Propname endeavors to autonomously predict missing links within expansive knowledge graphs. Recent years have seen a surge in state of the art Propname techniques presented at top conferences across various research domains, including data mining, machine learning, and natural language processing. However, a notable trend reveals that several recent papers boast exceedingly high performance, surpassing established state of the art methods. In this paper, we identify that this phenomenon can be attributed to the adoption of inappropriate evaluation protocols. We propose a straightforward yet effective evaluation protocol to rectify this issue, designed to handle model bias and mitigate its impact on final results. Extensive experiments are conducted, presenting the performance of numerous existing methods using our proposed protocol. We have made the reproducible code publicly available to facilitate transparency and reproducibility in the evaluation of Propname Propname Propname techniques.", PROPN PROPN PROPN VERB PART ADV VERB ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADJ NOUN AUX VERB DET NOUN ADP NOUN ADP DET NOUN PROPN NOUN VERB ADP ADJ NOUN ADP ADJ NOUN NOUN PUNCT VERB NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ ADJ NOUN NOUN PUNCT ADV PUNCT DET ADJ NOUN VERB SCONJ ADJ ADJ NOUN VERB ADV ADJ NOUN PUNCT VERB VERB NOUN ADP DET NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ DET NOUN AUX AUX VERB ADP DET NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB DET ADJ CCONJ ADJ NOUN NOUN PART VERB DET NOUN PUNCT VERB PART VERB NOUN NOUN CCONJ VERB PRON NOUN ADP ADJ NOUN PUNCT ADJ NOUN AUX VERB PUNCT VERB DET NOUN ADP ADJ VERB NOUN VERB PRON VERB NOUN PUNCT PRON AUX VERB DET ADJ NOUN ADV ADJ PART VERB NOUN CCONJ NOUN ADP DET NOUN ADP PROPN PROPN PROPN NOUN PUNCT,0.6794871794871795,22.285714285714285,5.660256410256411
13,13,GPT-3.5,"[' Autoregressive sequence models, while excelling in performance, suffer from significant latency during inference, prompting the exploration of non-autoregressive alternatives. However, existing non-autoregressive models, assuming conditional independence during token decoding, often produce inconsistent outputs, leading to inferior accuracy compared to autoregressive counterparts. To address this, we propose incorporating a structured inference module into non-autoregressive models. Specifically, we introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models. Additionally, we present a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation showcase that our model, while incurring minimal latency (8âˆ¼14ms), achieves significantly improved translation performance compared to previous non-autoregressive models across different datasets. Notably, on the WMT14 En-De dataset, our model attains a BLEU score of 26.80, outperforming prior non-autoregressive baselines and trailing purely autoregressive models by only 0.61 in BLEU.']",abstract_chunked," Autoregressive sequence models, while excelling in performance, suffer from significant latency during inference, prompting the exploration of non-autoregressive alternatives. However, existing non-autoregressive models, assuming conditional independence during token decoding, often produce inconsistent outputs, leading to inferior accuracy compared to autoregressive counterparts. To address this, we propose incorporating a structured inference module into non-autoregressive models. Specifically, we introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models. Additionally, we present a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation showcase that our model, while incurring minimal latency (8âˆ¼14ms), achieves significantly improved translation performance compared to previous non-autoregressive models across different datasets. Notably, on the WMT14 En-De dataset, our model attains a BLEU score of 26.80, outperforming prior non-autoregressive baselines and trailing purely autoregressive models by only 0.61 in BLEU.",-16.82167832167829,2.6406588729545177,13,0.36588218808174133," Autoregressive sequence models, while excelling in performance, suffer from significant latency during inference, prompting the exploration of non autoregressive alternatives. However, existing non autoregressive models, assuming conditional independence during token decoding, often produce inconsistent outputs, leading to inferior accuracy compared to autoregressive counterparts. To address this, we propose incorporating a structured inference module into non autoregressive models. Specifically, we introduce an efficient approximation for Propname Propname Propname tailored for non autoregressive sequence models. Additionally, we present a dynamic transition technique to capture positional contexts within the Propname. Experimental results in machine translation showcase that our model, while incurring minimal latency, achieves significantly improved translation performance compared to previous non autoregressive models across different datasets. Notably, on the Propname Propname Propname dataset, our model attains a Propname score of 00.00, outperforming prior non autoregressive baselines and trailing purely autoregressive models by only 0.00 in Propname."," Autoregressive sequence models, while excelling in performance, suffer from significant latency during inference, prompting the exploration of non autoregressive alternatives. However, existing non autoregressive models, assuming conditional independence during token decoding, often produce inconsistent outputs, leading to inferior accuracy compared to autoregressive counterparts. To address this, we propose incorporating a structured inference module into non autoregressive models. Specifically, we introduce an efficient approximation for Propname Propname Propname tailored for non autoregressive sequence models. Additionally, we present a dynamic transition technique to capture positional contexts within the Propname. Experimental results in machine translation showcase that our model, while incurring minimal latency, achieves significantly improved translation performance compared to previous non autoregressive models across different datasets. Notably, on the Propname Propname Propname dataset, our model attains a Propname score of 00.00, outperforming prior non autoregressive baselines and trailing purely autoregressive models by only 0.00 in Propname.", ADJ NOUN NOUN PUNCT SCONJ VERB ADP NOUN PUNCT VERB ADP ADJ NOUN ADP NOUN PUNCT VERB DET NOUN ADP ADJ ADJ NOUN PUNCT ADV PUNCT VERB ADJ ADJ NOUN PUNCT VERB ADJ NOUN ADP ADJ VERB PUNCT ADV VERB ADJ NOUN PUNCT VERB ADP ADJ NOUN VERB ADP ADJ NOUN PUNCT PART VERB PRON PUNCT PRON VERB VERB DET ADJ NOUN NOUN ADP ADJ ADJ NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADP PROPN PROPN PROPN VERB ADP ADJ ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN NOUN PART VERB ADJ NOUN ADP DET PROPN PUNCT ADJ NOUN ADP NOUN NOUN NOUN SCONJ PRON NOUN PUNCT SCONJ VERB ADJ NOUN PUNCT VERB ADV VERB NOUN NOUN VERB ADP ADJ ADJ ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT ADP DET PROPN PROPN PROPN NOUN PUNCT PRON NOUN VERB DET PROPN NOUN ADP NUM PUNCT VERB ADJ ADJ ADJ NOUN CCONJ VERB ADV ADJ NOUN ADP ADV NUM ADP PROPN PUNCT,0.5903614457831325,23.714285714285715,6.216867469879518
14,14,GPT-3.5,"[' Supervised Fine-Tuning (SFT) on response demonstrations coupled with Reinforcement Learning from Human Feedback (RLHF) offers a robust paradigm for aligning AI agents based on large language models (LLMs). However, the reliance on high-quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in-distribution preferences is challenging. This paper introduces SALMON (Self-ALignMent with principlefOllowiNg reward models), a novel approach to align base language models with minimal human supervision. SALMON utilizes a principle-following reward model, trained on synthetic preference data, to generate reward scores based on human-defined principles. By adjusting these principles during RL training, we gain precise control over preferences, influencing RL-trained policies and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method produces the AI assistant Dromedary-2, surpassing state-of-the-art AI systems with only 6 exemplars for in-context learning and 31 human-defined principles. We open-source the code and model weights, encouraging further research in aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.']",abstract_chunked," Supervised Fine-Tuning (SFT) on response demonstrations coupled with Reinforcement Learning from Human Feedback (RLHF) offers a robust paradigm for aligning AI agents based on large language models (LLMs). However, the reliance on high-quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in-distribution preferences is challenging. This paper introduces SALMON (Self-ALignMent with principlefOllowiNg reward models), a novel approach to align base language models with minimal human supervision. SALMON utilizes a principle-following reward model, trained on synthetic preference data, to generate reward scores based on human-defined principles. By adjusting these principles during RL training, we gain precise control over preferences, influencing RL-trained policies and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method produces the AI assistant Dromedary-2, surpassing state-of-the-art AI systems with only 6 exemplars for in-context learning and 31 human-defined principles. We open-source the code and model weights, encouraging further research in aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",1.3026011560693576,2.6406588729545177,14,0.6146210432052612," Supervised Fine Tuning on response demonstrations coupled with Propname Propname from Propname Propname offers a robust paradigm for aligning Propname agents based on large language models. However, the reliance on high quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in distribution preferences is challenging. This paper introduces Propname, a novel approach to align base language models with minimal human supervision. Propname utilizes a principle following reward model, trained on synthetic preference data, to generate reward scores based on human defined principles. By adjusting these principles during Propname training, we gain precise control over preferences, influencing Propname trained policies and eliminating the need for online human preferences. Applied to the Propname 0 00b base language model, our method produces the Propname assistant Propname 0, surpassing state of the art Propname systems with only 0 exemplars for in context learning and 00 human defined principles. We open source the code and model weights, encouraging further research in aligning Propname based Propname agents with enhanced supervision efficiency, improved controllability, and scalable oversight."," Supervised Fine Tuning on response demonstrations coupled with Propname Propname from Propname Propname offers a robust paradigm for aligning Propname agents based on large language models. However, the reliance on high quality human annotations poses a significant limitation, especially for complex tasks where obtaining consistent response demonstrations and in distribution preferences is challenging. This paper introduces Propname, a novel approach to align base language models with minimal human supervision. Propname utilizes a principle following reward model, trained on synthetic preference data, to generate reward scores based on human defined principles. By adjusting these principles during Propname training, we gain precise control over preferences, influencing Propname trained policies and eliminating the need for online human preferences. Applied to the Propname 0 00b base language model, our method produces the Propname assistant Propname 0, surpassing state of the art Propname systems with only 0 exemplars for in context learning and 00 human defined principles. We open source the code and model weights, encouraging further research in aligning Propname based Propname agents with enhanced supervision efficiency, improved controllability, and scalable oversight.", ADJ NOUN NOUN ADP NOUN NOUN VERB ADP PROPN PROPN ADP PROPN PROPN VERB DET ADJ NOUN ADP VERB PROPN NOUN VERB ADP ADJ NOUN NOUN PUNCT ADV PUNCT DET NOUN ADP ADJ NOUN ADJ NOUN VERB DET ADJ NOUN PUNCT ADV ADP ADJ NOUN SCONJ VERB ADJ NOUN NOUN CCONJ ADP NOUN NOUN AUX VERB PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN ADP ADJ NOUN NOUN NOUN ADP ADJ ADJ NOUN PUNCT PROPN VERB DET NOUN VERB NOUN NOUN PUNCT VERB ADP ADJ NOUN NOUN PUNCT PART VERB NOUN NOUN VERB ADP ADJ VERB NOUN PUNCT ADP VERB DET NOUN ADP PROPN NOUN PUNCT PRON VERB ADJ NOUN ADP NOUN PUNCT VERB PROPN VERB NOUN CCONJ VERB DET NOUN ADP ADJ ADJ NOUN PUNCT VERB ADP DET PROPN NUM NOUN NOUN NOUN NOUN PUNCT PRON NOUN VERB DET PROPN NOUN PROPN NUM PUNCT VERB NOUN ADP DET NOUN PROPN NOUN ADP ADV NUM NOUN ADP ADP NOUN NOUN CCONJ NUM ADJ VERB NOUN PUNCT PRON VERB NOUN DET NOUN CCONJ NOUN NOUN PUNCT VERB ADJ NOUN ADP VERB PROPN VERB PROPN NOUN ADP ADJ NOUN NOUN PUNCT VERB NOUN PUNCT CCONJ ADJ NOUN PUNCT,0.5786802030456852,28.142857142857142,5.66497461928934
15,15,GPT-3.5,"["" This paper addresses the challenge of learning representations for entities and relations in knowledge graphs, with the goal of predicting missing links. The effectiveness of this task hinges on the model's capacity to capture and infer intricate patterns within relations. We introduce RotatE, a novel knowledge graph embedding approach designed to model and infer diverse relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation from the source entity to the target entity in the complex vector space. Furthermore, we propose a novel self-adversarial negative sampling technique to efficiently and effectively train the RotatE model. Experimental results across multiple benchmark knowledge graphs demonstrate that RotatE is not only scalable but also adept at inferring and modeling various relation patterns, surpassing existing state-of-the-art models in link prediction.""]",abstract_chunked," This paper addresses the challenge of learning representations for entities and relations in knowledge graphs, with the goal of predicting missing links. The effectiveness of this task hinges on the model's capacity to capture and infer intricate patterns within relations. We introduce RotatE, a novel knowledge graph embedding approach designed to model and infer diverse relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation from the source entity to the target entity in the complex vector space. Furthermore, we propose a novel self-adversarial negative sampling technique to efficiently and effectively train the RotatE model. Experimental results across multiple benchmark knowledge graphs demonstrate that RotatE is not only scalable but also adept at inferring and modeling various relation patterns, surpassing existing state-of-the-art models in link prediction.",13.227562656641624,2.6406588729545177,15,0.5239368081092834," This paper addresses the challenge of learning representations for entities and relations in knowledge graphs, with the goal of predicting missing links. The effectiveness of this task hinges on the models capacity to capture and infer intricate patterns within relations. We introduce Propname, a novel knowledge graph embedding approach designed to model and infer diverse relation patterns, including symmetryantisymmetry, inversion, and composition. The Propname model represents each relation as a rotation from the source entity to the target entity in the complex vector space. Furthermore, we propose a novel self adversarial negative sampling technique to efficiently and effectively train the Propname model. Experimental results across multiple benchmark knowledge graphs demonstrate that Propname is not only scalable but also adept at inferring and modeling various relation patterns, surpassing existing state of the art models in link prediction."," This paper addresses the challenge of learning representations for entities and relations in knowledge graphs, with the goal of predicting missing links. The effectiveness of this task hinges on the models capacity to capture and infer intricate patterns within relations. We introduce Propname, a novel knowledge graph embedding approach designed to model and infer diverse relation patterns, including symmetryantisymmetry, inversion, and composition. The Propname model represents each relation as a rotation from the source entity to the target entity in the complex vector space. Furthermore, we propose a novel self adversarial negative sampling technique to efficiently and effectively train the Propname model. Experimental results across multiple benchmark knowledge graphs demonstrate that Propname is not only scalable but also adept at inferring and modeling various relation patterns, surpassing existing state of the art models in link prediction.", DET NOUN VERB DET NOUN ADP VERB NOUN ADP NOUN CCONJ NOUN ADP NOUN NOUN PUNCT ADP DET NOUN ADP VERB ADJ NOUN PUNCT DET NOUN ADP DET NOUN NOUN ADP DET NOUN NOUN PART VERB CCONJ VERB ADJ NOUN ADP NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN VERB NOUN VERB PART VERB CCONJ VERB ADJ NOUN NOUN PUNCT VERB NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT DET PROPN NOUN VERB DET NOUN ADP DET NOUN ADP DET NOUN NOUN ADP DET NOUN NOUN ADP DET ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADJ ADJ NOUN NOUN ADP ADV CCONJ ADV VERB DET PROPN NOUN PUNCT ADJ NOUN ADP ADJ ADJ NOUN NOUN VERB SCONJ PROPN AUX PART ADV ADJ CCONJ ADV ADJ ADP VERB CCONJ VERB ADJ NOUN NOUN PUNCT VERB VERB NOUN ADP DET NOUN NOUN ADP NOUN NOUN PUNCT,0.6577181208053692,24.833333333333332,5.543624161073826
16,16,GPT-3.5,"["" This paper introduces a novel paradigm, RECITation-augmented gEneration (RECITE), aimed at enhancing the accuracy of factual knowledge generation in Large Language Models (LLMs) without relying on external corpuses. Departing from conventional retrieval-augmented language models, RECITE employs a recite-and-answer approach, where relevant passages are sampled from the LLMs' internal memory before generating final responses. The proposed RECITE paradigm demonstrates significant efficacy in knowledge-intensive Natural Language Processing (NLP) tasks, achieving state-of-the-art performance in various closed-book question answering (CBQA) scenarios. Through comprehensive experiments on four pre-trained models (PaLM, UL2, OPT, and Codex) across three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA), we validate the effectiveness of RECITE. The code for our approach is publicly available for further exploration.""]",abstract_chunked," This paper introduces a novel paradigm, RECITation-augmented gEneration (RECITE), aimed at enhancing the accuracy of factual knowledge generation in Large Language Models (LLMs) without relying on external corpuses. Departing from conventional retrieval-augmented language models, RECITE employs a recite-and-answer approach, where relevant passages are sampled from the LLMs' internal memory before generating final responses. The proposed RECITE paradigm demonstrates significant efficacy in knowledge-intensive Natural Language Processing (NLP) tasks, achieving state-of-the-art performance in various closed-book question answering (CBQA) scenarios. Through comprehensive experiments on four pre-trained models (PaLM, UL2, OPT, and Codex) across three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA), we validate the effectiveness of RECITE. The code for our approach is publicly available for further exploration.",-4.836956521739097,2.6406588729545177,16,0.5909387469291687," This paper introduces a novel paradigm, Propname augmented Propname, aimed at enhancing the accuracy of factual knowledge generation in Large Propname Propname without relying on external corpuses. Departing from conventional retrieval augmented language models, Propname employs a recite and answer approach, where relevant passages are sampled from the Propname internal memory before generating final responses. The proposed Propname paradigm demonstrates significant efficacy in knowledge intensive Propname Propname Propname tasks, achieving state of the art performance in various closed book question answering scenarios. Through comprehensive experiments on four pre trained models across three Propname tasks, we validate the effectiveness of Propname. The code for our approach is publicly available for further exploration."," This paper introduces a novel paradigm, Propname augmented Propname, aimed at enhancing the accuracy of factual knowledge generation in Large Propname Propname without relying on external corpuses. Departing from conventional retrieval augmented language models, Propname employs a recite and answer approach, where relevant passages are sampled from the Propname internal memory before generating final responses. The proposed Propname paradigm demonstrates significant efficacy in knowledge intensive Propname Propname Propname tasks, achieving state of the art performance in various closed book question answering scenarios. Through comprehensive experiments on four pre trained models across three Propname tasks, we validate the effectiveness of Propname. The code for our approach is publicly available for further exploration.", DET NOUN VERB DET ADJ NOUN PUNCT PROPN VERB PROPN PUNCT VERB ADP VERB DET NOUN ADP ADJ NOUN NOUN ADP ADJ PROPN PROPN ADP VERB ADP ADJ NOUN PUNCT VERB ADP ADJ NOUN VERB NOUN NOUN PUNCT PROPN VERB DET NOUN CCONJ NOUN NOUN PUNCT SCONJ ADJ NOUN AUX VERB ADP DET PROPN ADJ NOUN ADP VERB ADJ NOUN PUNCT DET VERB PROPN NOUN VERB ADJ NOUN ADP NOUN ADJ PROPN PROPN PROPN NOUN PUNCT VERB NOUN ADP DET NOUN NOUN ADP ADJ ADJ NOUN NOUN VERB NOUN PUNCT ADP ADJ NOUN ADP NUM ADJ VERB NOUN ADP NUM PROPN NOUN PUNCT PRON VERB DET NOUN ADP PROPN PUNCT DET NOUN ADP PRON NOUN AUX ADV ADJ ADP ADJ NOUN PUNCT,0.6885245901639344,24.4,5.934426229508197
17,17,GPT-3.5,"["" Recent advances in Natural Language Processing (NLP) have seen significant success through the utilization of large pre-trained models with millions of parameters. However, the deployment of such models to resource-limited mobile devices is hindered by their substantial size and high latency. This paper introduces MobileBERT, a solution designed to compress and accelerate the widely used BERT model. MobileBERT retains the task-agnostic nature of the original BERT, allowing it to be applied to various NLP tasks through straightforward fine-tuning. Essentially, MobileBERT is a streamlined version of BERTLARGE, featuring bottleneck structures and a meticulously balanced combination of self-attentions and feed-forward networks. The training process involves the creation of a specially designed teacher model, an inverted-bottleneck incorporated BERTLARGE model, from which knowledge is transferred to MobileBERT. Empirical studies demonstrate that MobileBERT achieves a 4.3× reduction in size and a 5.5× improvement in speed compared to BERTBASE while delivering competitive results on established benchmarks. In particular, on GLUE's natural language inference tasks, MobileBERT attains a GLUE score of 77.7 (0.6 lower than BERTBASE) with a latency of 62 ms on a Pixel 4 phone. Additionally, on the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE).""]",abstract_chunked," Recent advances in Natural Language Processing (NLP) have seen significant success through the utilization of large pre-trained models with millions of parameters. However, the deployment of such models to resource-limited mobile devices is hindered by their substantial size and high latency. This paper introduces MobileBERT, a solution designed to compress and accelerate the widely used BERT model. MobileBERT retains the task-agnostic nature of the original BERT, allowing it to be applied to various NLP tasks through straightforward fine-tuning. Essentially, MobileBERT is a streamlined version of BERTLARGE, featuring bottleneck structures and a meticulously balanced combination of self-attentions and feed-forward networks. The training process involves the creation of a specially designed teacher model, an inverted-bottleneck incorporated BERTLARGE model, from which knowledge is transferred to MobileBERT. Empirical studies demonstrate that MobileBERT achieves a 4.3× reduction in size and a 5.5× improvement in speed compared to BERTBASE while delivering competitive results on established benchmarks. In particular, on GLUE's natural language inference tasks, MobileBERT attains a GLUE score of 77.7 (0.6 lower than BERTBASE) with a latency of 62 ms on a Pixel 4 phone. Additionally, on the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE).",19.6030434782609,2.6406588729545177,17,0.5207461714744568," Recent advances in Propname Propname Propname have seen significant success through the utilization of large pre trained models with millions of parameters. However, the deployment of such models to resource limited mobile devices is hindered by their substantial size and high latency. This paper introduces Propname, a solution designed to compress and accelerate the widely used Propname model. MobileBERT retains the task agnostic nature of the original Propname, allowing it to be applied to various Propname tasks through straightforward fine tuning. Essentially, Propname is a streamlined version of BERTLARGE, featuring bottleneck structures and a meticulously balanced combination of self attentions and feed forward networks. The training process involves the creation of a specially designed teacher model, an inverted bottleneck incorporated Propname model, from which knowledge is transferred to Propname. Empirical studies demonstrate that Propname achieves a 0.0 reduction in size and a 0.0 improvement in speed compared to Propname while delivering competitive results on established benchmarks. In particular, on Propname natural language inference tasks, MobileBERT attains a GLUE score of 00.0 with a latency of00 ms on a Propname 0 phone. Additionally, on the SQuAD v0.0v0.0 question answering task, Propname achieves a dev F0 score of 00.000.0."," Recent advances in Propname Propname Propname have seen significant success through the utilization of large pre trained models with millions of parameters. However, the deployment of such models to resource limited mobile devices is hindered by their substantial size and high latency. This paper introduces Propname, a solution designed to compress and accelerate the widely used Propname model. MobileBERT retains the task agnostic nature of the original Propname, allowing it to be applied to various Propname tasks through straightforward fine tuning. Essentially, Propname is a streamlined version of BERTLARGE, featuring bottleneck structures and a meticulously balanced combination of self attentions and feed forward networks. The training process involves the creation of a specially designed teacher model, an inverted bottleneck incorporated Propname model, from which knowledge is transferred to Propname. Empirical studies demonstrate that Propname achieves a 0.0 reduction in size and a 0.0 improvement in speed compared to Propname while delivering competitive results on established benchmarks. In particular, on Propname natural language inference tasks, MobileBERT attains a GLUE score of 00.0 with a latency of00 ms on a Propname 0 phone. Additionally, on the SQuAD v0.0v0.0 question answering task, Propname achieves a dev F0 score of 00.000.0.", ADJ NOUN ADP PROPN PROPN PROPN AUX VERB ADJ NOUN ADP DET NOUN ADP ADJ ADJ VERB NOUN ADP NOUN ADP NOUN PUNCT ADV PUNCT DET NOUN ADP ADJ NOUN PART VERB ADJ ADJ NOUN AUX VERB ADP PRON ADJ NOUN CCONJ ADJ NOUN PUNCT DET NOUN VERB PROPN PUNCT DET NOUN VERB PART VERB CCONJ VERB DET ADV VERB PROPN NOUN PUNCT NUM VERB DET NOUN ADJ NOUN ADP DET ADJ PROPN PUNCT VERB PRON PART AUX VERB ADP ADJ PROPN NOUN ADP ADJ ADJ NOUN PUNCT ADV PUNCT PROPN AUX DET ADJ NOUN ADP NOUN PUNCT VERB NOUN NOUN CCONJ DET ADV ADJ NOUN ADP NOUN NOUN CCONJ VERB ADJ NOUN PUNCT DET NOUN NOUN VERB DET NOUN ADP DET ADV VERB NOUN NOUN PUNCT DET VERB NOUN VERB PROPN NOUN PUNCT ADP PRON NOUN AUX VERB ADP PROPN PUNCT ADJ NOUN VERB SCONJ PROPN VERB DET NUM NOUN ADP NOUN CCONJ DET NUM NOUN ADP NOUN VERB ADP PROPN SCONJ VERB ADJ NOUN ADP VERB NOUN PUNCT ADP ADJ PUNCT ADP PROPN ADJ NOUN NOUN NOUN PUNCT NUM NOUN DET ADJ NOUN ADP NUM ADP DET NOUN ADP PUNCT NOUN ADP DET PROPN NUM NOUN PUNCT ADV PUNCT ADP DET ADJ NOUN NOUN VERB NOUN PUNCT PROPN VERB DET NOUN NOUN NOUN ADP NUM PUNCT,0.6036866359447005,24.11111111111111,5.387096774193548
18,18,GPT-3.5,"["" Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present PEER, a collaborative language model trained to imitate the entire writing process. PEER can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of PEER are trained to infill various parts of the writing process, leveraging self-training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate PEER's strong performance across diverse domains and editing tasks.""]",abstract_chunked," Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present PEER, a collaborative language model trained to imitate the entire writing process. PEER can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of PEER are trained to infill various parts of the writing process, leveraging self-training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate PEER's strong performance across diverse domains and editing tasks.",11.989097744360919,2.6406588729545177,18,0.6524873971939087," Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present Propname, a collaborative language model trained to imitate the entire writing process. Propname can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of Propname are trained to infill various parts of the writing process, leveraging self training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate Propname strong performance across diverse domains and editing tasks."," Collaborative writing involves a dynamic process of drafting, suggesting changes, and iterative revisions. Current language models, however, are trained solely to generate final outputs, lacking crucial abilities for collaborative writing. In response, we present Propname, a collaborative language model trained to imitate the entire writing process. Propname can generate drafts, propose edits, and provide explanations for its actions, addressing limitations in updating existing texts, controllability, and verbal planning. Notably, multiple instances of Propname are trained to infill various parts of the writing process, leveraging self training techniques to enhance the quality, quantity, and diversity of training data. This approach allows PEER to operate in domains without edit histories and improves its capacity to follow instructions, generate useful comments, and explain its actions. Extensive experiments demonstrate Propname strong performance across diverse domains and editing tasks.", ADJ NOUN VERB DET ADJ NOUN ADP NOUN PUNCT VERB NOUN PUNCT CCONJ ADJ NOUN PUNCT ADJ NOUN NOUN PUNCT ADV PUNCT AUX VERB ADV PART VERB ADJ NOUN PUNCT VERB ADJ NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN VERB PART VERB DET ADJ NOUN NOUN PUNCT PROPN AUX VERB NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN ADP PRON NOUN PUNCT VERB NOUN ADP VERB VERB NOUN PUNCT NOUN PUNCT CCONJ ADJ NOUN PUNCT ADV PUNCT ADJ NOUN ADP PROPN AUX VERB PART VERB ADJ NOUN ADP DET NOUN NOUN PUNCT VERB NOUN NOUN NOUN PART VERB DET NOUN PUNCT NOUN PUNCT CCONJ NOUN ADP NOUN NOUN PUNCT DET NOUN VERB ADJ PART VERB ADP NOUN ADP NOUN NOUN CCONJ VERB PRON NOUN PART VERB NOUN PUNCT VERB ADJ NOUN PUNCT CCONJ VERB PRON NOUN PUNCT ADJ NOUN VERB PROPN ADJ NOUN ADP ADJ NOUN CCONJ NOUN NOUN PUNCT,0.610062893081761,22.714285714285715,5.484276729559748
19,19,GPT-3.5,"["" The success of pretraining deep language models in NLP has been remarkable, but recent findings highlight their struggle to comprehend rare words. Addressing this limitation, we present BERTRAM, an architecture built upon BERT that excels in generating high-quality embeddings for rare words. Inspired by strategies employed in static word embeddings, BERTRAM enables the interaction between the surface form and contexts of a word within a deep architecture. Integrating BERTRAM into BERT yields substantial performance improvements, particularly in the representation of rare and medium-frequency words. This enhancement is evident across a rare word probing task and three downstream tasks, showcasing BERTRAM's effectiveness in improving the treatment of rare words in pretrained language models.""]",abstract_chunked," The success of pretraining deep language models in NLP has been remarkable, but recent findings highlight their struggle to comprehend rare words. Addressing this limitation, we present BERTRAM, an architecture built upon BERT that excels in generating high-quality embeddings for rare words. Inspired by strategies employed in static word embeddings, BERTRAM enables the interaction between the surface form and contexts of a word within a deep architecture. Integrating BERTRAM into BERT yields substantial performance improvements, particularly in the representation of rare and medium-frequency words. This enhancement is evident across a rare word probing task and three downstream tasks, showcasing BERTRAM's effectiveness in improving the treatment of rare words in pretrained language models.",22.452571428571446,2.6406588729545177,19,0.6189583539962769," The success of pretraining deep language models in Propname has been remarkable, but recent findings highlight their struggle to comprehend rare words. Addressing this limitation, we present Propname, an architecture built upon Propname that excels in generating high quality embeddings for rare words. Inspired by strategies employed in static word embeddings, Propname enables the interaction between the surface form and contexts of a word within a deep architecture. Integrating BERTRAM into Propname yields substantial performance improvements, particularly in the representation of rare and medium frequency words. This enhancement is evident across a rare word probing task and three downstream tasks, showcasing BERTRAMs effectiveness in improving the treatment of rare words in pretrained language models."," The success of pretraining deep language models in Propname has been remarkable, but recent findings highlight their struggle to comprehend rare words. Addressing this limitation, we present Propname, an architecture built upon Propname that excels in generating high quality embeddings for rare words. Inspired by strategies employed in static word embeddings, Propname enables the interaction between the surface form and contexts of a word within a deep architecture. Integrating BERTRAM into Propname yields substantial performance improvements, particularly in the representation of rare and medium frequency words. This enhancement is evident across a rare word probing task and three downstream tasks, showcasing BERTRAMs effectiveness in improving the treatment of rare words in pretrained language models.", DET NOUN ADP VERB ADJ NOUN NOUN ADP PROPN AUX AUX ADJ PUNCT CCONJ ADJ NOUN VERB PRON NOUN PART VERB ADJ NOUN PUNCT VERB DET NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN VERB SCONJ PROPN SCONJ NOUN ADP VERB ADJ NOUN NOUN ADP ADJ NOUN PUNCT VERB ADP NOUN VERB ADP ADJ NOUN NOUN PUNCT PROPN VERB DET NOUN ADP DET NOUN NOUN CCONJ NOUN ADP DET NOUN ADP DET ADJ NOUN PUNCT VERB ADV ADP PROPN NOUN ADJ NOUN NOUN PUNCT ADV ADP DET NOUN ADP ADJ CCONJ ADJ ADJ NOUN PUNCT DET NOUN AUX ADJ ADP DET ADJ NOUN VERB NOUN CCONJ NUM ADJ NOUN PUNCT VERB NOUN NOUN ADP VERB DET NOUN ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT,0.664,25.0,5.608
20,20,GPT-3.5,"[' While some natural language processing (NLP) tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Pattern-Exploiting Training (PET), a novel semi-supervised training approach that combines unsupervised and supervised learning. PET reformulates input examples as cloze-style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, PET demonstrates superior performance compared to both supervised training and strong semi-supervised approaches, particularly in low-resource settings.']",abstract_chunked," While some natural language processing (NLP) tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Pattern-Exploiting Training (PET), a novel semi-supervised training approach that combines unsupervised and supervised learning. PET reformulates input examples as cloze-style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, PET demonstrates superior performance compared to both supervised training and strong semi-supervised approaches, particularly in low-resource settings.",11.447873563218423,2.6406588729545177,20,0.5727148056030273," While some natural language processing tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Propname Propname Propname, a novel semi supervised training approach that combines unsupervised and supervised learning. Propname reformulates input examples as cloze style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, Propname demonstrates superior performance compared to both supervised training and strong semi supervised approaches, particularly in low resource settings."," While some natural language processing tasks can be addressed in an entirely unsupervised manner by providing pretrained language models with task descriptions, this approach tends to underperform compared to supervised methods. In this work, we propose Propname Propname Propname, a novel semi supervised training approach that combines unsupervised and supervised learning. Propname reformulates input examples as cloze style phrases to guide language models in understanding a specific task. These phrases are then utilized to assign soft labels to a large set of unlabeled examples. Subsequently, standard supervised training is conducted on the resulting training set. Across various tasks and languages, Propname demonstrates superior performance compared to both supervised training and strong semi supervised approaches, particularly in low resource settings.", SCONJ DET ADJ NOUN NOUN NOUN AUX AUX VERB ADP DET ADV ADJ NOUN ADP VERB VERB NOUN NOUN ADP NOUN NOUN PUNCT DET NOUN VERB PART VERB VERB ADP VERB NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN PROPN PROPN PUNCT DET ADJ ADJ ADJ NOUN NOUN PRON VERB ADJ CCONJ ADJ NOUN PUNCT PROPN VERB NOUN NOUN SCONJ VERB NOUN NOUN PART VERB NOUN NOUN ADP VERB DET ADJ NOUN PUNCT DET NOUN AUX ADV VERB PART VERB ADJ NOUN ADP DET ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT ADJ ADJ NOUN AUX VERB ADP DET VERB NOUN NOUN PUNCT ADP ADJ NOUN CCONJ NOUN PUNCT PROPN VERB ADJ NOUN VERB ADP DET ADJ NOUN CCONJ ADJ ADJ ADJ NOUN PUNCT ADV ADP ADJ NOUN NOUN PUNCT,0.648854961832061,21.833333333333332,5.717557251908397
21,21,GPT-3.5,"[' Word embeddings are integral to high-performing natural language processing (NLP) systems, yet learning effective representations for novel words remains challenging, especially for words absent in the training data. Existing approaches either focus on the surface-form of novel words or their contextual usage. In this paper, we introduce an architecture that combines information from both surface-form and context, demonstrating substantial improvements in embedding quality. Our model achieves state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, making it easily integrable into any NLP system to enhance its ability to handle novel words.']",abstract_chunked," Word embeddings are integral to high-performing natural language processing (NLP) systems, yet learning effective representations for novel words remains challenging, especially for words absent in the training data. Existing approaches either focus on the surface-form of novel words or their contextual usage. In this paper, we introduce an architecture that combines information from both surface-form and context, demonstrating substantial improvements in embedding quality. Our model achieves state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, making it easily integrable into any NLP system to enhance its ability to handle novel words.",14.332691588785082,2.6406588729545177,21,0.5655471086502075," Word embeddings are integral to high performing natural language processing systems, yet learning effective representations for novel words remains challenging, especially for words absent in the training data. Existing approaches either focus on the surface form of novel words or their contextual usage. In this paper, we introduce an architecture that combines information from both surface form and context, demonstrating substantial improvements in embedding quality. Our model achieves state of the art results on the Propname Propname and Propname Propname Propname datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, making it easily integrable into any NLP system to enhance its ability to handle novel words."," Word embeddings are integral to high performing natural language processing systems, yet learning effective representations for novel words remains challenging, especially for words absent in the training data. Existing approaches either focus on the surface form of novel words or their contextual usage. In this paper, we introduce an architecture that combines information from both surface form and context, demonstrating substantial improvements in embedding quality. Our model achieves state of the art results on the Propname Propname and Propname Propname Propname datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, making it easily integrable into any NLP system to enhance its ability to handle novel words.", NOUN NOUN AUX ADJ PART ADJ VERB ADJ NOUN NOUN NOUN PUNCT CCONJ VERB ADJ NOUN ADP ADJ NOUN VERB VERB PUNCT ADV ADP NOUN ADJ ADP DET NOUN NOUN PUNCT VERB NOUN CCONJ VERB ADP DET NOUN NOUN ADP ADJ NOUN CCONJ PRON ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN PRON VERB NOUN ADP DET NOUN NOUN CCONJ NOUN PUNCT VERB ADJ NOUN ADP VERB NOUN PUNCT PRON NOUN VERB NOUN ADP DET NOUN NOUN ADP DET PROPN PROPN CCONJ PROPN PROPN PROPN NOUN PUNCT ADV PUNCT PRON NOUN VERB ADV DET VERB NOUN CCONJ DET ADJ NOUN ADP NOUN PUNCT VERB PRON ADV ADJ ADP DET NOUN NOUN PART VERB PRON NOUN PART VERB ADJ NOUN PUNCT,0.7073170731707317,24.6,5.333333333333333
22,22,GPT-3.5,"[' This paper introduces an innovative approach to obtaining high-quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models (PLMs). Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high-performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique.']",abstract_chunked," This paper introduces an innovative approach to obtaining high-quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models (PLMs). Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high-performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique.",-1.567229357798169,2.6406588729545177,22,0.5419114828109741," This paper introduces an innovative approach to obtaining high quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models. Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique."," This paper introduces an innovative approach to obtaining high quality sentence embeddings without the reliance on labeled data, finetuning, or modifications to pretraining objectives in pretrained language models. Traditional methods either augment PLMs with additional pretraining objectives or necessitate finetuning on large sets of labeled text pairs, demanding significant human effort to generate suitable datasets. Our proposed methodology harnesses the generative capabilities of large and high performing PLMs to create entire datasets of labeled text pairs from scratch. These datasets are subsequently utilized for finetuning smaller and more efficient models. Our fully unsupervised approach surpasses strong baselines on various semantic textual similarity datasets, showcasing the efficacy of this novel technique.", DET NOUN VERB DET ADJ NOUN ADP VERB ADJ NOUN NOUN NOUN ADP DET NOUN ADP VERB NOUN PUNCT VERB PUNCT CCONJ NOUN ADP VERB NOUN ADP VERB NOUN NOUN PUNCT ADJ NOUN CCONJ NOUN NOUN ADP ADJ VERB NOUN CCONJ ADJ NOUN ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT VERB ADJ ADJ NOUN PART VERB ADJ NOUN PUNCT PRON VERB NOUN VERB DET ADJ NOUN ADP ADJ CCONJ ADJ VERB NOUN PART VERB ADJ NOUN ADP VERB NOUN NOUN ADP NOUN PUNCT DET NOUN AUX ADV VERB ADP VERB ADJ CCONJ ADV ADJ NOUN PUNCT PRON ADV ADJ NOUN VERB ADJ NOUN ADP ADJ ADJ ADJ NOUN NOUN PUNCT VERB DET NOUN ADP DET NOUN NOUN PUNCT,0.6974789915966386,23.8,6.050420168067227
23,23,GPT-3.5,"[' This paper explores the synergy between pretrained language models and simple task descriptions to address unsupervised tasks and enhance few-shot learning in text classification. The proposed approach, GENPET, leverages pattern-exploiting training, originally designed for classification tasks, to enable text generation. We demonstrate the efficacy of GENPET across various summarization and headline generation datasets, showcasing its consistent improvements over robust baselines in few-shot scenarios. The method addresses challenges such as finding easily understandable task descriptions for pretrained models and ensuring their effective utilization, while also implementing measures to mitigate overfitting. GENPET emerges as a promising solution to enhance data efficiency in generative settings, offering a novel perspective on combining task descriptions and example-based learning for text generation.']",abstract_chunked," This paper explores the synergy between pretrained language models and simple task descriptions to address unsupervised tasks and enhance few-shot learning in text classification. The proposed approach, GENPET, leverages pattern-exploiting training, originally designed for classification tasks, to enable text generation. We demonstrate the efficacy of GENPET across various summarization and headline generation datasets, showcasing its consistent improvements over robust baselines in few-shot scenarios. The method addresses challenges such as finding easily understandable task descriptions for pretrained models and ensuring their effective utilization, while also implementing measures to mitigate overfitting. GENPET emerges as a promising solution to enhance data efficiency in generative settings, offering a novel perspective on combining task descriptions and example-based learning for text generation.",-9.980241379310314,2.6406588729545177,23,0.6766180992126465," This paper explores the synergy between pretrained language models and simple task descriptions to address unsupervised tasks and enhance few shot learning in text classification. The proposed approach, Propname, leverages pattern exploiting training, originally designed for classification tasks, to enable text generation. We demonstrate the efficacy of GENPET across various summarization and headline generation datasets, showcasing its consistent improvements over robust baselines in few shot scenarios. The method addresses challenges such as finding easily understandable task descriptions for pretrained models and ensuring their effective utilization, while also implementing measures to mitigate overfitting. GENPET emerges as a promising solution to enhance data efficiency in generative settings, offering a novel perspective on combining task descriptions and example based learning for text generation."," This paper explores the synergy between pretrained language models and simple task descriptions to address unsupervised tasks and enhance few shot learning in text classification. The proposed approach, Propname, leverages pattern exploiting training, originally designed for classification tasks, to enable text generation. We demonstrate the efficacy of GENPET across various summarization and headline generation datasets, showcasing its consistent improvements over robust baselines in few shot scenarios. The method addresses challenges such as finding easily understandable task descriptions for pretrained models and ensuring their effective utilization, while also implementing measures to mitigate overfitting. GENPET emerges as a promising solution to enhance data efficiency in generative settings, offering a novel perspective on combining task descriptions and example based learning for text generation.", DET NOUN VERB DET NOUN ADP VERB NOUN NOUN CCONJ ADJ NOUN NOUN PART VERB ADJ NOUN CCONJ VERB ADJ NOUN NOUN ADP NOUN NOUN PUNCT DET VERB NOUN PUNCT PROPN PUNCT NOUN NOUN VERB NOUN PUNCT ADV VERB ADP NOUN NOUN PUNCT PART VERB NOUN NOUN PUNCT PRON VERB DET NOUN ADP NOUN ADP ADJ NOUN CCONJ NOUN NOUN NOUN PUNCT VERB PRON ADJ NOUN ADP ADJ NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN NOUN VERB ADJ ADP VERB ADV ADJ NOUN NOUN ADP ADJ NOUN CCONJ VERB PRON ADJ NOUN PUNCT SCONJ ADV VERB NOUN PART VERB VERB PUNCT NOUN VERB ADP DET ADJ NOUN PART VERB NOUN NOUN ADP ADJ NOUN PUNCT VERB DET ADJ NOUN ADP VERB NOUN NOUN CCONJ NOUN VERB VERB ADP NOUN NOUN PUNCT,0.6818181818181818,26.4,6.045454545454546
24,24,Aman Madaan,"[' Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%.2']",abstract_chunked," Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%.2",20.198717948717956,30.71625077440773,24,0.501285195350647," Large language models are now available in various sizes and configurations from cloud Propname providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present Propname, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller Propname. Central to Propname is a few shot self verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in Propname to refine the accuracy of these assessments. Our experiments using Propname Propname, on five context grounded reasoning datasets demonstrate that Propname surpasses established baselines, improving the incremental benefit per cost by up to 00.0"," Large language models are now available in various sizes and configurations from cloud Propname providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present Propname, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller Propname. Central to Propname is a few shot self verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in Propname to refine the accuracy of these assessments. Our experiments using Propname Propname, on five context grounded reasoning datasets demonstrate that Propname surpasses established baselines, improving the incremental benefit per cost by up to 00.0", ADJ NOUN NOUN AUX ADV ADJ ADP ADJ NOUN CCONJ NOUN ADP ADJ PROPN NOUN PUNCT SCONJ DET NOUN VERB DET ADJ NOUN ADP NOUN PUNCT ADV VERB DET NOUN PART VERB ADJ NOUN CCONJ NOUN VERB VERB PUNCT ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN PRON ADV VERB VERB ADP ADJ NOUN PUNCT VERB ADP DET ADJ NOUN ADP NOUN ADP DET ADJ PROPN PUNCT ADJ ADP PROPN AUX DET ADJ NOUN NOUN NOUN NOUN PUNCT PRON VERB DET NOUN ADP PRON ADJ NOUN ADP VERB NOUN PUNCT VERB SCONJ NOUN AUX AUX ADJ PUNCT PRON VERB DET ADJ NOUN ADP PROPN PART VERB DET NOUN ADP DET NOUN PUNCT PRON NOUN VERB PROPN PROPN PUNCT ADP NUM NOUN VERB NOUN NOUN VERB SCONJ PROPN VERB VERB NOUN PUNCT VERB DET ADJ NOUN ADP NOUN ADP ADP PART NUM,0.7062937062937062,23.833333333333332,5.391608391608392
25,25,Aman Madaan,"[' Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.']",abstract_chunked," Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ∼20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach.",27.016136363636377,30.71625077440773,25,0.6079075336456299," Like humans, large language models do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Propname Propname, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an Propname; then, the same Propname provides feedback for its output and uses it to refine itself, iteratively. Propname Propname does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single Propname as the generator, refiner and the feedback provider. We evaluate Propname Propname across 0 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state of the art LLMs. Across all evaluated tasks, outputs generated with Propname Propname are preferred by humans and automatic metrics over those generated with the same Propname using conventional one step generation, improving by 00 absolute on average in task performance. Our work demonstrates that even state of the art LLMs like Propname 0 can be further improved at test time using our simple, standalone approach."," Like humans, large language models do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Propname Propname, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an Propname; then, the same Propname provides feedback for its output and uses it to refine itself, iteratively. Propname Propname does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single Propname as the generator, refiner and the feedback provider. We evaluate Propname Propname across 0 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state of the art LLMs. Across all evaluated tasks, outputs generated with Propname Propname are preferred by humans and automatic metrics over those generated with the same Propname using conventional one step generation, improving by 00 absolute on average in task performance. Our work demonstrates that even state of the art LLMs like Propname 0 can be further improved at test time using our simple, standalone approach.", ADP NOUN PUNCT ADJ NOUN NOUN AUX PART ADV VERB DET ADJ NOUN ADP PRON ADJ NOUN PUNCT VERB ADP SCONJ NOUN VERB PRON VERB NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET NOUN ADP VERB ADJ NOUN ADP NOUN ADP ADJ NOUN CCONJ NOUN PUNCT DET ADJ NOUN AUX PART VERB DET ADJ NOUN VERB DET PROPN PUNCT ADV PUNCT DET ADJ PROPN VERB NOUN ADP PRON NOUN CCONJ VERB PRON PART VERB PRON PUNCT ADV PUNCT PROPN PROPN AUX PART VERB DET ADJ NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN NOUN PUNCT CCONJ ADV VERB DET ADJ PROPN ADP DET NOUN PUNCT NOUN CCONJ DET NOUN NOUN PUNCT PRON VERB PROPN PROPN ADP NUM ADJ NOUN PUNCT VERB ADP NOUN NOUN NOUN ADP ADJ NOUN PUNCT VERB NOUN ADP DET NOUN NOUN PUNCT ADP DET VERB NOUN PUNCT NOUN VERB ADP PROPN PROPN AUX VERB ADP NOUN CCONJ ADJ NOUN ADP PRON VERB ADP DET ADJ PROPN VERB ADJ NUM NOUN NOUN PUNCT VERB ADP NUM ADJ ADP ADJ ADP NOUN NOUN PUNCT PRON NOUN VERB SCONJ ADV NOUN ADP DET NOUN NOUN ADP PROPN NUM AUX AUX ADV VERB ADP NOUN NOUN VERB PRON ADJ PUNCT ADJ NOUN PUNCT,0.6059113300492611,29.0,4.975369458128079
26,26,Aman Madaan,"[' The waning of Moore’s Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program’s performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, we use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI’s CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5ˆ for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10ˆ smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.']",abstract_chunked," The waning of Moore’s Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program’s performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, we use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI’s CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5ˆ for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10ˆ smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.",29.820818965517276,30.71625077440773,26,0.5869742035865784," The waning of Propname Propname has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large scale dataset of Performance Improving Propname, Propname. Propname contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the programs performance. We use Propname to evaluate and improve the capacity of large language models. Specifically, we use examples from Propname to fine tune multiple variants of Propname, a billion scale Propname decoder model. Additionally, we use examples from Propname to prompt OpenAIs Propname using a few shot prompting. By leveraging Propname, we find that both Propname and Propname can generate performance improving edits, with speedups of more than 0.0 for over 00 of the programs, for Propname and Propname, even after the Propname programs were compiled using the Propname optimization level. Crucially, we show that Propname allows Propname, an open sourced and 00 smaller model than Propname, to match the performance of Propname on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code."," The waning of Propname Propname has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large scale dataset of Performance Improving Propname, Propname. Propname contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the programs performance. We use Propname to evaluate and improve the capacity of large language models. Specifically, we use examples from Propname to fine tune multiple variants of Propname, a billion scale Propname decoder model. Additionally, we use examples from Propname to prompt OpenAIs Propname using a few shot prompting. By leveraging Propname, we find that both Propname and Propname can generate performance improving edits, with speedups of more than 0.0 for over 00 of the programs, for Propname and Propname, even after the Propname programs were compiled using the Propname optimization level. Crucially, we show that Propname allows Propname, an open sourced and 00 smaller model than Propname, to match the performance of Propname on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.", DET NOUN ADP PROPN PROPN AUX VERB DET NOUN ADP DET NOUN NOUN ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT SCONJ VERB NOUN AUX DET ADJ NOUN PART VERB VERB NOUN NOUN PUNCT NOUN VERB PART VERB ADJ NOUN ADP VERB CCONJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP ADJ NOUN NOUN PART VERB ADV ADJ PUNCT NOUN VERB NOUN NOUN PUNCT PRON VERB SCONJ NOUN NOUN AUX VERB ADJ NOUN ADP NOUN PRON AUX AUX ADJ ADP ADJ NOUN ADV PUNCT PRON VERB DET NOUN ADP VERB DET ADJ NOUN NOUN ADP NOUN VERB PROPN PUNCT PROPN PUNCT PROPN VERB NOUN ADP NOUN PUNCT SCONJ DET NOUN VERB ADP DET ADJ PUNCT ADJ NOUN CCONJ ADV VERB NOUN PART VERB DET NOUN NOUN PUNCT PRON VERB PROPN PART VERB CCONJ VERB DET NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP PROPN ADP ADJ NOUN ADJ NOUN ADP PROPN PUNCT DET NUM NOUN PROPN NOUN NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP PROPN PART VERB NOUN PROPN VERB DET ADJ NOUN NOUN PUNCT ADP VERB PROPN PUNCT PRON VERB SCONJ PRON PROPN CCONJ PROPN AUX VERB NOUN VERB NOUN PUNCT ADP NOUN ADP ADJ ADP NUM ADP ADP NUM ADP DET NOUN PUNCT ADP PROPN CCONJ PROPN PUNCT ADV SCONJ DET PROPN NOUN AUX VERB VERB DET PROPN NOUN NOUN PUNCT ADV PUNCT PRON VERB SCONJ PROPN VERB PROPN PUNCT DET ADJ VERB CCONJ NUM ADJ NOUN ADP PROPN PUNCT PART VERB DET NOUN ADP PROPN ADP DET ADJ NOUN PUNCT ADV PUNCT DET NOUN VERB ADJ NOUN ADP VERB NOUN CCONJ NOUN PRON AUX VERB NOUN VERB ADJ NOUN PUNCT,0.5174825174825175,23.833333333333332,5.206293706293707
27,27,Aman Madaan,"[' We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches “serialize” the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the fewshot setting. Our code and data are available at https://github.com/madaan/ CoCoGen .']",abstract_chunked," We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches “serialize” the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the fewshot setting. Our code and data are available at https://github.com/madaan/ CoCoGen .",40.83640883977904,30.71625077440773,27,0.4305422306060791," We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning graph. To employ large language models for this task, existing approaches serialize the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language Propname that LMs were pre trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation Propname outperforms natural LMs that are fine tuned on the target task and other strong LMs such as Propname 0 in the fewshot setting. Our code and data are available at Propname."," We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning graph. To employ large language models for this task, existing approaches serialize the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language Propname that LMs were pre trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation Propname outperforms natural LMs that are fine tuned on the target task and other strong LMs such as Propname 0 in the fewshot setting. Our code and data are available at Propname.", PRON VERB DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT VERB DET ADJ NOUN NOUN PUNCT DET NOUN AUX PART VERB DET NOUN ADJ ADP DET NOUN CCONJ DET NOUN NOUN PUNCT PART VERB ADJ NOUN NOUN ADP DET NOUN PUNCT VERB NOUN VERB DET NOUN NOUN ADP DET ADJ NOUN ADP NOUN CCONJ NOUN PUNCT SCONJ ADJ PUNCT DET ADJ NOUN ADV VERB ADP DET ADJ NOUN PROPN SCONJ NOUN AUX VERB VERB ADP PUNCT VERB NOUN ADP VERB PRON ADV PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ SCONJ PRON ADV VERB VERB NOUN NOUN NOUN ADP NOUN NOUN NOUN PUNCT VERB VERB NOUN ADP NOUN AUX ADJ ADJ NOUN NOUN ADP NOUN ADP ADJ NOUN PUNCT ADV SCONJ DET ADJ NOUN AUX PART VERB NOUN NOUN ADV ADV PUNCT PRON VERB PRON NOUN ADP NUM ADJ ADJ NOUN NOUN NOUN PUNCT ADP DET DET ADJ NOUN NOUN PUNCT PRON VERB SCONJ VERB PRON NOUN PUNCT DET NOUN NOUN PROPN VERB ADJ NOUN PRON AUX ADJ VERB ADP DET NOUN NOUN CCONJ ADJ ADJ NOUN ADJ ADP PROPN NUM ADP DET ADJ NOUN PUNCT PRON NOUN CCONJ NOUN AUX ADJ ADP PROPN PUNCT,0.5282051282051282,27.857142857142858,4.758974358974359
28,28,Aman Madaan,"[' In the past decade, we witnessed dramatic gains in natural language processing and an\nunprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (COT) prompting. Specifically, COT pushes the performance of large language models in a few-shot setup by\naugmenting the prompts with intermediate steps. Despite impressive results across various\ntasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT-based few-shot prompting\nmechanisms in large language models. We first systematically identify and define the\nkey components of a prompt: symbols, patterns, and text. Then, we devise and conduct\nan exhaustive set of deliberated experiments across four different tasks, by querying the\nmodel with counterfactual prompts where only one of these components is altered. Our\nexperiments across three models—PaLM, GPT-3, and CODEX—reveal several surprising\nfindings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success\nof COT. Second, our results conclude that the primary role of intermediate steps may not\nbe to facilitate learning “how” to solve a task.', 'The intermediate steps are rather a beacon\nfor the model to realize “what” symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to “trick” the model into forming sentences\nthat resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis\nreveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and\npatterns enforce task understanding and direct text generation. Such systematic understanding of COT enables us to devise a concise chain of thought, dubbed as CCOT, where\ntext and patterns are pruned by over 20%, only retaining their key roles. We achieve this\nreduction in the number of tokens while delivering on par or slightly higher solve task rate.']",abstract_chunked," In the past decade, we witnessed dramatic gains in natural language processing and an
unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (COT) prompting. Specifically, COT pushes the performance of large language models in a few-shot setup by
augmenting the prompts with intermediate steps. Despite impressive results across various
tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT-based few-shot prompting
mechanisms in large language models. We first systematically identify and define the
key components of a prompt: symbols, patterns, and text. Then, we devise and conduct
an exhaustive set of deliberated experiments across four different tasks, by querying the
model with counterfactual prompts where only one of these components is altered. Our
experiments across three models—PaLM, GPT-3, and CODEX—reveal several surprising
findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success
of COT. Second, our results conclude that the primary role of intermediate steps may not
be to facilitate learning “how” to solve a task.",37.46164634146342,30.71625077440773,28,0.22856290638446808," In the past decade, we witnessed dramatic gains in natural language processing and an 
 unprecedented scaling of large language models. These developments have been accelerated by the advent of few shot techniques such as chain of thought prompting. Specifically, COT pushes the performance of large language models in a few shot setup by 
 augmenting the prompts with intermediate steps. Despite impressive results across various 
 tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT based few shot prompting 
 mechanisms in large language models. We first systematically identify and define the 
 key components of a prompt: symbols, patterns, and text. Then, we devise and conduct 
 an exhaustive set of deliberated experiments across four different tasks, by querying the 
 model with counterfactual prompts where only one of these components is altered. Our 
 experiments across three Propname, Propname 0, and CODEXreveal several surprising 
 findings and brings into question the conventional wisdom around few shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success 
 of Propname. Second, our results conclude that the primary role of intermediate steps may not 
 be to facilitate learning how to solve a task."," In the past decade, we witnessed dramatic gains in natural language processing and an 
 unprecedented scaling of large language models. These developments have been accelerated by the advent of few shot techniques such as chain of thought prompting. Specifically, COT pushes the performance of large language models in a few shot setup by 
 augmenting the prompts with intermediate steps. Despite impressive results across various 
 tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT based few shot prompting 
 mechanisms in large language models. We first systematically identify and define the 
 key components of a prompt: symbols, patterns, and text. Then, we devise and conduct 
 an exhaustive set of deliberated experiments across four different tasks, by querying the 
 model with counterfactual prompts where only one of these components is altered. Our 
 experiments across three Propname, Propname 0, and CODEXreveal several surprising 
 findings and brings into question the conventional wisdom around few shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success 
 of Propname. Second, our results conclude that the primary role of intermediate steps may not 
 be to facilitate learning how to solve a task.", ADP DET NOUN NOUN PUNCT PRON VERB ADJ NOUN ADP ADJ NOUN NOUN CCONJ DET SPACE ADJ NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN AUX AUX VERB ADP DET NOUN ADP ADJ VERB NOUN ADJ ADP NOUN ADP NOUN VERB PUNCT ADV PUNCT NOUN VERB DET NOUN ADP ADJ NOUN NOUN ADP DET ADJ NOUN NOUN ADP SPACE VERB DET NOUN ADP ADJ NOUN PUNCT SCONJ ADJ NOUN ADP ADJ SPACE NOUN PUNCT DET NOUN ADP PRON NOUN AUX PART AUX VERB PUNCT DET NOUN VERB ADJ NOUN PART VERB DET ADJ NOUN ADP NOUN VERB ADJ NOUN VERB SPACE NOUN ADP ADJ NOUN NOUN PUNCT PRON ADV ADV VERB CCONJ VERB DET SPACE ADJ NOUN ADP DET NOUN PUNCT NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT ADV PUNCT PRON VERB CCONJ VERB SPACE DET ADJ NOUN ADP VERB NOUN ADP NUM ADJ NOUN PUNCT ADP VERB DET SPACE NOUN ADP ADJ NOUN SCONJ ADV NUM ADP DET NOUN AUX VERB PUNCT PRON SPACE NOUN ADP NUM PROPN PUNCT PROPN NUM PUNCT CCONJ VERB ADJ ADJ SPACE NOUN CCONJ VERB ADP NOUN DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT DET NOUN ADP ADJ NOUN ADP DET NOUN AUX ADV ADJ ADP DET NOUN SPACE ADP PROPN PUNCT ADV PUNCT PRON NOUN VERB SCONJ DET ADJ NOUN ADP ADJ NOUN AUX PART SPACE AUX PART VERB VERB SCONJ PART VERB DET NOUN PUNCT,0.5777777777777777,22.5,5.066666666666666
29,29,Aman Madaan,"[' In the past decade, we witnessed dramatic gains in natural language processing and an\nunprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (COT) prompting. Specifically, COT pushes the performance of large language models in a few-shot setup by\naugmenting the prompts with intermediate steps. Despite impressive results across various\ntasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of COT-based few-shot prompting\nmechanisms in large language models. We first systematically identify and define the\nkey components of a prompt: symbols, patterns, and text. Then, we devise and conduct\nan exhaustive set of deliberated experiments across four different tasks, by querying the\nmodel with counterfactual prompts where only one of these components is altered. Our\nexperiments across three models—PaLM, GPT-3, and CODEX—reveal several surprising\nfindings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success\nof COT. Second, our results conclude that the primary role of intermediate steps may not\nbe to facilitate learning “how” to solve a task.', 'The intermediate steps are rather a beacon\nfor the model to realize “what” symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to “trick” the model into forming sentences\nthat resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis\nreveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and\npatterns enforce task understanding and direct text generation. Such systematic understanding of COT enables us to devise a concise chain of thought, dubbed as CCOT, where\ntext and patterns are pruned by over 20%, only retaining their key roles. We achieve this\nreduction in the number of tokens while delivering on par or slightly higher solve task rate.']",abstract_chunked,"The intermediate steps are rather a beacon
for the model to realize “what” symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to “trick” the model into forming sentences
that resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis
reveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and
patterns enforce task understanding and direct text generation. Such systematic understanding of COT enables us to devise a concise chain of thought, dubbed as CCOT, where
text and patterns are pruned by over 20%, only retaining their key roles. We achieve this
reduction in the number of tokens while delivering on par or slightly higher solve task rate.",40.46000000000001,30.71625077440773,29,0.5836986899375916," The intermediate steps are rather a beacon 
 for the model to realize what symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to trick the model into forming sentences 
 that resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis 
 reveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and 
 patterns enforce task understanding and direct text generation. Such systematic understanding of Propname enables us to devise a concise chain of thought, dubbed as Propname, where 
 text and patterns are pruned by over 00, only retaining their key roles. We achieve this 
 reduction in the number of Propname while delivering on par or slightly higher solve task rate."," The intermediate steps are rather a beacon 
 for the model to realize what symbols to replicate in the output to form a factual answer. As such, the patterns are merely a channel to trick the model into forming sentences 
 that resemble correct answers. This pathway is facilitated by text, which imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis 
 reveals that a symbiotic relationship between text and patterns explains the success of fewshot prompting: text helps extract commonsense from the question to help patterns, and 
 patterns enforce task understanding and direct text generation. Such systematic understanding of Propname enables us to devise a concise chain of thought, dubbed as Propname, where 
 text and patterns are pruned by over 00, only retaining their key roles. We achieve this 
 reduction in the number of Propname while delivering on par or slightly higher solve task rate.", DET ADJ NOUN AUX ADV DET NOUN SPACE ADP DET NOUN PART VERB PRON NOUN PART VERB ADP DET NOUN PART VERB DET ADJ NOUN PUNCT ADP ADJ PUNCT DET NOUN AUX ADV DET NOUN PART VERB DET NOUN ADP VERB NOUN SPACE PRON VERB ADJ NOUN PUNCT DET NOUN AUX VERB ADP NOUN PUNCT PRON VERB NOUN ADP NOUN NOUN CCONJ NOUN PUNCT PRON ADJ CCONJ ADJ NOUN SPACE VERB SCONJ DET ADJ NOUN ADP NOUN CCONJ NOUN VERB DET NOUN ADP ADJ NOUN PUNCT NOUN VERB VERB NOUN ADP DET NOUN PART VERB NOUN PUNCT CCONJ SPACE NOUN VERB NOUN NOUN CCONJ ADJ NOUN NOUN PUNCT ADJ ADJ NOUN ADP PROPN VERB PRON PART VERB DET NOUN NOUN ADP NOUN PUNCT VERB ADP PROPN PUNCT SCONJ SPACE NOUN CCONJ NOUN AUX VERB ADP ADP NUM PUNCT ADV VERB PRON ADJ NOUN PUNCT PRON VERB DET SPACE NOUN ADP DET NOUN ADP PROPN SCONJ VERB ADP NOUN CCONJ ADV ADJ VERB NOUN NOUN PUNCT,0.6666666666666666,26.5,4.943396226415095
30,30,Aman Madaan,"[' Machine learning systems typically apply the\nsame model to both easy and tough cases. This is\nin stark contrast with humans, who tend to evoke\neither fast (instinctive) or slow (analytical) thinking process, depending on the difficulty of the\nproblem—a property called the dual-process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual-process\ntheory of mind. Depending on the difficulty of\ngraph completion at the current step, the system either calls a FAST (weaker) module or a\nSLOW (stronger) module for the task. These modules have identical architectures, but vary in the\nnumber of parameters and consequently differ\nin generative power. Experiments on real-world\ngraphs show that FLOWGEN can successfully generate graphs similar to those generated by a single\nlarge model, while being up to 2x faster.']",abstract_chunked," Machine learning systems typically apply the
same model to both easy and tough cases. This is
in stark contrast with humans, who tend to evoke
either fast (instinctive) or slow (analytical) thinking process, depending on the difficulty of the
problem—a property called the dual-process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual-process
theory of mind. Depending on the difficulty of
graph completion at the current step, the system either calls a FAST (weaker) module or a
SLOW (stronger) module for the task. These modules have identical architectures, but vary in the
number of parameters and consequently differ
in generative power. Experiments on real-world
graphs show that FLOWGEN can successfully generate graphs similar to those generated by a single
large model, while being up to 2x faster.",35.49072055137847,30.71625077440773,30,0.3967522382736206," Machine learning systems typically apply the 
 same model to both easy and tough cases. This is 
 in stark contrast with humans, who tend to evoke 
 either fast or slow thinking process, depending on the difficulty of the 
 problema property called the dual process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual process 
 theory of mind. Depending on the difficulty of 
 graph completion at the current step, the system either calls a FAST module or a 
 SLOW module for the task. These modules have identical architectures, but vary in the 
 number of parameters and consequently differ 
 in generative power. Experiments on real world 
 graphs show that Propname can successfully generate graphs similar to those generated by a single 
 large model, while being up to Propname faster."," Machine learning systems typically apply the 
 same model to both easy and tough cases. This is 
 in stark contrast with humans, who tend to evoke 
 either fast or slow thinking process, depending on the difficulty of the 
 problema property called the dual process theory of mind. We present FLOWGEN, a graphgeneration model inspired by the dual process 
 theory of mind. Depending on the difficulty of 
 graph completion at the current step, the system either calls a FAST module or a 
 SLOW module for the task. These modules have identical architectures, but vary in the 
 number of parameters and consequently differ 
 in generative power. Experiments on real world 
 graphs show that Propname can successfully generate graphs similar to those generated by a single 
 large model, while being up to Propname faster.", NOUN NOUN NOUN ADV VERB DET SPACE ADJ NOUN ADP CCONJ ADJ CCONJ ADJ NOUN PUNCT PRON AUX SPACE ADP ADJ NOUN ADP NOUN PUNCT PRON VERB PART VERB SPACE CCONJ ADV CCONJ ADJ NOUN NOUN PUNCT VERB ADP DET NOUN ADP DET SPACE NOUN NOUN VERB DET ADJ NOUN NOUN ADP NOUN PUNCT PRON VERB NOUN PUNCT DET NOUN NOUN VERB ADP DET ADJ NOUN SPACE NOUN ADP NOUN PUNCT VERB ADP DET NOUN ADP SPACE NOUN NOUN ADP DET ADJ NOUN PUNCT DET NOUN CCONJ VERB DET ADJ NOUN CCONJ DET SPACE ADJ NOUN ADP DET NOUN PUNCT DET NOUN VERB ADJ NOUN PUNCT CCONJ VERB ADP DET SPACE NOUN ADP NOUN CCONJ ADV VERB SPACE ADP ADJ NOUN PUNCT NOUN ADP ADJ NOUN SPACE NOUN VERB SCONJ PROPN AUX ADV VERB NOUN ADJ ADP PRON VERB ADP DET ADJ SPACE ADJ NOUN PUNCT SCONJ AUX ADP ADP PROPN ADV PUNCT,0.6619718309859155,23.666666666666668,4.711267605633803
31,31,Aman Madaan,"[' Conditional set generation learns a mapping\nfrom an input sequence of tokens to a set. Several NLP tasks, such as entity typing and\ndialogue emotion tagging, are instances of\nset generation. SEQ2SEQ models, a popular choice for set generation, treat a set as\na sequence and do not fully leverage its key\nproperties, namely order-invariance and cardinality. We propose a novel algorithm for\neffectively sampling informative orders over\nthe combinatorial space of label orders. We\njointly model the set cardinality and output\nby prepending the set size and taking advantage of the autoregressive factorization used\nby SEQ2SEQ models. Our method is a modelindependent data augmentation approach that\nendows any SEQ2SEQ model with the signals\nof order-invariance and cardinality. Training a\nSEQ2SEQ model on this augmented data (without any additional annotations) gets an average\nrelative improvement of 20% on four benchmark datasets across various models: BARTbase, T5-11B, and GPT3-175B.']",abstract_chunked," Conditional set generation learns a mapping
from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and
dialogue emotion tagging, are instances of
set generation. SEQ2SEQ models, a popular choice for set generation, treat a set as
a sequence and do not fully leverage its key
properties, namely order-invariance and cardinality. We propose a novel algorithm for
effectively sampling informative orders over
the combinatorial space of label orders. We
jointly model the set cardinality and output
by prepending the set size and taking advantage of the autoregressive factorization used
by SEQ2SEQ models. Our method is a modelindependent data augmentation approach that
endows any SEQ2SEQ model with the signals
of order-invariance and cardinality. Training a
SEQ2SEQ model on this augmented data (without any additional annotations) gets an average
relative improvement of 20% on four benchmark datasets across various models: BARTbase, T5-11B, and GPT3-175B.",25.53032258064519,30.71625077440773,31,0.5514452457427979," Conditional set generation learns a mapping 
 from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and 
 dialogue emotion tagging, are instances of 
 set generation. Propname models, a popular choice for set generation, treat a set as 
 a sequence and do not fully leverage its key 
 properties, namely order invariance and cardinality. We propose a novel algorithm for 
 effectively sampling informative orders over 
 the combinatorial space of label orders. We 
 jointly model the set cardinality and output 
 by prepending the set size and taking advantage of the autoregressive factorization used 
 by Propname models. Our method is a modelindependent data augmentation approach that 
 endows any Propname model with the signals 
 of order invariance and cardinality. Training a 
 Propname model on this augmented data gets an average 
 relative improvement of 00 on four benchmark datasets across various models: Propname, Propname 00B, and Propname Propname"," Conditional set generation learns a mapping 
 from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and 
 dialogue emotion tagging, are instances of 
 set generation. Propname models, a popular choice for set generation, treat a set as 
 a sequence and do not fully leverage its key 
 properties, namely order invariance and cardinality. We propose a novel algorithm for 
 effectively sampling informative orders over 
 the combinatorial space of label orders. We 
 jointly model the set cardinality and output 
 by prepending the set size and taking advantage of the autoregressive factorization used 
 by Propname models. Our method is a modelindependent data augmentation approach that 
 endows any Propname model with the signals 
 of order invariance and cardinality. Training a 
 Propname model on this augmented data gets an average 
 relative improvement of 00 on four benchmark datasets across various models: Propname, Propname 00B, and Propname Propname", ADJ NOUN NOUN VERB DET NOUN SPACE ADP DET NOUN NOUN ADP NOUN ADP DET NOUN PUNCT ADJ NOUN NOUN PUNCT ADJ ADP NOUN NOUN CCONJ SPACE NOUN NOUN NOUN PUNCT AUX NOUN ADP SPACE VERB NOUN PUNCT PROPN NOUN PUNCT DET ADJ NOUN ADP ADJ NOUN PUNCT VERB DET NOUN ADP SPACE DET NOUN CCONJ AUX PART ADV VERB PRON ADJ SPACE NOUN PUNCT ADV VERB NOUN CCONJ NOUN PUNCT PRON VERB DET ADJ NOUN ADP SPACE ADV VERB ADJ NOUN ADP SPACE DET ADJ NOUN ADP NOUN NOUN PUNCT PRON SPACE ADV VERB DET VERB NOUN CCONJ NOUN SPACE ADP VERB DET VERB NOUN CCONJ VERB NOUN ADP DET ADJ NOUN VERB SPACE ADP PROPN NOUN PUNCT PRON NOUN AUX DET ADJ NOUN NOUN NOUN PRON SPACE VERB DET PROPN NOUN ADP DET NOUN SPACE ADP NOUN NOUN CCONJ NOUN PUNCT VERB DET SPACE PROPN NOUN ADP DET VERB NOUN VERB DET ADJ SPACE ADJ NOUN ADP NUM ADP NUM ADJ NOUN ADP ADJ NOUN PUNCT PROPN PUNCT PROPN NUM PUNCT CCONJ PROPN PROPN,0.5987654320987654,23.142857142857142,5.055555555555555
32,32,Aman Madaan,"[' Large LMs such as GPT-3 are powerful, but\ncan commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly\ninterpret ""What word is similar to good?"" to\nmean a homophone, while the user intended\na synonym. Our goal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be\nprohibitively costly. We pair GPT-3 with a\ngrowing memory of recorded cases where the\nmodel misunderstood the user’s intents, along\nwith user feedback for clarification. Such\na memory allows our system to produce enhanced prompts for any new query based on\nthe user feedback for error correction on similar cases in the past. On four tasks (two lexical\ntasks, two advanced ethical reasoning tasks),\nwe show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained\nLMs.']",abstract_chunked," Large LMs such as GPT-3 are powerful, but
can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly
interpret ""What word is similar to good?"" to
mean a homophone, while the user intended
a synonym. Our goal is to effectively correct such errors via user interactions with the
system but without retraining, which will be
prohibitively costly. We pair GPT-3 with a
growing memory of recorded cases where the
model misunderstood the user’s intents, along
with user feedback for clarification. Such
a memory allows our system to produce enhanced prompts for any new query based on
the user feedback for error correction on similar cases in the past. On four tasks (two lexical
tasks, two advanced ethical reasoning tasks),
we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT3. Our approach is a step towards the low-cost
utility enhancement for very large pre-trained
LMs.",39.3828879310345,30.71625077440773,32,0.33368945121765137," Large LMs such as Propname 0 are powerful, but 
 can commit mistakes that are obvious to humans. For example, Propname 0 would mistakenly 
 interpret What word is similar to good? to 
 mean a homophone, while the user intended 
 a synonym. Our goal is to effectively correct such errors via user interactions with the 
 system but without retraining, which will be 
 prohibitively costly. We pair Propname 0 with a 
 growing memory of recorded cases where the 
 model misunderstood the users intents, along 
 with user feedback for clarification. Such 
 a memory allows our system to produce enhanced prompts for any new query based on 
 the user feedback for error correction on similar cases in the past. On four tasks two lexical 
 tasks, two advanced ethical reasoning tasks, 
 we show how a user can interactively teach a deployed Propname 0, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the Propname. Our approach is a step towards the low cost 
 utility enhancement for very large pre trained 
 LMs."," Large LMs such as Propname 0 are powerful, but 
 can commit mistakes that are obvious to humans. For example, Propname 0 would mistakenly 
 interpret What word is similar to good? to 
 mean a homophone, while the user intended 
 a synonym. Our goal is to effectively correct such errors via user interactions with the 
 system but without retraining, which will be 
 prohibitively costly. We pair Propname 0 with a 
 growing memory of recorded cases where the 
 model misunderstood the users intents, along 
 with user feedback for clarification. Such 
 a memory allows our system to produce enhanced prompts for any new query based on 
 the user feedback for error correction on similar cases in the past. On four tasks two lexical 
 tasks, two advanced ethical reasoning tasks, 
 we show how a user can interactively teach a deployed Propname 0, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the Propname. Our approach is a step towards the low cost 
 utility enhancement for very large pre trained 
 LMs.", ADJ NOUN ADJ ADP PROPN NUM AUX ADJ PUNCT CCONJ SPACE AUX VERB NOUN PRON AUX ADJ ADP NOUN PUNCT ADP NOUN PUNCT PROPN NUM AUX ADV SPACE VERB DET NOUN AUX ADJ ADP ADJ PUNCT PART SPACE VERB DET NOUN PUNCT SCONJ DET NOUN VERB SPACE DET NOUN PUNCT PRON NOUN AUX PART ADV VERB ADJ NOUN ADP NOUN NOUN ADP DET SPACE NOUN CCONJ ADP NOUN PUNCT PRON AUX AUX SPACE ADV ADJ PUNCT PRON VERB PROPN NUM ADP DET SPACE VERB NOUN ADP VERB NOUN SCONJ DET SPACE NOUN NOUN DET NOUN NOUN PUNCT ADP SPACE ADP NOUN NOUN ADP NOUN PUNCT ADJ SPACE DET NOUN VERB PRON NOUN PART VERB ADJ NOUN ADP DET ADJ NOUN VERB ADP SPACE DET NOUN NOUN ADP NOUN NOUN ADP ADJ NOUN ADP DET NOUN PUNCT ADP NUM NOUN NUM ADJ SPACE NOUN PUNCT NUM ADJ ADJ NOUN NOUN PUNCT SPACE PRON VERB SCONJ DET NOUN AUX ADV VERB DET VERB PROPN NUM PUNCT ADV VERB PRON NOUN ADP DET NOUN ADP ADJ NOUN ADP NOUN ADP DET PROPN PUNCT PRON NOUN AUX DET NOUN ADP DET ADJ NOUN SPACE NOUN NOUN ADP ADV ADJ ADJ VERB SPACE NOUN PUNCT,0.6432432432432432,23.125,4.664864864864865
33,33,Aman Madaan,"[' Defeasible reasoning is the mode of reasoning\nwhere conclusions can be overturned by taking\ninto account new evidence. Existing cognitive\nscience literature on defeasible reasoning suggests that a person forms a mental model of the\nproblem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the\nquestion scenario before answering a defeasible query. Our approach is, given a question,\nto have a model first create a graph of relevant\ninfluences, and then leverage that graph as an\nadditional input when answering the question. Our system, CURIOUS, achieves a new stateof-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by\nguiding a system to “think about” a question\nand explicitly model the scenario, rather than\nanswering reflexively.']",abstract_chunked," Defeasible reasoning is the mode of reasoning
where conclusions can be overturned by taking
into account new evidence. Existing cognitive
science literature on defeasible reasoning suggests that a person forms a mental model of the
problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the
question scenario before answering a defeasible query. Our approach is, given a question,
to have a model first create a graph of relevant
influences, and then leverage that graph as an
additional input when answering the question. Our system, CURIOUS, achieves a new stateof-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by
guiding a system to “think about” a question
and explicitly model the scenario, rather than
answering reflexively.",23.570833333333354,30.71625077440773,33,0.21827168762683868," Defeasible reasoning is the mode of reasoning 
 where conclusions can be overturned by taking 
 into account new evidence. Existing cognitive 
 science literature on defeasible reasoning suggests that a person forms a mental model of the 
 problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the 
 question scenario before answering a defeasible query. Our approach is, given a question, 
 to have a model first create a graph of relevant 
 influences, and then leverage that graph as an 
 additional input when answering the question. Our system, Propname, achieves a new stateof the art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by 
 guiding a system to think about a question 
 and explicitly model the scenario, rather than 
 answering reflexively."," Defeasible reasoning is the mode of reasoning 
 where conclusions can be overturned by taking 
 into account new evidence. Existing cognitive 
 science literature on defeasible reasoning suggests that a person forms a mental model of the 
 problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the 
 question scenario before answering a defeasible query. Our approach is, given a question, 
 to have a model first create a graph of relevant 
 influences, and then leverage that graph as an 
 additional input when answering the question. Our system, Propname, achieves a new stateof the art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by 
 guiding a system to think about a question 
 and explicitly model the scenario, rather than 
 answering reflexively.", ADJ NOUN AUX DET NOUN ADP NOUN SPACE SCONJ NOUN AUX AUX VERB ADP VERB SPACE ADP NOUN ADJ NOUN PUNCT VERB ADJ SPACE NOUN NOUN ADP ADJ NOUN VERB SCONJ DET NOUN VERB DET ADJ NOUN ADP DET SPACE NOUN NOUN ADP VERB NOUN PUNCT PRON NOUN NOUN VERB SCONJ ADJ NOUN AUX ADV VERB ADP VERB DET SPACE NOUN NOUN ADP VERB DET ADJ NOUN PUNCT PRON NOUN AUX PUNCT VERB DET NOUN PUNCT SPACE PART VERB DET NOUN ADV VERB DET NOUN ADP ADJ SPACE NOUN PUNCT CCONJ ADV VERB DET NOUN ADP DET SPACE ADJ NOUN SCONJ VERB DET NOUN PUNCT PRON NOUN PUNCT PROPN PUNCT VERB DET ADJ NOUN DET NOUN ADP NUM ADJ ADJ NOUN NOUN PUNCT DET NOUN AUX ADJ SCONJ PRON VERB SCONJ NOUN AUX AUX VERB ADP SPACE VERB DET NOUN PART VERB ADP DET NOUN SPACE CCONJ ADV VERB DET NOUN PUNCT ADV ADP SPACE VERB ADV PUNCT,0.6054421768707483,24.5,5.163265306122449
34,34,Aman Madaan,"[' Defeasible reasoning is a mode of reasoning\nwhere conclusions can be overturned by taking into account new evidence. A commonly\nused method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference\ngraphs through transfer learning from a related\nNLP task that shares the kind of reasoning that\ninference graphs support. Through automated\nmetrics and human evaluation, we find that our\nmethod generates meaningful graphs for the\ndefeasible inference task. Human accuracy on\nthis task improves by 20% by consulting the\ngenerated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning.']",abstract_chunked," Defeasible reasoning is a mode of reasoning
where conclusions can be overturned by taking into account new evidence. A commonly
used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference
graphs through transfer learning from a related
NLP task that shares the kind of reasoning that
inference graphs support. Through automated
metrics and human evaluation, we find that our
method generates meaningful graphs for the
defeasible inference task. Human accuracy on
this task improves by 20% by consulting the
generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning.",30.107857142857142,30.71625077440773,34,0.6099236607551575," Defeasible reasoning is a mode of reasoning 
 where conclusions can be overturned by taking into account new evidence. A commonly 
 used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference 
 graphs through transfer learning from a related 
 Propname task that shares the kind of reasoning that 
 inference graphs support. Through automated 
 metrics and human evaluation, we find that our 
 method generates meaningful graphs for the 
 defeasible inference task. Human accuracy on 
 this task improves by 00 by consulting the 
 generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning."," Defeasible reasoning is a mode of reasoning 
 where conclusions can be overturned by taking into account new evidence. A commonly 
 used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference 
 graphs through transfer learning from a related 
 Propname task that shares the kind of reasoning that 
 inference graphs support. Through automated 
 metrics and human evaluation, we find that our 
 method generates meaningful graphs for the 
 defeasible inference task. Human accuracy on 
 this task improves by 00 by consulting the 
 generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning.", ADJ NOUN AUX DET NOUN ADP NOUN SPACE SCONJ NOUN AUX AUX VERB ADP VERB ADP NOUN ADJ NOUN PUNCT DET ADV SPACE VERB NOUN ADP ADJ NOUN CCONJ NOUN NOUN AUX PART VERB NOUN VERB NOUN NOUN PUNCT SCONJ NOUN VERB NOUN NOUN ADV ADJ ADP NOUN PUNCT VERB PRON ADP NOUN AUX ADJ PUNCT ADP DET NOUN PUNCT PRON ADV VERB ADJ NOUN SPACE NOUN ADP NOUN VERB ADP DET VERB SPACE PROPN NOUN PRON VERB DET NOUN ADP NOUN PRON SPACE NOUN NOUN NOUN PUNCT ADP VERB SPACE NOUN CCONJ ADJ NOUN PUNCT PRON VERB SCONJ PRON SPACE NOUN VERB ADJ NOUN ADP DET SPACE ADJ NOUN NOUN PUNCT ADJ NOUN ADP SPACE DET NOUN VERB ADP NUM ADP VERB DET SPACE VERB NOUN PUNCT PRON NOUN VERB ADP ADJ ADJ NOUN NOUN ADP NOUN SCONJ NOUN NOUN AUX VERB ADJ NOUN PUNCT,0.6691176470588235,19.428571428571427,5.338235294117647
35,35,Aman Madaan,"[' A class of explainable NLP models for reasoning tasks support their decisions by generating free-form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce MERCURIE- an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains.']",abstract_chunked," A class of explainable NLP models for reasoning tasks support their decisions by generating free-form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce MERCURIE- an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains.",24.896245283018885,30.71625077440773,35,0.35036012530326843," A class of explainable NLP models for reasoning tasks support their decisions by generating free form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce Propname an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 00 fewer inconsistencies as compared with the off the shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 0.0 points on accuracy on defeasible reasoning across all three domains."," A class of explainable NLP models for reasoning tasks support their decisions by generating free form or structured explanations, but what happens when these supporting structures contain errors? Our goal is to allow users to interactively correct explanation structures through natural language feedback. We introduce Propname an interactive system that refines its explanations for a given reasoning task by getting human feedback in natural language. Our approach generates graphs that have 00 fewer inconsistencies as compared with the off the shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 0.0 points on accuracy on defeasible reasoning across all three domains.", DET NOUN ADP ADJ NOUN NOUN ADP NOUN NOUN VERB PRON NOUN ADP VERB ADJ NOUN CCONJ ADJ NOUN PUNCT CCONJ PRON VERB SCONJ DET VERB NOUN VERB NOUN PUNCT PRON NOUN AUX PART VERB NOUN PART ADV VERB NOUN NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB PROPN DET ADJ NOUN PRON VERB PRON NOUN ADP DET VERB NOUN NOUN ADP VERB ADJ NOUN ADP ADJ NOUN PUNCT PRON NOUN VERB NOUN PRON VERB NUM ADJ NOUN SCONJ VERB ADP DET ADP DET NOUN NOUN PUNCT ADV PUNCT ADV VERB DET VERB NOUN NOUN ADP DET NOUN VERB ADP DET NOUN ADP NUM NOUN ADP NOUN ADP ADJ NOUN ADP DET NUM NOUN PUNCT,0.7652173913043478,23.0,5.417391304347826
36,36,Aman Madaan,"[' Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a “what-if” Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning']",abstract_chunked," Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a “what-if” Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning",26.356297520661172,30.71625077440773,36,0.615426242351532," Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present Propname a method to leverage pre trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by Propname improve the performance on a what if Propname Answering benchmark, especially for questions that require background knowledge and multi hop reasoning"," Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present Propname a method to leverage pre trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by Propname improve the performance on a what if Propname Answering benchmark, especially for questions that require background knowledge and multi hop reasoning", VERB ADP NOUN CCONJ VERB PRON NOUN AUX ADJ ADP NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN DET NOUN PART VERB ADJ VERB NOUN NOUN PART VERB NOUN NOUN VERB ADP DET NOUN PUNCT NOUN ADP PRON NOUN PUNCT CCONJ DET NOUN ADP DET NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN ADP NOUN CCONJ NOUN ADP NOUN ADP NOUN NOUN NOUN PUNCT NOUN VERB ADJ NOUN CCONJ ADP NOUN ADP VERB NOUN NOUN CCONJ ADJ NOUN ADP NOUN ADP NOUN CCONJ NOUN ADP NOUN PUNCT ADV PUNCT PRON VERB SCONJ DET NOUN NOUN VERB ADP PROPN VERB DET NOUN ADP DET PRON SCONJ PROPN VERB NOUN PUNCT ADV ADP NOUN PRON VERB NOUN NOUN CCONJ VERB NOUN VERB,0.6666666666666666,24.6,5.390243902439025
37,37,Aman Madaan,"[' This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with humanannotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the systeminduced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-ofdomain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting']",abstract_chunked," This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with humanannotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the systeminduced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-ofdomain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting",15.855604838709695,30.71625077440773,37,0.42146939039230347," This paper presents the first study on using large scale pre trained language models for automated generation of an event level temporal graph for a document. Despite the huge success of neural pre training methods in Propname tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training Propname with humanannotated events and temporal links. We address this challenge by using existing Propname tools to automatically generate a large quantity of system produced document graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence to sequence mapping task. These strategies enable us to leverage and fine tune pre trained language models on the systeminduced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand labeled, out Propname Propname shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open ended temporal questions in a reading comprehension setting"," This paper presents the first study on using large scale pre trained language models for automated generation of an event level temporal graph for a document. Despite the huge success of neural pre training methods in Propname tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training Propname with humanannotated events and temporal links. We address this challenge by using existing Propname tools to automatically generate a large quantity of system produced document graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence to sequence mapping task. These strategies enable us to leverage and fine tune pre trained language models on the systeminduced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand labeled, out Propname Propname shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open ended temporal questions in a reading comprehension setting", DET NOUN VERB DET ADJ NOUN ADP VERB ADJ NOUN VERB VERB NOUN NOUN ADP VERB NOUN ADP DET NOUN NOUN ADJ NOUN ADP DET NOUN PUNCT SCONJ DET ADJ NOUN ADP ADJ ADJ NOUN NOUN ADP PROPN NOUN PUNCT PRON NOUN ADP ADJ NOUN ADP NOUN NOUN AUX PART AUX ADV VERB PUNCT NOUN ADP DET NOUN AUX DET NOUN ADP VERB ADJ NOUN PROPN ADP VERB NOUN CCONJ ADJ NOUN PUNCT PRON VERB DET NOUN ADP VERB VERB PROPN NOUN PART ADV VERB DET ADJ NOUN ADP NOUN VERB NOUN NOUN NOUN PUNCT CCONJ VERB DET ADJ NOUN ADP DET VERB NOUN NOUN NOUN ADP DET NOUN PART VERB NOUN NOUN PUNCT DET NOUN VERB PRON ADP NOUN CCONJ ADJ NOUN VERB VERB NOUN NOUN ADP DET VERB NOUN NOUN ADP DET NOUN NOUN NOUN PUNCT PRON NOUN VERB SCONJ PRON NOUN AUX ADV ADJ ADP VERB ADV CCONJ ADV ADJ NOUN PUNCT ADV PUNCT NOUN ADP DET ADJ NOUN VERB PUNCT ADV PROPN PROPN VERB SCONJ PRON NOUN VERB DET ADJ VERB NOUN ADP DET ADJ NOUN ADP ADJ NOUN PUNCT PRON ADV VERB DET ADJ NOUN ADP PRON NOUN ADP VERB PRON PART VERB ADJ VERB ADJ NOUN ADP DET NOUN NOUN VERB,0.6038647342995169,25.875,5.280193236714976
38,38,Aman Madaan,"[' This paper introduces a new task of politeness\ntransfer which involves converting non-polite\nsentences to polite sentences while preserving\nthe meaning. We also provide a dataset of\nmore than 1.39 million instances automatically\nlabeled for politeness to encourage benchmark\nevaluations on this new task. We design a tag\nand generate pipeline that identifies stylistic attributes and subsequently generates a sentence\nin the target style while preserving most of the\nsource content. For politeness as well as five\nother transfer tasks, our model outperforms the\nstate-of-the-art methods on automatic metrics\nfor content preservation, with a comparable\nor better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer\naccuracy across all the six style transfer tasks. The data and code is located at https://\ngithub.com/tag-and-generate/']",abstract_chunked," This paper introduces a new task of politeness
transfer which involves converting non-polite
sentences to polite sentences while preserving
the meaning. We also provide a dataset of
more than 1.39 million instances automatically
labeled for politeness to encourage benchmark
evaluations on this new task. We design a tag
and generate pipeline that identifies stylistic attributes and subsequently generates a sentence
in the target style while preserving most of the
source content. For politeness as well as five
other transfer tasks, our model outperforms the
state-of-the-art methods on automatic metrics
for content preservation, with a comparable
or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer
accuracy across all the six style transfer tasks. The data and code is located at https://
github.com/tag-and-generate/",14.797500000000014,30.71625077440773,38,0.6314231157302856," This paper introduces a new task of politeness 
 transfer which involves converting non polite 
 sentences to polite sentences while preserving 
 the meaning. We also provide a dataset of 
 more than 0.00 million instances automatically 
 labeled for politeness to encourage benchmark 
 evaluations on this new task. We design a tag 
 and generate pipeline that identifies stylistic attributes and subsequently generates a sentence 
 in the target style while preserving most of the 
 source content. For politeness as well as five 
 other transfer tasks, our model outperforms the 
 state of the art methods on automatic metrics 
 for content preservation, with a comparable 
 or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer 
 accuracy across all the six style transfer tasks. The data and code is located at https: 
 github.comtag and generate"," This paper introduces a new task of politeness 
 transfer which involves converting non polite 
 sentences to polite sentences while preserving 
 the meaning. We also provide a dataset of 
 more than 0.00 million instances automatically 
 labeled for politeness to encourage benchmark 
 evaluations on this new task. We design a tag 
 and generate pipeline that identifies stylistic attributes and subsequently generates a sentence 
 in the target style while preserving most of the 
 source content. For politeness as well as five 
 other transfer tasks, our model outperforms the 
 state of the art methods on automatic metrics 
 for content preservation, with a comparable 
 or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer 
 accuracy across all the six style transfer tasks. The data and code is located at https: 
 github.comtag and generate", DET NOUN VERB DET ADJ NOUN ADP NOUN SPACE NOUN PRON VERB VERB ADJ ADJ SPACE NOUN ADP ADJ NOUN SCONJ VERB SPACE DET NOUN PUNCT PRON ADV VERB DET NOUN ADP SPACE ADJ ADP NUM NUM NOUN ADV SPACE VERB ADP NOUN PART VERB NOUN SPACE NOUN ADP DET ADJ NOUN PUNCT PRON VERB DET NOUN SPACE CCONJ VERB NOUN PRON VERB ADJ NOUN CCONJ ADV VERB DET NOUN SPACE ADP DET NOUN NOUN SCONJ VERB ADJ ADP DET SPACE NOUN NOUN PUNCT ADP NOUN ADV ADV ADP NUM SPACE ADJ NOUN NOUN PUNCT PRON NOUN VERB DET SPACE NOUN ADP DET NOUN NOUN ADP ADJ NOUN SPACE ADP NOUN NOUN PUNCT ADP DET ADJ SPACE CCONJ ADJ NOUN ADP NOUN NOUN NOUN PUNCT ADV PUNCT PRON NOUN VERB VERB NOUN ADP ADJ NOUN ADP NOUN PUNCT VERB NOUN CCONJ VERB SPACE NOUN ADP DET DET NUM NOUN NOUN NOUN PUNCT DET NOUN CCONJ NOUN AUX VERB ADP NOUN PUNCT SPACE NOUN CCONJ VERB,0.6333333333333333,25.0,5.36
39,39,Aman Madaan,"[' We propose a method of curating high-quality comparable training data\nfor low-resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the\nsource and target languages by getting captions for such images in both\nlanguages independently. Human evaluations on the English-Hindi comparable corpora created with our method show that 81.1% of the pairs\nare acceptable translations, and only 2.47% of the pairs are not translations at all. We further establish the potential of the dataset collected\nthrough our approach by experimenting on two downstream tasks – machine translation and dictionary extraction. All code and data are available\nat https://github.com/madaan/PML4DC-Comparable-Data-Collection']",abstract_chunked," We propose a method of curating high-quality comparable training data
for low-resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the
source and target languages by getting captions for such images in both
languages independently. Human evaluations on the English-Hindi comparable corpora created with our method show that 81.1% of the pairs
are acceptable translations, and only 2.47% of the pairs are not translations at all. We further establish the potential of the dataset collected
through our approach by experimenting on two downstream tasks – machine translation and dictionary extraction. All code and data are available
at https://github.com/madaan/PML4DC-Comparable-Data-Collection",20.411,30.71625077440773,39,0.49926865100860596," We propose a method of curating high quality comparable training data 
 for low resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the 
 source and target languages by getting captions for such images in both 
 languages independently. Human evaluations on the Propname Propname comparable corpora created with our method show that 00.0 of the pairs 
 are acceptable translations, and only 0.00 of the pairs are not translations at all. We further establish the potential of the dataset collected 
 through our approach by experimenting on two downstream tasks machine translation and dictionary extraction. All code and data are available 
 at Propname Propname Propname"," We propose a method of curating high quality comparable training data 
 for low resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the 
 source and target languages by getting captions for such images in both 
 languages independently. Human evaluations on the Propname Propname comparable corpora created with our method show that 00.0 of the pairs 
 are acceptable translations, and only 0.00 of the pairs are not translations at all. We further establish the potential of the dataset collected 
 through our approach by experimenting on two downstream tasks machine translation and dictionary extraction. All code and data are available 
 at Propname Propname Propname", PRON VERB DET NOUN ADP VERB ADJ NOUN ADJ NOUN NOUN SPACE ADP ADJ NOUN NOUN ADP ADJ NOUN PUNCT PRON NOUN VERB VERB DET ADV VERB NOUN ADP NOUN ADP DET NOUN ADP DET SPACE NOUN CCONJ NOUN NOUN ADP VERB NOUN ADP ADJ NOUN ADP DET SPACE NOUN ADV PUNCT ADJ NOUN ADP DET PROPN PROPN ADJ NOUN VERB ADP PRON NOUN VERB SCONJ NUM ADP DET NOUN SPACE AUX ADJ NOUN PUNCT CCONJ ADV NUM ADP DET NOUN AUX PART NOUN ADV ADV PUNCT PRON ADV VERB DET NOUN ADP DET NOUN VERB SPACE ADP PRON NOUN ADP VERB ADP NUM ADJ NOUN NOUN NOUN CCONJ ADJ NOUN PUNCT DET NOUN CCONJ NOUN AUX ADJ SPACE ADP PROPN PROPN PROPN,0.6666666666666666,23.4,5.3418803418803416
40,40,Aman Madaan,"[' We study a novel task of numerical relation extraction with\nthe goal of extracting relations where one of the arguments\nis a number or a quantity (e.g., atomic number(Aluminium,\n13), inflation rate(India, 10.9%)). This task presents peculiar\nchallenges not found in standard Information Extraction (IE),\nsuch as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: (1) NumberRule, a rule based extractor, and (2) NumberTron, a probabilistic graphical model. We find that both\nsystems dramatically outperform MultiR, a state-of-the-art\nnon-numerical IE model, obtaining up to 25 points F-score\nimprovement.']",abstract_chunked," We study a novel task of numerical relation extraction with
the goal of extracting relations where one of the arguments
is a number or a quantity (e.g., atomic number(Aluminium,
13), inflation rate(India, 10.9%)). This task presents peculiar
challenges not found in standard Information Extraction (IE),
such as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: (1) NumberRule, a rule based extractor, and (2) NumberTron, a probabilistic graphical model. We find that both
systems dramatically outperform MultiR, a state-of-the-art
non-numerical IE model, obtaining up to 25 points F-score
improvement.",17.28,30.71625077440773,40,0.336382657289505," We study a novel task of Propname relation extraction with 
 the goal of extracting relations where one of the arguments 
 is a number or a quantity eg, atomic numberAluminium, 
 00, inflation rate. This task presents peculiar 
 challenges not found in standard Propname Propname, 
 such as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: Propname, a rule based extractor, and Propname, a probabilistic graphical model. We find that both 
 systems dramatically outperform Propname, a state of the art 
 Propname Propname Propname model, obtaining up to 00 points Propname score 
 improvement."," We study a novel task of Propname relation extraction with 
 the goal of extracting relations where one of the arguments 
 is a number or a quantity eg, atomic numberAluminium, 
 00, inflation rate. This task presents peculiar 
 challenges not found in standard Propname Propname, 
 such as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: Propname, a rule based extractor, and Propname, a probabilistic graphical model. We find that both 
 systems dramatically outperform Propname, a state of the art 
 Propname Propname Propname model, obtaining up to 00 points Propname score 
 improvement.", PRON VERB DET ADJ NOUN ADP PROPN NOUN NOUN ADP SPACE DET NOUN ADP VERB NOUN SCONJ NUM ADP DET NOUN SPACE AUX DET NOUN CCONJ DET NOUN NOUN PUNCT ADJ NOUN PUNCT SPACE NUM PUNCT NOUN NOUN PUNCT DET NOUN VERB ADJ SPACE NOUN PART VERB ADP ADJ PROPN PROPN PUNCT SPACE ADJ ADP DET NOUN ADP NOUN NOUN ADP ADJ NOUN CCONJ DET NOUN ADP NOUN PUNCT PRON VERB NUM NOUN NOUN PRON VERB ADJ ADJ NOUN ADP NOUN PUNCT PROPN PUNCT DET NOUN VERB NOUN PUNCT CCONJ PROPN PUNCT DET ADJ ADJ NOUN PUNCT PRON VERB SCONJ DET SPACE NOUN ADV VERB PROPN PUNCT DET NOUN ADP DET NOUN SPACE PROPN PROPN PROPN NOUN PUNCT VERB ADP PART NUM NOUN PROPN NOUN SPACE NOUN PUNCT,0.6166666666666667,30.0,4.966666666666667
41,41,Hugo Touvron,"[' In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.']",abstract_chunked," In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",31.83141355140188,30.655460127574653,41,0.7385146021842957," In this work, we develop and release Propname 0, a collection of pretrained and fine tuned large language models ranging in scale from 0 billion to 00 billion parameters. Our fine tuned LLMs, called Propname 0 Chat, are optimized for dialogue use cases. Our models outperform open source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine tuning and safety improvements of Propname 0 Chat in order to enable the community to build on our work and contribute to the responsible development of Propname."," In this work, we develop and release Propname 0, a collection of pretrained and fine tuned large language models ranging in scale from 0 billion to 00 billion parameters. Our fine tuned LLMs, called Propname 0 Chat, are optimized for dialogue use cases. Our models outperform open source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine tuning and safety improvements of Propname 0 Chat in order to enable the community to build on our work and contribute to the responsible development of Propname.", ADP DET NOUN PUNCT PRON VERB CCONJ VERB PROPN NUM PUNCT DET NOUN ADP VERB CCONJ ADJ VERB ADJ NOUN NOUN VERB ADP NOUN ADP NUM NUM PART NUM NUM NOUN PUNCT PRON ADJ VERB NOUN PUNCT VERB PROPN NUM NOUN PUNCT AUX VERB ADP NOUN NOUN NOUN PUNCT PRON NOUN VERB ADJ NOUN NOUN NOUN ADP ADJ NOUN PRON VERB PUNCT CCONJ VERB ADP PRON ADJ NOUN ADP NOUN CCONJ NOUN PUNCT AUX AUX DET ADJ NOUN ADP NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP PRON NOUN ADP ADJ NOUN CCONJ NOUN NOUN ADP PROPN NUM NOUN ADP NOUN PART VERB DET NOUN PART VERB ADP PRON NOUN CCONJ VERB ADP DET ADJ NOUN ADP PROPN PUNCT,0.6,30.0,4.65
42,42,Hugo Touvron,"[' We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly available datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community.']",abstract_chunked," We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B
parameters. We train our models on trillions
of tokens, and show that it is possible to train
state-of-the-art models using publicly available datasets exclusively, without resorting
to proprietary and inaccessible datasets. In
particular, LLaMA-13B outperforms GPT-3
(175B) on most benchmarks, and LLaMA65B is competitive with the best models,
Chinchilla-70B and PaLM-540B. We release
all our models to the research community.",30.655460127574653,30.655460127574653,42,0.7741825580596924," We introduce Propname, a collection of foundation language models ranging from Propname to 00B 
 parameters. We train our models on trillions 
 of tokens, and show that it is possible to train 
 state of the art models using publicly available datasets exclusively, without resorting 
 to proprietary and inaccessible datasets. In 
 particular, Propname 00B outperforms Propname 0 on most benchmarks, and Propname is competitive with the best models, 
 Chinchilla 00B and Propname Propname We release 
 all our models to the research community."," We introduce Propname, a collection of foundation language models ranging from Propname to 00B 
 parameters. We train our models on trillions 
 of tokens, and show that it is possible to train 
 state of the art models using publicly available datasets exclusively, without resorting 
 to proprietary and inaccessible datasets. In 
 particular, Propname 00B outperforms Propname 0 on most benchmarks, and Propname is competitive with the best models, 
 Chinchilla 00B and Propname Propname We release 
 all our models to the research community.", PRON VERB PROPN PUNCT DET NOUN ADP NOUN NOUN NOUN VERB ADP PROPN ADP NUM SPACE NOUN PUNCT PRON VERB PRON NOUN ADP NOUN SPACE ADP NOUN PUNCT CCONJ VERB SCONJ PRON AUX ADJ PART VERB SPACE NOUN ADP DET NOUN NOUN VERB ADV ADJ NOUN ADV PUNCT ADP VERB SPACE ADP ADJ CCONJ ADJ NOUN PUNCT ADP SPACE ADJ PUNCT PROPN NUM VERB PROPN NUM ADP ADJ NOUN PUNCT CCONJ PROPN AUX ADJ ADP DET ADJ NOUN PUNCT SPACE NOUN NOUN CCONJ PROPN PROPN PRON VERB SPACE DET PRON NOUN ADP DET NOUN NOUN PUNCT,0.5955056179775281,29.666666666666668,5.067415730337078
43,43,Hugo Touvron,"[' We introduce submodel co-training, a regularization\nmethod related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each\nsample we implicitly instantiate two altered networks, “submodels”, with stochastic depth: we activate only a subset\nof the layers. Each network serves as a soft teacher to the\nother, by providing a loss that complements the regular loss\nprovided by the one-hot label. Our approach, dubbed “cosub”, uses a single set of weights, and does not involve a\npre-trained external model or temporal averaging. Experimentally, we show that submodel co-training is\neffective to train backbones for recognition tasks such as\nimage classification and semantic segmentation. Our approach is compatible with multiple architectures, including\nRegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training\nstrategy improves their results in comparable settings. For\ninstance, a ViT-B pretrained with cosub on ImageNet-21k\nobtains 87.4% top-1 acc. @448 on ImageNet-val.']",abstract_chunked," We introduce submodel co-training, a regularization
method related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each
sample we implicitly instantiate two altered networks, “submodels”, with stochastic depth: we activate only a subset
of the layers. Each network serves as a soft teacher to the
other, by providing a loss that complements the regular loss
provided by the one-hot label. Our approach, dubbed “cosub”, uses a single set of weights, and does not involve a
pre-trained external model or temporal averaging. Experimentally, we show that submodel co-training is
effective to train backbones for recognition tasks such as
image classification and semantic segmentation. Our approach is compatible with multiple architectures, including
RegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training
strategy improves their results in comparable settings. For
instance, a ViT-B pretrained with cosub on ImageNet-21k
obtains 87.4% top-1 acc. @448 on ImageNet-val.",33.45163780663782,30.655460127574653,43,0.49933937191963196," We introduce submodel co training, a regularization 
 method related to co training, self distillation and stochastic depth. Given a neural network to be trained, for each 
 sample we implicitly instantiate two altered networks, submodels, with stochastic depth: we activate only a subset 
 of the layers. Each network serves as a soft teacher to the 
 other, by providing a loss that complements the regular loss 
 provided by the one hot label. Our approach, dubbed cosub, uses a single set of weights, and does not involve a 
 pre trained external model or temporal averaging. Experimentally, we show that Propname co training is 
 effective to train backbones for recognition tasks such as 
 image classification and semantic segmentation. Our approach is compatible with multiple architectures, including 
 Propname, Propname, Propname, Propname, Propname and Propname. Our training 
 strategy improves their results in comparable settings. For 
 instance, a Propname Propname pretrained with cosub on Propname 00k 
 obtains 00.0 top 0 acc. 000 on Propname val."," We introduce submodel co training, a regularization 
 method related to co training, self distillation and stochastic depth. Given a neural network to be trained, for each 
 sample we implicitly instantiate two altered networks, submodels, with stochastic depth: we activate only a subset 
 of the layers. Each network serves as a soft teacher to the 
 other, by providing a loss that complements the regular loss 
 provided by the one hot label. Our approach, dubbed cosub, uses a single set of weights, and does not involve a 
 pre trained external model or temporal averaging. Experimentally, we show that Propname co training is 
 effective to train backbones for recognition tasks such as 
 image classification and semantic segmentation. Our approach is compatible with multiple architectures, including 
 Propname, Propname, Propname, Propname, Propname and Propname. Our training 
 strategy improves their results in comparable settings. For 
 instance, a Propname Propname pretrained with cosub on Propname 00k 
 obtains 00.0 top 0 acc. 000 on Propname val.", PRON VERB NOUN NOUN NOUN PUNCT DET NOUN SPACE NOUN VERB ADP NOUN NOUN PUNCT NOUN NOUN CCONJ NOUN NOUN PUNCT VERB DET ADJ NOUN PART AUX VERB PUNCT ADP DET SPACE NOUN PRON ADV VERB NUM ADJ NOUN PUNCT NOUN PUNCT ADP NOUN NOUN PUNCT PRON VERB ADV DET NOUN SPACE ADP DET NOUN PUNCT DET NOUN VERB ADP DET ADJ NOUN ADP DET SPACE ADJ PUNCT ADP VERB DET NOUN PRON VERB DET ADJ NOUN SPACE VERB ADP DET NUM ADJ NOUN PUNCT PRON NOUN PUNCT VERB NOUN PUNCT VERB DET ADJ NOUN ADP NOUN PUNCT CCONJ AUX PART VERB DET SPACE ADJ VERB ADJ NOUN CCONJ ADJ NOUN PUNCT ADV PUNCT PRON VERB SCONJ PROPN NOUN NOUN AUX SPACE ADJ PART VERB NOUN ADP NOUN NOUN ADJ ADP SPACE NOUN NOUN CCONJ ADJ NOUN PUNCT PRON NOUN AUX ADJ ADP ADJ NOUN PUNCT VERB SPACE PROPN PUNCT PROPN PUNCT PROPN PUNCT PROPN PUNCT PROPN CCONJ PROPN PUNCT PRON NOUN SPACE NOUN VERB PRON NOUN ADP ADJ NOUN PUNCT ADP SPACE NOUN PUNCT DET PROPN PROPN VERB SCONJ NOUN ADP PROPN NOUN SPACE VERB NUM ADJ NUM NOUN PUNCT NUM ADP PROPN NOUN PUNCT,0.5978260869565217,20.444444444444443,4.809782608695652
44,44,Hugo Touvron,"[' A Vision Transformer (ViT) is a simple neural architecture amenable to serve\nseveral computer vision tasks. It has limited built-in architectural priors, in\ncontrast to more recent architectures that incorporate priors either about the\ninput data or of specific tasks. Recent works show that ViTs benefit from selfsupervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the supervised training of ViTs. Our procedure\nbuilds upon and simplifies a recipe introduced for training ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations,\ncloser to the practice in self-supervised learning. Our evaluations on Image\nclassification (ImageNet-1k with and without pre-training on ImageNet-21k),\ntransfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It\nalso reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better\nbaselines for recent self-supervised approaches demonstrated on ViT.']",abstract_chunked," A Vision Transformer (ViT) is a simple neural architecture amenable to serve
several computer vision tasks. It has limited built-in architectural priors, in
contrast to more recent architectures that incorporate priors either about the
input data or of specific tasks. Recent works show that ViTs benefit from selfsupervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the supervised training of ViTs. Our procedure
builds upon and simplifies a recipe introduced for training ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations,
closer to the practice in self-supervised learning. Our evaluations on Image
classification (ImageNet-1k with and without pre-training on ImageNet-21k),
transfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It
also reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better
baselines for recent self-supervised approaches demonstrated on ViT.",24.153939393939424,30.655460127574653,44,0.5779136419296265," A Propname Propname is a simple neural architecture amenable to serve 
 several computer vision tasks. It has limited built in architectural priors, in 
 contrast to more recent architectures that incorporate priors either about the 
 input data or of specific tasks. Recent works show that Propname benefit from selfsupervised pre training, in particular Propname like pre training like Propname In this paper, we revisit the supervised training of Propname. Our procedure 
 builds upon and simplifies a recipe introduced for training ResNet 00. It includes a new simple data augmentation procedure with only 0 augmentations, 
 closer to the practice in self supervised learning. Our evaluations on Image 
 classification, 
 transfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It 
 also reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better 
 baselines for recent self supervised approaches demonstrated on ViT."," A Propname Propname is a simple neural architecture amenable to serve 
 several computer vision tasks. It has limited built in architectural priors, in 
 contrast to more recent architectures that incorporate priors either about the 
 input data or of specific tasks. Recent works show that Propname benefit from selfsupervised pre training, in particular Propname like pre training like Propname In this paper, we revisit the supervised training of Propname. Our procedure 
 builds upon and simplifies a recipe introduced for training ResNet 00. It includes a new simple data augmentation procedure with only 0 augmentations, 
 closer to the practice in self supervised learning. Our evaluations on Image 
 classification, 
 transfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It 
 also reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better 
 baselines for recent self supervised approaches demonstrated on ViT.", DET PROPN PROPN AUX DET ADJ ADJ NOUN ADJ PART VERB SPACE ADJ NOUN NOUN NOUN PUNCT PRON AUX VERB VERB ADP ADJ NOUN PUNCT ADP SPACE NOUN ADP ADV ADJ NOUN PRON VERB NOUN CCONJ ADP DET SPACE NOUN NOUN CCONJ ADP ADJ NOUN PUNCT ADJ NOUN VERB SCONJ PROPN NOUN ADP VERB ADJ NOUN PUNCT ADP ADJ PROPN VERB ADJ NOUN ADP PROPN ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP PROPN PUNCT PRON NOUN SPACE VERB SCONJ CCONJ NOUN DET NOUN VERB ADP VERB NOUN NUM PUNCT PRON VERB DET ADJ ADJ NOUN NOUN NOUN ADP ADV NUM NOUN PUNCT SPACE ADV ADP DET NOUN ADP NOUN ADJ NOUN PUNCT PRON NOUN ADP NOUN SPACE NOUN PUNCT SPACE NOUN NOUN CCONJ ADJ NOUN VERB SCONJ PRON NOUN NOUN ADP DET ADJ NOUN ADJ ADV ADJ NOUN NOUN ADP NOUN PRON SPACE ADV VERB SCONJ DET NOUN ADP PRON NOUN VERB ADP NOUN AUX ADJ ADP PRON ADP ADV ADJ NOUN PUNCT PRON NOUN AUX VERB ADP ADJ SPACE NOUN ADP ADJ NOUN VERB NOUN VERB ADP NOUN,0.5942857142857143,21.875,5.3028571428571425
45,45,Hugo Touvron,"[' After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing stateof-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.']",abstract_chunked," After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing stateof-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.",18.921250000000043,30.655460127574653,45,0.7115664482116699," After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing stateof the art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. Fine tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine tuning time, and allows sharing the majority of weights across tasks. Adding Propname based patch pre processing layers improves Propname like self supervised training based on patch masking. We evaluate the impact of these design choices using the Propname 0k dataset, and confirm our findings on the Propname Propname test set. Transfer performance is measured across six smaller datasets."," After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing stateof the art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. Fine tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine tuning time, and allows sharing the majority of weights across tasks. Adding Propname based patch pre processing layers improves Propname like self supervised training based on patch masking. We evaluate the impact of these design choices using the Propname 0k dataset, and confirm our findings on the Propname Propname test set. Transfer performance is measured across six smaller datasets.", ADP PRON ADJ NOUN ADP ADJ NOUN NOUN PUNCT NOUN NOUN AUX ADV VERB NOUN ADP NOUN NOUN PUNCT VERB VERB DET NOUN NOUN ADP NOUN ADJ ADP NOUN NOUN PUNCT NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN PUNCT PRON VERB NUM NOUN VERB ADP ADJ CCONJ ADJ PART VERB NOUN ADP NOUN NOUN PUNCT DET ADJ NOUN ADP NOUN NOUN PUNCT PRON AUX ADV VERB ADV PUNCT AUX ADP DET NOUN AUX VERB ADV ADP NOUN ADP ADV VERB DET NOUN PUNCT ADJ VERB DET NOUN ADP DET NOUN NOUN AUX ADJ PART VERB NOUN NOUN ADP DET ADJ NOUN CCONJ ADP ADJ NOUN NOUN PUNCT PRON VERB NOUN PUNCT VERB DET NOUN NOUN NOUN ADP ADJ NOUN NOUN PUNCT CCONJ VERB VERB DET NOUN ADP NOUN ADP NOUN PUNCT VERB PROPN VERB NOUN NOUN NOUN NOUN VERB PROPN ADP NOUN VERB NOUN VERB ADP NOUN NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN NOUN VERB DET PROPN NOUN NOUN PUNCT CCONJ VERB PRON NOUN ADP DET PROPN PROPN NOUN NOUN PUNCT NOUN NOUN AUX VERB ADP NUM ADJ NOUN PUNCT,0.6502732240437158,22.875,5.3224043715847
46,46,Hugo Touvron,"[' Nowadays, machine learning and more particularly deep learning have an increasing impact in our society. This field has become prevalent, for instance in natural language processing where it has led to concrete applications to hate speech detection and document summarization. Sim- ilarly for computer vision, it enables better image interpretation, medical diagnosis, and major steps towards autonomous driving. Deep learning success is often associated with emblematic architectures, like AlexNet [123], ResNet [94] or GPT [155]. These successes were also powered by well-designed optimisation procedures, which are usually not the main focus of discussions. In image classification, the ImageNet [168] challenge was an accelerator for the development of new architectures but also for new optimisation recipes. In this thesis, we discuss the interactions between architectures and training procedures. We study more specifically Transformers [204] architecture applied for visual understanding. Cur- rently, transformers training procedures are less mature than those employed with convolutional networks (convnets). However, training is key to overcoming the limited architectural priors of transformers. For this reason, we focus on training procedures capable of obtaining interesting performance for transformers or even simpler architectures close to the multi-layer perceptron. We start by studying the possibility of learning with coarse labels through a modification of the training procedure. We then study different kinds of architectures for computer vision. We study their features, their advantages, their drawbacks and how to train them. Finally, we study the impact of the interaction between architecture and training process. All our approaches are evalu- ated in image classification on ImageNet and in transfer learning. We also evaluate our methods on additional tasks such as semantic segmentation.']",abstract_chunked," Nowadays, machine learning and more particularly deep learning have an increasing impact in our society. This field has become prevalent, for instance in natural language processing where it has led to concrete applications to hate speech detection and document summarization. Sim- ilarly for computer vision, it enables better image interpretation, medical diagnosis, and major steps towards autonomous driving. Deep learning success is often associated with emblematic architectures, like AlexNet [123], ResNet [94] or GPT [155]. These successes were also powered by well-designed optimisation procedures, which are usually not the main focus of discussions. In image classification, the ImageNet [168] challenge was an accelerator for the development of new architectures but also for new optimisation recipes. In this thesis, we discuss the interactions between architectures and training procedures. We study more specifically Transformers [204] architecture applied for visual understanding. Cur- rently, transformers training procedures are less mature than those employed with convolutional networks (convnets). However, training is key to overcoming the limited architectural priors of transformers. For this reason, we focus on training procedures capable of obtaining interesting performance for transformers or even simpler architectures close to the multi-layer perceptron. We start by studying the possibility of learning with coarse labels through a modification of the training procedure. We then study different kinds of architectures for computer vision. We study their features, their advantages, their drawbacks and how to train them. Finally, we study the impact of the interaction between architecture and training process. All our approaches are evalu- ated in image classification on ImageNet and in transfer learning. We also evaluate our methods on additional tasks such as semantic segmentation.",25.44949579831936,30.655460127574653,46,0.37463438510894775," Nowadays, machine learning and more particularly deep learning have an increasing impact in our society. This field has become prevalent, for instance in natural language processing where it has led to concrete applications to hate speech detection and document summarization. Sim ilarly for computer vision, it enables better image interpretation, medical diagnosis, and major steps towards autonomous driving. Deep learning success is often associated with emblematic architectures, like Propname, ResNet or Propname. These successes were also powered by well designed optimisation procedures, which are usually not the main focus of discussions. In image classification, the Propname challenge was an accelerator for the development of new architectures but also for new optimisation recipes. In this thesis, we discuss the interactions between architectures and training procedures. We study more specifically Propname architecture applied for visual understanding. Propname rently, transformers training procedures are less mature than those employed with convolutional networks. However, training is key to overcoming the limited architectural priors of transformers. For this reason, we focus on training procedures capable of obtaining interesting performance for transformers or even simpler architectures close to the Propname layer perceptron. We start by studying the possibility of learning with coarse labels through a modification of the training procedure. We then study different kinds of architectures for computer vision. We study their features, their advantages, their drawbacks and how to train them. Finally, we study the impact of the interaction between architecture and training process. All our approaches are Propname ated in image classification on Propname and in transfer learning. We also evaluate our methods on additional tasks such as semantic segmentation."," Nowadays, machine learning and more particularly deep learning have an increasing impact in our society. This field has become prevalent, for instance in natural language processing where it has led to concrete applications to hate speech detection and document summarization. Sim ilarly for computer vision, it enables better image interpretation, medical diagnosis, and major steps towards autonomous driving. Deep learning success is often associated with emblematic architectures, like Propname, ResNet or Propname. These successes were also powered by well designed optimisation procedures, which are usually not the main focus of discussions. In image classification, the Propname challenge was an accelerator for the development of new architectures but also for new optimisation recipes. In this thesis, we discuss the interactions between architectures and training procedures. We study more specifically Propname architecture applied for visual understanding. Propname rently, transformers training procedures are less mature than those employed with convolutional networks. However, training is key to overcoming the limited architectural priors of transformers. For this reason, we focus on training procedures capable of obtaining interesting performance for transformers or even simpler architectures close to the Propname layer perceptron. We start by studying the possibility of learning with coarse labels through a modification of the training procedure. We then study different kinds of architectures for computer vision. We study their features, their advantages, their drawbacks and how to train them. Finally, we study the impact of the interaction between architecture and training process. All our approaches are Propname ated in image classification on Propname and in transfer learning. We also evaluate our methods on additional tasks such as semantic segmentation.", ADV PUNCT NOUN NOUN CCONJ ADV ADV ADJ NOUN VERB DET VERB NOUN ADP PRON NOUN PUNCT DET NOUN AUX VERB ADJ PUNCT ADP NOUN ADP ADJ NOUN NOUN SCONJ PRON AUX VERB ADP ADJ NOUN PART VERB NOUN NOUN CCONJ NOUN NOUN PUNCT NOUN ADV ADP NOUN NOUN PUNCT PRON VERB ADJ NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ ADJ NOUN ADP ADJ NOUN PUNCT ADJ VERB NOUN AUX ADV VERB ADP ADJ NOUN PUNCT ADP PROPN PUNCT NOUN CCONJ PROPN PUNCT DET NOUN AUX ADV VERB ADP ADV VERB NOUN NOUN PUNCT PRON AUX ADV PART DET ADJ NOUN ADP NOUN PUNCT ADP NOUN NOUN PUNCT DET PROPN NOUN AUX DET NOUN ADP DET NOUN ADP ADJ NOUN CCONJ ADV ADP ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT PRON VERB ADV ADV PROPN NOUN VERB ADP ADJ NOUN PUNCT PROPN ADV PUNCT NOUN NOUN NOUN AUX ADV ADJ ADP PRON VERB ADP ADJ NOUN PUNCT ADV PUNCT NOUN AUX ADJ ADP VERB DET ADJ ADJ NOUN ADP NOUN PUNCT ADP DET NOUN PUNCT PRON VERB ADP NOUN NOUN ADJ ADP VERB ADJ NOUN ADP NOUN CCONJ ADV ADJ NOUN ADV ADP DET PROPN NOUN NOUN PUNCT PRON VERB ADP VERB DET NOUN ADP VERB ADP ADJ NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT PRON ADV VERB ADJ NOUN ADP NOUN ADP NOUN NOUN PUNCT PRON VERB PRON NOUN PUNCT PRON NOUN PUNCT PRON NOUN CCONJ SCONJ PART VERB PRON PUNCT ADV PUNCT PRON VERB DET NOUN ADP DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT DET PRON NOUN AUX PROPN VERB ADP NOUN NOUN ADP PROPN CCONJ ADP NOUN NOUN PUNCT PRON ADV VERB PRON NOUN ADP ADJ NOUN ADJ ADP ADJ NOUN PUNCT,0.5585284280936454,17.58823529411765,5.441471571906354
47,47,Hugo Touvron,"[' We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attentionbased aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.']",abstract_chunked," We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attentionbased aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.",10.040368421052648,30.655460127574653,47,0.5050764083862305," We show how to augment any convolutional network with an attention based global map to achieve non local reasoning. We replace the final average pooling by an attentionbased aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch based convolutional network parametrized by 0 parameters. In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection."," We show how to augment any convolutional network with an attention based global map to achieve non local reasoning. We replace the final average pooling by an attentionbased aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch based convolutional network parametrized by 0 parameters. In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.", PRON VERB SCONJ PART VERB DET ADJ NOUN ADP DET NOUN VERB ADJ NOUN PART VERB ADJ ADJ NOUN PUNCT PRON VERB DET ADJ ADJ NOUN ADP DET VERB NOUN NOUN ADJ ADP DET ADJ NOUN NOUN PUNCT PRON VERB SCONJ DET NOUN AUX VERB ADP DET NOUN NOUN PUNCT PRON VERB PRON VERB NOUN NOUN ADP DET ADJ NOUN VERB ADJ NOUN VERB ADP NUM NOUN PUNCT ADP NOUN ADP DET ADJ NOUN PUNCT DET NOUN NOUN VERB DET NOUN NOUN NOUN ADP DET DET NOUN PUNCT PRON VERB ADV ADJ NOUN NOUN ADP NOUN CCONJ NOUN PUNCT ADP ADJ ADP NOUN ADP NOUN NOUN PUNCT SCONJ VERB ADP PRON NOUN ADP ADJ NOUN NOUN NOUN PUNCT NOUN NOUN PUNCT NOUN NOUN CCONJ NOUN PUNCT,0.7222222222222222,25.2,5.349206349206349
48,48,Hugo Touvron,"[' Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions\nof images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers by\ntraining on Imagenet only. We train them on a single computer in less than\n3 days. Our reference vision transformer (86M parameters) achieves top-1\naccuracy of 83.1% (single-crop) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet\n(where we obtain up to 85.2% accuracy) and when transferring to other\ntasks. We share our code and models.']",abstract_chunked," Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions
of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers by
training on Imagenet only. We train them on a single computer in less than
3 days. Our reference vision transformer (86M parameters) achieves top-1
accuracy of 83.1% (single-crop) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to
transformers. It relies on a distillation token ensuring that the student
learns from the teacher through attention. We show the interest of this
token-based distillation, especially when using a convnet as a teacher. This
leads us to report results competitive with convnets for both Imagenet
(where we obtain up to 85.2% accuracy) and when transferring to other
tasks. We share our code and models.",34.816384615384635,30.655460127574653,48,0.6875571012496948," Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre trained with hundreds of millions 
 of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution free transformers by 
 training on Propname only. We train them on a single computer in less than 
 0 days. Our reference vision transformer achieves top 0 
 accuracy of 00.0 on Propname with no external data. More importantly, we introduce a teacher student strategy specific to 
 transformers. It relies on a distillation token ensuring that the student 
 learns from the teacher through attention. We show the interest of this 
 token based distillation, especially when using a convnet as a teacher. This 
 leads us to report results competitive with convnets for both Propname and when transferring to other 
 tasks. We share our code and models."," Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre trained with hundreds of millions 
 of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution free transformers by 
 training on Propname only. We train them on a single computer in less than 
 0 days. Our reference vision transformer achieves top 0 
 accuracy of 00.0 on Propname with no external data. More importantly, we introduce a teacher student strategy specific to 
 transformers. It relies on a distillation token ensuring that the student 
 learns from the teacher through attention. We show the interest of this 
 token based distillation, especially when using a convnet as a teacher. This 
 leads us to report results competitive with convnets for both Propname and when transferring to other 
 tasks. We share our code and models.", ADV PUNCT ADJ NOUN ADV VERB ADP NOUN AUX VERB PART VERB NOUN VERB NOUN ADJ ADP NOUN NOUN PUNCT DET NOUN NOUN NOUN AUX VERB VERB ADP NOUN ADP NOUN SPACE ADP NOUN VERB DET ADJ NOUN PUNCT ADV VERB PRON NOUN PUNCT ADP DET NOUN PUNCT PRON VERB ADJ NOUN ADJ NOUN ADP SPACE NOUN ADP PROPN ADV PUNCT PRON VERB PRON ADP DET ADJ NOUN ADP ADJ ADP SPACE NUM NOUN PUNCT PRON NOUN NOUN NOUN VERB ADJ NUM SPACE NOUN ADP NUM ADP PROPN ADP DET ADJ NOUN PUNCT ADV ADV PUNCT PRON VERB DET NOUN NOUN NOUN ADJ ADP SPACE NOUN PUNCT PRON VERB ADP DET NOUN NOUN VERB SCONJ DET NOUN SPACE NOUN ADP DET NOUN ADP NOUN PUNCT PRON VERB DET NOUN ADP DET SPACE VERB VERB NOUN PUNCT ADV SCONJ VERB DET NOUN ADP DET NOUN PUNCT PRON SPACE VERB PRON PART VERB NOUN ADJ ADP NOUN ADP DET PROPN CCONJ SCONJ VERB ADP ADJ SPACE NOUN PUNCT PRON VERB PRON NOUN CCONJ NOUN PUNCT,0.6585365853658537,16.4,5.012195121951219
49,49,Hugo Touvron,"[' We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity tradeoffs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.']",abstract_chunked," We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity tradeoffs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.",15.524476401179953,30.655460127574653,49,0.8408218622207642," We present Propname, an architecture built entirely upon multi layer perceptrons for image classification. It is a simple residual network that alternates a linear layer in which image patches interact, independently and identically across channels, and a two layer feed forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data augmentation and optionally distillation, it attains surprisingly good accuracycomplexity tradeoffs on Propname. We also train Propname models in a self supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre trained models and our code based on the Propname library."," We present Propname, an architecture built entirely upon multi layer perceptrons for image classification. It is a simple residual network that alternates a linear layer in which image patches interact, independently and identically across channels, and a two layer feed forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data augmentation and optionally distillation, it attains surprisingly good accuracycomplexity tradeoffs on Propname. We also train Propname models in a self supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre trained models and our code based on the Propname library.", PRON VERB PROPN PUNCT DET NOUN VERB ADV SCONJ ADJ NOUN NOUN ADP NOUN NOUN PUNCT PRON AUX DET ADJ ADJ NOUN PRON VERB DET ADJ NOUN ADP PRON NOUN NOUN VERB PUNCT ADV CCONJ ADV ADP NOUN PUNCT CCONJ DET NUM NOUN NOUN ADV NOUN ADP PRON NOUN VERB ADV ADP NOUN PUNCT SCONJ VERB ADP DET ADJ NOUN NOUN VERB ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT PRON VERB ADV ADJ NOUN NOUN ADP PROPN PUNCT PRON ADV VERB PROPN NOUN ADP DET NOUN VERB NOUN PUNCT PART ADV VERB NOUN ADP VERB DET VERB NOUN PUNCT ADV PUNCT ADP VERB PRON NOUN ADP NOUN NOUN PRON VERB ADV ADJ NOUN PUNCT PRON VERB ADJ VERB NOUN CCONJ PRON NOUN VERB ADP DET PROPN NOUN PUNCT,0.6875,21.333333333333332,5.3125
50,50,Hugo Touvron,"[' Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers\nhas been little studied so far. In this work, we build and optimize deeper\ntransformer networks for image classification. In particular, we investigate\nthe interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models\nwhose performance does not saturate early with more depth, for instance\nwe obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet\nwith Reassessed labels and Imagenet-V2 / match frequency, in the setting\nwith no additional training data. We share our code and models.']",abstract_chunked," Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers
has been little studied so far. In this work, we build and optimize deeper
transformer networks for image classification. In particular, we investigate
the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models
whose performance does not saturate early with more depth, for instance
we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet
with Reassessed labels and Imagenet-V2 / match frequency, in the setting
with no additional training data. We share our code and models.",31.70789473684212,30.655460127574653,50,0.5411679744720459," Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers 
 has been little studied so far. In this work, we build and optimize deeper 
 transformer networks for image classification. In particular, we investigate 
 the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models 
 whose performance does not saturate early with more depth, for instance 
 we obtain 00.0 top 0 accuracy on Propname when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Propname 
 with Propname labels and Propname V0 match frequency, in the setting 
 with no additional training data. We share our code and models."," Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers 
 has been little studied so far. In this work, we build and optimize deeper 
 transformer networks for image classification. In particular, we investigate 
 the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models 
 whose performance does not saturate early with more depth, for instance 
 we obtain 00.0 top 0 accuracy on Propname when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Propname 
 with Propname labels and Propname V0 match frequency, in the setting 
 with no additional training data. We share our code and models.", NOUN AUX AUX ADV VERB ADP ADJ NOUN NOUN NOUN PUNCT VERB ADJ NOUN VERB ADP DET ADJ NOUN ADP ADJ ADJ NOUN PUNCT ADV DET NOUN ADP NOUN NOUN SPACE AUX AUX ADV VERB ADV ADV PUNCT ADP DET NOUN PUNCT PRON VERB CCONJ VERB ADJ SPACE NOUN NOUN ADP NOUN NOUN PUNCT ADP ADJ PUNCT PRON VERB SPACE DET NOUN ADP NOUN CCONJ NOUN ADP ADJ ADJ NOUN PUNCT PRON VERB NUM NOUN NOUN NOUN PRON ADV VERB DET NOUN ADP ADJ NOUN PUNCT PRON VERB PRON PART VERB NOUN SPACE DET NOUN AUX PART VERB ADV ADP ADJ NOUN PUNCT ADP NOUN SPACE PRON VERB NUM ADJ NUM NOUN ADP PROPN SCONJ VERB ADP DET ADJ NOUN PUNCT PRON ADV VERB DET ADJ NOUN ADP ADJ NOUN CCONJ NOUN PUNCT ADV PUNCT PRON ADJ NOUN VERB DET ADJ NOUN ADP DET NOUN ADP PROPN SPACE ADP PROPN NOUN CCONJ PROPN NOUN NOUN NOUN PUNCT ADP DET NOUN SPACE ADP DET ADJ NOUN NOUN PUNCT PRON VERB PRON NOUN CCONJ NOUN PUNCT,0.6407185628742516,20.875,5.1017964071856285
51,51,Hugo Touvron,"[' This paper tackles the problem of learning a finer representation than the one provided by training labels. This\nenables fine-grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest-neighbor classifier\nobjective, and an instance loss inspired by self-supervised\nlearning. By jointly leveraging the coarse labels and the underlying fine-grained latent space, it significantly improves\nthe accuracy of category-level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than\nthat available at train time. It also improves the accuracy\nfor transfer learning tasks to fine-grained datasets, thereby\nestablishing the new state of the art on five public benchmarks, like iNaturalist-2018.']",abstract_chunked," This paper tackles the problem of learning a finer representation than the one provided by training labels. This
enables fine-grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest-neighbor classifier
objective, and an instance loss inspired by self-supervised
learning. By jointly leveraging the coarse labels and the underlying fine-grained latent space, it significantly improves
the accuracy of category-level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than
that available at train time. It also improves the accuracy
for transfer learning tasks to fine-grained datasets, thereby
establishing the new state of the art on five public benchmarks, like iNaturalist-2018.",18.39028248587573,30.655460127574653,51,0.5622329115867615," This paper tackles the problem of learning a finer representation than the one provided by training labels. This 
 enables fine grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest neighbor classifier 
 objective, and an instance loss inspired by self supervised 
 learning. By jointly leveraging the coarse labels and the underlying fine grained latent space, it significantly improves 
 the accuracy of category level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than 
 that available at train time. It also improves the accuracy 
 for transfer learning tasks to fine grained datasets, thereby 
 establishing the new state of the art on five public benchmarks, like Propname 0000."," This paper tackles the problem of learning a finer representation than the one provided by training labels. This 
 enables fine grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest neighbor classifier 
 objective, and an instance loss inspired by self supervised 
 learning. By jointly leveraging the coarse labels and the underlying fine grained latent space, it significantly improves 
 the accuracy of category level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than 
 that available at train time. It also improves the accuracy 
 for transfer learning tasks to fine grained datasets, thereby 
 establishing the new state of the art on five public benchmarks, like Propname 0000.", DET NOUN VERB DET NOUN ADP VERB DET ADJ NOUN ADP DET NOUN VERB ADP NOUN NOUN PUNCT PRON SPACE VERB ADJ VERB NOUN NOUN ADP NOUN ADP DET NOUN VERB ADP ADJ NOUN ADV PUNCT PRON NOUN AUX VERB ADP DET ADJ NOUN NOUN SPACE NOUN PUNCT CCONJ DET NOUN NOUN VERB ADP NOUN VERB SPACE NOUN PUNCT ADP ADV VERB DET ADJ NOUN CCONJ DET ADJ NOUN VERB NOUN NOUN PUNCT PRON ADV VERB SPACE DET NOUN ADP ADJ NOUN NOUN NOUN PUNCT PRON NOUN VERB DET VERB NOUN ADP VERB CCONJ VERB NOUN ADP DET ADJ NOUN ADP SPACE PRON ADJ ADP NOUN NOUN PUNCT PRON ADV VERB DET NOUN SPACE ADP NOUN VERB NOUN PART ADJ VERB NOUN PUNCT ADV SPACE VERB DET ADJ NOUN ADP DET NOUN ADP NUM ADJ NOUN PUNCT ADP PROPN NUM PUNCT,0.664179104477612,22.333333333333332,5.2164179104477615
52,52,Hugo Touvron,"[' We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising,\ndeblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a\nresidual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training\nschedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number\nof weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with\nthe number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance\nof our model is comparable or better than CycleGAN with significantly fewer parameters.']",abstract_chunked," We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising,
deblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a
residual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training
schedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number
of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with
the number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance
of our model is comparable or better than CycleGAN with significantly fewer parameters.",29.91906887755104,30.655460127574653,52,0.545376718044281," We propose a simple architecture to address unpaired image to image translation tasks: style or class transfer, denoising, 
 deblurring, deblocking, etc . We start from an image autoencoder architecture with fixed weights. For each task we learn a 
 residual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training 
 schedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number 
 of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with 
 the number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance 
 of our model is comparable or better than Propname with significantly fewer parameters."," We propose a simple architecture to address unpaired image to image translation tasks: style or class transfer, denoising, 
 deblurring, deblocking, etc . We start from an image autoencoder architecture with fixed weights. For each task we learn a 
 residual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training 
 schedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number 
 of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with 
 the number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance 
 of our model is comparable or better than Propname with significantly fewer parameters.", PRON VERB DET ADJ NOUN PART VERB ADJ NOUN ADP NOUN NOUN NOUN PUNCT NOUN CCONJ NOUN NOUN PUNCT NOUN PUNCT SPACE VERB PUNCT ADJ PUNCT X X PRON VERB ADP DET NOUN NOUN NOUN ADP VERB NOUN PUNCT ADP DET NOUN PRON VERB DET SPACE ADJ NOUN VERB ADP DET ADJ NOUN PUNCT PRON AUX ADV VERB SCONJ DET NOUN NOUN AUX VERB PUNCT DET ADJ NOUN SPACE NOUN AUX VERB PART VERB DET NOUN NOUN ADP DET NOUN PUNCT ADP NOUN NOUN PUNCT PRON VERB ADJ NOUN PUNCT DET NOUN SPACE ADP NOUN NOUN AUX ADJ CCONJ DET ADJ NOUN VERB NUM PART VERB DET NOUN ADP DET NOUN ADP SPACE DET NOUN ADP NOUN PUNCT PRON AUX ADJ PUNCT ADP NOUN PUNCT SCONJ DET NOUN CCONJ NOUN ADP NOUN PART VERB AUX PART VERB ADP NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP NOUN VERB DET NOUN ADP PRON NOUN ADP ADJ NOUN PUNCT DET NOUN SPACE ADP PRON NOUN AUX ADJ CCONJ ADJ ADP PROPN ADP ADV ADJ NOUN PUNCT,0.6488095238095238,21.0,4.886904761904762
53,53,Hugo Touvron,"[' Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date']",abstract_chunked," Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date",41.46626059322034,30.655460127574653,53,0.8000166416168213," Data augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 00.0 top 0 accuracy on Propname with a ResNet 00 trained on 000000 images, and 00.0 with one trained at 000000. A ResNeXt 000 00x00d pre trained with weak supervision on 000 million 000000 images and further optimized with our technique for test resolution 000000 achieves 00.0 top0 accuracy. To the best of our knowledge this is the highest Propname single crop accuracy to date"," Data augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 00.0 top 0 accuracy on Propname with a ResNet 00 trained on 000000 images, and 00.0 with one trained at 000000. A ResNeXt 000 00x00d pre trained with weak supervision on 000 million 000000 images and further optimized with our technique for test resolution 000000 achieves 00.0 top0 accuracy. To the best of our knowledge this is the highest Propname single crop accuracy to date", NOUN NOUN AUX ADJ ADP DET NOUN ADP ADJ NOUN ADP NOUN NOUN PUNCT DET NOUN ADV VERB SCONJ VERB NOUN VERB DET ADJ NOUN ADP DET NOUN ADP DET NOUN VERB ADP DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT ADP NOUN PUNCT DET ADJ NOUN NOUN VERB DET NOUN ADP NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN PART VERB DET ADJ NOUN PUNCT PRON VERB ADJ NOUN CCONJ NOUN NOUN PUNCT PRON VERB ADP DET ADV ADJ ADJ NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT PRON VERB VERB ADJ NOUN VERB ADJ NOUN NOUN PUNCT CCONJ ADV ADV VERB DET NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB NUM ADJ NUM NOUN ADP PROPN ADP DET NOUN NUM VERB ADP NUM NOUN PUNCT CCONJ NUM ADP NUM VERB ADP NUM PUNCT DET NOUN NUM X VERB VERB ADP ADJ NOUN ADP NUM NUM NUM NOUN CCONJ ADV VERB ADP PRON NOUN ADP NOUN NOUN NUM VERB NUM NOUN NOUN PUNCT ADP DET ADJ ADP PRON NOUN PRON AUX DET ADJ PROPN ADJ NOUN NOUN ADP NOUN,0.5934065934065934,22.75,4.928571428571429
54,54,Hugo Touvron,"[' The growing availability of large neuroimaging databases\noffers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these\ndatabases may be prone to several sources of variability (e.g.,\nage, gender, acquisition parameters,...). These nuisance variables can hamper the performance of a classification method\nand can even lead to misinterpret its behavior. We focus in\nthis paper on how to account for data coming from different\ndatabases. First, we present experiments on simulated data\nthat illustrate how interactions with other confounds such as\nage can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust\ndata and evaluate them in the real scenario of a ComputerAided Diagnosis (CAD) system that discriminates healthy\nfrom Alzheimer’s Disease (AD) subjects based on volumetric\ncharacteristics derived from structural MRI.']",abstract_chunked," The growing availability of large neuroimaging databases
offers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these
databases may be prone to several sources of variability (e.g.,
age, gender, acquisition parameters,...). These nuisance variables can hamper the performance of a classification method
and can even lead to misinterpret its behavior. We focus in
this paper on how to account for data coming from different
databases. First, we present experiments on simulated data
that illustrate how interactions with other confounds such as
age can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust
data and evaluate them in the real scenario of a ComputerAided Diagnosis (CAD) system that discriminates healthy
from Alzheimer’s Disease (AD) subjects based on volumetric
characteristics derived from structural MRI.",19.194347826086982,30.655460127574653,54,0.2473350316286087," The growing availability of large neuroimaging databases 
 offers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these 
 databases may be prone to several sources of variability Propname, 
 age, gender, acquisition parameters,.... These nuisance variables can hamper the performance of a classification method 
 and can even lead to misinterpret its behavior. We focus in 
 this paper on how to account for data coming from different 
 databases. First, we present experiments on simulated data 
 that illustrate how interactions with other confounds such as 
 age can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust 
 data and evaluate them in the real scenario of a Propname Propname system that discriminates healthy 
 from Propname Propname subjects based on volumetric 
 characteristics derived from structural Propname."," The growing availability of large neuroimaging databases 
 offers exceptional opportunities to train more and more efficient machine learning algorithms. Nevertheless, these 
 databases may be prone to several sources of variability Propname, 
 age, gender, acquisition parameters,.... These nuisance variables can hamper the performance of a classification method 
 and can even lead to misinterpret its behavior. We focus in 
 this paper on how to account for data coming from different 
 databases. First, we present experiments on simulated data 
 that illustrate how interactions with other confounds such as 
 age can be problematic for the adjustment of data from multiple databases. Then, we compare three strategies to adjust 
 data and evaluate them in the real scenario of a Propname Propname system that discriminates healthy 
 from Propname Propname subjects based on volumetric 
 characteristics derived from structural Propname.", DET VERB NOUN ADP ADJ NOUN NOUN SPACE VERB ADJ NOUN PART VERB ADJ CCONJ ADV ADJ NOUN NOUN NOUN PUNCT ADV PUNCT DET SPACE NOUN AUX AUX ADJ ADP ADJ NOUN ADP NOUN PROPN PUNCT SPACE NOUN PUNCT NOUN PUNCT NOUN NOUN PUNCT PUNCT DET ADJ NOUN AUX VERB DET NOUN ADP DET NOUN NOUN SPACE CCONJ AUX ADV VERB PART VERB PRON NOUN PUNCT PRON VERB ADP SPACE DET NOUN ADP SCONJ PART VERB ADP NOUN VERB ADP ADJ SPACE NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP VERB NOUN SPACE PRON VERB SCONJ NOUN ADP ADJ NOUN ADJ ADP SPACE NOUN AUX AUX ADJ ADP DET NOUN ADP NOUN ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB NUM NOUN PART VERB SPACE NOUN CCONJ VERB PRON ADP DET ADJ NOUN ADP DET PROPN PROPN NOUN PRON VERB ADJ SPACE ADP PROPN PROPN NOUN VERB ADP NOUN SPACE NOUN VERB ADP ADJ PROPN PUNCT,0.6620689655172414,24.166666666666668,5.448275862068965
55,55,Hugo Touvron,"[' Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top-1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date.']",abstract_chunked," Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 79.8% with one trained at 224×224. A ResNeXt-101 32x48d pre-trained with weak supervision on 940 million 224×224 images and further optimized with our technique for test resolution 320×320 achieves 86.4% top-1 accuracy (top-5: 98.0%). To the best of our knowledge this is the highest ImageNet single-crop accuracy to date.",41.46626059322034,30.655460127574653,55,0.7947770357131958," Data augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 00.0 top 0 accuracy on Propname with a ResNet 00 trained on 000000 images, and 00.0 with one trained at 000000. A ResNeXt 000 00x00d pre trained with weak supervision on 000 million 000000 images and further optimized with our technique for test resolution 000000 achieves 00.0 top 0 accuracy. To the best of our knowledge this is the highest Propname single crop accuracy to date."," Data augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! We then propose a simple strategy to optimize the classifier performance, that employs different train and test resolutions. It relies on a computationally cheap fine tuning of the network at the test resolution. This enables training strong classifiers using small training images, and therefore significantly reduce the training time. For instance, we obtain 00.0 top 0 accuracy on Propname with a ResNet 00 trained on 000000 images, and 00.0 with one trained at 000000. A ResNeXt 000 00x00d pre trained with weak supervision on 000 million 000000 images and further optimized with our technique for test resolution 000000 achieves 00.0 top 0 accuracy. To the best of our knowledge this is the highest Propname single crop accuracy to date.", NOUN NOUN AUX ADJ ADP DET NOUN ADP ADJ NOUN ADP NOUN NOUN PUNCT DET NOUN ADV VERB SCONJ VERB NOUN VERB DET ADJ NOUN ADP DET NOUN ADP DET NOUN VERB ADP DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT ADP NOUN PUNCT DET ADJ NOUN NOUN VERB DET NOUN ADP NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN PART VERB DET ADJ NOUN PUNCT PRON VERB ADJ NOUN CCONJ NOUN NOUN PUNCT PRON VERB ADP DET ADV ADJ ADJ NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT PRON VERB VERB ADJ NOUN VERB ADJ NOUN NOUN PUNCT CCONJ ADV ADV VERB DET NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB NUM ADJ NUM NOUN ADP PROPN ADP DET NOUN NUM VERB ADP NUM NOUN PUNCT CCONJ NUM ADP NUM VERB ADP NUM PUNCT DET NOUN NUM X VERB VERB ADP ADJ NOUN ADP NUM NUM NUM NOUN CCONJ ADV VERB ADP PRON NOUN ADP NOUN NOUN NUM VERB NUM ADJ NUM NOUN PUNCT ADP DET ADJ ADP PRON NOUN PRON AUX DET ADJ PROPN ADJ NOUN NOUN ADP NOUN PUNCT,0.5815217391304348,23.0,4.880434782608695
56,56,Zhiqing Sun,"[' Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principlefOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.']",abstract_chunked," Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principlefOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",1.9895210727969186,22.733194790065724,56,0.6166015267372131," Supervised Fine Tuning on response demonstrations combined with Propname Propname from Propname Propname constitutes a powerful paradigm for aligning Propname based Propname agents. However, a significant limitation of such an approach is its dependency on high quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in distribution response preferences. This paper presents a novel approach, namely Propname, to align base language models with minimal human supervision, using only a small set of human defined principles, yet achieving superior performance. Central to our approach is a principle following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human defined principles. By merely adjusting these principles during the Propname training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the Propname policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the Propname 0 00b base language model, we developed an Propname assistant named Propname 0. With only 0 exemplars for in context learning and 00 human defined principles, Propname 0 significantly surpasses the performance of several state of the art Propname systems, including Propname 0 Propname 00b, on various benchmark datasets. We have open sourced the code and model weights to encourage further research into aligning Propname Propname agents with enhanced supervision efficiency, improved controllability, and scalable oversight."," Supervised Fine Tuning on response demonstrations combined with Propname Propname from Propname Propname constitutes a powerful paradigm for aligning Propname based Propname agents. However, a significant limitation of such an approach is its dependency on high quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in distribution response preferences. This paper presents a novel approach, namely Propname, to align base language models with minimal human supervision, using only a small set of human defined principles, yet achieving superior performance. Central to our approach is a principle following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human defined principles. By merely adjusting these principles during the Propname training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the Propname policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the Propname 0 00b base language model, we developed an Propname assistant named Propname 0. With only 0 exemplars for in context learning and 00 human defined principles, Propname 0 significantly surpasses the performance of several state of the art Propname systems, including Propname 0 Propname 00b, on various benchmark datasets. We have open sourced the code and model weights to encourage further research into aligning Propname Propname agents with enhanced supervision efficiency, improved controllability, and scalable oversight.", ADJ NOUN NOUN ADP NOUN NOUN VERB ADP PROPN PROPN ADP PROPN PROPN VERB DET ADJ NOUN ADP VERB PROPN VERB PROPN NOUN PUNCT ADV PUNCT DET ADJ NOUN ADP DET DET NOUN AUX PRON NOUN ADP ADJ NOUN ADJ NOUN PUNCT VERB PRON NOUN ADP ADJ NOUN VERB ADP ADP NOUN ADP VERB ADJ NOUN NOUN CCONJ ADP NOUN NOUN NOUN PUNCT DET NOUN VERB DET ADJ NOUN PUNCT ADV PROPN PUNCT PART VERB NOUN NOUN NOUN ADP ADJ ADJ NOUN PUNCT VERB ADV DET ADJ NOUN ADP ADJ VERB NOUN PUNCT CCONJ VERB ADJ NOUN PUNCT ADJ ADP PRON NOUN AUX DET ADJ VERB NOUN NOUN PUNCT VERB ADP ADJ NOUN NOUN PUNCT DET NOUN AUX VERB NOUN NOUN VERB ADP ADJ ADJ VERB NOUN PUNCT ADP ADV VERB DET NOUN ADP DET PROPN NOUN NOUN PUNCT PRON VERB ADJ NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT ADV VERB DET NOUN ADP DET PROPN NOUN PUNCT CCONJ VERB DET NOUN ADP DET NOUN ADP ADJ ADJ NOUN PUNCT VERB PRON NOUN ADP DET PROPN NUM NOUN NOUN NOUN NOUN PUNCT PRON VERB DET PROPN NOUN VERB PROPN NUM PUNCT ADP ADV NUM NOUN ADP ADP NOUN NOUN CCONJ NUM ADJ VERB NOUN PUNCT PROPN NUM ADV VERB DET NOUN ADP ADJ NOUN ADP DET NOUN PROPN NOUN PUNCT VERB PROPN NUM PROPN NOUN PUNCT ADP ADJ ADJ NOUN PUNCT PRON AUX ADJ VERB DET NOUN CCONJ NOUN NOUN PART VERB ADJ NOUN ADP VERB PROPN PROPN NOUN ADP ADJ NOUN NOUN PUNCT VERB NOUN PUNCT CCONJ ADJ NOUN PUNCT,0.5437262357414449,29.22222222222222,5.494296577946768
57,57,Zhiqing Sun,"[' Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in “hallucination”, generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at.']",abstract_chunked," Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in “hallucination”, generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at.",14.222368421052636,22.733194790065724,57,0.666664183139801," Large Propname Propname are built across modalities and the misalignment between two modalities can result in hallucination, generating textual outputs that are not grounded by the multimodal information in context. To address the Propname misalignment issue, we adapt the Propname Propname from Propname Propname from the text domain to the task of vision language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Propname Propname that augments the reward model with additional factual information such as image captions and ground truth multi choice options, which alleviates the reward hacking phenomenon in Propname and further improves the performance. We also enhance the Propname 0 generated training data with previously available human written image text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real world scenarios, we develop a new evaluation benchmark Propname Propname with a special focus on penalizing hallucinations. As the first Propname trained with Propname, our approach achieves remarkable improvement on the Propname Propname dataset with the 00 performance level of the text only Propname 0, and an improvement by 00 on Propname Propname over other baselines. We opensource our code, model, data at."," Large Propname Propname are built across modalities and the misalignment between two modalities can result in hallucination, generating textual outputs that are not grounded by the multimodal information in context. To address the Propname misalignment issue, we adapt the Propname Propname from Propname Propname from the text domain to the task of vision language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Propname Propname that augments the reward model with additional factual information such as image captions and ground truth multi choice options, which alleviates the reward hacking phenomenon in Propname and further improves the performance. We also enhance the Propname 0 generated training data with previously available human written image text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real world scenarios, we develop a new evaluation benchmark Propname Propname with a special focus on penalizing hallucinations. As the first Propname trained with Propname, our approach achieves remarkable improvement on the Propname Propname dataset with the 00 performance level of the text only Propname 0, and an improvement by 00 on Propname Propname over other baselines. We opensource our code, model, data at.", ADJ PROPN PROPN AUX VERB ADP NOUN CCONJ DET NOUN ADP NUM NOUN AUX VERB ADP NOUN PUNCT VERB ADJ NOUN PRON AUX PART VERB ADP DET ADJ NOUN ADP NOUN PUNCT PART VERB DET PROPN NOUN NOUN PUNCT PRON VERB DET PROPN PROPN ADP PROPN PROPN ADP DET NOUN NOUN ADP DET NOUN ADP NOUN NOUN NOUN PUNCT SCONJ ADJ NOUN AUX VERB PART VERB NUM NOUN CCONJ VERB DET ADV VERB NUM PUNCT CCONJ DET NOUN NOUN NOUN AUX VERB PART VERB DET ADJ ADJ NOUN PUNCT PRON VERB DET ADJ NOUN NOUN VERB ADV PROPN PROPN PRON VERB DET NOUN NOUN ADP ADJ ADJ NOUN ADJ ADP NOUN NOUN CCONJ NOUN NOUN NOUN NOUN NOUN PUNCT PRON VERB DET NOUN VERB NOUN ADP PROPN CCONJ ADV VERB DET NOUN PUNCT PRON ADV VERB DET PROPN NUM VERB NOUN NOUN ADP ADV ADJ ADJ VERB NOUN NOUN VERB PART VERB DET ADJ NOUN ADP PRON NOUN PUNCT PART VERB DET VERB NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB DET ADJ NOUN NOUN PROPN PROPN ADP DET ADJ NOUN ADP VERB NOUN PUNCT SCONJ DET ADJ PROPN VERB ADP PROPN PUNCT PRON NOUN VERB ADJ NOUN ADP DET PROPN PROPN VERB ADP DET NUM NOUN NOUN ADP DET NOUN ADV PROPN NUM PUNCT CCONJ DET NOUN ADP NUM ADP PROPN PROPN ADP ADJ NOUN PUNCT PRON VERB PRON NOUN PUNCT NOUN PUNCT NOUN ADP PUNCT,0.5443037974683544,33.857142857142854,5.286919831223629
58,58,Zhiqing Sun,"[' Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of the AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user’s queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary . With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning), Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings. We have open-sourced the code, LoRA weights of Dromedary, and our synthetic training data to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, reduced biases, and improved controllability']",abstract_chunked," Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of the AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user’s queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary . With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning), Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings. We have open-sourced the code, LoRA weights of Dromedary, and our synthetic training data to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, reduced biases, and improved controllability",-3.140882352941162,22.733194790065724,58,0.5977641344070435," Recent Propname assistant agents, such as ChatGPT, predominantly rely on supervised fine tuning with human annotations and reinforcement learning from human feedback to align the output of large language models with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of Propname assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self consistency, and undesirable biases. To address these challenges, we propose a novel approach called Propname Propname, which combines principle driven reasoning and the generative power of LLMs for the self alignment of the Propname agents with minimal human supervision. Our approach encompasses four stages: first, we use an Propname to generate synthetic prompts, and a topic guided method to augment the prompt diversity; second, we use a small set of human written principles for Propname models to follow, and guide the Propname through in context learning from demonstrations to produce helpful, ethical, and reliable responses to users queries; third, we fine tune the original Propname with the high quality self aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly brief or indirect responses. Applying SELF Propname to the Propname 00b base language model, we develop an Propname assistant named Propname. With fewer than 000 lines of human annotations, Propname significantly surpasses the performance of several state of the art Propname systems, including Propname Propname 000 and Propname, on benchmark datasets with various settings. We have open sourced the code, Propname weights of Propname, and our synthetic training data to encourage further research into aligning Propname based Propname agents with enhanced supervision efficiency, reduced biases, and improved controllability"," Recent Propname assistant agents, such as ChatGPT, predominantly rely on supervised fine tuning with human annotations and reinforcement learning from human feedback to align the output of large language models with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of Propname assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self consistency, and undesirable biases. To address these challenges, we propose a novel approach called Propname Propname, which combines principle driven reasoning and the generative power of LLMs for the self alignment of the Propname agents with minimal human supervision. Our approach encompasses four stages: first, we use an Propname to generate synthetic prompts, and a topic guided method to augment the prompt diversity; second, we use a small set of human written principles for Propname models to follow, and guide the Propname through in context learning from demonstrations to produce helpful, ethical, and reliable responses to users queries; third, we fine tune the original Propname with the high quality self aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly brief or indirect responses. Applying SELF Propname to the Propname 00b base language model, we develop an Propname assistant named Propname. With fewer than 000 lines of human annotations, Propname significantly surpasses the performance of several state of the art Propname systems, including Propname Propname 000 and Propname, on benchmark datasets with various settings. We have open sourced the code, Propname weights of Propname, and our synthetic training data to encourage further research into aligning Propname based Propname agents with enhanced supervision efficiency, reduced biases, and improved controllability", ADJ PROPN ADJ NOUN PUNCT ADJ ADP NOUN PUNCT ADV VERB ADP VERB ADJ NOUN ADP ADJ NOUN CCONJ NOUN NOUN ADP ADJ NOUN PART VERB DET NOUN ADP ADJ NOUN NOUN ADP ADJ NOUN PUNCT VERB PRON AUX ADJ PUNCT ADJ PUNCT CCONJ ADJ PUNCT ADV PUNCT DET NOUN AUX ADV VERB DET ADJ NOUN ADP PROPN ADJ NOUN ADP ADP DET ADJ NOUN ADP VERB ADJ NOUN CCONJ DET ADJ NOUN ADP NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN NOUN PUNCT CCONJ ADJ NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB DET ADJ NOUN VERB PROPN PROPN PUNCT PRON VERB ADJ ADJ NOUN CCONJ DET ADJ NOUN ADP NOUN ADP DET NOUN NOUN ADP DET PROPN NOUN ADP ADJ ADJ NOUN PUNCT PRON NOUN VERB NUM NOUN PUNCT ADV PUNCT PRON VERB DET PROPN PART VERB ADJ NOUN PUNCT CCONJ DET NOUN VERB NOUN PART VERB DET ADJ NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADP ADJ VERB NOUN ADP PROPN NOUN PART VERB PUNCT CCONJ VERB DET PROPN ADP ADP NOUN VERB ADP NOUN PART VERB ADJ PUNCT ADJ PUNCT CCONJ ADJ NOUN ADP NOUN NOUN PUNCT ADV PUNCT PRON ADJ NOUN DET ADJ PROPN ADP DET ADJ NOUN NOUN VERB NOUN SCONJ SCONJ DET VERB NOUN AUX VERB ADJ NOUN ADP DET NOUN ADV ADP DET NOUN VERB CCONJ DET NOUN ADV PUNCT CCONJ ADV PUNCT PRON VERB DET ADJ NOUN PART VERB DET NOUN ADP ADV ADJ CCONJ ADJ NOUN PUNCT VERB NOUN PROPN ADP DET PROPN NOUN NOUN NOUN NOUN PUNCT PRON VERB DET PROPN NOUN VERB PROPN PUNCT ADP ADJ ADP NUM NOUN ADP ADJ NOUN PUNCT PROPN ADV VERB DET NOUN ADP ADJ NOUN ADP DET NOUN PROPN NOUN PUNCT VERB PROPN PROPN NUM CCONJ PROPN PUNCT ADP ADJ NOUN ADP ADJ NOUN PUNCT PRON AUX ADJ VERB DET NOUN PUNCT PROPN NOUN ADP PROPN PUNCT CCONJ PRON ADJ NOUN NOUN PART VERB ADJ NOUN ADP VERB PROPN VERB PROPN NOUN ADP ADJ NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ VERB NOUN,0.5160349854227405,49.0,5.186588921282799
59,59,Zhiqing Sun,"[' Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark. Our code is available at.']",abstract_chunked," Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark. Our code is available at.",22.531867774566507,22.733194790065724,59,0.6314787268638611," Neural network based Propname Propname methods have shown promising results in solving various Propname complete problems without relying on hand crafted domain knowledge. This paper broadens the current scope of neural solvers for Propname problems by introducing a new graph based diffusion framework, namely DIFUSCO. Our framework casts Propname problems as discrete 0, 0 vector optimization problems and leverages graph based denoising diffusion models to generate high quality solutions. We investigate two types of diffusion models with Propname and Propname noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well studied Propname combinatorial optimization problems: Traveling Propname Propname and Propname Propname Propname. Experimental results show that Propname strongly outperforms the previous state of the art neural solvers, improving the performance gap between ground truth and neural solvers from 0.00 to 0.00 on Propname 000, from 0.00 to 0.00 on Propname 0000, and from 0.00 to 0.00 on Propname 00000. For the Propname problem, DIFUSCO outperforms the previous state of the art neural solver on the challenging Propname benchmark. Our code is available at."," Neural network based Propname Propname methods have shown promising results in solving various Propname complete problems without relying on hand crafted domain knowledge. This paper broadens the current scope of neural solvers for Propname problems by introducing a new graph based diffusion framework, namely DIFUSCO. Our framework casts Propname problems as discrete 0, 0 vector optimization problems and leverages graph based denoising diffusion models to generate high quality solutions. We investigate two types of diffusion models with Propname and Propname noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well studied Propname combinatorial optimization problems: Traveling Propname Propname and Propname Propname Propname. Experimental results show that Propname strongly outperforms the previous state of the art neural solvers, improving the performance gap between ground truth and neural solvers from 0.00 to 0.00 on Propname 000, from 0.00 to 0.00 on Propname 0000, and from 0.00 to 0.00 on Propname 00000. For the Propname problem, DIFUSCO outperforms the previous state of the art neural solver on the challenging Propname benchmark. Our code is available at.", ADJ NOUN VERB PROPN PROPN NOUN AUX VERB ADJ NOUN ADP VERB ADJ PROPN ADJ NOUN ADP VERB ADP NOUN VERB NOUN NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP ADJ NOUN ADP PROPN NOUN ADP VERB DET ADJ NOUN VERB NOUN NOUN PUNCT ADV NOUN PUNCT PRON NOUN VERB PROPN NOUN ADP ADJ NUM PUNCT NUM NOUN NOUN NOUN CCONJ NOUN NOUN VERB VERB NOUN NOUN PART VERB ADJ NOUN NOUN PUNCT PRON VERB NUM NOUN ADP NOUN NOUN ADP PROPN CCONJ PROPN NOUN PUNCT ADV PUNCT CCONJ VERB DET ADJ NOUN NOUN PART VERB DET NOUN NOUN PUNCT PRON VERB PRON NOUN ADP NUM ADV VERB PROPN NOUN NOUN NOUN PUNCT VERB PROPN PROPN CCONJ PROPN PROPN PROPN PUNCT ADJ NOUN VERB SCONJ PROPN ADV VERB DET ADJ NOUN ADP DET NOUN ADJ NOUN PUNCT VERB DET NOUN NOUN ADP NOUN NOUN CCONJ ADJ NOUN ADP NUM ADP NUM ADP PROPN NUM PUNCT ADP NUM ADP NUM ADP PROPN NUM PUNCT CCONJ ADP NUM ADP NUM ADP PROPN NUM PUNCT ADP DET PROPN NOUN PUNCT NOUN VERB DET ADJ NOUN ADP DET NOUN ADJ NOUN ADP DET ADJ PROPN NOUN PUNCT PRON NOUN AUX ADJ ADP PUNCT,0.535,25.0,5.365
60,60,Zhiqing Sun,"[' Numerical simulation of non-linear partial differential equations plays a crucial role in modeling physical science and engineering phenomena, such as weather, climate, and aerodynamics. Recent Machine Learning (ML) models trained on low-resolution spatio-temporal signals have shown new promises in capturing important dynamics in high-resolution signals, under the condition that the models can effectively recover the missing details. However, this study shows that significant information is often lost in the lowresolution down-sampled features. To address such issues, we propose a new approach, namely Temporal Stencil Modeling (TSM), which combines the strengths of advanced time-series sequence modeling (with the HiPPO features) and state-of-the-art neural PDE solvers (with learnable stencil modeling). TSM aims to recover the lost information from the PDE trajectories and can be regarded as a temporal generalization of classic finite volume methods such as WENO. Our experimental results show that TSM achieves the new state-of-the-art simulation accuracy for 2-D incompressible Navier-Stokes turbulent flows: it significantly outperforms the previously reported best results by 19.9% in terms of the highly-correlated duration time and reduces the inference latency into 80%. We also show a strong generalization ability of the proposed method to various out-of-distribution turbulent flow settings. Our code is available at https: //github.com/Edward-Sun/TSM-PDE.']",abstract_chunked," Numerical simulation of non-linear partial differential equations plays a crucial role in modeling physical science and engineering phenomena, such as weather, climate, and aerodynamics. Recent Machine Learning (ML) models trained on low-resolution spatio-temporal signals have shown new promises in capturing important dynamics in high-resolution signals, under the condition that the models can effectively recover the missing details. However, this study shows that significant information is often lost in the lowresolution down-sampled features. To address such issues, we propose a new approach, namely Temporal Stencil Modeling (TSM), which combines the strengths of advanced time-series sequence modeling (with the HiPPO features) and state-of-the-art neural PDE solvers (with learnable stencil modeling). TSM aims to recover the lost information from the PDE trajectories and can be regarded as a temporal generalization of classic finite volume methods such as WENO. Our experimental results show that TSM achieves the new state-of-the-art simulation accuracy for 2-D incompressible Navier-Stokes turbulent flows: it significantly outperforms the previously reported best results by 19.9% in terms of the highly-correlated duration time and reduces the inference latency into 80%. We also show a strong generalization ability of the proposed method to various out-of-distribution turbulent flow settings. Our code is available at https: //github.com/Edward-Sun/TSM-PDE.",4.886448019802003,22.733194790065724,60,0.5935629606246948," Numerical simulation of non linear partial differential equations plays a crucial role in modeling physical science and engineering phenomena, such as weather, climate, and aerodynamics. Recent Propname Propname models trained on low resolution spatio temporal signals have shown new promises in capturing important dynamics in high resolution signals, under the condition that the models can effectively recover the missing details. However, this study shows that significant information is often lost in the lowresolution down sampled features. To address such issues, we propose a new approach, namely Propname Propname Propname, which combines the strengths of advanced time series sequence modeling and state of the art neural Propname solvers. Propname aims to recover the lost information from the Propname trajectories and can be regarded as a temporal generalization of classic finite volume methods such as Propname. Our experimental results show that Propname achieves the new state of the art simulation accuracy for 0 D incompressible Propname Propname turbulent flows: it significantly outperforms the previously reported best results by 00.0 in terms of the highly correlated duration time and reduces the inference latency into 00. We also show a strong generalization ability of the proposed method to various out of distribution turbulent flow settings. Our code is available at https: Propname Propname Propname."," Numerical simulation of non linear partial differential equations plays a crucial role in modeling physical science and engineering phenomena, such as weather, climate, and aerodynamics. Recent Propname Propname models trained on low resolution spatio temporal signals have shown new promises in capturing important dynamics in high resolution signals, under the condition that the models can effectively recover the missing details. However, this study shows that significant information is often lost in the lowresolution down sampled features. To address such issues, we propose a new approach, namely Propname Propname Propname, which combines the strengths of advanced time series sequence modeling and state of the art neural Propname solvers. Propname aims to recover the lost information from the Propname trajectories and can be regarded as a temporal generalization of classic finite volume methods such as Propname. Our experimental results show that Propname achieves the new state of the art simulation accuracy for 0 D incompressible Propname Propname turbulent flows: it significantly outperforms the previously reported best results by 00.0 in terms of the highly correlated duration time and reduces the inference latency into 00. We also show a strong generalization ability of the proposed method to various out of distribution turbulent flow settings. Our code is available at https: Propname Propname Propname.", ADJ NOUN ADP ADJ ADJ ADJ ADJ NOUN VERB DET ADJ NOUN ADP VERB ADJ NOUN CCONJ NOUN NOUN PUNCT ADJ ADP NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT ADJ PROPN PROPN NOUN VERB ADP ADJ NOUN NOUN ADJ NOUN AUX VERB ADJ NOUN ADP VERB ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADP DET NOUN SCONJ DET NOUN AUX ADV VERB DET VERB NOUN PUNCT ADV PUNCT DET NOUN VERB SCONJ ADJ NOUN AUX ADV VERB ADP DET NOUN ADP VERB NOUN PUNCT PART VERB ADJ NOUN PUNCT PRON VERB DET ADJ NOUN PUNCT ADV PROPN PROPN PROPN PUNCT PRON VERB DET NOUN ADP ADJ NOUN NOUN NOUN NOUN CCONJ NOUN ADP DET NOUN ADJ PROPN NOUN PUNCT PROPN VERB PART VERB DET VERB NOUN ADP DET PROPN NOUN CCONJ AUX AUX VERB ADP DET ADJ NOUN ADP ADJ ADJ NOUN NOUN ADJ ADP PROPN PUNCT PRON ADJ NOUN VERB SCONJ PROPN VERB DET ADJ NOUN ADP DET NOUN NOUN NOUN ADP NUM ADJ ADJ PROPN PROPN ADJ NOUN PUNCT PRON ADV VERB DET ADV VERB ADJ NOUN ADP NUM ADP NOUN ADP DET ADV VERB NOUN NOUN CCONJ VERB DET NOUN NOUN ADP NUM PUNCT PRON ADV VERB DET ADJ NOUN NOUN ADP DET VERB NOUN AUX ADJ ADP ADP NOUN ADJ NOUN NOUN PUNCT PRON NOUN AUX ADJ ADP NOUN PUNCT PROPN PROPN PROPN PUNCT,0.6140350877192983,28.5,5.359649122807017
61,61,Zhiqing Sun,"[' We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at.']",abstract_chunked," We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at.",19.15045454545455,22.733194790065724,61,0.5414963960647583," We propose a new paradigm to help Large Propname Propname generate more accurate factual knowledge without retrieving from an external corpus, called RECITation augmented Propname. Different from retrieval augmented language models that retrieve relevant documents before generating the outputs, given an input, Propname first recites one or several relevant passages from Propname own memory via sampling, and then produces the final answers. We show that Propname is a powerful paradigm for knowledge intensive Propname tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite and answer scheme can achieve new state of the art performance in various closed book question answering tasks. In experiments, we verify the effectiveness of Propname on four pre trained models and three CBQA tasks. Our code is available at."," We propose a new paradigm to help Large Propname Propname generate more accurate factual knowledge without retrieving from an external corpus, called RECITation augmented Propname. Different from retrieval augmented language models that retrieve relevant documents before generating the outputs, given an input, Propname first recites one or several relevant passages from Propname own memory via sampling, and then produces the final answers. We show that Propname is a powerful paradigm for knowledge intensive Propname tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite and answer scheme can achieve new state of the art performance in various closed book question answering tasks. In experiments, we verify the effectiveness of Propname on four pre trained models and three CBQA tasks. Our code is available at.", PRON VERB DET ADJ NOUN PART VERB ADJ PROPN PROPN VERB ADV ADJ ADJ NOUN ADP VERB ADP DET ADJ NOUN PUNCT VERB NOUN VERB PROPN PUNCT ADJ ADP NOUN VERB NOUN NOUN PRON VERB ADJ NOUN ADP VERB DET NOUN PUNCT VERB DET NOUN PUNCT PROPN ADJ VERB NUM CCONJ ADJ ADJ NOUN ADP PROPN VERB NOUN ADP NOUN PUNCT CCONJ ADV VERB DET ADJ NOUN PUNCT PRON VERB SCONJ PROPN AUX DET ADJ NOUN ADP NOUN ADJ PROPN NOUN PUNCT ADV PUNCT PRON VERB SCONJ ADP VERB NOUN SCONJ DET ADJ NOUN PUNCT DET NOUN CCONJ NOUN NOUN AUX VERB ADJ NOUN ADP DET NOUN NOUN ADP ADJ ADJ NOUN NOUN NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB DET NOUN ADP PROPN ADP NUM ADJ VERB NOUN CCONJ NUM NOUN NOUN PUNCT PRON NOUN AUX ADJ ADP PUNCT,0.6879432624113475,23.5,5.177304964539007
62,62,Zhiqing Sun,"[' Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learning-to-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants']",abstract_chunked," Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learning-to-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants",24.798940568475444,22.733194790065724,62,0.40212517976760864," Propname has become ubiquitous in sequence modeling tasks. As a key component of Propname, self attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Propname Propname Propname methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those Propname methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the Propname methods such as Propname Propname Propname are randomized and can not fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely Propname, which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of Propname is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre trained Propname models. Our experiments on evaluation of the Propname 000 dataset for language modeling, the Propname benchmark for natural language understanding, and the Propname Propname Propname benchmark for multiple tasks show the superior performance of Propname over other strong Propname variants"," Propname has become ubiquitous in sequence modeling tasks. As a key component of Propname, self attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Propname Propname Propname methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those Propname methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the Propname methods such as Propname Propname Propname are randomized and can not fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely Propname, which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of Propname is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre trained Propname models. Our experiments on evaluation of the Propname 000 dataset for language modeling, the Propname benchmark for natural language understanding, and the Propname Propname Propname benchmark for multiple tasks show the superior performance of Propname over other strong Propname variants", PROPN AUX VERB ADJ ADP NOUN NOUN NOUN PUNCT ADP DET ADJ NOUN ADP PROPN PUNCT NOUN NOUN AUX PART VERB ADP ADJ NOUN ADP ADP PRON ADJ NOUN CCONJ NOUN NOUN ADP NOUN ADP DET NOUN NOUN PUNCT PART VERB DET NOUN PUNCT ADJ NOUN VERB ADJ NOUN NOUN NOUN VERB ADP PROPN PROPN PROPN NOUN PUNCT SCONJ ADJ NOUN CCONJ NOUN AUX VERB ADP DET ADJ NOUN NOUN ADP ADJ NOUN PUNCT ADV PUNCT DET NOUN ADP DET PROPN NOUN VERB ADP DET NOUN PRON VERB CCONJ NOUN AUX VERB ADP DET ADJ NOUN PUNCT PRON AUX PART ADV ADJ PUNCT ADV PUNCT PRON ADP DET PROPN NOUN ADJ ADP PROPN PROPN PROPN AUX VERB CCONJ AUX PART ADV VERB DET ADJ ADJ NOUN NOUN PUNCT PART VERB DET NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP ADJ NOUN PUNCT ADV PROPN PUNCT PRON ADV VERB ADJ ADJ NOUN NOUN ADP NOUN CCONJ NOUN PUNCT ADV PUNCT DET NOUN ADP PROPN AUX SCONJ PRON AUX PART VERB ADJ NOUN ADP NOUN CCONJ NOUN PUNCT PRON VERB PRON ADJ ADP DET ADJ NOUN ADP ADJ VERB PROPN NOUN PUNCT PRON NOUN ADP NOUN ADP DET PROPN NUM NOUN ADP NOUN NOUN PUNCT DET PROPN NOUN ADP ADJ NOUN NOUN PUNCT CCONJ DET PROPN PROPN PROPN NOUN ADP ADJ NOUN VERB DET ADJ NOUN ADP PROPN ADP ADJ ADJ PROPN NOUN,0.5517241379310345,29.0,5.176724137931035
63,63,Zhiqing Sun,"[' DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly out-perform DETR and other baselines in terms of detection accuracy. Code is released at.']",abstract_chunked," DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly out-perform DETR and other baselines in terms of detection accuracy. Code is released at.",22.54420765027325,22.733194790065724,63,0.29406246542930603," Propname is a recently proposed Propname based method which views object detection as a set prediction problem and achieves state of the art performance but demands extra long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of Propname. Our examinations reveal several factors contributing to the slow convergence of Propname, primarily the issues with the Hungarian loss and the Propname cross attention mechanism. To overcome these issues we propose two solutions, namely, Propname Propname and Propname Propname. Experimental results show that the proposed methods not only converge much faster than the original Propname, but also significantly out perform Propname and other baselines in terms of detection accuracy. Propname is released at."," Propname is a recently proposed Propname based method which views object detection as a set prediction problem and achieves state of the art performance but demands extra long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of Propname. Our examinations reveal several factors contributing to the slow convergence of Propname, primarily the issues with the Hungarian loss and the Propname cross attention mechanism. To overcome these issues we propose two solutions, namely, Propname Propname and Propname Propname. Experimental results show that the proposed methods not only converge much faster than the original Propname, but also significantly out perform Propname and other baselines in terms of detection accuracy. Propname is released at.", PROPN AUX DET ADV VERB PROPN VERB NOUN PRON VERB NOUN NOUN ADP DET VERB NOUN NOUN CCONJ VERB NOUN ADP DET NOUN NOUN CCONJ VERB ADJ ADJ NOUN NOUN PART VERB PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN NOUN ADP DET NOUN ADP PROPN PUNCT PRON NOUN VERB ADJ NOUN VERB ADP DET ADJ NOUN ADP PROPN PUNCT ADV DET NOUN ADP DET ADJ NOUN CCONJ DET PROPN VERB NOUN NOUN PUNCT PART VERB DET NOUN PRON VERB NUM NOUN PUNCT ADV PUNCT PROPN PROPN CCONJ PROPN PROPN PUNCT ADJ NOUN VERB SCONJ DET VERB NOUN PART ADV VERB ADV ADV ADP DET ADJ PROPN PUNCT CCONJ ADV ADV ADV VERB PROPN CCONJ ADJ NOUN ADP NOUN ADP NOUN NOUN PUNCT PROPN AUX VERB ADP PUNCT,0.6439393939393939,22.0,5.25
64,64,Zhiqing Sun,"[' Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3× smaller and 5.5× faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE)']",abstract_chunked," Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3× smaller and 5.5× faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE)",31.71700000000004,22.733194790065724,64,0.5241109728813171," Propname Propname Propname has recently achieved great success by using huge pre trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they can not be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular Propname model. Like the original Propname, Propname is task agnostic, that is, it can be generically applied to various downstream Propname tasks via simple fine tuning. Basically, Propname is a thin version of Propname, while equipped with bottleneck structures and a carefully designed balance between self attentions and feed forward networks. To train Propname, we first train a specially designed teacher model, an invertedbottleneck incorporated Propname model. Then, we conduct knowledge transfer from this teacher to Propname. Empirical studies show that Propname is 0.0 smaller and 0.0 faster than Propname while achieving competitive results on well known benchmarks. On the natural language inference tasks of Propname, Propname achieves a GLUE score of 00.0, and00 ms latency on a Propname 0 phone. On the SQuAD v0.0v0.0 question answering task, Propname achieves a dev F0 score of00.000.0"," Propname Propname Propname has recently achieved great success by using huge pre trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they can not be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular Propname model. Like the original Propname, Propname is task agnostic, that is, it can be generically applied to various downstream Propname tasks via simple fine tuning. Basically, Propname is a thin version of Propname, while equipped with bottleneck structures and a carefully designed balance between self attentions and feed forward networks. To train Propname, we first train a specially designed teacher model, an invertedbottleneck incorporated Propname model. Then, we conduct knowledge transfer from this teacher to Propname. Empirical studies show that Propname is 0.0 smaller and 0.0 faster than Propname while achieving competitive results on well known benchmarks. On the natural language inference tasks of Propname, Propname achieves a GLUE score of 00.0, and00 ms latency on a Propname 0 phone. On the SQuAD v0.0v0.0 question answering task, Propname achieves a dev F0 score of00.000.0", PROPN PROPN PROPN AUX ADV VERB ADJ NOUN ADP VERB ADJ ADJ VERB NOUN ADP NOUN ADP NOUN ADP NOUN PUNCT ADV PUNCT DET NOUN VERB ADP ADJ NOUN NOUN CCONJ ADJ NOUN ADJ SCONJ PRON AUX PART AUX VERB ADP VERB ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB NOUN ADP VERB CCONJ VERB DET ADJ PROPN NOUN PUNCT ADP DET ADJ PROPN PUNCT PROPN AUX NOUN ADJ PUNCT ADV ADV PUNCT PRON AUX AUX ADV VERB ADP ADJ ADJ PROPN NOUN ADP ADJ ADJ NOUN PUNCT ADV PUNCT PROPN AUX DET ADJ NOUN ADP PROPN PUNCT SCONJ VERB ADP NOUN NOUN CCONJ DET ADV VERB NOUN ADP NOUN NOUN CCONJ VERB ADJ NOUN PUNCT PART VERB PROPN PUNCT PRON ADV VERB DET ADV VERB NOUN NOUN PUNCT DET NOUN VERB PROPN NOUN PUNCT ADV PUNCT PRON VERB NOUN NOUN ADP DET NOUN ADP PROPN PUNCT ADJ NOUN VERB SCONJ PROPN AUX NUM ADJ CCONJ NUM ADV ADP PROPN SCONJ VERB ADJ NOUN ADP ADV VERB NOUN PUNCT ADP DET ADJ NOUN NOUN NOUN ADP PROPN PUNCT PROPN VERB DET ADJ NOUN ADP NUM PUNCT CCONJ PUNCT ADJ NOUN ADP DET PROPN NUM NOUN PUNCT ADP DET ADJ NOUN NOUN VERB NOUN PUNCT PROPN VERB DET NOUN NOUN NOUN ADP PUNCT,0.6066350710900474,21.1,5.080568720379147
65,65,Zhiqing Sun,"[' Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.']",abstract_chunked," Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.",20.86360000000002,22.733194790065724,65,0.24376142024993896," Propname Propname Propname aims at automatically predicting missing links for large scale knowledge graphs. A vast number of state of the art Propname techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state of the art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available."," Propname Propname Propname aims at automatically predicting missing links for large scale knowledge graphs. A vast number of state of the art Propname techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state of the art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.", PROPN PROPN PROPN VERB ADP ADV VERB VERB NOUN ADP ADJ NOUN NOUN NOUN PUNCT DET ADJ NOUN ADP NOUN ADP DET NOUN PROPN NOUN AUX AUX VERB ADP ADJ NOUN ADP ADJ NOUN NOUN PUNCT VERB NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB SCONJ ADJ ADJ NOUN VERB ADV ADJ NOUN PUNCT PRON ADV VERB ADJ NOUN ADP DET NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ PRON AUX AUX VERB ADP DET ADJ NOUN NOUN VERB ADP PRON CCONJ VERB DET ADJ NOUN NOUN PART VERB DET NOUN PUNCT DET VERB NOUN AUX ADJ PART VERB NOUN ADP DET NOUN PUNCT PRON AUX ADV VERB DET ADJ NOUN PUNCT PRON VERB ADJ NOUN CCONJ VERB NOUN ADP ADJ VERB NOUN VERB PRON NOUN PUNCT DET ADJ NOUN AUX AUX VERB ADV ADJ PUNCT,0.6827586206896552,20.714285714285715,5.172413793103448
66,66,Zhiqing Sun,"[' Autoregressive (AR) models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non-autoregressive (NAR) models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi-modality in sequence generation. This paper proposes a new approach that jointly optimizes both AR and NAR models in a unified Expectation-Maximization (EM) framework. In the E-step, an AR model learns to approximate the regularized posterior of the NAR model. In the M-step, the NAR model is updated on the new posterior and selects the training examples for the next AR model. This iterative process can effectively guide the system to remove the multi-modality in the output sequences. To our knowledge, this is the first EM approach to NAR sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.']",abstract_chunked," Autoregressive (AR) models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non-autoregressive (NAR) models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi-modality in sequence generation. This paper proposes a new approach that jointly optimizes both AR and NAR models in a unified Expectation-Maximization (EM) framework. In the E-step, an AR model learns to approximate the regularized posterior of the NAR model. In the M-step, the NAR model is updated on the new posterior and selects the training examples for the next AR model. This iterative process can effectively guide the system to remove the multi-modality in the output sequences. To our knowledge, this is the first EM approach to NAR sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.",23.1827777777778,22.733194790065724,66,0.4947231113910675," Autoregressive models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non autoregressive models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi modality in sequence generation. This paper proposes a new approach that jointly optimizes both Propname and Propname models in a unified Propname Propname framework. In the E step, an Propname model learns to approximate the regularized posterior of the Propname model. In the Propname step, the Propname model is updated on the new posterior and selects the training examples for the next Propname model. This iterative process can effectively guide the system to remove the multi modality in the output sequences. To our knowledge, this is the first Propname approach to Propname sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency."," Autoregressive models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency. Non autoregressive models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi modality in sequence generation. This paper proposes a new approach that jointly optimizes both Propname and Propname models in a unified Propname Propname framework. In the E step, an Propname model learns to approximate the regularized posterior of the Propname model. In the Propname step, the Propname model is updated on the new posterior and selects the training examples for the next Propname model. This iterative process can effectively guide the system to remove the multi modality in the output sequences. To our knowledge, this is the first Propname approach to Propname sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.", ADJ NOUN AUX AUX DET VERB NOUN ADP ADJ NOUN NOUN PUNCT CCONJ AUX VERB ADP DET NOUN ADP ADJ NOUN NOUN PUNCT ADJ ADJ NOUN AUX AUX ADV VERB PART VERB DET NOUN ADP VERB DET NOUN NOUN ADP NOUN CCONJ AUX ADV VERB ADJ NOUN VERB ADP PRON ADJ NOUN PUNCT ADV ADJ ADP DET NOUN ADP VERB ADP DET ADJ NOUN ADP NOUN NOUN PUNCT DET NOUN VERB DET ADJ NOUN PRON ADV VERB CCONJ PROPN CCONJ PROPN NOUN ADP DET ADJ PROPN PROPN NOUN PUNCT ADP DET NOUN NOUN PUNCT DET PROPN NOUN VERB PART VERB DET VERB NOUN ADP DET PROPN NOUN PUNCT ADP DET PROPN NOUN PUNCT DET PROPN NOUN AUX VERB ADP DET ADJ NOUN CCONJ VERB DET NOUN NOUN ADP DET ADJ PROPN NOUN PUNCT DET ADJ NOUN AUX ADV VERB DET NOUN PART VERB DET ADJ NOUN ADP DET NOUN NOUN PUNCT ADP PRON NOUN PUNCT PRON AUX DET ADJ PROPN NOUN ADP PROPN NOUN NOUN PUNCT PRON VERB PRON NOUN ADP DET NOUN ADP NOUN NOUN PUNCT ADJ NOUN ADP ADJ NOUN NOUN VERB SCONJ DET VERB NOUN VERB ADJ PUNCT SCONJ PART ADJ PUNCT NOUN ADP VERB NOUN NOUN CCONJ ADV VERB DET NOUN NOUN PUNCT,0.5507246376811594,23.0,5.251207729468599
67,67,Zhiqing Sun,"[' Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8∼14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models.']",abstract_chunked," Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8∼14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models.",4.252295544275256,22.733194790065724,67,0.2966288626194," Autoregressive sequence models achieve state of the art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non autoregressive models. Specifically, we design an efficient approximation for Propname Propname Propname for non autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the Propname. Experiments in machine translation show that while increasing little latency, our model could achieve significantly better translation performance than previous non autoregressive models on different translation datasets. In particular, for the Propname Propname Propname dataset, our model obtains a Propname score of 00.00, which largely outperforms the previous non autoregressive baselines and is only 0.00 lower in Propname than purely autoregressive models."," Autoregressive sequence models achieve state of the art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non autoregressive models. Specifically, we design an efficient approximation for Propname Propname Propname for non autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the Propname. Experiments in machine translation show that while increasing little latency, our model could achieve significantly better translation performance than previous non autoregressive models on different translation datasets. In particular, for the Propname Propname Propname dataset, our model obtains a Propname score of 00.00, which largely outperforms the previous non autoregressive baselines and is only 0.00 lower in Propname than purely autoregressive models.", ADJ NOUN NOUN VERB NOUN ADP DET NOUN NOUN ADP NOUN ADP NOUN NOUN PUNCT ADV PUNCT ADP ADP DET ADJ NOUN NOUN PUNCT DET NOUN VERB ADP ADJ NOUN ADP NOUN PUNCT ADV PUNCT ADJ ADJ NOUN NOUN AUX VERB PART VERB DET NOUN NOUN PUNCT ADV PUNCT DET NOUN VERB SCONJ DET VERB NOUN ADP DET NOUN AUX ADV ADJ ADP NOUN PUNCT DET DET NOUN NOUN ADV VERB DET NOUN NOUN NOUN PUNCT CCONJ ADV DET VERB ADJ ADJ NOUN AUX ADV VERB ADJ NOUN VERB ADP PRON ADJ NOUN PUNCT PART VERB DET VERB NOUN CCONJ VERB DET NOUN NOUN ADP DET ADJ NOUN PUNCT PRON VERB PART VERB DET ADJ NOUN NOUN ADP DET ADJ ADJ NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADP PROPN PROPN PROPN ADP ADJ ADJ NOUN NOUN PUNCT CCONJ ADV VERB DET ADJ NOUN NOUN AUX VERB ADJ NOUN ADP DET PROPN PUNCT NOUN ADP NOUN NOUN VERB SCONJ SCONJ VERB ADJ NOUN PUNCT PRON NOUN AUX VERB ADV ADJ NOUN NOUN ADP ADJ ADJ ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADP ADJ PUNCT ADP DET PROPN PROPN PROPN NOUN PUNCT PRON NOUN VERB DET PROPN NOUN ADP NUM PUNCT PRON ADV VERB DET ADJ ADJ ADJ NOUN CCONJ AUX ADV NUM ADJ ADP PROPN ADP ADV ADJ NOUN PUNCT,0.515695067264574,24.77777777777778,5.744394618834081
68,68,Zhiqing Sun,"[' Keyphrase extraction from documents is useful to a variety of applications such as information retrieval and document summarization. This paper presents an end-to-end method called DivGraphPointer for extracting a set of diversified keyphrases from a document. DivGraphPointer combines the advantages of traditional graph-based ranking methods and recent neural network-based approaches. Specifically, given a document, a word graph is constructed from the document based on word proximity and is encoded with graph convolutional networks, which effectively capture document-level word salience by modeling long-range dependency between words in the document and aggregating multiple appearances of identical words into one node. Furthermore, we propose a diversified point network to generate a set of diverse keyphrases out of the word graph in the decoding process. Experimental results on five benchmark data sets show that our proposed method significantly outperforms the existing state-of-the-art approaches.']",abstract_chunked," Keyphrase extraction from documents is useful to a variety of applications such as information retrieval and document summarization. This paper presents an end-to-end method called DivGraphPointer for extracting a set of diversified keyphrases from a document. DivGraphPointer combines the advantages of traditional graph-based ranking methods and recent neural network-based approaches. Specifically, given a document, a word graph is constructed from the document based on word proximity and is encoded with graph convolutional networks, which effectively capture document-level word salience by modeling long-range dependency between words in the document and aggregating multiple appearances of identical words into one node. Furthermore, we propose a diversified point network to generate a set of diverse keyphrases out of the word graph in the decoding process. Experimental results on five benchmark data sets show that our proposed method significantly outperforms the existing state-of-the-art approaches.",8.643135491606756,22.733194790065724,68,0.6390833854675293," Keyphrase extraction from documents is useful to a variety of applications such as information retrieval and document summarization. This paper presents an end to end method called DivGraphPointer for extracting a set of diversified keyphrases from a document. DivGraphPointer combines the advantages of traditional graph based ranking methods and recent neural network based approaches. Specifically, given a document, a word graph is constructed from the document based on word proximity and is encoded with graph convolutional networks, which effectively capture document level word salience by modeling long range dependency between words in the document and aggregating multiple appearances of identical words into one node. Furthermore, we propose a diversified point network to generate a set of diverse keyphrases out of the word graph in the decoding process. Experimental results on five benchmark data sets show that our proposed method significantly outperforms the existing state of the art approaches."," Keyphrase extraction from documents is useful to a variety of applications such as information retrieval and document summarization. This paper presents an end to end method called DivGraphPointer for extracting a set of diversified keyphrases from a document. DivGraphPointer combines the advantages of traditional graph based ranking methods and recent neural network based approaches. Specifically, given a document, a word graph is constructed from the document based on word proximity and is encoded with graph convolutional networks, which effectively capture document level word salience by modeling long range dependency between words in the document and aggregating multiple appearances of identical words into one node. Furthermore, we propose a diversified point network to generate a set of diverse keyphrases out of the word graph in the decoding process. Experimental results on five benchmark data sets show that our proposed method significantly outperforms the existing state of the art approaches.", NOUN NOUN ADP NOUN AUX ADJ ADP DET NOUN ADP NOUN ADJ ADP NOUN NOUN CCONJ NOUN NOUN PUNCT DET NOUN VERB DET NOUN PART NOUN NOUN VERB NOUN ADP VERB DET NOUN ADP ADJ NOUN ADP DET NOUN PUNCT NOUN VERB DET NOUN ADP ADJ NOUN VERB ADJ NOUN CCONJ ADJ ADJ NOUN VERB NOUN PUNCT ADV PUNCT VERB DET NOUN PUNCT DET NOUN NOUN AUX VERB ADP DET NOUN VERB ADP NOUN NOUN CCONJ AUX VERB ADP NOUN ADJ NOUN PUNCT PRON ADV VERB NOUN NOUN NOUN NOUN ADP VERB ADJ NOUN NOUN ADP NOUN ADP DET NOUN CCONJ VERB ADJ NOUN ADP ADJ NOUN ADP NUM NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN NOUN PART VERB DET NOUN ADP ADJ NOUN ADP ADP DET NOUN NOUN ADP DET VERB NOUN PUNCT ADJ NOUN ADP NUM ADJ NOUN NOUN VERB SCONJ PRON VERB NOUN ADV VERB DET VERB NOUN ADP DET NOUN NOUN PUNCT,0.6265822784810127,26.333333333333332,5.493670886075949
69,69,Zhiqing Sun,"[' We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.']",abstract_chunked," We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",16.79607142857145,22.733194790065724,69,0.49413204193115234," We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of the relations. In this paper, we present a new approach for knowledge graph embedding called Propname, which is able to model and infer various relation patterns including: symmetryantisymmetry, inversion, and composition. Specifically, the Propname model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self adversarial negative sampling technique for efficiently and effectively training the Propname model. Experimental results on multiple benchmark knowledge graphs show that the proposed Propname model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state of the art models for link prediction."," We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of the relations. In this paper, we present a new approach for knowledge graph embedding called Propname, which is able to model and infer various relation patterns including: symmetryantisymmetry, inversion, and composition. Specifically, the Propname model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self adversarial negative sampling technique for efficiently and effectively training the Propname model. Experimental results on multiple benchmark knowledge graphs show that the proposed Propname model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state of the art models for link prediction.", PRON VERB DET NOUN ADP VERB NOUN ADP NOUN CCONJ NOUN ADP NOUN NOUN ADP VERB ADJ NOUN PUNCT DET NOUN ADP DET DET NOUN ADV VERB ADP DET NOUN ADP NOUN CCONJ VERB DET NOUN ADP DET NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP NOUN NOUN VERB VERB PROPN PUNCT PRON AUX ADJ PART VERB CCONJ VERB ADJ NOUN NOUN VERB PUNCT NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT ADV PUNCT DET PROPN NOUN VERB DET NOUN ADP DET NOUN ADP DET NOUN NOUN ADP DET NOUN NOUN ADP DET ADJ NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NOUN ADJ ADJ NOUN NOUN ADP ADV CCONJ ADV VERB DET PROPN NOUN PUNCT ADJ NOUN ADP ADJ ADJ NOUN NOUN VERB SCONJ DET VERB PROPN NOUN AUX PART ADV ADJ PUNCT CCONJ ADV ADJ PART VERB CCONJ VERB ADJ NOUN NOUN CCONJ ADV VERB VERB NOUN ADP DET NOUN NOUN ADP NOUN NOUN PUNCT,0.6049382716049383,27.0,5.191358024691358
70,70,Zhiqing Sun,"[' Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.']",abstract_chunked," Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.",20.78238410596029,22.733194790065724,70,0.4423273801803589," Previous traditional approaches to unsupervised Chinese word segmentation can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non trivial. In this paper, we propose the segmental language models for Propname. Our approach explicitly focuses on the segmental nature of Propname, as well as preserves several properties of language models. In Propname, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised Propname and achieve competitive performance to the state of the art statistical models on four different datasets from Propname 0000 bakeoff."," Previous traditional approaches to unsupervised Chinese word segmentation can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non trivial. In this paper, we propose the segmental language models for Propname. Our approach explicitly focuses on the segmental nature of Propname, as well as preserves several properties of language models. In Propname, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised Propname and achieve competitive performance to the state of the art statistical models on four different datasets from Propname 0000 bakeoff.", ADJ ADJ NOUN PART VERB ADJ NOUN NOUN AUX AUX ADV VERB ADP ADJ CCONJ ADJ NOUN PUNCT DET ADJ VERB DET ADV VERB NOUN NOUN ADP NOUN NOUN PUNCT SCONJ DET ADJ VERB ADP VERB DET ADJ NOUN ADP DET ADJ ADJ NOUN PUNCT ADV PUNCT SCONJ PRON VERB DET ADJ NOUN PART VERB DET ADJ NOUN ADP ADJ NOUN ADP VERB ADJ NOUN NOUN PUNCT PRON ADP ADJ NOUN AUX ADV ADJ PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN NOUN ADP PROPN PUNCT PRON NOUN ADV VERB ADP DET ADJ NOUN ADP PROPN PUNCT ADV ADV ADP VERB ADJ NOUN ADP NOUN NOUN PUNCT ADP PROPN PUNCT DET NOUN NOUN VERB DET ADJ NOUN CCONJ DET NOUN NOUN VERB DET NOUN ADV PUNCT ADV ADV SCONJ PRON VERB PUNCT PRON AUX DET ADJ PART VERB DET ADJ NOUN ADP ADJ PROPN CCONJ VERB ADJ NOUN ADP DET NOUN ADP DET NOUN ADJ NOUN ADP NUM ADJ NOUN ADP PROPN NUM NOUN PUNCT,0.5748502994011976,23.857142857142858,5.323353293413174
71,71,Timo Schick,"[' Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with “task descriptions” in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.']",abstract_chunked," Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with “task descriptions” in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.",28.453818181818207,32.83053957444222,71,0.5623278021812439," Some Propname tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with task descriptions in natural language. While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Propname Propname Propname, a semi supervised training procedure that reformulates input examples as cloze style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, Propname outperforms supervised training and strong semi supervised approaches in low resource settings by a large margin."," Some Propname tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with task descriptions in natural language. While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Propname Propname Propname, a semi supervised training procedure that reformulates input examples as cloze style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, Propname outperforms supervised training and strong semi supervised approaches in low resource settings by a large margin.", DET PROPN NOUN AUX AUX VERB ADP DET ADV ADJ NOUN ADP VERB DET VERB NOUN NOUN ADP NOUN NOUN ADP ADJ NOUN PUNCT SCONJ DET NOUN VERB PRON ADJ NOUN PUNCT PRON VERB ADP DET NOUN SCONJ DET NUM NOUN AUX AUX VERB PUNCT PRON VERB PROPN PROPN PROPN PUNCT DET ADJ ADJ NOUN NOUN PRON VERB NOUN NOUN SCONJ VERB NOUN NOUN PART VERB NOUN NOUN VERB DET VERB NOUN PUNCT DET NOUN AUX ADV VERB PART VERB ADJ NOUN ADP DET ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT ADJ ADJ NOUN AUX VERB ADP DET VERB NOUN NOUN PUNCT ADP ADJ NOUN CCONJ NOUN PUNCT PROPN NOUN VERB NOUN CCONJ ADJ ADJ VERB NOUN ADP ADJ NOUN NOUN ADP DET ADJ NOUN PUNCT,0.6535433070866141,25.4,5.21259842519685
72,72,Timo Schick,"[' When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.']",abstract_chunked," When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",26.790419354838747,32.83053957444222,72,0.32861679792404175," When scaled to hundreds of billions of parameters, pretrained language models such as Propname 0 achieve remarkable few shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to Propname 0 can be obtained with language models that are much greener in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models."," When scaled to hundreds of billions of parameters, pretrained language models such as Propname 0 achieve remarkable few shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to Propname 0 can be obtained with language models that are much greener in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.", SCONJ VERB ADP NOUN ADP NOUN ADP NOUN PUNCT VERB NOUN NOUN ADJ ADP PROPN NUM VERB ADJ ADJ NOUN NOUN PUNCT ADV PUNCT ADJ NOUN ADP NOUN AUX VERB ADP NOUN CCONJ VERB ADJ ADJ NOUN PUNCT VERB ADP DET ADJ NOUN NOUN CCONJ VERB PRON ADJ SCONJ NOUN CCONJ NOUN PART VERB PRON PUNCT PRON VERB SCONJ NOUN ADJ ADP PROPN NUM AUX AUX VERB ADP NOUN NOUN PRON AUX ADV ADJ SCONJ SCONJ PRON NOUN NOUN AUX ADJ NOUN ADP NOUN ADJ PUNCT PRON AUX VERB ADP VERB ADJ NOUN ADP VERB NOUN PRON VERB DET NOUN NOUN PUNCT VERB ADP NOUN VERB NOUN PUNCT VERB ADJ NOUN VERB ADJ NOUN PUNCT PRON VERB ADJ NOUN VERB ADP ADJ ADJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT,0.7153846153846154,26.0,5.384615384615385
73,73,Timo Schick,"[' Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.']",abstract_chunked," Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",33.73223602484475,32.83053957444222,73,0.7291655540466309," Language models exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Propname, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self supervised way, requiring nothing more than a handful of demonstrations for each Propname. We incorporate a range of tools, including a calculator, a QA system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."," Language models exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Propname, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self supervised way, requiring nothing more than a handful of demonstrations for each Propname. We incorporate a range of tools, including a calculator, a QA system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", NOUN NOUN VERB ADJ NOUN PART VERB ADJ NOUN ADP ADV DET ADJ NOUN CCONJ ADJ NOUN PUNCT ADV ADP NOUN PUNCT PRON ADV PUNCT ADV PUNCT VERB ADP ADJ NOUN PUNCT ADJ ADP ADJ CCONJ ADJ NOUN PUNCT SCONJ ADV ADJ CCONJ ADJ NOUN VERB PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ NOUN AUX VERB PRON PART VERB ADJ NOUN ADP ADJ NOUN CCONJ VERB DET ADJ ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN VERB PART VERB DET NOUN PART VERB PUNCT SCONJ PART VERB PRON PUNCT PRON NOUN PART VERB PUNCT CCONJ SCONJ PART ADV VERB DET NOUN ADP ADJ ADJ NOUN PUNCT PRON AUX VERB ADP DET NOUN VERB NOUN PUNCT VERB PRON ADJ ADP DET NOUN ADP NOUN ADP DET PROPN PUNCT PRON VERB DET NOUN ADP NOUN PUNCT VERB DET NOUN PUNCT DET NOUN NOUN PUNCT DET NOUN NOUN PUNCT DET NOUN NOUN PUNCT CCONJ DET NOUN PUNCT NOUN VERB ADV VERB NUM NOUN NOUN ADP DET NOUN ADP ADJ NOUN PUNCT ADV ADJ ADP ADV ADJ NOUN PUNCT ADP VERB PRON NOUN NOUN VERB NOUN PUNCT,0.6720430107526881,26.571428571428573,4.682795698924731
74,74,Timo Schick,"[' When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.']",abstract_chunked," When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.",33.14108695652175,32.83053957444222,74,0.11431405693292618," When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self diagnosis. Based on this finding, we then propose a decoding Propname that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self debiasing. Self debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the models parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction."," When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self diagnosis. Based on this finding, we then propose a decoding Propname that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self debiasing. Self debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the models parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.", SCONJ VERB ADP ADJ PUNCT ADJ NOUN ADP DET NOUN PUNCT NOUN NOUN VERB ADP CCONJ VERB DET NOUN ADP ADJ NOUN PRON AUX AUX VERB ADP DET NOUN PUNCT PRON ADV VERB ADJ PUNCT ADJ PUNCT ADJ CCONJ ADV ADJ NOUN PUNCT SCONJ ADJ NOUN VERB NOUN ADP NOUN NOUN PART VERB ADJ NOUN PUNCT PRON AUX ADJ PART ADV VERB PRON ADP AUX VERB ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON ADV VERB DET ADJ NOUN PUNCT VERB NOUN NOUN VERB PUNCT ADP DET ADJ NOUN PUNCT PRON ADJ NOUN CCONJ DET NOUN ADP DET NOUN PRON VERB PUNCT PRON VERB ADP DET NOUN ADP NOUN NOUN PUNCT VERB ADP DET NOUN PUNCT PRON ADV VERB DET VERB PROPN SCONJ PUNCT VERB ADV DET ADJ NOUN ADP DET ADJ NOUN PUNCT VERB DET NOUN ADP DET NOUN NOUN VERB ADJ NOUN PUNCT PRON VERB ADP DET NOUN ADP NOUN VERB PUNCT NOUN NOUN AUX PART VERB ADP ADV VERB NOUN NOUN PUNCT CCONJ AUX PRON VERB DET NOUN NOUN CCONJ NOUN ADP DET NOUN NOUN PUNCT SCONJ PRON ADP DET NOUN VERB DET NOUN ADP NOUN NOUN VERB ADJ NOUN PUNCT PRON VERB PRON NOUN PART AUX DET ADJ NOUN ADP DET NOUN PUNCT,0.5961538461538461,26.0,4.711538461538462
75,75,Timo Schick,"[' A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language model’s abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand-crafted label-to-word mappings.']",abstract_chunked," A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language model’s abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand-crafted label-to-word mappings.",32.83053957444222,32.83053957444222,75,0.4040681719779968," A recent approach for few shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language models abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand crafted label to word mappings."," A recent approach for few shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language models abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand crafted label to word mappings.", DET ADJ NOUN ADP ADJ NOUN NOUN NOUN AUX PART VERB ADJ NOUN ADP VERB NOUN PRON VERB DET NOUN ADP NOUN NOUN PUNCT VERB PRON ADP DET VERB NOUN NOUN CCONJ VERB DET VERB NOUN ADP NOUN PUNCT ADV VERB DET NOUN ADP NOUN CCONJ NOUN VERB DET NOUN NOUN CCONJ DET NOUN ADP DET NOUN NOUN NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB DET NOUN PRON ADV VERB DET DET NOUN VERB ADJ NOUN ADP NOUN NOUN PUNCT ADP DET NOUN ADP NOUN PUNCT DET NOUN VERB ADP PRON NOUN VERB ADV ADV ADV ADP NOUN VERB NOUN PART NOUN NOUN PUNCT,0.7358490566037735,26.5,4.867924528301887
76,76,Timo Schick,"[' To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pre-training objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.']",abstract_chunked," To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pre-training objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.",11.53159836065575,32.83053957444222,76,0.595647394657135," To obtain high quality sentence embeddings from pretrained language models, they must either be augmented with additional pre training objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets."," To obtain high quality sentence embeddings from pretrained language models, they must either be augmented with additional pre training objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.", PART VERB ADJ NOUN NOUN NOUN ADP VERB NOUN NOUN PUNCT PRON AUX CCONJ AUX VERB ADP ADJ ADJ NOUN NOUN CCONJ VERB ADP DET ADJ NOUN ADP VERB NOUN NOUN PUNCT SCONJ DET ADJ NOUN ADV VERB DET ADJ PUNCT PRON VERB ADJ ADJ NOUN PART VERB ADJ NOUN ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ NOUN AUX AUX VERB PART VERB ADJ NOUN NOUN NOUN ADP DET NOUN ADP VERB NOUN PUNCT VERB CCONJ NOUN ADP DET VERB NOUN PUNCT PRON VERB DET ADJ NOUN ADP ADJ CCONJ ADJ VERB NOUN PART VERB ADJ NOUN ADP VERB NOUN NOUN ADP NOUN PUNCT PRON PRON ADV VERB ADP VERB ADV ADJ CCONJ ADV ADJ NOUN PUNCT PRON ADV ADJ NOUN VERB ADJ NOUN ADP ADJ ADJ ADJ NOUN NOUN PUNCT,0.6814814814814815,33.75,5.296296296296297
77,77,Timo Schick,"[' Pretraining deep neural network architectures with a language modeling objective has brought large improvements for many natural language processing tasks. Exemplified by BERT, a recently proposed such architecture, we demonstrate that despite being trained on huge amounts of data, deep language models still struggle to understand rare words. To fix this problem, we adapt Attentive Mimicking, a method that was designed to explicitly learn embeddings for rare words, to deep language models. In order to make this possible, we introduce one-token approximation, a procedure that enables us to use Attentive Mimicking even when the underlying language model uses subword-based tokenization, i.e., it does not assign embeddings to all words. To evaluate our method, we create a novel dataset that tests the ability of language models to capture semantic properties of words without any task-specific fine-tuning. Using this dataset, we show that adding our adapted version of Attentive Mimicking to BERT does substantially improve its understanding of rare words.']",abstract_chunked," Pretraining deep neural network architectures with a language modeling objective has brought large improvements for many natural language processing tasks. Exemplified by BERT, a recently proposed such architecture, we demonstrate that despite being trained on huge amounts of data, deep language models still struggle to understand rare words. To fix this problem, we adapt Attentive Mimicking, a method that was designed to explicitly learn embeddings for rare words, to deep language models. In order to make this possible, we introduce one-token approximation, a procedure that enables us to use Attentive Mimicking even when the underlying language model uses subword-based tokenization, i.e., it does not assign embeddings to all words. To evaluate our method, we create a novel dataset that tests the ability of language models to capture semantic properties of words without any task-specific fine-tuning. Using this dataset, we show that adding our adapted version of Attentive Mimicking to BERT does substantially improve its understanding of rare words.",24.03938679245283,32.83053957444222,77,0.5313282012939453," Pretraining deep neural network architectures with a language modeling objective has brought large improvements for many natural language processing tasks. Exemplified by Propname, a recently proposed such architecture, we demonstrate that despite being trained on huge amounts of data, deep language models still struggle to understand rare words. To fix this problem, we adapt Propname Propname, a method that was designed to explicitly learn embeddings for rare words, to deep language models. In order to make this possible, we introduce one token approximation, a procedure that enables us to use Propname Propname even when the underlying language model uses Propname based Propname, ie, it does not assign embeddings to all words. To evaluate our method, we create a novel dataset that tests the ability of language models to capture semantic properties of words without any task specific fine tuning. Using this dataset, we show that adding our adapted version of Propname Propname to Propname does substantially improve its understanding of rare words."," Pretraining deep neural network architectures with a language modeling objective has brought large improvements for many natural language processing tasks. Exemplified by Propname, a recently proposed such architecture, we demonstrate that despite being trained on huge amounts of data, deep language models still struggle to understand rare words. To fix this problem, we adapt Propname Propname, a method that was designed to explicitly learn embeddings for rare words, to deep language models. In order to make this possible, we introduce one token approximation, a procedure that enables us to use Propname Propname even when the underlying language model uses Propname based Propname, ie, it does not assign embeddings to all words. To evaluate our method, we create a novel dataset that tests the ability of language models to capture semantic properties of words without any task specific fine tuning. Using this dataset, we show that adding our adapted version of Propname Propname to Propname does substantially improve its understanding of rare words.", VERB ADJ ADJ NOUN VERB ADP DET NOUN NOUN NOUN AUX VERB ADJ NOUN ADP ADJ ADJ NOUN NOUN NOUN PUNCT VERB ADP PROPN PUNCT DET ADV VERB ADJ NOUN PUNCT PRON VERB SCONJ SCONJ AUX VERB ADP ADJ NOUN ADP NOUN PUNCT ADJ NOUN NOUN ADV VERB PART VERB ADJ NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET NOUN PRON AUX VERB PART ADV VERB NOUN ADP ADJ NOUN PUNCT ADP ADJ NOUN NOUN PUNCT ADP NOUN PART VERB PRON ADJ PUNCT PRON VERB NUM NOUN NOUN PUNCT DET NOUN PRON VERB PRON PART VERB PROPN PROPN ADV SCONJ DET ADJ NOUN NOUN VERB PROPN VERB PROPN PUNCT ADV PUNCT PRON AUX PART VERB NOUN ADP DET NOUN PUNCT PART VERB PRON NOUN PUNCT PRON VERB DET ADJ NOUN PRON VERB DET NOUN ADP NOUN NOUN PART VERB ADJ NOUN ADP NOUN ADP DET NOUN ADJ ADJ NOUN PUNCT VERB DET NOUN PUNCT PRON VERB SCONJ VERB PRON VERB NOUN ADP PROPN PROPN ADP PROPN AUX ADV VERB PRON NOUN ADP ADJ NOUN PUNCT,0.5944444444444444,30.0,5.0
78,78,Timo Schick,"[' Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few-shot results for a wide range of text classification tasks when combined with gradient-based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Pattern-Exploiting Training (PET), a recently proposed few-shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of PET gives consistent improvements over a strong baseline in few-shot settings.']",abstract_chunked," Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few-shot results for a wide range of text classification tasks when combined with gradient-based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Pattern-Exploiting Training (PET), a recently proposed few-shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of PET gives consistent improvements over a strong baseline in few-shot settings.",32.83053957444222,32.83053957444222,78,0.5951589941978455," Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few shot results for a wide range of text classification tasks when combined with gradient based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Propname Propname Propname, a recently proposed few shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of Propname gives consistent improvements over a strong baseline in few shot settings."," Providing pretrained language models with simple task descriptions or prompts in natural language yields impressive few shot results for a wide range of text classification tasks when combined with gradient based learning from examples. In this paper, we show that the underlying idea can also be applied to text generation tasks: We adapt Propname Propname Propname, a recently proposed few shot approach, for finetuning generative language models on text generation tasks. On several text summarization and headline generation datasets, our proposed variant of Propname gives consistent improvements over a strong baseline in few shot settings.", VERB VERB NOUN NOUN ADP ADJ NOUN NOUN CCONJ NOUN ADP ADJ NOUN NOUN ADJ ADJ NOUN NOUN ADP DET ADJ NOUN ADP NOUN NOUN NOUN SCONJ VERB ADP NOUN VERB VERB ADP NOUN PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ DET ADJ NOUN AUX ADV AUX VERB ADP NOUN NOUN NOUN PUNCT PRON VERB PROPN PROPN PROPN PUNCT DET ADV VERB ADJ NOUN NOUN PUNCT ADP VERB ADJ NOUN NOUN ADP NOUN NOUN NOUN PUNCT ADP ADJ NOUN NOUN CCONJ NOUN NOUN NOUN PUNCT PRON VERB NOUN ADP PROPN VERB ADJ NOUN ADP DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT,0.7184466019417476,34.333333333333336,5.359223300970874
79,79,Timo Schick,"[' Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GENPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GENPET gives consistent improvements over strong baselines in few-shot\nsettings.']",abstract_chunked," Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GENPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GENPET gives consistent improvements over strong baselines in few-shot
settings.",18.60223684210527,32.83053957444222,79,0.614845335483551," Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GENPET, a method for text generation that is based on pattern exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GENPET gives consistent improvements over strong baselines in few shot 
 settings."," Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GENPET, a method for text generation that is based on pattern exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GENPET gives consistent improvements over strong baselines in few shot 
 settings.", VERB VERB NOUN NOUN ADP ADJ NOUN NOUN ADP ADJ NOUN VERB PRON PART VERB DET NOUN ADP DET ADV ADJ NOUN PUNCT ADV PUNCT SCONJ VERB ADP ADJ NOUN ADP NOUN PUNCT DET NOUN VERB ADJ ADJ NOUN NOUN ADP DET ADJ NOUN ADP NOUN NOUN NOUN PUNCT PRON AUX ADV DET ADJ NOUN PART VERB NOUN NOUN ADP ADJ NOUN PUNCT CCONJ PRON VERB ADJ NOUN ADP VERB DET NOUN ADP NOUN NOUN CCONJ NOUN VERB VERB ADP NOUN NOUN PUNCT ADP ADJ PUNCT PRON AUX ADJ PART VERB NOUN NOUN PRON AUX ADJ PART VERB ADP DET VERB NOUN CCONJ PART VERB SCONJ PRON ADV VERB ADJ NOUN ADP PRON PUNCT ADV PUNCT ADJ NOUN ADP NOUN VERB PART AUX VERB PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ DET NOUN AUX AUX VERB PUNCT PRON VERB NOUN PUNCT DET NOUN ADP NOUN NOUN PRON AUX VERB ADP NOUN VERB NOUN PUNCT DET ADJ NOUN ADP VERB ADJ NOUN ADP ADJ NOUN PRON ADV VERB ADP NOUN NOUN PUNCT ADP ADJ NOUN CCONJ NOUN NOUN NOUN PUNCT NOUN VERB ADJ NOUN ADP ADJ NOUN ADP ADJ NOUN SPACE NOUN PUNCT,0.6302083333333334,32.0,5.182291666666667
80,80,Timo Schick,"[' Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today’s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER’s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.']",abstract_chunked," Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today’s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER’s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.",27.711443850267386,32.83053957444222,80,0.5870611071586609," Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, todays language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce Propname, a collaborative language model that is trained to imitate the entire writing process itself: Propname can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEERs full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that Propname achieves strong performance across various domains and editing tasks."," Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, todays language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce Propname, a collaborative language model that is trained to imitate the entire writing process itself: Propname can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEERs full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that Propname achieves strong performance across various domains and editing tasks.", ADJ NOUN AUX ADV DET NOUN ADP DET ADJ NOUN NOUN PUNCT PRON VERB ADP DET ADJ NOUN PUNCT VERB ADP NOUN PUNCT CCONJ ADV VERB NOUN PUNCT ADJ ADP DET NOUN PUNCT VERB NOUN NOUN AUX VERB PART VERB ADV DET ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB ADJ NOUN ADJ ADP ADJ NOUN PUNCT PRON AUX ADJ PART VERB VERB NOUN PUNCT ADJ PART VERB CCONJ ADJ ADP ADV VERB CCONJ VERB PRON NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN PRON AUX VERB PART VERB DET ADJ NOUN NOUN PRON PUNCT PROPN AUX VERB NOUN PUNCT VERB NOUN PUNCT VERB NOUN CCONJ VERB NOUN ADP PRON NOUN PUNCT ADV PUNCT PRON VERB ADJ NOUN ADP ADJ ADJ PART VERB ADJ NOUN ADP DET NOUN NOUN PUNCT VERB DET NOUN ADP VERB NOUN ADP VERB DET NOUN PUNCT NOUN CCONJ NOUN ADP NOUN NOUN PUNCT DET NOUN VERB ADJ NOUN ADP VERB PRON ADJ ADP NOUN ADP PRON DET NOUN NOUN AUX ADJ CCONJ VERB PRON NOUN PART VERB NOUN PUNCT PART VERB ADJ NOUN PUNCT CCONJ PART VERB PRON NOUN PUNCT PRON VERB SCONJ PROPN VERB ADJ NOUN ADP ADJ NOUN CCONJ NOUN NOUN PUNCT,0.6280193236714976,29.571428571428573,5.024154589371981
81,81,Timo Schick,"[' Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word’s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium-frequency range.']",abstract_chunked," Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al., 2017) has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word’s surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium-frequency range.",31.212000000000018,32.83053957444222,81,0.428235799074173," Learning high quality embeddings for rare words is a hard problem because of sparse context information. Mimicking has been proposed as a solution: given embeddings learned by a standard Propname, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a words surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium frequency range."," Learning high quality embeddings for rare words is a hard problem because of sparse context information. Mimicking has been proposed as a solution: given embeddings learned by a standard Propname, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a words surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the medium frequency range.", VERB ADJ NOUN NOUN ADP ADJ NOUN AUX DET ADJ NOUN SCONJ ADP ADJ NOUN NOUN PUNCT NOUN AUX AUX VERB ADP DET NOUN PUNCT VERB NOUN VERB ADP DET ADJ PROPN PUNCT DET NOUN AUX ADV VERB PART VERB NOUN ADP ADJ NOUN ADP PRON NOUN NOUN CCONJ ADV VERB PART VERB NOUN ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB ADJ NOUN PUNCT DET ADJ NOUN AUX VERB NOUN PART ADV ADP DET NOUN NOUN NOUN PUNCT CCONJ ADV ADP DET ADJ NOUN CCONJ VERB PART VERB ADP DET ADV ADJ CCONJ ADJ NOUN ADP VERB DET NOUN PUNCT ADP DET NOUN ADP NUM NOUN PUNCT PRON VERB PRON ADJ VERB NOUN ADJ NOUN ADP CCONJ ADJ CCONJ ADJ ADJ NOUN PUNCT ADV PUNCT VERB ADP ADJ NOUN PUNCT ADJ NOUN VERB NOUN ADP DET ADV ADJ NOUN ADP DET NOUN PUNCT VERB DET ADJ ADJ NOUN PUNCT,0.5855263157894737,30.4,4.848684210526316
82,82,Timo Schick,"[' Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch¨utze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.']",abstract_chunked," Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch¨utze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.",42.117955635491626,32.83053957444222,82,0.6015511155128479," Pretraining deep language models has led to large performance gains in Propname. Despite this success, Propname and Propname recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce Propname, a powerful architecture based on Propname that is capable of inferring high quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into Propname leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks."," Pretraining deep language models has led to large performance gains in Propname. Despite this success, Propname and Propname recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce Propname, a powerful architecture based on Propname that is capable of inferring high quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into Propname leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.", VERB ADJ NOUN NOUN AUX VERB ADP ADJ NOUN NOUN ADP PROPN PUNCT SCONJ DET NOUN PUNCT PROPN CCONJ PROPN ADV VERB SCONJ DET NOUN VERB PART VERB ADJ NOUN PUNCT ADP ADJ NOUN NOUN PUNCT DET NOUN AUX AUX VERB ADP ADV VERB NOUN ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN PART VERB NOUN NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN VERB ADP PROPN PRON AUX ADJ ADP VERB ADJ NOUN NOUN ADP ADJ NOUN PRON AUX ADJ ADP NOUN NOUN ADP ADJ NOUN NOUN PUNCT PRON AUX VERB ADP VERB DET NOUN NOUN CCONJ NOUN ADP DET NOUN PART VERB ADP DET ADJ ADP DET ADJ NOUN PUNCT VERB ADV ADP PROPN VERB ADP ADJ NOUN NOUN ADJ ADP VERB NOUN ADP ADJ CCONJ ADJ ADJ NOUN ADP CCONJ DET ADJ NOUN VERB NOUN CCONJ NUM ADJ NOUN PUNCT,0.5945945945945946,24.666666666666668,5.141891891891892
83,83,Timo Schick,"[' Prompt-based approaches are strong at fewshot learning. However, Perez et al. (2021) have recently cast doubt on their performance because they had difficulty getting good results in a “true” few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of PET, a method that combines textual instructions with examplebased finetuning. We show that, if correctly configured, PET performs strongly in a true few-shot setting, i.e., without a dev set. Crucial for this strong performance is PET’s ability to intelligently handle multiple prompts. We then put our findings to a real-world test by running PET on RAFT, a benchmark of tasks taken directly from realistic NLP applications for which no labeled dev or test sets are available. PET achieves a new state of the art on RAFT and performs close to nonexpert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners like PET excel at true few-shot learning and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities.']",abstract_chunked," Prompt-based approaches are strong at fewshot learning. However, Perez et al. (2021) have recently cast doubt on their performance because they had difficulty getting good results in a “true” few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of PET, a method that combines textual instructions with examplebased finetuning. We show that, if correctly configured, PET performs strongly in a true few-shot setting, i.e., without a dev set. Crucial for this strong performance is PET’s ability to intelligently handle multiple prompts. We then put our findings to a real-world test by running PET on RAFT, a benchmark of tasks taken directly from realistic NLP applications for which no labeled dev or test sets are available. PET achieves a new state of the art on RAFT and performs close to nonexpert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners like PET excel at true few-shot learning and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities.",50.047222222222246,32.83053957444222,83,0.45713871717453003," Propname based approaches are strong at fewshot learning. However, Propname Propname Propname. have recently cast doubt on their performance because they had difficulty getting good results in a true few shot setting in which prompts and hyperparameters can not be tuned on a dev set. In view of this, we conduct an extensive study of Propname, a method that combines textual instructions with examplebased finetuning. We show that, if correctly configured, Propname performs strongly in a true few shot setting, ie, without a dev set. Crucial for this strong performance is PETs ability to intelligently handle multiple prompts. We then put our findings to a real world test by running Propname on Propname, a benchmark of tasks taken directly from realistic Propname applications for which no labeled dev or test sets are available. Propname achieves a new state of the art on Propname and performs close to nonexpert humans for 0 out of 00 tasks. These results demonstrate that prompt based learners like Propname excel at true few shot learning and underpin our belief that learning from instructions will play an important role on the path towards human like few shot learning capabilities."," Propname based approaches are strong at fewshot learning. However, Propname Propname Propname. have recently cast doubt on their performance because they had difficulty getting good results in a true few shot setting in which prompts and hyperparameters can not be tuned on a dev set. In view of this, we conduct an extensive study of Propname, a method that combines textual instructions with examplebased finetuning. We show that, if correctly configured, Propname performs strongly in a true few shot setting, ie, without a dev set. Crucial for this strong performance is PETs ability to intelligently handle multiple prompts. We then put our findings to a real world test by running Propname on Propname, a benchmark of tasks taken directly from realistic Propname applications for which no labeled dev or test sets are available. Propname achieves a new state of the art on Propname and performs close to nonexpert humans for 0 out of 00 tasks. These results demonstrate that prompt based learners like Propname excel at true few shot learning and underpin our belief that learning from instructions will play an important role on the path towards human like few shot learning capabilities.", PROPN VERB NOUN AUX ADJ ADP ADJ NOUN PUNCT ADV PUNCT PROPN PROPN PROPN PUNCT AUX ADV VERB NOUN ADP PRON NOUN SCONJ PRON VERB NOUN VERB ADJ NOUN ADP DET ADJ ADJ NOUN NOUN ADP PRON NOUN CCONJ NOUN AUX PART AUX VERB ADP DET NOUN NOUN PUNCT ADP NOUN ADP PRON PUNCT PRON VERB DET ADJ NOUN ADP PROPN PUNCT DET NOUN PRON VERB ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB SCONJ PUNCT SCONJ ADV VERB PUNCT PROPN VERB ADV ADP DET ADJ ADJ NOUN NOUN PUNCT ADV PUNCT ADP DET NOUN NOUN PUNCT ADJ ADP DET ADJ NOUN AUX ADJ NOUN PART ADV VERB ADJ NOUN PUNCT PRON ADV VERB PRON NOUN ADP DET ADJ NOUN NOUN ADP VERB PROPN ADP PROPN PUNCT DET NOUN ADP NOUN VERB ADV ADP ADJ PROPN NOUN ADP PRON DET VERB NOUN CCONJ NOUN NOUN AUX ADJ PUNCT PROPN VERB DET ADJ NOUN ADP DET NOUN ADP PROPN CCONJ VERB ADV PART VERB NOUN ADP NUM ADP ADP NUM NOUN PUNCT DET NOUN VERB SCONJ NOUN VERB NOUN ADP PROPN VERB ADP ADJ ADJ NOUN NOUN CCONJ VERB PRON NOUN SCONJ VERB ADP NOUN AUX VERB DET ADJ NOUN ADP DET NOUN ADP ADJ ADP ADJ NOUN VERB NOUN PUNCT,0.5904761904761905,23.333333333333332,4.776190476190476
84,84,Timo Schick,"[' Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for novel words on the fly, i.e., for words that did not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: (i) learning an embedding from the novel word’s surface-form (e.g., subword n-grams) and (ii) learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information – surface-form and context – and show that it results in large increases in embedding quality. Our architecture obtains state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words.']",abstract_chunked," Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for novel words on the fly, i.e., for words that did not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: (i) learning an embedding from the novel word’s surface-form (e.g., subword n-grams) and (ii) learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information – surface-form and context – and show that it results in large increases in embedding quality. Our architecture obtains state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words.",32.35384615384615,32.83053957444222,84,0.37341034412384033," Word embeddings are a key component of high performing natural language processing systems, but it remains a challenge to learn good representations for novel words on the fly, ie, for words that did not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: learning an embedding from the novel words surface form and learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information surface form and context and show that it results in large increases in embedding quality. Our architecture obtains state of the art results on the Propname Propname and Propname Propname Propname datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words."," Word embeddings are a key component of high performing natural language processing systems, but it remains a challenge to learn good representations for novel words on the fly, ie, for words that did not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: learning an embedding from the novel words surface form and learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information surface form and context and show that it results in large increases in embedding quality. Our architecture obtains state of the art results on the Propname Propname and Propname Propname Propname datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words.", NOUN NOUN AUX DET ADJ NOUN ADP ADJ VERB ADJ NOUN NOUN NOUN PUNCT CCONJ PRON VERB DET NOUN PART VERB ADJ NOUN ADP ADJ NOUN ADP DET NOUN PUNCT ADV PUNCT ADP NOUN PRON AUX PART VERB ADP DET NOUN NOUN PUNCT DET ADJ NOUN VERB AUX SCONJ NOUN NOUN AUX VERB ADP DET ADJ NOUN NOUN CCONJ ADV DET NOUN AUX VERB SCONJ VERB ADJ NOUN ADP DET VERB VERB NOUN PUNCT ADV PUNCT NUM NOUN ADP VERB NOUN ADP ADJ NOUN VERB PUNCT VERB DET VERB ADP DET ADJ NOUN NOUN NOUN CCONJ VERB DET VERB ADP DET NOUN ADP PRON PRON VERB PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN PRON VERB DET NOUN ADP NOUN NOUN NOUN CCONJ NOUN CCONJ VERB SCONJ PRON VERB ADP ADJ NOUN ADP VERB NOUN PUNCT PRON NOUN VERB NOUN ADP DET NOUN NOUN ADP DET PROPN PROPN CCONJ PROPN PROPN PROPN NOUN PUNCT ADP NOUN PUNCT PRON ADV VERB DET VERB NOUN CCONJ DET ADJ NOUN ADP VERB PRON NOUN PART VERB NOUN ADJ ADP DET VERB VERB NOUN PUNCT ADV PUNCT PRON NOUN AUX ADV AUX VERB ADP DET VERB NOUN NOUN CCONJ VERB PRON NOUN PART VERB ADJ NOUN PUNCT,0.5343137254901961,29.142857142857142,4.867647058823529
85,85,Timo Schick,"[' This work addresses the task of generating English sentences from Abstract Meaning Representation (AMR) graphs. To cope with this task, we transform each input AMR graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an algorithm that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Bleu score of 27.4 on the LDC2014T12 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data.']",abstract_chunked," This work addresses the task of generating English sentences from Abstract Meaning Representation (AMR) graphs. To cope with this task, we transform each input AMR graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an algorithm that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Bleu score of 27.4 on the LDC2014T12 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data.",31.122418604651187,32.83053957444222,85,0.7665858864784241," This work addresses the task of generating English sentences from Propname Propname Propname graphs. To cope with this task, we transform each input Propname graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an Propname that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Propname score of 00.0 on the LDC0000T00 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data."," This work addresses the task of generating English sentences from Propname Propname Propname graphs. To cope with this task, we transform each input Propname graph into a structure similar to a dependency tree and annotate it with syntactic information by applying various predefined actions to it. Subsequently, a sentence is obtained from this tree structure by visiting its nodes in a specific order. We train maximum entropy models to estimate the probability of each individual action and devise an Propname that efficiently approximates the best sequence of actions to be applied. Using a substandard language model, our generator achieves a Propname score of 00.0 on the LDC0000T00 test set, the best result reported so far without using silver standard annotations from another corpus as additional training data.", DET NOUN VERB DET NOUN ADP VERB ADJ NOUN ADP PROPN PROPN PROPN NOUN PUNCT PART VERB ADP DET NOUN PUNCT PRON VERB DET NOUN PROPN NOUN ADP DET NOUN ADJ ADP DET NOUN NOUN CCONJ VERB PRON ADP ADJ NOUN ADP VERB ADJ VERB NOUN ADP PRON PUNCT ADV PUNCT DET NOUN AUX VERB ADP DET NOUN NOUN ADP VERB PRON NOUN ADP DET ADJ NOUN PUNCT PRON VERB ADJ NOUN NOUN PART VERB DET NOUN ADP DET ADJ NOUN CCONJ VERB DET PROPN PRON ADV VERB DET ADJ NOUN ADP NOUN PART AUX VERB PUNCT VERB DET ADJ NOUN NOUN PUNCT PRON NOUN VERB DET PROPN NOUN ADP NUM ADP DET NOUN NOUN NOUN PUNCT DET ADJ NOUN VERB ADV ADV ADP VERB ADJ ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT,0.7058823529411765,27.2,5.110294117647059
86,0,GPT-3.5,"[' The availability of diverse Large Language Models (LLMs) through cloud API providers offers a spectrum of choices, but the challenge lies in efficiently utilizing these options to strike a balance between computational cost and performance optimization. This work presents AutoMix, a novel approach addressing this challenge by intelligently directing queries to larger LLMs based on the reliability of outputs from a smaller LM. The key innovation lies in a few-shot self-verification mechanism, eliminating the need for extensive training to estimate output reliability. Recognizing the potential noise in verifications, AutoMix incorporates a meta verifier to refine the accuracy of these assessments. The experimental evaluation, conducted on LLAMA2-13/70B across five context-grounded reasoning datasets, demonstrates the superiority of AutoMix over established baselines, showcasing a remarkable up to 89% enhancement in the incremental benefit per cost.']",intro_chunked," The availability of diverse Large Language Models (LLMs) through cloud API providers offers a spectrum of choices, but the challenge lies in efficiently utilizing these options to strike a balance between computational cost and performance optimization. This work presents AutoMix, a novel approach addressing this challenge by intelligently directing queries to larger LLMs based on the reliability of outputs from a smaller LM. The key innovation lies in a few-shot self-verification mechanism, eliminating the need for extensive training to estimate output reliability. Recognizing the potential noise in verifications, AutoMix incorporates a meta verifier to refine the accuracy of these assessments. The experimental evaluation, conducted on LLAMA2-13/70B across five context-grounded reasoning datasets, demonstrates the superiority of AutoMix over established baselines, showcasing a remarkable up to 89% enhancement in the incremental benefit per cost.",3.488223880597019,2.6406588729545177,86,0.6620028018951416," The availability of diverse Large Propname Propname through cloud API providers offers a spectrum of choices, but the challenge lies in efficiently utilizing these options to strike a balance between computational cost and performance optimization. This work presents Propname, a novel approach addressing this challenge by intelligently directing queries to larger LLMs based on the reliability of outputs from a smaller Propname. The key innovation lies in a few shot self verification mechanism, eliminating the need for extensive training to estimate output reliability. Recognizing the potential noise in verifications, Propname incorporates a meta verifier to refine the accuracy of these assessments. The experimental evaluation, conducted on Propname Propname across five context grounded reasoning datasets, demonstrates the superiority of Propname over established baselines, showcasing a remarkable up to 00 enhancement in the incremental benefit per cost."," The availability of diverse Large Propname Propname through cloud API providers offers a spectrum of choices, but the challenge lies in efficiently utilizing these options to strike a balance between computational cost and performance optimization. This work presents Propname, a novel approach addressing this challenge by intelligently directing queries to larger LLMs based on the reliability of outputs from a smaller Propname. The key innovation lies in a few shot self verification mechanism, eliminating the need for extensive training to estimate output reliability. Recognizing the potential noise in verifications, Propname incorporates a meta verifier to refine the accuracy of these assessments. The experimental evaluation, conducted on Propname Propname across five context grounded reasoning datasets, demonstrates the superiority of Propname over established baselines, showcasing a remarkable up to 00 enhancement in the incremental benefit per cost.", DET NOUN ADP ADJ ADJ PROPN PROPN ADP ADJ NOUN NOUN VERB DET NOUN ADP NOUN PUNCT CCONJ DET NOUN VERB ADP ADV VERB DET NOUN PART VERB DET NOUN ADP ADJ NOUN CCONJ NOUN NOUN PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN VERB DET NOUN ADP ADV VERB NOUN ADP ADJ NOUN VERB ADP DET NOUN ADP NOUN ADP DET ADJ PROPN PUNCT DET ADJ NOUN VERB ADP DET ADJ NOUN NOUN NOUN NOUN PUNCT VERB DET NOUN ADP ADJ NOUN PART VERB NOUN NOUN PUNCT VERB DET ADJ NOUN ADP NOUN PUNCT PROPN VERB DET ADJ NOUN PART VERB DET NOUN ADP DET NOUN PUNCT DET ADJ NOUN PUNCT VERB ADP PROPN PROPN ADP NUM NOUN VERB NOUN NOUN PUNCT VERB DET NOUN ADP PROPN ADP VERB NOUN PUNCT VERB DET ADJ ADP PART NUM NOUN ADP DET ADJ NOUN ADP NOUN PUNCT,0.673469387755102,29.4,5.666666666666667
87,1,GPT-3.5,"[' Large Language Models (LLMs) have exhibited remarkable capabilities in natural language processing tasks. However, their performance is not infallible, often producing suboptimal outputs in the first attempt. This motivates the development of SELF-REFINE, an innovative approach aimed at refining LLM outputs through an iterative feedback loop. Inspired by human writing refinement processes, SELF-REFINE leverages a single LLM for the entire feedback and improvement pipeline. Unlike existing methods, our approach circumvents the need for additional supervised training data, specialized training, or reinforcement learning. In this paper, we explore the efficacy of SELF-REFINE across a spectrum of 7 diverse tasks, spanning conversational contexts to intricate mathematical reasoning. Utilizing cutting-edge LLMs, including GPT-3.5 and GPT-4, we present compelling evidence that SELF-REFINE consistently outperforms conventional one-step generation, showcasing an average improvement of around 20% in task performance. Our work underscores the untapped potential for refining even the most advanced LLMs at test-time with a straightforward, standalone strategy.']",intro_chunked," Large Language Models (LLMs) have exhibited remarkable capabilities in natural language processing tasks. However, their performance is not infallible, often producing suboptimal outputs in the first attempt. This motivates the development of SELF-REFINE, an innovative approach aimed at refining LLM outputs through an iterative feedback loop. Inspired by human writing refinement processes, SELF-REFINE leverages a single LLM for the entire feedback and improvement pipeline. Unlike existing methods, our approach circumvents the need for additional supervised training data, specialized training, or reinforcement learning. In this paper, we explore the efficacy of SELF-REFINE across a spectrum of 7 diverse tasks, spanning conversational contexts to intricate mathematical reasoning. Utilizing cutting-edge LLMs, including GPT-3.5 and GPT-4, we present compelling evidence that SELF-REFINE consistently outperforms conventional one-step generation, showcasing an average improvement of around 20% in task performance. Our work underscores the untapped potential for refining even the most advanced LLMs at test-time with a straightforward, standalone strategy.",10.328084677419355,2.6406588729545177,87,0.551087498664856," Large Propname Propname have exhibited remarkable capabilities in natural language processing tasks. However, their performance is not infallible, often producing suboptimal outputs in the first attempt. This motivates the development of Propname Propname, an innovative approach aimed at refining Propname outputs through an iterative feedback loop. Inspired by human writing refinement processes, Propname Propname leverages a single LLM for the entire feedback and improvement pipeline. Unlike existing methods, our approach circumvents the need for additional supervised training data, specialized training, or reinforcement learning. In this paper, we explore the efficacy of Propname Propname across a spectrum of 0 diverse tasks, spanning conversational contexts to intricate mathematical reasoning. Utilizing cutting edge LLMs, including Propname 0.0 and Propname 0, we present compelling evidence that Propname Propname consistently outperforms conventional one step generation, showcasing an average improvement of around 00 in task performance. Our work underscores the untapped potential for refining even the most advanced LLMs at test time with a straightforward, standalone strategy."," Large Propname Propname have exhibited remarkable capabilities in natural language processing tasks. However, their performance is not infallible, often producing suboptimal outputs in the first attempt. This motivates the development of Propname Propname, an innovative approach aimed at refining Propname outputs through an iterative feedback loop. Inspired by human writing refinement processes, Propname Propname leverages a single LLM for the entire feedback and improvement pipeline. Unlike existing methods, our approach circumvents the need for additional supervised training data, specialized training, or reinforcement learning. In this paper, we explore the efficacy of Propname Propname across a spectrum of 0 diverse tasks, spanning conversational contexts to intricate mathematical reasoning. Utilizing cutting edge LLMs, including Propname 0.0 and Propname 0, we present compelling evidence that Propname Propname consistently outperforms conventional one step generation, showcasing an average improvement of around 00 in task performance. Our work underscores the untapped potential for refining even the most advanced LLMs at test time with a straightforward, standalone strategy.", ADJ PROPN PROPN AUX VERB ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT ADV PUNCT PRON NOUN AUX PART ADJ PUNCT ADV VERB ADJ NOUN ADP DET ADJ NOUN PUNCT PRON VERB DET NOUN ADP PROPN PROPN PUNCT DET ADJ NOUN VERB ADP VERB PROPN NOUN ADP DET ADJ NOUN NOUN PUNCT VERB ADP ADJ NOUN NOUN NOUN PUNCT PROPN PROPN VERB DET ADJ NOUN ADP DET ADJ NOUN CCONJ NOUN NOUN PUNCT ADP VERB NOUN PUNCT PRON NOUN VERB DET NOUN ADP ADJ ADJ NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP PROPN PROPN ADP DET NOUN ADP NUM ADJ NOUN PUNCT VERB ADJ NOUN ADP VERB ADJ NOUN PUNCT VERB VERB NOUN NOUN PUNCT VERB PROPN NUM CCONJ PROPN NUM PUNCT PRON VERB ADJ NOUN SCONJ PROPN PROPN ADV VERB ADJ NUM NOUN NOUN PUNCT VERB DET ADJ NOUN ADP ADP NUM ADP NOUN NOUN PUNCT PRON NOUN VERB DET ADJ NOUN ADP VERB ADV DET ADV ADJ NOUN ADP NOUN NOUN ADP DET NOUN PUNCT ADJ NOUN PUNCT,0.6648351648351648,22.75,5.7032967032967035
88,2,GPT-3.5,"[' Machine learning models commonly lack the nuanced adaptability demonstrated by human cognitive processes. Humans instinctively switch between fast and slow thinking modes based on the difficulty of a task, as articulated in the dual-process theory of mind. Addressing this disparity, we present FLOWGEN, a novel graph generation model inspired by the dual-process theory. FLOWGEN incorporates both FAST (weaker) and SLOW (stronger) modules, sharing architectural similarities but diverging in parameter count and generative capabilities. The model dynamically selects the appropriate module based on the difficulty of the current graph completion step. Through empirical evaluations on real-world graphs, FLOWGEN exhibits the capability to generate graphs comparable to those produced by a single large model, with the added advantage of being up to 2x faster.']",intro_chunked," Machine learning models commonly lack the nuanced adaptability demonstrated by human cognitive processes. Humans instinctively switch between fast and slow thinking modes based on the difficulty of a task, as articulated in the dual-process theory of mind. Addressing this disparity, we present FLOWGEN, a novel graph generation model inspired by the dual-process theory. FLOWGEN incorporates both FAST (weaker) and SLOW (stronger) modules, sharing architectural similarities but diverging in parameter count and generative capabilities. The model dynamically selects the appropriate module based on the difficulty of the current graph completion step. Through empirical evaluations on real-world graphs, FLOWGEN exhibits the capability to generate graphs comparable to those produced by a single large model, with the added advantage of being up to 2x faster.",14.222896174863422,2.6406588729545177,88,0.5236122608184814," Machine learning models commonly lack the nuanced adaptability demonstrated by human cognitive processes. Humans instinctively switch between fast and slow thinking modes based on the difficulty of a task, as articulated in the dual process theory of mind. Addressing this disparity, we present Propname, a novel graph generation model inspired by the dual process theory. Propname incorporates both FAST and SLOW modules, sharing architectural similarities but diverging in parameter count and generative capabilities. The model dynamically selects the appropriate module based on the difficulty of the current graph completion step. Through empirical evaluations on real world graphs, Propname exhibits the capability to generate graphs comparable to those produced by a single large model, with the added advantage of being up to Propname faster."," Machine learning models commonly lack the nuanced adaptability demonstrated by human cognitive processes. Humans instinctively switch between fast and slow thinking modes based on the difficulty of a task, as articulated in the dual process theory of mind. Addressing this disparity, we present Propname, a novel graph generation model inspired by the dual process theory. Propname incorporates both FAST and SLOW modules, sharing architectural similarities but diverging in parameter count and generative capabilities. The model dynamically selects the appropriate module based on the difficulty of the current graph completion step. Through empirical evaluations on real world graphs, Propname exhibits the capability to generate graphs comparable to those produced by a single large model, with the added advantage of being up to Propname faster.", NOUN NOUN NOUN ADV VERB DET VERB NOUN VERB ADP ADJ ADJ NOUN PUNCT NOUN ADV VERB ADP ADJ CCONJ ADJ NOUN NOUN VERB ADP DET NOUN ADP DET NOUN PUNCT SCONJ VERB ADP DET ADJ NOUN NOUN ADP NOUN PUNCT VERB DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN NOUN VERB ADP DET ADJ NOUN NOUN PUNCT PROPN VERB CCONJ ADJ CCONJ ADJ NOUN PUNCT VERB ADJ NOUN CCONJ VERB ADP NOUN NOUN CCONJ ADJ NOUN PUNCT DET NOUN ADV VERB DET ADJ NOUN VERB ADP DET NOUN ADP DET ADJ NOUN NOUN NOUN PUNCT ADP ADJ NOUN ADP ADJ NOUN NOUN PUNCT PROPN VERB DET NOUN PART VERB NOUN ADJ ADP PRON VERB ADP DET ADJ ADJ NOUN PUNCT ADP DET VERB NOUN ADP AUX ADP ADP PROPN ADV PUNCT,0.674074074074074,22.5,5.385185185185185
89,3,GPT-3.5,"[' Training data scarcity remains a formidable challenge in low-resource language domains. In response, we propose a novel method for curating high-quality comparable training data, leveraging the capabilities of monolingual annotators. Our methodology revolves around the selection of a judicious set of images that act as linguistic pivots between source and target languages. By independently eliciting captions for these images in both languages, we establish a robust and comparable corpus. To assess the efficacy of our approach, we conduct human evaluations on the English-Hindi comparable corpora, revealing an impressive 81.1% acceptability rate for translation pairs, with a mere 2.47% identified as non-translations. Additionally, we explore the potential of the curated dataset through experiments on two downstream tasksâ€”machine translation and dictionary extraction.']",intro_chunked," Training data scarcity remains a formidable challenge in low-resource language domains. In response, we propose a novel method for curating high-quality comparable training data, leveraging the capabilities of monolingual annotators. Our methodology revolves around the selection of a judicious set of images that act as linguistic pivots between source and target languages. By independently eliciting captions for these images in both languages, we establish a robust and comparable corpus. To assess the efficacy of our approach, we conduct human evaluations on the English-Hindi comparable corpora, revealing an impressive 81.1% acceptability rate for translation pairs, with a mere 2.47% identified as non-translations. Additionally, we explore the potential of the curated dataset through experiments on two downstream tasksâ€”machine translation and dictionary extraction.",11.325060975609773,2.6406588729545177,89,0.5913411974906921," Training data scarcity remains a formidable challenge in low resource language domains. In response, we propose a novel method for curating high quality comparable training data, leveraging the capabilities of monolingual annotators. Our methodology revolves around the selection of a judicious set of images that act as linguistic pivots between source and target languages. By independently eliciting captions for these images in both languages, we establish a robust and comparable Propname. To assess the efficacy of our approach, we conduct human evaluations on the Propname Propname comparable Propname, revealing an impressive 00.0 acceptability rate for translation pairs, with a mere 0.00 identified as non translations. Additionally, we explore the potential of the curated dataset through experiments on two downstream tasksmachine translation and dictionary extraction."," Training data scarcity remains a formidable challenge in low resource language domains. In response, we propose a novel method for curating high quality comparable training data, leveraging the capabilities of monolingual annotators. Our methodology revolves around the selection of a judicious set of images that act as linguistic pivots between source and target languages. By independently eliciting captions for these images in both languages, we establish a robust and comparable Propname. To assess the efficacy of our approach, we conduct human evaluations on the Propname Propname comparable Propname, revealing an impressive 00.0 acceptability rate for translation pairs, with a mere 0.00 identified as non translations. Additionally, we explore the potential of the curated dataset through experiments on two downstream tasksmachine translation and dictionary extraction.", NOUN NOUN NOUN VERB DET ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NOUN ADP VERB ADJ NOUN ADJ NOUN NOUN PUNCT VERB DET NOUN ADP ADJ NOUN PUNCT PRON NOUN VERB ADP DET NOUN ADP DET ADJ NOUN ADP NOUN PRON VERB ADP ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT ADP ADV VERB NOUN ADP DET NOUN ADP DET NOUN PUNCT PRON VERB DET ADJ CCONJ ADJ PROPN PUNCT PART VERB DET NOUN ADP PRON NOUN PUNCT PRON VERB ADJ NOUN ADP DET PROPN PROPN ADJ PROPN PUNCT VERB DET ADJ NUM NOUN NOUN ADP NOUN NOUN PUNCT ADP DET ADJ NUM VERB ADP NOUN NOUN PUNCT ADV PUNCT PRON VERB DET NOUN ADP DET VERB NOUN ADP NOUN ADP NUM ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT,0.6861313868613139,22.833333333333332,5.532846715328467
90,4,GPT-3.5,"[' In the realm of natural language processing, this paper pioneers the task of politeness transfer – a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 1.39 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag-and-generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state-of-the-art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open-sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research.']",intro_chunked," In the realm of natural language processing, this paper pioneers the task of politeness transfer – a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 1.39 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag-and-generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state-of-the-art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open-sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research.",-4.23545454545453,2.6406588729545177,90,0.730229377746582," In the realm of natural language processing, this paper pioneers the task of politeness transfer a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 0.00 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag and generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state of the art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research."," In the realm of natural language processing, this paper pioneers the task of politeness transfer a challenge centered on transforming impolite expressions into polite ones while maintaining semantic fidelity. With the aim of fostering progress in this nascent area, we present a vast dataset meticulously annotated for politeness, exceeding 0.00 million instances. This dataset serves as a crucial resource for evaluating and comparing politeness transfer models. Our approach leverages a tag and generate pipeline, employing an innovative methodology that identifies and transforms stylistic attributes to achieve effective politeness transfer. We extend our investigation beyond politeness transfer to encompass five additional transfer tasks, consistently outperforming current state of the art methods in content preservation according to automatic metrics. Moreover, our model surpasses existing approaches in human evaluations, excelling in grammaticality, meaning preservation, and overall transfer accuracy across all six style transfer tasks. To facilitate further exploration and collaboration, we have open sourced both our code and dataset, inviting the community to contribute to the advancement of politeness transfer and related research.", ADP DET NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN VERB DET NOUN ADP NOUN NOUN DET NOUN VERB ADP VERB ADJ NOUN ADP ADJ NOUN SCONJ VERB ADJ NOUN PUNCT ADP DET NOUN ADP VERB NOUN ADP DET NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADV VERB ADP NOUN PUNCT VERB NUM NUM NOUN PUNCT DET NOUN VERB ADP DET ADJ NOUN ADP VERB CCONJ VERB NOUN NOUN NOUN PUNCT PRON NOUN VERB DET NOUN CCONJ VERB NOUN PUNCT VERB DET ADJ NOUN PRON VERB CCONJ VERB ADJ NOUN PART VERB ADJ NOUN NOUN PUNCT PRON VERB PRON NOUN ADP NOUN NOUN PART VERB NUM ADJ NOUN NOUN PUNCT ADV VERB ADJ NOUN ADP DET NOUN NOUN ADP ADJ NOUN VERB ADP ADJ NOUN PUNCT ADV PUNCT PRON NOUN VERB VERB NOUN ADP ADJ NOUN PUNCT VERB ADP NOUN PUNCT VERB NOUN PUNCT CCONJ ADJ NOUN NOUN ADP DET NUM NOUN NOUN NOUN PUNCT PART VERB ADJ NOUN CCONJ NOUN PUNCT PRON AUX ADJ VERB CCONJ PRON NOUN CCONJ NOUN PUNCT VERB DET NOUN PART VERB ADP DET NOUN ADP NOUN NOUN CCONJ ADJ NOUN PUNCT,0.6702127659574468,26.857142857142858,5.76595744680851
91,5,GPT-3.5,"[' Comprehending dynamic processes necessitates the ability to analyze events and comprehend their intricate interconnections. This paper introduces EIGEN, an innovative approach leveraging pre-trained language models for the generation of event influences. By considering context, the nature of influence, and the position in a reasoning chain, EIGEN excels in producing high-quality event influence representations. We address the current gap in research with the introduction of a dedicated dataset, providing a standardized platform for evaluating methods in the realm of event influence generation. Through rigorous evaluation against strong baselines, EIGEN emerges as a superior solution, not only in terms of automated metrics but also in human assessments of generated content, affirming its effectiveness in capturing both reference closeness and relevance.']",intro_chunked," Comprehending dynamic processes necessitates the ability to analyze events and comprehend their intricate interconnections. This paper introduces EIGEN, an innovative approach leveraging pre-trained language models for the generation of event influences. By considering context, the nature of influence, and the position in a reasoning chain, EIGEN excels in producing high-quality event influence representations. We address the current gap in research with the introduction of a dedicated dataset, providing a standardized platform for evaluating methods in the realm of event influence generation. Through rigorous evaluation against strong baselines, EIGEN emerges as a superior solution, not only in terms of automated metrics but also in human assessments of generated content, affirming its effectiveness in capturing both reference closeness and relevance.",2.9267627118643986,2.6406588729545177,91,0.713211715221405," Comprehending dynamic processes necessitates the ability to analyze events and comprehend their intricate interconnections. This paper introduces Propname, an innovative approach leveraging pre trained language models for the generation of event influences. By considering context, the nature of influence, and the position in a reasoning chain, EIGEN excels in producing high quality event influence representations. We address the current gap in research with the introduction of a dedicated dataset, providing a standardized platform for evaluating methods in the realm of event influence generation. Through rigorous evaluation against strong baselines, Propname emerges as a superior solution, not only in terms of automated metrics but also in human assessments of generated content, affirming its effectiveness in capturing both reference closeness and relevance."," Comprehending dynamic processes necessitates the ability to analyze events and comprehend their intricate interconnections. This paper introduces Propname, an innovative approach leveraging pre trained language models for the generation of event influences. By considering context, the nature of influence, and the position in a reasoning chain, EIGEN excels in producing high quality event influence representations. We address the current gap in research with the introduction of a dedicated dataset, providing a standardized platform for evaluating methods in the realm of event influence generation. Through rigorous evaluation against strong baselines, Propname emerges as a superior solution, not only in terms of automated metrics but also in human assessments of generated content, affirming its effectiveness in capturing both reference closeness and relevance.", VERB ADJ NOUN VERB DET NOUN PART VERB NOUN CCONJ VERB PRON ADJ NOUN PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN VERB ADJ VERB NOUN NOUN ADP DET NOUN ADP NOUN NOUN PUNCT ADP VERB NOUN PUNCT DET NOUN ADP NOUN PUNCT CCONJ DET NOUN ADP DET NOUN NOUN PUNCT NOUN NOUN ADP VERB ADJ NOUN NOUN NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP NOUN ADP DET NOUN ADP DET ADJ NOUN PUNCT VERB DET ADJ NOUN ADP VERB NOUN ADP DET NOUN ADP NOUN NOUN NOUN PUNCT ADP ADJ NOUN ADP ADJ NOUN PUNCT PROPN VERB ADP DET ADJ NOUN PUNCT PART ADV ADP NOUN ADP VERB NOUN CCONJ ADV ADP ADJ NOUN ADP VERB NOUN PUNCT VERB PRON NOUN ADP VERB DET NOUN NOUN CCONJ NOUN PUNCT,0.6992481203007519,26.6,5.646616541353383
92,6,GPT-3.5,"[' Convolutional Neural Networks (CNNs) have long reigned supreme in the realm of image classification, but recent breakthroughs have demonstrated the potential of transformers in this domain. This work delves into the unexplored territory of optimizing image transformers for large-scale image classification tasks. The primary focus is on understanding and enhancing the interplay between the architecture and optimization techniques of dedicated transformers. Two crucial architectural modifications are introduced, each contributing significantly to the accuracy of deep transformers. These changes not only address early saturation issues associated with increased model depth but also result in models that outperform existing state-of-the-art solutions on ImageNet. The achieved 86.5% top-1 accuracy without external data showcases the effectiveness of the proposed approach in achieving superior performance with reduced computational complexity and model parameters.']",intro_chunked," Convolutional Neural Networks (CNNs) have long reigned supreme in the realm of image classification, but recent breakthroughs have demonstrated the potential of transformers in this domain. This work delves into the unexplored territory of optimizing image transformers for large-scale image classification tasks. The primary focus is on understanding and enhancing the interplay between the architecture and optimization techniques of dedicated transformers. Two crucial architectural modifications are introduced, each contributing significantly to the accuracy of deep transformers. These changes not only address early saturation issues associated with increased model depth but also result in models that outperform existing state-of-the-art solutions on ImageNet. The achieved 86.5% top-1 accuracy without external data showcases the effectiveness of the proposed approach in achieving superior performance with reduced computational complexity and model parameters.",-2.524583333333311,2.6406588729545177,92,0.6097698211669922," Propname Propname Propname have long reigned supreme in the realm of image classification, but recent breakthroughs have demonstrated the potential of transformers in this domain. This work delves into the unexplored territory of optimizing image transformers for large scale image classification tasks. The primary focus is on understanding and enhancing the interplay between the architecture and optimization techniques of dedicated transformers. Two crucial architectural modifications are introduced, each contributing significantly to the accuracy of deep transformers. These changes not only address early saturation issues associated with increased model depth but also result in models that outperform existing state of the art solutions on Propname. The achieved 00.0 top 0 accuracy without external data showcases the effectiveness of the proposed approach in achieving superior performance with reduced computational complexity and model parameters."," Propname Propname Propname have long reigned supreme in the realm of image classification, but recent breakthroughs have demonstrated the potential of transformers in this domain. This work delves into the unexplored territory of optimizing image transformers for large scale image classification tasks. The primary focus is on understanding and enhancing the interplay between the architecture and optimization techniques of dedicated transformers. Two crucial architectural modifications are introduced, each contributing significantly to the accuracy of deep transformers. These changes not only address early saturation issues associated with increased model depth but also result in models that outperform existing state of the art solutions on Propname. The achieved 00.0 top 0 accuracy without external data showcases the effectiveness of the proposed approach in achieving superior performance with reduced computational complexity and model parameters.", PROPN PROPN PROPN AUX ADV VERB ADV ADP DET NOUN ADP NOUN NOUN PUNCT CCONJ ADJ NOUN AUX VERB DET NOUN ADP NOUN ADP DET NOUN PUNCT DET NOUN VERB ADP DET ADJ NOUN ADP VERB NOUN NOUN ADP ADJ NOUN NOUN NOUN NOUN PUNCT DET ADJ NOUN AUX ADP NOUN CCONJ VERB DET NOUN ADP DET NOUN CCONJ NOUN NOUN ADP ADJ NOUN PUNCT NUM ADJ ADJ NOUN AUX VERB PUNCT PRON VERB ADV ADP DET NOUN ADP ADJ NOUN PUNCT DET NOUN PART ADV VERB ADJ NOUN NOUN VERB ADP VERB NOUN NOUN CCONJ ADV VERB ADP NOUN PRON VERB VERB NOUN ADP DET NOUN NOUN ADP PROPN PUNCT DET VERB NUM ADJ NUM NOUN ADP ADJ NOUN VERB DET NOUN ADP DET VERB NOUN ADP VERB ADJ NOUN ADP VERB ADJ NOUN CCONJ NOUN NOUN PUNCT,0.7050359712230215,23.166666666666668,5.9784172661870505
93,7,GPT-3.5,"[' Addressing unpaired image-to-image translation tasks, such as style or class transfer, denoising, and more, requires efficient architectures that balance simplicity and effectiveness. In this work, we present a novel approach starting from a fixed-weight image autoencoder architecture. Our method introduces task-specific residual blocks operating in the latent space, employed iteratively until the target domain is reached. To counteract the exponentiation effect inherent in iterative processes, a specific training schedule is implemented. At the testing phase, our approach stands out due to its limited number of weight parameters and a compositional design allowing the modulation of transformation strength based on the number of iterations. This proves particularly advantageous when the characteristics of the noise to be suppressed are uncertain. We provide experimental evidence supporting the efficacy of our method, demonstrating its utility for various transformations, and showcasing comparable or superior performance to CycleGAN while utilizing significantly fewer parameters.']",intro_chunked," Addressing unpaired image-to-image translation tasks, such as style or class transfer, denoising, and more, requires efficient architectures that balance simplicity and effectiveness. In this work, we present a novel approach starting from a fixed-weight image autoencoder architecture. Our method introduces task-specific residual blocks operating in the latent space, employed iteratively until the target domain is reached. To counteract the exponentiation effect inherent in iterative processes, a specific training schedule is implemented. At the testing phase, our approach stands out due to its limited number of weight parameters and a compositional design allowing the modulation of transformation strength based on the number of iterations. This proves particularly advantageous when the characteristics of the noise to be suppressed are uncertain. We provide experimental evidence supporting the efficacy of our method, demonstrating its utility for various transformations, and showcasing comparable or superior performance to CycleGAN while utilizing significantly fewer parameters.",0.2057142857142935,2.6406588729545177,93,0.4251134991645813," Addressing unpaired image to image translation tasks, such as style or class transfer, denoising, and more, requires efficient architectures that balance simplicity and effectiveness. In this work, we present a novel approach starting from a fixed weight image autoencoder architecture. Our method introduces task specific residual blocks operating in the latent space, employed iteratively until the target domain is reached. To counteract the exponentiation effect inherent in iterative processes, a specific training schedule is implemented. At the testing phase, our approach stands out due to its limited number of weight parameters and a compositional design allowing the modulation of transformation strength based on the number of iterations. This proves particularly advantageous when the characteristics of the noise to be suppressed are uncertain. We provide experimental evidence supporting the efficacy of our method, demonstrating its utility for various transformations, and showcasing comparable or superior performance to Propname while utilizing significantly fewer parameters."," Addressing unpaired image to image translation tasks, such as style or class transfer, denoising, and more, requires efficient architectures that balance simplicity and effectiveness. In this work, we present a novel approach starting from a fixed weight image autoencoder architecture. Our method introduces task specific residual blocks operating in the latent space, employed iteratively until the target domain is reached. To counteract the exponentiation effect inherent in iterative processes, a specific training schedule is implemented. At the testing phase, our approach stands out due to its limited number of weight parameters and a compositional design allowing the modulation of transformation strength based on the number of iterations. This proves particularly advantageous when the characteristics of the noise to be suppressed are uncertain. We provide experimental evidence supporting the efficacy of our method, demonstrating its utility for various transformations, and showcasing comparable or superior performance to Propname while utilizing significantly fewer parameters.", VERB ADJ NOUN ADP NOUN NOUN NOUN PUNCT ADJ ADP NOUN CCONJ NOUN NOUN PUNCT NOUN PUNCT CCONJ ADJ PUNCT VERB ADJ NOUN SCONJ NOUN NOUN CCONJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN VERB ADP DET VERB NOUN NOUN NOUN NOUN PUNCT PRON NOUN VERB NOUN ADJ ADJ NOUN VERB ADP DET ADJ NOUN PUNCT VERB ADV SCONJ DET NOUN NOUN AUX VERB PUNCT PART VERB DET NOUN NOUN ADJ ADP ADJ NOUN PUNCT DET ADJ NOUN NOUN AUX VERB PUNCT ADP DET NOUN NOUN PUNCT PRON NOUN VERB ADP ADP ADP PRON ADJ NOUN ADP NOUN NOUN CCONJ DET ADJ NOUN VERB DET NOUN ADP NOUN NOUN VERB ADP DET NOUN ADP NOUN PUNCT PRON VERB ADV ADJ SCONJ DET NOUN ADP DET NOUN PART AUX VERB AUX ADJ PUNCT PRON VERB ADJ NOUN VERB DET NOUN ADP PRON NOUN PUNCT VERB PRON NOUN ADP ADJ NOUN PUNCT CCONJ VERB ADJ CCONJ ADJ NOUN ADP PROPN SCONJ VERB ADV ADJ NOUN PUNCT,0.7083333333333334,24.0,5.625
94,8,GPT-3.5,"[' In response to the growing demand for sophisticated language models, we present Llama 2, a significant advancement in the landscape of pretrained and fine-tuned large language models (LLMs). Llama 2 encompasses a diverse range of models, spanning from 7 billion to 70 billion parameters. Notably, our focus lies on Llama 2-Chat, a set of fine-tuned models tailored for dialogue-based applications. Through rigorous benchmark evaluations, we establish the superior performance of our models compared to existing open-source chat models. Human evaluations, particularly in terms of helpfulness and safety, suggest the potential suitability of Llama 2-Chat as a substitute for closed-source alternatives. This paper outlines our approach to fine-tuning and safety enhancements, aiming to facilitate knowledge-sharing within the community and foster responsible advancements in LLM development.']",intro_chunked," In response to the growing demand for sophisticated language models, we present Llama 2, a significant advancement in the landscape of pretrained and fine-tuned large language models (LLMs). Llama 2 encompasses a diverse range of models, spanning from 7 billion to 70 billion parameters. Notably, our focus lies on Llama 2-Chat, a set of fine-tuned models tailored for dialogue-based applications. Through rigorous benchmark evaluations, we establish the superior performance of our models compared to existing open-source chat models. Human evaluations, particularly in terms of helpfulness and safety, suggest the potential suitability of Llama 2-Chat as a substitute for closed-source alternatives. This paper outlines our approach to fine-tuning and safety enhancements, aiming to facilitate knowledge-sharing within the community and foster responsible advancements in LLM development.",12.291428571428611,2.6406588729545177,94,0.7379023432731628," In response to the growing demand for sophisticated language models, we present Propname 0, a significant advancement in the landscape of pretrained and fine tuned large language models. Propname 0 encompasses a diverse range of models, spanning from 0 billion to 00 billion parameters. Notably, our focus lies on Propname 0 Propname, a set of fine tuned models tailored for dialogue based applications. Through rigorous benchmark evaluations, we establish the superior performance of our models compared to existing open source chat models. Human evaluations, particularly in terms of helpfulness and safety, suggest the potential suitability of Propname 0 Chat as a substitute for closed source alternatives. This paper outlines our approach to fine tuning and safety enhancements, aiming to facilitate knowledge sharing within the community and foster responsible advancements in Propname development."," In response to the growing demand for sophisticated language models, we present Propname 0, a significant advancement in the landscape of pretrained and fine tuned large language models. Propname 0 encompasses a diverse range of models, spanning from 0 billion to 00 billion parameters. Notably, our focus lies on Propname 0 Propname, a set of fine tuned models tailored for dialogue based applications. Through rigorous benchmark evaluations, we establish the superior performance of our models compared to existing open source chat models. Human evaluations, particularly in terms of helpfulness and safety, suggest the potential suitability of Propname 0 Chat as a substitute for closed source alternatives. This paper outlines our approach to fine tuning and safety enhancements, aiming to facilitate knowledge sharing within the community and foster responsible advancements in Propname development.", ADP NOUN ADP DET VERB NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB PROPN NUM PUNCT DET ADJ NOUN ADP DET NOUN ADP VERB CCONJ ADJ VERB ADJ NOUN NOUN PUNCT PROPN NUM VERB DET ADJ NOUN ADP NOUN PUNCT VERB ADP NUM NUM PART NUM NUM NOUN PUNCT ADV PUNCT PRON NOUN VERB ADP PROPN NUM PROPN PUNCT DET NOUN ADP ADJ VERB NOUN VERB ADP NOUN VERB NOUN PUNCT ADP ADJ NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP PRON NOUN VERB ADP VERB ADJ NOUN NOUN NOUN PUNCT ADJ NOUN PUNCT ADV ADP NOUN ADP NOUN CCONJ NOUN PUNCT VERB DET ADJ NOUN ADP PROPN NUM NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN VERB PRON NOUN ADP ADJ NOUN CCONJ NOUN NOUN PUNCT VERB PART VERB NOUN NOUN ADP DET NOUN CCONJ VERB ADJ NOUN ADP PROPN NOUN PUNCT,0.5850340136054422,24.5,5.258503401360544
95,9,GPT-3.5,"["" The growing availability of extensive neuroimaging databases has opened up remarkable opportunities for advancing machine learning algorithms. However, the inherent variability in these databases, arising from factors such as age, gender, and acquisition parameters, poses significant challenges to classification methods. These nuisance variables not only affect performance but can also introduce complexities in interpreting the model's behavior. This paper addresses the crucial issue of handling data from different databases. Initial experiments, conducted on simulated data, illustrate the potential pitfalls associated with interactions between data and confounding variables, particularly age. Subsequently, we delve into a comparison of three strategies for data adjustment, with a specific focus on their application in a real-world scenario involving a Computer-Aided Diagnosis (CAD) system. This system aims to distinguish between healthy and Alzheimer's Disease (AD) subjects, utilizing volumetric characteristics derived from structural MRI.""]",intro_chunked," The growing availability of extensive neuroimaging databases has opened up remarkable opportunities for advancing machine learning algorithms. However, the inherent variability in these databases, arising from factors such as age, gender, and acquisition parameters, poses significant challenges to classification methods. These nuisance variables not only affect performance but can also introduce complexities in interpreting the model's behavior. This paper addresses the crucial issue of handling data from different databases. Initial experiments, conducted on simulated data, illustrate the potential pitfalls associated with interactions between data and confounding variables, particularly age. Subsequently, we delve into a comparison of three strategies for data adjustment, with a specific focus on their application in a real-world scenario involving a Computer-Aided Diagnosis (CAD) system. This system aims to distinguish between healthy and Alzheimer's Disease (AD) subjects, utilizing volumetric characteristics derived from structural MRI.",-3.225620437956195,2.6406588729545177,95,0.44264212250709534," The growing availability of extensive neuroimaging databases has opened up remarkable opportunities for advancing machine learning algorithms. However, the inherent variability in these databases, arising from factors such as age, gender, and acquisition parameters, poses significant challenges to classification methods. These nuisance variables not only affect performance but can also introduce complexities in interpreting the models behavior. This paper addresses the crucial issue of handling data from different databases. Initial experiments, conducted on simulated data, illustrate the potential pitfalls associated with interactions between data and confounding variables, particularly age. Subsequently, we delve into a comparison of three strategies for data adjustment, with a specific focus on their application in a real world scenario involving a Propname Propname Propname system. This system aims to distinguish between healthy and Propname Propname subjects, utilizing volumetric characteristics derived from structural Propname."," The growing availability of extensive neuroimaging databases has opened up remarkable opportunities for advancing machine learning algorithms. However, the inherent variability in these databases, arising from factors such as age, gender, and acquisition parameters, poses significant challenges to classification methods. These nuisance variables not only affect performance but can also introduce complexities in interpreting the models behavior. This paper addresses the crucial issue of handling data from different databases. Initial experiments, conducted on simulated data, illustrate the potential pitfalls associated with interactions between data and confounding variables, particularly age. Subsequently, we delve into a comparison of three strategies for data adjustment, with a specific focus on their application in a real world scenario involving a Propname Propname Propname system. This system aims to distinguish between healthy and Propname Propname subjects, utilizing volumetric characteristics derived from structural Propname.", DET VERB NOUN ADP ADJ NOUN NOUN AUX VERB ADP ADJ NOUN ADP VERB NOUN VERB NOUN PUNCT ADV PUNCT DET ADJ NOUN ADP DET NOUN PUNCT VERB ADP NOUN ADJ ADP NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN PUNCT VERB ADJ NOUN ADP NOUN NOUN PUNCT DET ADJ NOUN PART ADV VERB NOUN CCONJ AUX ADV VERB NOUN ADP VERB DET NOUN NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP VERB NOUN ADP ADJ NOUN PUNCT ADJ NOUN PUNCT VERB ADP VERB NOUN PUNCT VERB DET ADJ NOUN VERB ADP NOUN ADP NOUN CCONJ VERB NOUN PUNCT ADV NOUN PUNCT ADV PUNCT PRON VERB ADP DET NOUN ADP NUM NOUN ADP NOUN NOUN PUNCT ADP DET ADJ NOUN ADP PRON NOUN ADP DET ADJ NOUN NOUN VERB DET PROPN PROPN PROPN NOUN PUNCT DET NOUN VERB PART VERB ADP ADJ CCONJ PROPN PROPN NOUN PUNCT VERB NOUN NOUN VERB ADP ADJ PROPN PUNCT,0.6838709677419355,22.142857142857142,5.890322580645162
96,10,GPT-3.5,"["" The ubiquity of machine learning, particularly deep learning, has ushered in a transformative era across diverse societal realms. Noteworthy applications in natural language processing and computer vision have demonstrated the prowess of emblematic architectures like AlexNet, ResNet, and GPT. While these architectures have rightfully garnered attention, the complementary role of well-designed optimization procedures remains a critical, albeit often overlooked, component of their success. This thesis embarks on an exploration of the intricate dynamics between architectures and training procedures, with a specific focus on the application of Transformer architectures to visual understanding. Unlike their convolutional counterparts, transformers' training procedures are in a nascent stage, prompting an imperative need for refinement. The pivotal role of training in mitigating the architectural constraints of transformers serves as the guiding principle for our investigation. The research journey unfolds by delving into the potential of learning with coarse labels, introducing a modification to the training procedure. Subsequent chapters undertake a comprehensive study of various computer vision architectures, dissecting their attributes, advantages, drawbacks, and the nuances of effective training methodologies. This exploration ultimately leads to an analysis of the symbiotic relationship between architecture and training processes.""]",intro_chunked," The ubiquity of machine learning, particularly deep learning, has ushered in a transformative era across diverse societal realms. Noteworthy applications in natural language processing and computer vision have demonstrated the prowess of emblematic architectures like AlexNet, ResNet, and GPT. While these architectures have rightfully garnered attention, the complementary role of well-designed optimization procedures remains a critical, albeit often overlooked, component of their success. This thesis embarks on an exploration of the intricate dynamics between architectures and training procedures, with a specific focus on the application of Transformer architectures to visual understanding. Unlike their convolutional counterparts, transformers' training procedures are in a nascent stage, prompting an imperative need for refinement. The pivotal role of training in mitigating the architectural constraints of transformers serves as the guiding principle for our investigation. The research journey unfolds by delving into the potential of learning with coarse labels, introducing a modification to the training procedure. Subsequent chapters undertake a comprehensive study of various computer vision architectures, dissecting their attributes, advantages, drawbacks, and the nuances of effective training methodologies. This exploration ultimately leads to an analysis of the symbiotic relationship between architecture and training processes.",0.6533333333333644,2.6406588729545177,96,0.6002388596534729," The ubiquity of machine learning, particularly deep learning, has ushered in a transformative era across diverse societal realms. Noteworthy applications in natural language processing and computer vision have demonstrated the prowess of emblematic architectures like Propname, ResNet, and Propname. While these architectures have rightfully garnered attention, the complementary role of well designed optimization procedures remains a critical, albeit often overlooked, component of their success. This thesis embarks on an exploration of the intricate dynamics between architectures and training procedures, with a specific focus on the application of Propname architectures to visual understanding. Unlike their convolutional counterparts, transformers training procedures are in a nascent stage, prompting an imperative need for refinement. The pivotal role of training in mitigating the architectural constraints of transformers serves as the guiding principle for our investigation. The research journey unfolds by delving into the potential of learning with coarse labels, introducing a modification to the training procedure. Subsequent chapters undertake a comprehensive study of various computer vision architectures, dissecting their attributes, advantages, drawbacks, and the nuances of effective training methodologies. This exploration ultimately leads to an analysis of the symbiotic relationship between architecture and training processes."," The ubiquity of machine learning, particularly deep learning, has ushered in a transformative era across diverse societal realms. Noteworthy applications in natural language processing and computer vision have demonstrated the prowess of emblematic architectures like Propname, ResNet, and Propname. While these architectures have rightfully garnered attention, the complementary role of well designed optimization procedures remains a critical, albeit often overlooked, component of their success. This thesis embarks on an exploration of the intricate dynamics between architectures and training procedures, with a specific focus on the application of Propname architectures to visual understanding. Unlike their convolutional counterparts, transformers training procedures are in a nascent stage, prompting an imperative need for refinement. The pivotal role of training in mitigating the architectural constraints of transformers serves as the guiding principle for our investigation. The research journey unfolds by delving into the potential of learning with coarse labels, introducing a modification to the training procedure. Subsequent chapters undertake a comprehensive study of various computer vision architectures, dissecting their attributes, advantages, drawbacks, and the nuances of effective training methodologies. This exploration ultimately leads to an analysis of the symbiotic relationship between architecture and training processes.", DET NOUN ADP NOUN NOUN PUNCT ADV ADJ NOUN PUNCT AUX VERB ADP DET ADJ NOUN ADP ADJ ADJ NOUN PUNCT ADJ NOUN ADP ADJ NOUN NOUN CCONJ NOUN NOUN AUX VERB DET NOUN ADP ADJ NOUN ADP PROPN PUNCT NOUN PUNCT CCONJ PROPN PUNCT SCONJ DET NOUN AUX ADV VERB NOUN PUNCT DET ADJ NOUN ADP ADV VERB NOUN NOUN VERB DET ADJ PUNCT ADV ADV VERB PUNCT NOUN ADP PRON NOUN PUNCT DET NOUN NOUN ADP DET NOUN ADP DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT ADP DET ADJ NOUN ADP DET NOUN ADP PROPN VERB ADP ADJ NOUN PUNCT ADP PRON ADJ NOUN PUNCT NOUN NOUN NOUN AUX ADP DET NOUN NOUN PUNCT VERB DET ADJ NOUN ADP NOUN PUNCT DET ADJ NOUN ADP NOUN ADP VERB DET ADJ NOUN ADP NOUN VERB ADP DET VERB NOUN ADP PRON NOUN PUNCT DET NOUN NOUN VERB ADP VERB ADP DET NOUN ADP VERB ADP ADJ NOUN PUNCT VERB DET NOUN ADP DET NOUN NOUN PUNCT ADJ NOUN VERB DET ADJ NOUN ADP ADJ NOUN NOUN VERB PUNCT VERB PRON NOUN PUNCT NOUN PUNCT NOUN PUNCT CCONJ DET NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN ADV VERB ADP DET NOUN ADP DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT,0.5887850467289719,23.77777777777778,5.920560747663552
97,11,GPT-3.5,"[' Traditional convolutional neural networks (CNNs) have been the cornerstone of image classification architectures, but this paper proposes a paradigm shift with ResMLP. Built solely upon multi-layer perceptrons, ResMLP introduces a straightforward yet highly effective residual network. The architecture alternates between two key components: a linear layer facilitating interaction among image patches across channels, and a two-layer feed-forward network promoting independent channel interaction per patch. This design choice, coupled with contemporary training strategies encompassing robust data augmentation and optional distillation, yields unexpectedly favorable results on the ImageNet dataset. Moreover, this paper extends the application of ResMLP to self-supervised learning, showcasing its ability to learn meaningful representations without explicit supervision. By reducing reliance on labeled datasets, ResMLP demonstrates versatility and robustness across diverse training scenarios. The paper also explores the adaptability of ResMLP to machine translation, achieving surprisingly competitive results in this domain.']",intro_chunked," Traditional convolutional neural networks (CNNs) have been the cornerstone of image classification architectures, but this paper proposes a paradigm shift with ResMLP. Built solely upon multi-layer perceptrons, ResMLP introduces a straightforward yet highly effective residual network. The architecture alternates between two key components: a linear layer facilitating interaction among image patches across channels, and a two-layer feed-forward network promoting independent channel interaction per patch. This design choice, coupled with contemporary training strategies encompassing robust data augmentation and optional distillation, yields unexpectedly favorable results on the ImageNet dataset. Moreover, this paper extends the application of ResMLP to self-supervised learning, showcasing its ability to learn meaningful representations without explicit supervision. By reducing reliance on labeled datasets, ResMLP demonstrates versatility and robustness across diverse training scenarios. The paper also explores the adaptability of ResMLP to machine translation, achieving surprisingly competitive results in this domain.",-6.2099999999999795,2.6406588729545177,97,0.8137733936309814," Traditional convolutional neural networks have been the cornerstone of image classification architectures, but this paper proposes a paradigm shift with Propname. Built solely upon multi layer perceptrons, Propname introduces a straightforward yet highly effective residual network. The architecture alternates between two key components: a linear layer facilitating interaction among image patches across channels, and a two layer feed forward network promoting independent channel interaction per patch. This design choice, coupled with contemporary training strategies encompassing robust data augmentation and optional distillation, yields unexpectedly favorable results on the Propname dataset. Moreover, this paper extends the application of Propname to self supervised learning, showcasing its ability to learn meaningful representations without explicit supervision. By reducing reliance on labeled datasets, Propname demonstrates versatility and robustness across diverse training scenarios. The paper also explores the adaptability of Propname to machine translation, achieving surprisingly competitive results in this domain."," Traditional convolutional neural networks have been the cornerstone of image classification architectures, but this paper proposes a paradigm shift with Propname. Built solely upon multi layer perceptrons, Propname introduces a straightforward yet highly effective residual network. The architecture alternates between two key components: a linear layer facilitating interaction among image patches across channels, and a two layer feed forward network promoting independent channel interaction per patch. This design choice, coupled with contemporary training strategies encompassing robust data augmentation and optional distillation, yields unexpectedly favorable results on the Propname dataset. Moreover, this paper extends the application of Propname to self supervised learning, showcasing its ability to learn meaningful representations without explicit supervision. By reducing reliance on labeled datasets, Propname demonstrates versatility and robustness across diverse training scenarios. The paper also explores the adaptability of Propname to machine translation, achieving surprisingly competitive results in this domain.", ADJ ADJ ADJ NOUN AUX AUX DET NOUN ADP NOUN NOUN NOUN PUNCT CCONJ DET NOUN VERB DET NOUN NOUN ADP PROPN PUNCT VERB ADV SCONJ ADJ NOUN NOUN PUNCT PROPN VERB DET ADJ CCONJ ADV ADJ ADJ NOUN PUNCT DET NOUN NOUN ADP NUM ADJ NOUN PUNCT DET ADJ NOUN VERB NOUN ADP NOUN NOUN ADP NOUN PUNCT CCONJ DET NUM NOUN NOUN ADV NOUN VERB ADJ NOUN NOUN ADP NOUN PUNCT DET NOUN NOUN PUNCT VERB ADP ADJ NOUN NOUN VERB ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT VERB ADV ADJ NOUN ADP DET PROPN NOUN PUNCT ADV PUNCT DET NOUN VERB DET NOUN ADP PROPN PART VERB ADJ NOUN PUNCT VERB PRON NOUN PART VERB ADJ NOUN ADP ADJ NOUN PUNCT ADP VERB NOUN ADP VERB NOUN PUNCT PROPN VERB NOUN CCONJ NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN ADV VERB DET NOUN ADP PROPN PART VERB NOUN PUNCT VERB ADV ADJ NOUN ADP DET NOUN PUNCT,0.7080745341614907,23.0,6.167701863354037
98,12,GPT-3.5,"[' The dynamic landscape of Knowledge Graph Completion (KGC) has witnessed a proliferation of advanced techniques, each vying for superiority in predicting missing links within expansive knowledge graphs. Notably, recent publications have showcased seemingly exceptional performance, surpassing established state-of-the-art methods. However, a critical examination reveals that the reported advancements may be linked to the adoption of inappropriate evaluation protocols. In this paper, we address this issue by proposing a simple yet robust evaluation protocol. This protocol aims to rectify the inadequacies of existing evaluation methodologies and, crucially, is designed to handle model bias effectively. We embark on extensive experiments to assess the performance of several existing KGC methods using our proposed protocol. The openness of our reproducible code further enhances the transparency and replicability of our findings.']",intro_chunked," The dynamic landscape of Knowledge Graph Completion (KGC) has witnessed a proliferation of advanced techniques, each vying for superiority in predicting missing links within expansive knowledge graphs. Notably, recent publications have showcased seemingly exceptional performance, surpassing established state-of-the-art methods. However, a critical examination reveals that the reported advancements may be linked to the adoption of inappropriate evaluation protocols. In this paper, we address this issue by proposing a simple yet robust evaluation protocol. This protocol aims to rectify the inadequacies of existing evaluation methodologies and, crucially, is designed to handle model bias effectively. We embark on extensive experiments to assess the performance of several existing KGC methods using our proposed protocol. The openness of our reproducible code further enhances the transparency and replicability of our findings.",1.9078571428571536,2.6406588729545177,98,0.40003713965415955," The dynamic landscape of Propname Propname Propname has witnessed a proliferation of advanced techniques, each vying for superiority in predicting missing links within expansive knowledge graphs. Notably, recent publications have showcased seemingly exceptional performance, surpassing established state of the art methods. However, a critical examination reveals that the reported advancements may be linked to the adoption of inappropriate evaluation protocols. In this paper, we address this issue by proposing a simple yet robust evaluation protocol. This protocol aims to rectify the inadequacies of existing evaluation methodologies and, crucially, is designed to handle model bias effectively. We embark on extensive experiments to assess the performance of several existing Propname methods using our proposed protocol. The openness of our reproducible code further enhances the transparency and replicability of our findings."," The dynamic landscape of Propname Propname Propname has witnessed a proliferation of advanced techniques, each vying for superiority in predicting missing links within expansive knowledge graphs. Notably, recent publications have showcased seemingly exceptional performance, surpassing established state of the art methods. However, a critical examination reveals that the reported advancements may be linked to the adoption of inappropriate evaluation protocols. In this paper, we address this issue by proposing a simple yet robust evaluation protocol. This protocol aims to rectify the inadequacies of existing evaluation methodologies and, crucially, is designed to handle model bias effectively. We embark on extensive experiments to assess the performance of several existing Propname methods using our proposed protocol. The openness of our reproducible code further enhances the transparency and replicability of our findings.", DET ADJ NOUN ADP PROPN PROPN PROPN AUX VERB DET NOUN ADP ADJ NOUN PUNCT PRON VERB ADP NOUN ADP VERB VERB NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT ADJ NOUN AUX VERB ADV ADJ NOUN PUNCT VERB VERB NOUN ADP DET NOUN NOUN PUNCT ADV PUNCT DET ADJ NOUN VERB SCONJ DET VERB NOUN AUX AUX VERB ADP DET NOUN ADP ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP VERB DET ADJ CCONJ ADJ NOUN NOUN PUNCT DET NOUN VERB PART VERB DET NOUN ADP VERB NOUN NOUN CCONJ PUNCT ADV PUNCT AUX VERB PART VERB NOUN NOUN ADV PUNCT PRON VERB ADP ADJ NOUN PART VERB DET NOUN ADP ADJ VERB PROPN NOUN VERB PRON VERB NOUN PUNCT DET NOUN ADP PRON ADJ NOUN ADV VERB DET NOUN CCONJ NOUN ADP PRON NOUN PUNCT,0.6901408450704225,20.285714285714285,5.683098591549296
99,13,GPT-3.5,"[' Autoregressive sequence models have demonstrated state-of-the-art performance in diverse domains such as machine translation. Despite their prowess, these models suffer from substantial inference latency due to their autoregressive factorization nature. Non-autoregressive sequence models have been introduced as a solution to reduce inference time, but their assumption of conditional independence during token decoding often leads to output inconsistencies and inferior accuracy compared to autoregressive counterparts. This paper proposes a novel approach to enhance decoding consistency and reduce inference cost simultaneously by integrating a structured inference module into non-autoregressive models. We introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models and incorporate a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation demonstrate that our model achieves significantly improved translation performance compared to previous non-autoregressive models, with minimal additional latency (8âˆ¼14ms). Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and closely approaches the accuracy of purely autoregressive models, trailing by only 0.61 in BLEU.']",intro_chunked," Autoregressive sequence models have demonstrated state-of-the-art performance in diverse domains such as machine translation. Despite their prowess, these models suffer from substantial inference latency due to their autoregressive factorization nature. Non-autoregressive sequence models have been introduced as a solution to reduce inference time, but their assumption of conditional independence during token decoding often leads to output inconsistencies and inferior accuracy compared to autoregressive counterparts. This paper proposes a novel approach to enhance decoding consistency and reduce inference cost simultaneously by integrating a structured inference module into non-autoregressive models. We introduce an efficient approximation for Conditional Random Fields (CRF) tailored for non-autoregressive sequence models and incorporate a dynamic transition technique to capture positional contexts within the CRF. Experimental results in machine translation demonstrate that our model achieves significantly improved translation performance compared to previous non-autoregressive models, with minimal additional latency (8âˆ¼14ms). Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and closely approaches the accuracy of purely autoregressive models, trailing by only 0.61 in BLEU.",-11.896842105263119,2.6406588729545177,99,0.40233734250068665," Autoregressive sequence models have demonstrated state of the art performance in diverse domains such as machine translation. Despite their prowess, these models suffer from substantial inference latency due to their autoregressive factorization nature. Non autoregressive sequence models have been introduced as a solution to reduce inference time, but their assumption of conditional independence during Propname decoding often leads to output inconsistencies and inferior accuracy compared to autoregressive counterparts. This paper proposes a novel approach to enhance decoding consistency and reduce inference cost simultaneously by integrating a structured inference module into non autoregressive models. We introduce an efficient approximation for Propname Propname Propname tailored for non autoregressive sequence models and incorporate a dynamic transition technique to capture positional contexts within the Propname. Experimental results in machine translation demonstrate that our model achieves significantly improved translation performance compared to previous non autoregressive models, with minimal additional latency. Notably, on the Propname Propname Propname dataset, our model outperforms prior non autoregressive baselines and closely approaches the accuracy of purely autoregressive models, trailing by only 0.00 in Propname."," Autoregressive sequence models have demonstrated state of the art performance in diverse domains such as machine translation. Despite their prowess, these models suffer from substantial inference latency due to their autoregressive factorization nature. Non autoregressive sequence models have been introduced as a solution to reduce inference time, but their assumption of conditional independence during Propname decoding often leads to output inconsistencies and inferior accuracy compared to autoregressive counterparts. This paper proposes a novel approach to enhance decoding consistency and reduce inference cost simultaneously by integrating a structured inference module into non autoregressive models. We introduce an efficient approximation for Propname Propname Propname tailored for non autoregressive sequence models and incorporate a dynamic transition technique to capture positional contexts within the Propname. Experimental results in machine translation demonstrate that our model achieves significantly improved translation performance compared to previous non autoregressive models, with minimal additional latency. Notably, on the Propname Propname Propname dataset, our model outperforms prior non autoregressive baselines and closely approaches the accuracy of purely autoregressive models, trailing by only 0.00 in Propname.", ADJ NOUN NOUN AUX VERB NOUN ADP DET NOUN NOUN ADP ADJ NOUN ADJ ADP NOUN NOUN PUNCT SCONJ PRON NOUN PUNCT DET NOUN VERB ADP ADJ NOUN NOUN ADP ADP PRON ADJ NOUN NOUN PUNCT ADJ ADJ NOUN NOUN AUX AUX VERB ADP DET NOUN PART VERB NOUN NOUN PUNCT CCONJ PRON NOUN ADP ADJ NOUN ADP PROPN VERB ADV VERB AUX VERB NOUN CCONJ ADJ NOUN VERB ADP ADJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN PART VERB VERB NOUN CCONJ VERB NOUN NOUN ADV ADP VERB DET ADJ NOUN NOUN ADP ADJ ADJ NOUN PUNCT PRON VERB DET ADJ NOUN ADP PROPN PROPN PROPN VERB ADP ADJ ADJ NOUN NOUN CCONJ VERB DET ADJ NOUN NOUN PART VERB ADJ NOUN ADP DET PROPN PUNCT ADJ NOUN ADP NOUN NOUN VERB SCONJ PRON NOUN VERB ADV VERB NOUN NOUN VERB ADP ADJ ADJ ADJ NOUN PUNCT ADP ADJ ADJ NOUN PUNCT ADV PUNCT ADP DET PROPN PROPN PROPN NOUN PUNCT PRON NOUN VERB ADJ ADJ ADJ NOUN CCONJ ADV VERB DET NOUN ADP ADV ADJ NOUN PUNCT VERB ADP ADV NUM ADP PROPN PUNCT,0.5935828877005348,26.714285714285715,6.294117647058823
100,14,GPT-3.5,"[' Supervised Fine-Tuning (SFT) in conjunction with Reinforcement Learning from Human Feedback (RLHF) has proven effective for aligning AI agents based on large language models (LLMs). However, the necessity for high-quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in-distribution preferences is non-trivial. This paper presents SALMON (Self-ALignMent with principlefOllowiNg reward models), a pioneering approach to aligning base language models with minimal human supervision. Central to SALMON is a principle-following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human-defined principles. By adjusting these principles during RL training, we achieve precise control over preferences, influencing the behavior of RL-trained policies, and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method results in the creation of Dromedary-2, an AI assistant outperforming state-of-the-art systems with only 6 exemplars for in-context learning and 31 human-defined principles.']",intro_chunked," Supervised Fine-Tuning (SFT) in conjunction with Reinforcement Learning from Human Feedback (RLHF) has proven effective for aligning AI agents based on large language models (LLMs). However, the necessity for high-quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in-distribution preferences is non-trivial. This paper presents SALMON (Self-ALignMent with principlefOllowiNg reward models), a pioneering approach to aligning base language models with minimal human supervision. Central to SALMON is a principle-following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human-defined principles. By adjusting these principles during RL training, we achieve precise control over preferences, influencing the behavior of RL-trained policies, and eliminating the need for online human preferences. Applied to the LLaMA-2-70b base language model, our method results in the creation of Dromedary-2, an AI assistant outperforming state-of-the-art systems with only 6 exemplars for in-context learning and 31 human-defined principles.",2.681263440860249,2.6406588729545177,100,0.5361313223838806," Supervised Fine Tuning in conjunction with Propname Propname from Propname Propname has proven effective for aligning Propname agents based on large language models. However, the necessity for high quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in distribution preferences is non trivial. This paper presents Propname, a pioneering approach to aligning base language models with minimal human supervision. Central to Propname is a principle following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human defined principles. By adjusting these principles during Propname training, we achieve precise control over preferences, influencing the behavior of Propname trained policies, and eliminating the need for online human preferences. Applied to the Propname 0 00b base language model, our method results in the creation of Propname 0, an Propname assistant outperforming state of the art systems with only 0 exemplars for in context learning and 00 human defined principles."," Supervised Fine Tuning in conjunction with Propname Propname from Propname Propname has proven effective for aligning Propname agents based on large language models. However, the necessity for high quality human annotations poses a challenge, particularly for intricate tasks where obtaining consistent response demonstrations and in distribution preferences is non trivial. This paper presents Propname, a pioneering approach to aligning base language models with minimal human supervision. Central to Propname is a principle following reward model trained on synthetic preference data, capable of generating reward scores based on arbitrary human defined principles. By adjusting these principles during Propname training, we achieve precise control over preferences, influencing the behavior of Propname trained policies, and eliminating the need for online human preferences. Applied to the Propname 0 00b base language model, our method results in the creation of Propname 0, an Propname assistant outperforming state of the art systems with only 0 exemplars for in context learning and 00 human defined principles.", ADJ ADJ NOUN ADP NOUN ADP PROPN PROPN ADP PROPN PROPN AUX VERB ADJ ADP VERB PROPN NOUN VERB ADP ADJ NOUN NOUN PUNCT ADV PUNCT DET NOUN ADP ADJ NOUN ADJ NOUN VERB DET NOUN PUNCT ADV ADP ADJ NOUN SCONJ VERB ADJ NOUN NOUN CCONJ ADP NOUN NOUN AUX ADV ADJ PUNCT DET NOUN VERB PROPN PUNCT DET VERB NOUN ADP VERB NOUN NOUN NOUN ADP ADJ ADJ NOUN PUNCT ADJ ADP PROPN AUX DET ADJ VERB NOUN NOUN VERB ADP ADJ NOUN NOUN PUNCT ADJ ADP VERB NOUN NOUN VERB ADP ADJ ADJ VERB NOUN PUNCT ADP VERB DET NOUN ADP PROPN NOUN PUNCT PRON VERB ADJ NOUN ADP NOUN PUNCT VERB DET NOUN ADP PROPN VERB NOUN PUNCT CCONJ VERB DET NOUN ADP ADJ ADJ NOUN PUNCT VERB ADP DET PROPN NUM NOUN NOUN NOUN NOUN PUNCT PRON NOUN VERB ADP DET NOUN ADP PROPN NUM PUNCT DET PROPN NOUN VERB NOUN ADP DET NOUN NOUN ADP ADV NUM NOUN ADP ADP NOUN NOUN CCONJ NUM ADJ VERB NOUN PUNCT,0.5977011494252874,29.0,5.528735632183908
101,15,GPT-3.5,"["" Learning effective representations of entities and relations in knowledge graphs is pivotal for predicting missing links. The success of this task crucially depends on the model's ability to capture and infer intricate patterns within relations. In response, we present RotatE, a novel knowledge graph embedding approach specifically designed to model and infer a range of relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation in the complex vector space, providing a versatile framework for capturing diverse relation characteristics. Additionally, we introduce a novel self-adversarial negative sampling technique, enhancing the efficiency and effectiveness of RotatE model training. Through comprehensive experiments on multiple benchmark knowledge graphs, we demonstrate that RotatE is not only scalable but also excels in inferring and modeling various relation patterns, surpassing the performance of existing state-of-the-art models in link prediction.""]",intro_chunked," Learning effective representations of entities and relations in knowledge graphs is pivotal for predicting missing links. The success of this task crucially depends on the model's ability to capture and infer intricate patterns within relations. In response, we present RotatE, a novel knowledge graph embedding approach specifically designed to model and infer a range of relation patterns, including symmetry/antisymmetry, inversion, and composition. The RotatE model represents each relation as a rotation in the complex vector space, providing a versatile framework for capturing diverse relation characteristics. Additionally, we introduce a novel self-adversarial negative sampling technique, enhancing the efficiency and effectiveness of RotatE model training. Through comprehensive experiments on multiple benchmark knowledge graphs, we demonstrate that RotatE is not only scalable but also excels in inferring and modeling various relation patterns, surpassing the performance of existing state-of-the-art models in link prediction.",2.470238095238102,2.6406588729545177,101,0.6585707068443298," Learning effective representations of entities and relations in knowledge graphs is pivotal for predicting missing links. The success of this task crucially depends on the models ability to capture and infer intricate patterns within relations. In response, we present Propname, a novel knowledge graph embedding approach specifically designed to model and infer a range of relation patterns, including symmetryantisymmetry, inversion, and composition. The Propname model represents each relation as a rotation in the complex vector space, providing a versatile framework for capturing diverse relation characteristics. Additionally, we introduce a novel self adversarial negative sampling technique, enhancing the efficiency and effectiveness of Propname model training. Through comprehensive experiments on multiple benchmark knowledge graphs, we demonstrate that Propname is not only scalable but also excels in inferring and modeling various relation patterns, surpassing the performance of existing state of the art models in link prediction."," Learning effective representations of entities and relations in knowledge graphs is pivotal for predicting missing links. The success of this task crucially depends on the models ability to capture and infer intricate patterns within relations. In response, we present Propname, a novel knowledge graph embedding approach specifically designed to model and infer a range of relation patterns, including symmetryantisymmetry, inversion, and composition. The Propname model represents each relation as a rotation in the complex vector space, providing a versatile framework for capturing diverse relation characteristics. Additionally, we introduce a novel self adversarial negative sampling technique, enhancing the efficiency and effectiveness of Propname model training. Through comprehensive experiments on multiple benchmark knowledge graphs, we demonstrate that Propname is not only scalable but also excels in inferring and modeling various relation patterns, surpassing the performance of existing state of the art models in link prediction.", VERB ADJ NOUN ADP NOUN CCONJ NOUN ADP NOUN NOUN AUX ADJ ADP VERB ADJ NOUN PUNCT DET NOUN ADP DET NOUN ADV VERB ADP DET NOUN NOUN PART VERB CCONJ VERB ADJ NOUN ADP NOUN PUNCT ADP NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN VERB NOUN ADV VERB PART VERB CCONJ VERB DET NOUN ADP NOUN NOUN PUNCT VERB NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT DET PROPN NOUN VERB DET NOUN ADP DET NOUN ADP DET ADJ NOUN NOUN PUNCT VERB DET ADJ NOUN ADP VERB ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADJ ADJ NOUN NOUN PUNCT VERB DET NOUN CCONJ NOUN ADP PROPN NOUN NOUN PUNCT ADP ADJ NOUN ADP ADJ ADJ NOUN NOUN PUNCT PRON VERB SCONJ PROPN AUX PART ADV ADJ CCONJ ADV NOUN ADP VERB CCONJ VERB ADJ NOUN NOUN PUNCT VERB DET NOUN ADP VERB NOUN ADP DET NOUN NOUN ADP NOUN NOUN PUNCT,0.6289308176100629,26.5,5.6918238993710695
102,16,GPT-3.5,"["" In the realm of Large Language Models (LLMs), the quest for more accurate factual knowledge generation has led to the development of RECITation-augmented gEneration (RECITE). In contrast to traditional retrieval-augmented models, RECITE pioneers a recite-first approach, where pertinent passages are drawn from the LLMs' internal memory through sampling before the final answers are generated. This innovative strategy is tailored for knowledge-intensive Natural Language Processing (NLP) tasks, specifically closed-book question answering (CBQA). Our motivation stems from the need to reduce reliance on external corpuses and enhance the model's autonomy in generating precise and contextually relevant information. In this paper, we present RECITE as a powerful paradigm by demonstrating its applicability across diverse CBQA tasks, including Natural Questions, TriviaQA, and HotpotQA. We conduct extensive experiments employing four pre-trained models—PaLM, UL2, OPT, and Codex—to validate RECITE's effectiveness. The results showcase that the recite-and-answer scheme consistently achieves new state-of-the-art performance, establishing RECITE as a promising avenue for advancing factual knowledge generation within LLMs.""]",intro_chunked," In the realm of Large Language Models (LLMs), the quest for more accurate factual knowledge generation has led to the development of RECITation-augmented gEneration (RECITE). In contrast to traditional retrieval-augmented models, RECITE pioneers a recite-first approach, where pertinent passages are drawn from the LLMs' internal memory through sampling before the final answers are generated. This innovative strategy is tailored for knowledge-intensive Natural Language Processing (NLP) tasks, specifically closed-book question answering (CBQA). Our motivation stems from the need to reduce reliance on external corpuses and enhance the model's autonomy in generating precise and contextually relevant information. In this paper, we present RECITE as a powerful paradigm by demonstrating its applicability across diverse CBQA tasks, including Natural Questions, TriviaQA, and HotpotQA. We conduct extensive experiments employing four pre-trained models—PaLM, UL2, OPT, and Codex—to validate RECITE's effectiveness. The results showcase that the recite-and-answer scheme consistently achieves new state-of-the-art performance, establishing RECITE as a promising avenue for advancing factual knowledge generation within LLMs.",5.6957055214724335,2.6406588729545177,102,0.7704802751541138," In the realm of Large Propname Propname, the quest for more accurate factual knowledge generation has led to the development of Propname augmented Propname. In contrast to traditional retrieval augmented models, Propname pioneers a recite first approach, where pertinent passages are drawn from the Propname internal memory through sampling before the final answers are generated. This innovative strategy is tailored for knowledge intensive Propname Propname Propname tasks, specifically closed book question answering. Our motivation stems from the need to reduce reliance on external corpuses and enhance the models autonomy in generating precise and contextually relevant information. In this paper, we present Propname as a powerful paradigm by demonstrating its applicability across diverse CBQA tasks, including Propname Propname, Propname, and Propname. We conduct extensive experiments employing four pre trained Propname, Propname, Propname, and Propname validate RECITEs effectiveness. The results showcase that the recite and answer scheme consistently achieves new state of the art performance, establishing Propname as a promising avenue for advancing factual knowledge generation within LLMs."," In the realm of Large Propname Propname, the quest for more accurate factual knowledge generation has led to the development of Propname augmented Propname. In contrast to traditional retrieval augmented models, Propname pioneers a recite first approach, where pertinent passages are drawn from the Propname internal memory through sampling before the final answers are generated. This innovative strategy is tailored for knowledge intensive Propname Propname Propname tasks, specifically closed book question answering. Our motivation stems from the need to reduce reliance on external corpuses and enhance the models autonomy in generating precise and contextually relevant information. In this paper, we present Propname as a powerful paradigm by demonstrating its applicability across diverse CBQA tasks, including Propname Propname, Propname, and Propname. We conduct extensive experiments employing four pre trained Propname, Propname, Propname, and Propname validate RECITEs effectiveness. The results showcase that the recite and answer scheme consistently achieves new state of the art performance, establishing Propname as a promising avenue for advancing factual knowledge generation within LLMs.", ADP DET NOUN ADP ADJ PROPN PROPN PUNCT DET NOUN ADP ADV ADJ ADJ NOUN NOUN AUX VERB ADP DET NOUN ADP PROPN VERB PROPN PUNCT ADP NOUN ADP ADJ NOUN VERB NOUN PUNCT PROPN VERB DET ADJ ADJ NOUN PUNCT SCONJ ADJ NOUN AUX VERB ADP DET PROPN ADJ NOUN ADP VERB SCONJ DET ADJ NOUN AUX VERB PUNCT DET ADJ NOUN AUX VERB ADP NOUN ADJ PROPN PROPN PROPN NOUN PUNCT ADV VERB NOUN NOUN VERB PUNCT PRON NOUN VERB ADP DET NOUN PART VERB NOUN ADP ADJ NOUN CCONJ VERB DET NOUN NOUN ADP VERB ADJ CCONJ ADV ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN ADP DET ADJ NOUN ADP VERB PRON NOUN ADP ADJ NOUN NOUN PUNCT VERB PROPN PROPN PUNCT PROPN PUNCT CCONJ PROPN PUNCT PRON VERB ADJ NOUN VERB NUM ADJ VERB PROPN PUNCT PROPN PUNCT PROPN PUNCT CCONJ PROPN NOUN NOUN NOUN PUNCT DET NOUN VERB SCONJ DET NOUN CCONJ VERB NOUN ADV VERB ADJ NOUN ADP DET NOUN NOUN PUNCT VERB PROPN ADP DET ADJ NOUN ADP VERB ADJ NOUN NOUN ADP NOUN PUNCT,0.6324324324324324,26.428571428571427,5.632432432432433
103,17,GPT-3.5,"[' Natural Language Processing (NLP) has made remarkable strides, leveraging massive pre-trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces MobileBERT, a solution that addresses these challenges by compressing and accelerating the popular BERT model. Maintaining the versatility of the original BERT, MobileBERT is designed to be task-agnostic, allowing seamless application to diverse NLP tasks with minimal fine-tuning. The architecture of MobileBERT draws inspiration from BERTLARGE but incorporates bottleneck structures and a finely tuned balance between self-attentions and feed-forward networks. The training strategy involves a specially crafted teacher model, an inverted-bottleneck incorporated BERTLARGE, from which knowledge is transferred to MobileBERT. Empirical results demonstrate the efficiency of MobileBERT, achieving a 4.3× reduction in size and a 5.5× acceleration compared to BERTBASE, while still maintaining competitive performance on established benchmarks.']",intro_chunked," Natural Language Processing (NLP) has made remarkable strides, leveraging massive pre-trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces MobileBERT, a solution that addresses these challenges by compressing and accelerating the popular BERT model. Maintaining the versatility of the original BERT, MobileBERT is designed to be task-agnostic, allowing seamless application to diverse NLP tasks with minimal fine-tuning. The architecture of MobileBERT draws inspiration from BERTLARGE but incorporates bottleneck structures and a finely tuned balance between self-attentions and feed-forward networks. The training strategy involves a specially crafted teacher model, an inverted-bottleneck incorporated BERTLARGE, from which knowledge is transferred to MobileBERT. Empirical results demonstrate the efficiency of MobileBERT, achieving a 4.3× reduction in size and a 5.5× acceleration compared to BERTBASE, while still maintaining competitive performance on established benchmarks.",4.3575862068965705,2.6406588729545177,103,0.6360481977462769," Propname Propname Propname has made remarkable strides, leveraging massive pre trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces Propname, a solution that addresses these challenges by compressing and accelerating the popular Propname model. Maintaining the versatility of the original Propname, Propname is designed to be task agnostic, allowing seamless application to diverse Propname tasks with minimal fine tuning. The architecture of MobileBERT draws inspiration from Propname but incorporates bottleneck structures and a finely tuned balance between self attentions and feed forward networks. The training strategy involves a specially crafted teacher model, an inverted bottleneck incorporated BERTLARGE, from which knowledge is transferred to Propname. Empirical results demonstrate the efficiency of Propname, achieving a 0.0 reduction in size and a 0.0 acceleration compared to Propname, while still maintaining competitive performance on established benchmarks."," Propname Propname Propname has made remarkable strides, leveraging massive pre trained models for various applications. However, the deployment of these models on mobile devices faces challenges due to their size and latency. This paper introduces Propname, a solution that addresses these challenges by compressing and accelerating the popular Propname model. Maintaining the versatility of the original Propname, Propname is designed to be task agnostic, allowing seamless application to diverse Propname tasks with minimal fine tuning. The architecture of MobileBERT draws inspiration from Propname but incorporates bottleneck structures and a finely tuned balance between self attentions and feed forward networks. The training strategy involves a specially crafted teacher model, an inverted bottleneck incorporated BERTLARGE, from which knowledge is transferred to Propname. Empirical results demonstrate the efficiency of Propname, achieving a 0.0 reduction in size and a 0.0 acceleration compared to Propname, while still maintaining competitive performance on established benchmarks.", PROPN PROPN PROPN AUX VERB ADJ NOUN PUNCT VERB ADJ ADJ VERB NOUN ADP ADJ NOUN PUNCT ADV PUNCT DET NOUN ADP DET NOUN ADP ADJ NOUN VERB NOUN ADP ADP PRON NOUN CCONJ NOUN PUNCT DET NOUN VERB PROPN PUNCT DET NOUN PRON VERB DET NOUN ADP VERB CCONJ VERB DET ADJ PROPN NOUN PUNCT VERB DET NOUN ADP DET ADJ PROPN PUNCT PROPN AUX VERB PART AUX NOUN ADJ PUNCT VERB ADJ NOUN PART VERB PROPN NOUN ADP ADJ ADJ NOUN PUNCT DET NOUN ADP NUM VERB NOUN ADP PROPN CCONJ VERB NOUN NOUN CCONJ DET ADV VERB NOUN ADP NOUN NOUN CCONJ VERB ADJ NOUN PUNCT DET NOUN NOUN VERB DET ADV VERB NOUN NOUN PUNCT DET VERB NOUN VERB NOUN PUNCT ADP PRON NOUN AUX VERB ADP PROPN PUNCT ADJ NOUN VERB DET NOUN ADP PROPN PUNCT VERB DET NUM NOUN ADP NOUN CCONJ DET NUM NOUN VERB ADP PROPN PUNCT SCONJ ADV VERB ADJ NOUN ADP VERB NOUN PUNCT,0.6646341463414634,23.428571428571427,5.762195121951219
104,18,GPT-3.5,"["" Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce PEER, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of PEER to cover various parts of the writing process, leveraging self-training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEER's applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEER's robust performance across a range of domains and editing tasks.""]",intro_chunked," Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce PEER, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of PEER to cover various parts of the writing process, leveraging self-training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEER's applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEER's robust performance across a range of domains and editing tasks.",9.701935483871011,2.6406588729545177,104,0.6495702862739563," Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce Propname, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of Propname to cover various parts of the writing process, leveraging self training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEERs applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEERs robust performance across a range of domains and editing tasks."," Despite the collaborative nature of writing processes involving drafting, suggesting changes, and repeated revisions, current language models are primarily trained to generate final outputs. This limitation hinders their effectiveness in collaborative writing scenarios. To address this gap, we introduce Propname, a collaborative language model capable of imitating the entire writing process. PEER exhibits the ability to generate drafts, propose edits, and provide explanations for its actions, addressing crucial aspects of collaborative writing such as updating existing texts, controllability, and verbal planning. A key innovation is the training of multiple instances of Propname to cover various parts of the writing process, leveraging self training techniques to enrich the training data in terms of quality, quantity, and diversity. This approach extends PEERs applicability to domains lacking edit histories and enhances its proficiency in following instructions, generating meaningful comments, and explaining its actions. Through extensive experiments, we showcase PEERs robust performance across a range of domains and editing tasks.", SCONJ DET ADJ NOUN ADP NOUN NOUN VERB NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN PUNCT ADJ NOUN NOUN AUX ADV VERB PART VERB ADJ NOUN PUNCT DET NOUN VERB PRON NOUN ADP ADJ NOUN NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN NOUN ADJ ADP VERB DET ADJ NOUN NOUN PUNCT ADJ VERB DET NOUN PART VERB NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN ADP PRON NOUN PUNCT VERB ADJ NOUN ADP ADJ NOUN ADJ ADP VERB VERB NOUN PUNCT NOUN PUNCT CCONJ ADJ NOUN PUNCT DET ADJ NOUN AUX DET NOUN ADP ADJ NOUN ADP PROPN PART VERB ADJ NOUN ADP DET NOUN NOUN PUNCT VERB NOUN NOUN NOUN PART VERB DET NOUN NOUN ADP NOUN ADP NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT DET NOUN VERB VERB NOUN ADP NOUN VERB NOUN NOUN CCONJ VERB PRON NOUN ADP VERB NOUN PUNCT VERB ADJ NOUN PUNCT CCONJ VERB PRON NOUN PUNCT ADP ADJ NOUN PUNCT PRON VERB VERB ADJ NOUN ADP DET NOUN ADP NOUN CCONJ NOUN NOUN PUNCT,0.6312849162011173,25.571428571428573,5.553072625698324
105,19,GPT-3.5,"["" Pretraining deep language models, exemplified by BERT, has yielded significant advancements in natural language processing (NLP). However, recent observations underscore the challenge these models face in comprehending rare words. This work addresses the issue by introducing BERTRAM, an innovative architecture based on BERT designed to generate high-quality embeddings for rare words. Drawing inspiration from strategies applied to static word embeddings, BERTRAM facilitates the interaction between a word's surface form and contexts within a deep architecture. When integrated into BERT, BERTRAM achieves substantial performance boosts, particularly in enhancing representations of rare and medium-frequency words. This improvement is demonstrated across various tasks, including a rare word probing task and three downstream applications, highlighting BERTRAM's efficacy in enhancing the treatment of rare words within pretrained language models.""]",intro_chunked," Pretraining deep language models, exemplified by BERT, has yielded significant advancements in natural language processing (NLP). However, recent observations underscore the challenge these models face in comprehending rare words. This work addresses the issue by introducing BERTRAM, an innovative architecture based on BERT designed to generate high-quality embeddings for rare words. Drawing inspiration from strategies applied to static word embeddings, BERTRAM facilitates the interaction between a word's surface form and contexts within a deep architecture. When integrated into BERT, BERTRAM achieves substantial performance boosts, particularly in enhancing representations of rare and medium-frequency words. This improvement is demonstrated across various tasks, including a rare word probing task and three downstream applications, highlighting BERTRAM's efficacy in enhancing the treatment of rare words within pretrained language models.",11.200268817204346,2.6406588729545177,105,0.681515097618103," Pretraining deep language models, exemplified by Propname, has yielded significant advancements in natural language processing. However, recent observations underscore the challenge these models face in comprehending rare words. This work addresses the issue by introducing Propname, an innovative architecture based on Propname designed to generate high quality embeddings for rare words. Drawing inspiration from strategies applied to static word embeddings, Propname facilitates the interaction between a words surface form and contexts within a deep architecture. When integrated into Propname, Propname achieves substantial performance boosts, particularly in enhancing representations of rare and medium frequency words. This improvement is demonstrated across various tasks, including a rare word probing task and three downstream applications, highlighting BERTRAMs efficacy in enhancing the treatment of rare words within pretrained language models."," Pretraining deep language models, exemplified by Propname, has yielded significant advancements in natural language processing. However, recent observations underscore the challenge these models face in comprehending rare words. This work addresses the issue by introducing Propname, an innovative architecture based on Propname designed to generate high quality embeddings for rare words. Drawing inspiration from strategies applied to static word embeddings, Propname facilitates the interaction between a words surface form and contexts within a deep architecture. When integrated into Propname, Propname achieves substantial performance boosts, particularly in enhancing representations of rare and medium frequency words. This improvement is demonstrated across various tasks, including a rare word probing task and three downstream applications, highlighting BERTRAMs efficacy in enhancing the treatment of rare words within pretrained language models.", VERB ADJ NOUN NOUN PUNCT VERB ADP PROPN PUNCT AUX VERB ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT ADJ NOUN VERB DET NOUN DET NOUN VERB ADP VERB ADJ NOUN PUNCT DET NOUN VERB DET NOUN ADP VERB PROPN PUNCT DET ADJ NOUN VERB ADP PROPN VERB PART VERB ADJ NOUN NOUN ADP ADJ NOUN PUNCT VERB NOUN ADP NOUN VERB ADP ADJ NOUN NOUN PUNCT PROPN VERB DET NOUN ADP DET NOUN NOUN NOUN CCONJ NOUN ADP DET ADJ NOUN PUNCT SCONJ VERB ADP PROPN PUNCT PROPN VERB ADJ NOUN NOUN PUNCT ADV ADP VERB NOUN ADP ADJ CCONJ ADJ ADJ NOUN PUNCT DET NOUN AUX VERB ADP ADJ NOUN PUNCT VERB DET ADJ NOUN VERB NOUN CCONJ NUM ADJ NOUN PUNCT VERB NOUN NOUN ADP VERB DET NOUN ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT,0.6428571428571429,23.333333333333332,5.957142857142857
106,20,GPT-3.5,"[' The prospect of solving natural language processing (NLP) tasks in a fully unsupervised manner by providing pretrained language models with task descriptions has been explored but often falls short of supervised counterparts. In this work, we introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that seeks to bridge the gap between unsupervised and supervised learning. PET transforms input examples into cloze-style phrases, serving as guidance for language models in understanding the underlying task. These phrases are then leveraged to assign soft labels to a substantial set of unlabeled examples. Finally, standard supervised training is applied to the resulting training set. Across diverse tasks and languages, PET showcases superior performance compared to both supervised training and robust semi-supervised approaches, particularly demonstrating efficacy in low-resource settings.']",intro_chunked," The prospect of solving natural language processing (NLP) tasks in a fully unsupervised manner by providing pretrained language models with task descriptions has been explored but often falls short of supervised counterparts. In this work, we introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that seeks to bridge the gap between unsupervised and supervised learning. PET transforms input examples into cloze-style phrases, serving as guidance for language models in understanding the underlying task. These phrases are then leveraged to assign soft labels to a substantial set of unlabeled examples. Finally, standard supervised training is applied to the resulting training set. Across diverse tasks and languages, PET showcases superior performance compared to both supervised training and robust semi-supervised approaches, particularly demonstrating efficacy in low-resource settings.",14.611559139784958,2.6406588729545177,106,0.5687777400016785," The prospect of solving natural language processing tasks in a fully unsupervised manner by providing pretrained language models with task descriptions has been explored but often falls short of supervised counterparts. In this work, we introduce Propname Propname Propname, a semi supervised training procedure that seeks to bridge the gap between unsupervised and supervised learning. Propname transforms input examples into cloze style phrases, serving as guidance for language models in understanding the underlying task. These phrases are then leveraged to assign soft labels to a substantial set of unlabeled examples. Finally, standard supervised training is applied to the resulting training set. Across diverse tasks and languages, Propname showcases superior performance compared to both supervised training and robust semi supervised approaches, particularly demonstrating efficacy in low resource settings."," The prospect of solving natural language processing tasks in a fully unsupervised manner by providing pretrained language models with task descriptions has been explored but often falls short of supervised counterparts. In this work, we introduce Propname Propname Propname, a semi supervised training procedure that seeks to bridge the gap between unsupervised and supervised learning. Propname transforms input examples into cloze style phrases, serving as guidance for language models in understanding the underlying task. These phrases are then leveraged to assign soft labels to a substantial set of unlabeled examples. Finally, standard supervised training is applied to the resulting training set. Across diverse tasks and languages, Propname showcases superior performance compared to both supervised training and robust semi supervised approaches, particularly demonstrating efficacy in low resource settings.", DET NOUN ADP VERB ADJ NOUN NOUN NOUN ADP DET ADV ADJ NOUN ADP VERB VERB NOUN NOUN ADP NOUN NOUN AUX AUX VERB CCONJ ADV VERB ADJ ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN PROPN PROPN PUNCT DET ADJ ADJ NOUN NOUN PRON VERB PART VERB DET NOUN ADP ADJ CCONJ ADJ NOUN PUNCT PROPN VERB ADJ NOUN ADP ADJ NOUN NOUN PUNCT VERB ADP NOUN ADP NOUN NOUN ADP VERB DET VERB NOUN PUNCT DET NOUN AUX ADV ADJ PART VERB ADJ NOUN ADP DET ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT ADJ ADJ NOUN AUX VERB ADP DET VERB NOUN NOUN PUNCT ADP ADJ NOUN CCONJ NOUN PUNCT PROPN VERB ADJ NOUN VERB ADP DET ADJ NOUN CCONJ ADJ ADJ ADJ NOUN PUNCT ADV VERB NOUN ADP ADJ NOUN NOUN PUNCT,0.6690647482014388,23.166666666666668,5.697841726618705
107,21,GPT-3.5,"[' Effective representation learning for novel words in natural language processing (NLP) systems remains a significant challenge. Existing approaches tackle this issue by either focusing on the surface-form or contextual usage of novel words. In this paper, we present an innovative architecture that leverages both surface-form and context information to enhance the quality of word embeddings. Our model achieves state-of-the-art results on benchmark datasets such as Definitional Nonce and Contextual Rare Words. Crucially, our approach requires only an embedding set and an unlabeled corpus for training, offering a versatile solution that can be seamlessly integrated into any existing NLP system, augmenting its capability to handle novel words.']",intro_chunked," Effective representation learning for novel words in natural language processing (NLP) systems remains a significant challenge. Existing approaches tackle this issue by either focusing on the surface-form or contextual usage of novel words. In this paper, we present an innovative architecture that leverages both surface-form and context information to enhance the quality of word embeddings. Our model achieves state-of-the-art results on benchmark datasets such as Definitional Nonce and Contextual Rare Words. Crucially, our approach requires only an embedding set and an unlabeled corpus for training, offering a versatile solution that can be seamlessly integrated into any existing NLP system, augmenting its capability to handle novel words.",14.520773584905669,2.6406588729545177,107,0.6278308033943176," Effective representation learning for novel words in natural language processing systems remains a significant challenge. Existing approaches tackle this issue by either focusing on the surface form or contextual usage of novel words. In this paper, we present an innovative architecture that leverages both surface form and context information to enhance the quality of word embeddings. Our model achieves state of the art results on benchmark datasets such as Propname Propname and Propname Propname Propname. Crucially, our approach requires only an embedding set and an unlabeled corpus for training, offering a versatile solution that can be seamlessly integrated into any existing NLP system, augmenting its capability to handle novel words."," Effective representation learning for novel words in natural language processing systems remains a significant challenge. Existing approaches tackle this issue by either focusing on the surface form or contextual usage of novel words. In this paper, we present an innovative architecture that leverages both surface form and context information to enhance the quality of word embeddings. Our model achieves state of the art results on benchmark datasets such as Propname Propname and Propname Propname Propname. Crucially, our approach requires only an embedding set and an unlabeled corpus for training, offering a versatile solution that can be seamlessly integrated into any existing NLP system, augmenting its capability to handle novel words.", ADJ NOUN VERB ADP ADJ NOUN ADP ADJ NOUN NOUN NOUN VERB DET ADJ NOUN PUNCT VERB NOUN VERB DET NOUN ADP CCONJ VERB ADP DET NOUN NOUN CCONJ ADJ NOUN ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN PRON VERB DET NOUN NOUN CCONJ NOUN NOUN PART VERB DET NOUN ADP NOUN NOUN PUNCT PRON NOUN VERB NOUN ADP DET NOUN NOUN ADP ADJ NOUN ADJ ADP PROPN PROPN CCONJ PROPN PROPN PROPN PUNCT ADV PUNCT PRON NOUN VERB ADV DET VERB NOUN CCONJ DET ADJ NOUN ADP NOUN PUNCT VERB DET ADJ NOUN PRON AUX AUX ADV VERB ADP DET VERB NOUN NOUN PUNCT VERB PRON NOUN PART VERB ADJ NOUN PUNCT,0.7394957983193278,23.8,5.361344537815126
108,22,GPT-3.5,"[' The quest for high-quality sentence embeddings has been marked by the dichotomy between augmenting pretrained language models (PLMs) with supplementary pretraining objectives and relying on finetuning with large labeled datasets. While the latter typically outperforms the former, the formidable challenge lies in the human effort required to generate expansive, suitable datasets. This paper presents a groundbreaking alternative, demonstrating how the generative capabilities of large and high-performing PLMs can be harnessed to overcome this challenge. By generating entire datasets of labeled text pairs de novo, our approach eliminates the need for labeled data, finetuning, or modifications to pretraining objectives. This innovative strategy facilitates the finetuning of smaller and more efficient models, achieving superior performance compared to established baselines on diverse semantic textual similarity datasets.']",intro_chunked," The quest for high-quality sentence embeddings has been marked by the dichotomy between augmenting pretrained language models (PLMs) with supplementary pretraining objectives and relying on finetuning with large labeled datasets. While the latter typically outperforms the former, the formidable challenge lies in the human effort required to generate expansive, suitable datasets. This paper presents a groundbreaking alternative, demonstrating how the generative capabilities of large and high-performing PLMs can be harnessed to overcome this challenge. By generating entire datasets of labeled text pairs de novo, our approach eliminates the need for labeled data, finetuning, or modifications to pretraining objectives. This innovative strategy facilitates the finetuning of smaller and more efficient models, achieving superior performance compared to established baselines on diverse semantic textual similarity datasets.",-2.4657073170731394,2.6406588729545177,108,0.7058013081550598," The quest for high quality sentence embeddings has been marked by the dichotomy between augmenting pretrained language models with supplementary pretraining objectives and relying on finetuning with large labeled datasets. While the latter typically outperforms the former, the formidable challenge lies in the human effort required to generate expansive, suitable datasets. This paper presents a groundbreaking alternative, demonstrating how the generative capabilities of large and high performing PLMs can be harnessed to overcome this challenge. By generating entire datasets of labeled text pairs de Propname, our approach eliminates the need for labeled data, finetuning, or modifications to pretraining objectives. This innovative strategy facilitates the finetuning of smaller and more efficient models, achieving superior performance compared to established baselines on diverse semantic textual similarity datasets."," The quest for high quality sentence embeddings has been marked by the dichotomy between augmenting pretrained language models with supplementary pretraining objectives and relying on finetuning with large labeled datasets. While the latter typically outperforms the former, the formidable challenge lies in the human effort required to generate expansive, suitable datasets. This paper presents a groundbreaking alternative, demonstrating how the generative capabilities of large and high performing PLMs can be harnessed to overcome this challenge. By generating entire datasets of labeled text pairs de Propname, our approach eliminates the need for labeled data, finetuning, or modifications to pretraining objectives. This innovative strategy facilitates the finetuning of smaller and more efficient models, achieving superior performance compared to established baselines on diverse semantic textual similarity datasets.", DET NOUN ADP ADJ NOUN NOUN NOUN AUX AUX VERB ADP DET NOUN ADP VERB VERB NOUN NOUN ADP ADJ VERB NOUN CCONJ VERB ADP VERB ADP ADJ VERB NOUN PUNCT SCONJ DET ADJ ADV VERB DET ADJ PUNCT DET ADJ NOUN VERB ADP DET ADJ NOUN VERB PART VERB ADJ PUNCT ADJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN PUNCT VERB SCONJ DET ADJ NOUN ADP ADJ CCONJ ADJ VERB NOUN AUX AUX VERB PART VERB DET NOUN PUNCT ADP VERB ADJ NOUN ADP VERB NOUN VERB X PROPN PUNCT PRON NOUN VERB DET NOUN ADP VERB NOUN PUNCT VERB PUNCT CCONJ NOUN ADP VERB NOUN PUNCT DET ADJ NOUN VERB DET NOUN ADP ADJ CCONJ ADV ADJ NOUN PUNCT VERB ADJ NOUN VERB ADP VERB NOUN ADP ADJ ADJ ADJ NOUN NOUN PUNCT,0.6985294117647058,27.2,5.897058823529412
109,23,GPT-3.5,"[' The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text-related tasks and improving few-shot learning outcomes. This paper introduces GENPET, a method rooted in pattern-exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few-shot text generation.']",intro_chunked," The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text-related tasks and improving few-shot learning outcomes. This paper introduces GENPET, a method rooted in pattern-exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few-shot text generation.",-6.146529411764703,2.6406588729545177,109,0.7787601947784424," The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text related tasks and improving few shot learning outcomes. This paper introduces GENPET, a method rooted in pattern exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few shot text generation."," The fusion of pretrained language models with simple task descriptions presents a compelling avenue for advancing unsupervised text related tasks and improving few shot learning outcomes. This paper introduces GENPET, a method rooted in pattern exploiting training, an approach initially tailored for combining textual instructions with supervised learning in classification tasks. The primary objective is to extend the applicability of this methodology to text generation tasks, with a specific focus on summarization and headline generation. Addressing the critical challenges associated with comprehensible task descriptions, effective utilization by pretrained models, and the prevention of overfitting, GENPET aims to push the boundaries of data efficiency in generative settings. Through extensive experimentation on diverse datasets, we showcase the consistent enhancements achieved by GENPET over strong baselines, underscoring its potential as a versatile and effective solution in the realm of few shot text generation.", DET NOUN ADP VERB NOUN NOUN ADP ADJ NOUN NOUN VERB DET ADJ NOUN ADP VERB ADJ NOUN VERB NOUN CCONJ VERB ADJ NOUN VERB NOUN PUNCT DET NOUN VERB NOUN PUNCT DET NOUN VERB ADP NOUN VERB NOUN PUNCT DET NOUN ADV VERB ADP VERB ADJ NOUN ADP ADJ NOUN ADP NOUN NOUN PUNCT DET ADJ NOUN AUX PART VERB DET NOUN ADP DET NOUN PART NOUN NOUN NOUN PUNCT ADP DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT VERB DET ADJ NOUN VERB ADP ADJ NOUN NOUN PUNCT ADJ NOUN ADP VERB NOUN PUNCT CCONJ DET NOUN ADP NOUN PUNCT NOUN VERB PART VERB DET NOUN ADP NOUN NOUN ADP ADJ NOUN PUNCT ADP ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET ADJ NOUN VERB ADP NOUN ADP ADJ NOUN PUNCT VERB PRON NOUN ADP DET ADJ CCONJ ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN NOUN PUNCT,0.6470588235294118,30.6,5.823529411764706
110,24,Aman Madaan,"[' Human problem-solving inherently follows a multi-step process: generate a solution, verify its\nvalidity, and refine it further based on verification outcomes. The emulation of this self-refinement\nand reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan\net al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,\ndemonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the\nintrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable\ndata) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step\nreasoning), motivate an alternative approach of model switching. Model switching iteratively queries\nover models of disparate sizes and capabilities, verifying feedback at each step and determining\nwhether to accept the output or route to a more capable, albeit computationally intensive, model (Liu\net al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly\nfor each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,\n2022).', 'However, modern LLM often provide access solely through black-box APIs, restricting\ndirect model optimization and adaptability due to the unavailability of fine-tuning capabilities and\nweight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM\nAPIs, circumventing the necessity for separate models or logits access by adopting few-shot learning\nstrategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies\nfor each step of problem-solving: solution generation, verification, and routing, all assuming we only\nhave access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model\nrouting, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified\nearly. This consideration allows AutoMix to judiciously allocate computational resources, preventing\nunwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with\nthe provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan\net al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic\nlife"" would be flagged as inconsistent.', 'However, recognizing that self-verification can sometimes\nbe inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability\nof the initial verification. The meta-verifier acts as a secondary check, providing an additional layer\nof confidence assessment to ensure that the decision to route a task to a larger or smaller model is\nwell-founded. In summary, our contributions are:\n• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating\na solution, verifying the solution, and switching to a larger language model, everything without\naccess to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps\nimprove the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure\nthat quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets\nusing the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large\n(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing\nthe incremental benefit per cost by up to 89%.']",intro_chunked," Human problem-solving inherently follows a multi-step process: generate a solution, verify its
validity, and refine it further based on verification outcomes. The emulation of this self-refinement
and reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan
et al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,
demonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the
intrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable
data) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step
reasoning), motivate an alternative approach of model switching. Model switching iteratively queries
over models of disparate sizes and capabilities, verifying feedback at each step and determining
whether to accept the output or route to a more capable, albeit computationally intensive, model (Liu
et al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly
for each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,
2022).",21.044285714285735,30.71625077440773,110,0.544819176197052," Human problem solving inherently follows a multi step process: generate a solution, verify its 
 validity, and refine it further based on verification outcomes. The emulation of this self refinement 
 and reflective behavior has gained attention in the recent research Propname Propname Propname Propname, 0000a; Propname 
 Propname Propname Propname, 0000; Propname and Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname et Propname Propname, 0000. Classic self refine paradigms consistently employ a single model across all problem solving stages, 
 demonstrating effectiveness in certain scenarios. Yet, the 
 intrinsic complexity and variability of tasks, from Propname Propname, binary classification on separable 
 data to complex and potentially unsolvable eg, certain forms of multi step 
 reasoning, motivate an alternative approach of model switching. Model switching iteratively queries 
 over models of disparate sizes and capabilities, verifying feedback at each step and determining 
 whether to accept the output or route to a more capable, albeit computationally intensive, model Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. Past studies in model switching strategies predominantly rely on separate models trained explicitly 
 for each step or require access to logitsChen Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and Propname, 
 0000."," Human problem solving inherently follows a multi step process: generate a solution, verify its 
 validity, and refine it further based on verification outcomes. The emulation of this self refinement 
 and reflective behavior has gained attention in the recent research Propname Propname Propname Propname, 0000a; Propname 
 Propname Propname Propname, 0000; Propname and Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname et Propname Propname, 0000. Classic self refine paradigms consistently employ a single model across all problem solving stages, 
 demonstrating effectiveness in certain scenarios. Yet, the 
 intrinsic complexity and variability of tasks, from Propname Propname, binary classification on separable 
 data to complex and potentially unsolvable eg, certain forms of multi step 
 reasoning, motivate an alternative approach of model switching. Model switching iteratively queries 
 over models of disparate sizes and capabilities, verifying feedback at each step and determining 
 whether to accept the output or route to a more capable, albeit computationally intensive, model Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. Past studies in model switching strategies predominantly rely on separate models trained explicitly 
 for each step or require access to logitsChen Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and Propname, 
 0000.", ADJ NOUN VERB ADV VERB DET ADJ NOUN NOUN PUNCT VERB DET NOUN PUNCT VERB PRON SPACE NOUN PUNCT CCONJ VERB PRON ADV VERB ADP NOUN NOUN PUNCT DET NOUN ADP DET NOUN NOUN SPACE CCONJ ADJ NOUN AUX VERB NOUN ADP DET ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN NOUN PROPN PROPN PUNCT NUM PUNCT ADJ NOUN VERB NOUN ADV VERB DET ADJ NOUN ADP DET NOUN VERB NOUN PUNCT SPACE VERB NOUN ADP ADJ NOUN PUNCT ADV PUNCT DET SPACE ADJ NOUN CCONJ NOUN ADP NOUN PUNCT ADP PROPN PROPN PUNCT ADJ NOUN ADP ADJ SPACE NOUN ADP ADJ CCONJ ADV ADJ NOUN PUNCT ADJ NOUN ADP ADJ NOUN SPACE NOUN PUNCT VERB DET ADJ NOUN ADP NOUN NOUN PUNCT NOUN VERB ADV NOUN SPACE ADP NOUN ADP ADJ NOUN CCONJ NOUN PUNCT VERB NOUN ADP DET NOUN CCONJ VERB SPACE SCONJ PART VERB DET NOUN CCONJ NOUN ADP DET ADV ADJ PUNCT SCONJ ADV ADJ PUNCT NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADJ NOUN ADP NOUN VERB NOUN ADV VERB ADP ADJ NOUN VERB ADV SPACE ADP DET NOUN CCONJ VERB NOUN ADP INTJ PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN PUNCT SPACE NUM PUNCT,0.44921875,42.666666666666664,5.3828125
111,25,Aman Madaan,"[' Human problem-solving inherently follows a multi-step process: generate a solution, verify its\nvalidity, and refine it further based on verification outcomes. The emulation of this self-refinement\nand reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan\net al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,\ndemonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the\nintrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable\ndata) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step\nreasoning), motivate an alternative approach of model switching. Model switching iteratively queries\nover models of disparate sizes and capabilities, verifying feedback at each step and determining\nwhether to accept the output or route to a more capable, albeit computationally intensive, model (Liu\net al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly\nfor each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,\n2022).', 'However, modern LLM often provide access solely through black-box APIs, restricting\ndirect model optimization and adaptability due to the unavailability of fine-tuning capabilities and\nweight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM\nAPIs, circumventing the necessity for separate models or logits access by adopting few-shot learning\nstrategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies\nfor each step of problem-solving: solution generation, verification, and routing, all assuming we only\nhave access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model\nrouting, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified\nearly. This consideration allows AutoMix to judiciously allocate computational resources, preventing\nunwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with\nthe provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan\net al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic\nlife"" would be flagged as inconsistent.', 'However, recognizing that self-verification can sometimes\nbe inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability\nof the initial verification. The meta-verifier acts as a secondary check, providing an additional layer\nof confidence assessment to ensure that the decision to route a task to a larger or smaller model is\nwell-founded. In summary, our contributions are:\n• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating\na solution, verifying the solution, and switching to a larger language model, everything without\naccess to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps\nimprove the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure\nthat quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets\nusing the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large\n(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing\nthe incremental benefit per cost by up to 89%.']",intro_chunked,"However, modern LLM often provide access solely through black-box APIs, restricting
direct model optimization and adaptability due to the unavailability of fine-tuning capabilities and
weight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM
APIs, circumventing the necessity for separate models or logits access by adopting few-shot learning
strategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies
for each step of problem-solving: solution generation, verification, and routing, all assuming we only
have access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model
routing, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified
early. This consideration allows AutoMix to judiciously allocate computational resources, preventing
unwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with
the provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan
et al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic
life"" would be flagged as inconsistent.",3.8601492537313504,30.71625077440773,111,0.21481262147426605," However, modern Propname often provide access solely through black box APIs, restricting 
 direct model optimization and adaptability due to the unavailability of fine tuning capabilities and 
 weight access. In response to this, we introduce Propname, a method that utilizes Propname box Propname 
 APIs, circumventing the necessity for separate models or logits access by adopting few shot learning 
 strategies and implementing self verification. Our method proposes strategies 
 for each step of problem solving: solution generation, verification, and routing, all assuming we only 
 have access to black box LLMs. In contrast to existing approaches, which generally classify tasks as Propname or Propname for model 
 routing, Propname integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Propname Propname Propname and should not be routed to larger models if identified 
 early. This consideration allows Propname to judiciously allocate computational resources, preventing 
 unwarranted computational spending on these particularly challenging instances. We use context grounded few shot entailment to evaluate the consistency of generated answers with 
 the provided context, without requiring a large amount of human labeled data Propname, 0000; Propname 
 Propname Propname Propname, 0000. For example, an answer discussing desert animals in a context focused on aquatic 
 life would be flagged as inconsistent."," However, modern Propname often provide access solely through black box APIs, restricting 
 direct model optimization and adaptability due to the unavailability of fine tuning capabilities and 
 weight access. In response to this, we introduce Propname, a method that utilizes Propname box Propname 
 APIs, circumventing the necessity for separate models or logits access by adopting few shot learning 
 strategies and implementing self verification. Our method proposes strategies 
 for each step of problem solving: solution generation, verification, and routing, all assuming we only 
 have access to black box LLMs. In contrast to existing approaches, which generally classify tasks as Propname or Propname for model 
 routing, Propname integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Propname Propname Propname and should not be routed to larger models if identified 
 early. This consideration allows Propname to judiciously allocate computational resources, preventing 
 unwarranted computational spending on these particularly challenging instances. We use context grounded few shot entailment to evaluate the consistency of generated answers with 
 the provided context, without requiring a large amount of human labeled data Propname, 0000; Propname 
 Propname Propname Propname, 0000. For example, an answer discussing desert animals in a context focused on aquatic 
 life would be flagged as inconsistent.", ADV PUNCT ADJ PROPN ADV VERB NOUN ADV ADP ADJ NOUN NOUN PUNCT VERB SPACE ADJ NOUN NOUN CCONJ NOUN ADP ADP DET NOUN ADP ADJ NOUN NOUN CCONJ SPACE NOUN NOUN PUNCT ADP NOUN ADP PRON PUNCT PRON VERB PROPN PUNCT DET NOUN PRON VERB PROPN NOUN PROPN SPACE NOUN PUNCT VERB DET NOUN ADP ADJ NOUN CCONJ NOUN NOUN ADP VERB ADJ NOUN NOUN SPACE NOUN CCONJ VERB NOUN NOUN PUNCT PRON NOUN VERB NOUN SPACE ADP DET NOUN ADP NOUN VERB PUNCT NOUN NOUN PUNCT NOUN PUNCT CCONJ VERB PUNCT PRON VERB PRON ADV SPACE VERB NOUN ADP ADJ NOUN NOUN PUNCT ADP NOUN ADP VERB NOUN PUNCT PRON ADV VERB NOUN ADP PROPN CCONJ PROPN ADP NOUN SPACE NOUN PUNCT PROPN VERB DET ADJ NOUN ADP ADJ NOUN PUNCT DET NOUN AUX ADV ADJ ADV ADP DET PROPN PROPN PROPN CCONJ AUX PART AUX VERB ADP ADJ NOUN SCONJ VERB SPACE ADV PUNCT DET NOUN VERB PROPN PART ADV VERB ADJ NOUN PUNCT VERB SPACE ADJ ADJ NOUN ADP DET ADV ADJ NOUN PUNCT PRON VERB NOUN VERB ADJ NOUN NOUN PART VERB DET NOUN ADP VERB NOUN ADP SPACE DET VERB NOUN PUNCT ADP VERB DET ADJ NOUN ADP ADJ VERB NOUN PROPN PUNCT NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT ADP NOUN PUNCT DET NOUN VERB NOUN NOUN ADP DET NOUN VERB ADP ADJ SPACE NOUN AUX AUX VERB ADP NOUN PUNCT,0.6244541484716157,28.625,5.406113537117904
112,26,Aman Madaan,"[' Human problem-solving inherently follows a multi-step process: generate a solution, verify its\nvalidity, and refine it further based on verification outcomes. The emulation of this self-refinement\nand reflective behavior has gained attention in the recent research (Pan et al., 2023a; Madaan\net al., 2023; Reid and Neubig, 2022; Schick et al., 2022; Welleck et al., 2022; Shinn et al., 2023). Classic self-refine paradigms consistently employ a single model across all problem-solving stages,\ndemonstrating effectiveness in certain scenarios (Madaan et al., 2023; Shinn et al., 2023). Yet, the\nintrinsic complexity and variability of tasks, from simplistic (e.g., binary classification on separable\ndata) to complex (e.g., code generation) and potentially unsolvable (e.g., certain forms of multi-step\nreasoning), motivate an alternative approach of model switching. Model switching iteratively queries\nover models of disparate sizes and capabilities, verifying feedback at each step and determining\nwhether to accept the output or route to a more capable, albeit computationally intensive, model (Liu\net al., 2020; Zhou et al., 2020; Madaan and Yang, 2022; Geng et al., 2021; Schuster et al., 2022). Past studies in model-switching strategies predominantly rely on separate models trained explicitly\nfor each step or require access to logits(Chen et al., 2023; Welleck et al., 2022; Reid and Neubig,\n2022).', 'However, modern LLM often provide access solely through black-box APIs, restricting\ndirect model optimization and adaptability due to the unavailability of fine-tuning capabilities and\nweight access. In response to this, we introduce AutoMix, a method that utilizes black-box LLM\nAPIs, circumventing the necessity for separate models or logits access by adopting few-shot learning\nstrategies (Brown et al., 2020) and implementing self-verification. Our method proposes strategies\nfor each step of problem-solving: solution generation, verification, and routing, all assuming we only\nhave access to black-box LLMs.In contrast to existing approaches, which generally classify tasks as Simple or Complex for model\nrouting, AutoMix integrates a third category of Unsolvable queries. These queries are likely unsolvable even by a Large Language Model (LLM) and should not be routed to larger models if identified\nearly. This consideration allows AutoMix to judiciously allocate computational resources, preventing\nunwarranted computational spending on these particularly challenging instances. We use context-grounded few-shot entailment to evaluate the consistency of generated answers with\nthe provided context, without requiring a large amount of human-labeled data (Poliak, 2020; Dagan\net al., 2022). For example, an answer discussing ""desert animals"" in a context focused on ""aquatic\nlife"" would be flagged as inconsistent.', 'However, recognizing that self-verification can sometimes\nbe inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability\nof the initial verification. The meta-verifier acts as a secondary check, providing an additional layer\nof confidence assessment to ensure that the decision to route a task to a larger or smaller model is\nwell-founded. In summary, our contributions are:\n• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating\na solution, verifying the solution, and switching to a larger language model, everything without\naccess to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps\nimprove the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure\nthat quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets\nusing the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large\n(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing\nthe incremental benefit per cost by up to 89%.']",intro_chunked,"However, recognizing that self-verification can sometimes
be inconsistent or noisy (Huang et al., 2023), we introduce a meta-verifier to evaluate the reliability
of the initial verification. The meta-verifier acts as a secondary check, providing an additional layer
of confidence assessment to ensure that the decision to route a task to a larger or smaller model is
well-founded. In summary, our contributions are:
• We introduce AutoMix, a method that strategically leverages black-box LLM APIs for generating
a solution, verifying the solution, and switching to a larger language model, everything without
access to model weights, gradients, or logits. • We also show that context-grounded entailment is a reasonable but noisy proxy for self-verification. To deal with this noise, we propose a POMDP-based meta-verification mechanism that helps
improve the reliability of the final decision. • We propose and introduce the Incremental Benefit Per Unit Cost (IBC) metric, a novel measure
that quantifies the efficiency of integrating smaller and larger language models. • We present empirical evidence from experiments on five context-grounded reasoning datasets
using the language models LLAMA2-13B and LLAMA2-70B as the small (SLM) and large
(LLM) language models. Our results demonstrate that AutoMix surpasses baselines, enhancing
the incremental benefit per cost by up to 89%.",17.752692307692342,30.71625077440773,112,0.5683186054229736," However, recognizing that self verification can sometimes 
 be inconsistent or noisy, we introduce a meta verifier to evaluate the reliability 
 of the initial verification. The meta verifier acts as a secondary check, providing an additional layer 
 of confidence assessment to ensure that the decision to route a task to a larger or smaller model is 
 well founded. In summary, our contributions are: We introduce Propname, a method that strategically leverages black box Propname APIs for generating 
 a solution, verifying the solution, and switching to a larger language model, everything without 
 access to model weights, gradients, or logits. We also show that context grounded entailment is a reasonable but noisy proxy for self verification. To deal with this noise, we propose a Propname based meta verification mechanism that helps 
 improve the reliability of the final decision. We propose and introduce the Propname Benefit Per Propname Cost metric, a novel measure 
 that quantifies the efficiency of integrating smaller and larger language models. We present empirical evidence from experiments on five context grounded reasoning datasets 
 using the language models Propname 00B and Propname 00B as the small and large language models. Our results demonstrate that Propname surpasses baselines, enhancing 
 the incremental benefit per cost by up to 00."," However, recognizing that self verification can sometimes 
 be inconsistent or noisy, we introduce a meta verifier to evaluate the reliability 
 of the initial verification. The meta verifier acts as a secondary check, providing an additional layer 
 of confidence assessment to ensure that the decision to route a task to a larger or smaller model is 
 well founded. In summary, our contributions are: We introduce Propname, a method that strategically leverages black box Propname APIs for generating 
 a solution, verifying the solution, and switching to a larger language model, everything without 
 access to model weights, gradients, or logits. We also show that context grounded entailment is a reasonable but noisy proxy for self verification. To deal with this noise, we propose a Propname based meta verification mechanism that helps 
 improve the reliability of the final decision. We propose and introduce the Propname Benefit Per Propname Cost metric, a novel measure 
 that quantifies the efficiency of integrating smaller and larger language models. We present empirical evidence from experiments on five context grounded reasoning datasets 
 using the language models Propname 00B and Propname 00B as the small and large language models. Our results demonstrate that Propname surpasses baselines, enhancing 
 the incremental benefit per cost by up to 00.", ADV PUNCT VERB SCONJ NOUN NOUN AUX ADV SPACE AUX ADJ CCONJ ADJ PUNCT PRON VERB DET ADJ NOUN PART VERB DET NOUN SPACE ADP DET ADJ NOUN PUNCT DET ADJ NOUN VERB ADP DET ADJ NOUN PUNCT VERB DET ADJ NOUN SPACE ADP NOUN NOUN PART VERB SCONJ DET NOUN PART VERB DET NOUN ADP DET ADJ CCONJ ADJ NOUN AUX SPACE ADV VERB PUNCT ADP NOUN PUNCT PRON NOUN AUX PUNCT PRON VERB PROPN PUNCT DET NOUN PRON ADV VERB ADJ NOUN PROPN NOUN ADP VERB SPACE DET NOUN PUNCT VERB DET NOUN PUNCT CCONJ VERB ADP DET ADJ NOUN NOUN PUNCT PRON ADP SPACE NOUN ADP NOUN NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT PRON ADV VERB DET NOUN VERB NOUN AUX DET ADJ CCONJ ADJ NOUN ADP NOUN NOUN PUNCT PART VERB ADP DET NOUN PUNCT PRON VERB DET PROPN VERB ADJ NOUN NOUN PRON AUX SPACE VERB DET NOUN ADP DET ADJ NOUN PUNCT PRON VERB CCONJ VERB DET PROPN NOUN ADP PROPN NOUN NOUN PUNCT DET ADJ NOUN SPACE PRON VERB DET NOUN ADP VERB ADJ CCONJ ADJ NOUN NOUN PUNCT PRON VERB ADJ NOUN ADP NOUN ADP NUM NOUN VERB NOUN NOUN SPACE VERB DET NOUN NOUN PROPN NUM CCONJ PROPN NUM ADP DET ADJ CCONJ ADJ NOUN NOUN PUNCT PRON NOUN VERB SCONJ PROPN VERB NOUN PUNCT VERB SPACE DET ADJ NOUN ADP NOUN ADP ADP PART NUM PUNCT,0.5594713656387665,28.375,5.048458149779735
113,27,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked," Although large language models (LLMs) can generate coherent outputs, they often fall short in
addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such
as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program
readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may
benefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved
one—to ensure that the desired quality is achieved. Iterative refinement typically involves training
a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models
require large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),
which may not always be feasible to obtain. These limitations underscore the need for an effective
refinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;
Flower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating
an initial draft and subsequently refining it based on self-provided feedback.",23.51981818181818,30.71625077440773,113,0.5293113589286804," Although large language models can generate coherent outputs, they often fall short in 
 addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such 
 as dialogue response generation, or tasks with hard to define goals, such as enhancing program 
 readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may 
 benefit from further iterative refinementie, iteratively mapping a candidate output to an improved 
 oneto ensure that the desired quality is achieved. Propname refinement typically involves training 
 a refinement model that relies on domain specific data; Propname Propname Propname.; Propname Propname Propname.. Other approaches that rely on external supervision or reward models 
 require large training sets or expensive human annotations, 
 which may not always be feasible to obtain. These limitations underscore the need for an effective 
 refinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self refinement is a fundamental characteristic of human problem solving Propname, 0000; 
 Propname and Propname, 0000; Propname, 0000. Iterative self refinement is a process that involves creating 
 an initial draft and subsequently refining it based on self provided feedback."," Although large language models can generate coherent outputs, they often fall short in 
 addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such 
 as dialogue response generation, or tasks with hard to define goals, such as enhancing program 
 readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may 
 benefit from further iterative refinementie, iteratively mapping a candidate output to an improved 
 oneto ensure that the desired quality is achieved. Propname refinement typically involves training 
 a refinement model that relies on domain specific data; Propname Propname Propname.; Propname Propname Propname.. Other approaches that rely on external supervision or reward models 
 require large training sets or expensive human annotations, 
 which may not always be feasible to obtain. These limitations underscore the need for an effective 
 refinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self refinement is a fundamental characteristic of human problem solving Propname, 0000; 
 Propname and Propname, 0000; Propname, 0000. Iterative self refinement is a process that involves creating 
 an initial draft and subsequently refining it based on self provided feedback.", SCONJ ADJ NOUN NOUN AUX VERB ADJ NOUN PUNCT PRON ADV VERB ADJ ADP SPACE VERB ADJ NOUN PUNCT PRON ADV VERB NOUN ADP ADJ NOUN PUNCT ADJ SPACE SCONJ NOUN NOUN NOUN PUNCT CCONJ NOUN ADP NOUN PART VERB NOUN PUNCT ADJ ADP VERB NOUN SPACE NOUN PUNCT ADP DET NOUN PUNCT ADJ NOUN AUX VERB DET ADJ ADJ NOUN PUNCT CCONJ AUX SPACE VERB ADP ADJ ADJ NOUN PUNCT ADV VERB DET NOUN NOUN ADP DET VERB SPACE NOUN VERB SCONJ DET VERB NOUN AUX VERB PUNCT PROPN NOUN ADV VERB NOUN SPACE DET ADJ NOUN PRON VERB ADP NOUN ADJ NOUN PUNCT PROPN PROPN PROPN PUNCT PUNCT PROPN PROPN PROPN PUNCT PUNCT ADJ NOUN PRON VERB ADP ADJ NOUN CCONJ NOUN NOUN SPACE VERB ADJ NOUN NOUN CCONJ ADJ ADJ NOUN PUNCT SPACE PRON AUX PART ADV AUX ADJ PART VERB PUNCT DET NOUN VERB DET NOUN ADP DET ADJ SPACE ADJ NOUN PRON AUX AUX VERB ADP ADJ NOUN ADP VERB ADJ NOUN PUNCT ADJ NOUN NOUN AUX DET ADJ NOUN ADP ADJ NOUN VERB PROPN PUNCT NUM PUNCT SPACE PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PUNCT NUM PUNCT ADJ NOUN NOUN AUX DET NOUN PRON VERB VERB SPACE DET ADJ NOUN CCONJ ADV VERB PRON VERB ADP NOUN VERB NOUN PUNCT,0.624390243902439,25.625,5.4487804878048784
114,28,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked,"For example, when drafting an email to request a document from a colleague, an individual may initially write a direct
request such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the
potential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data
at your earliest convenience?"". When writing code, a programmer may implement an initial “quick
and dirty” implementation, and then, upon reflection, refactor their code to a solution that is more
efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement
without additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get
feedback. Then, the feedback is passed back to the same model to refine the previously-generated
draft. This process is repeated either for a specified number of iterations or until M determines that
no further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to
both generate feedback and incorporate the feedback into an improved draft.",37.762831838565035,30.71625077440773,114,0.45218583941459656," For example, when drafting an email to request a document from a colleague, an individual may initially write a direct 
 request such as Send me the data ASAP. Upon reflection, however, the writer recognizes the 
 potential impoliteness of the phrasing and revises it to Propname Propname, could you please send me the data 
 at your earliest convenience?. When writing code, a programmer may implement an initial quick 
 and dirty implementation, and then, upon reflection, refactor their code to a solution that is more 
 efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self refinement 
 without additional training, leading to higher quality outputs on a wide range of tasks. We present Propname Propname: an iterative self refinement Propname that alternates between two generative stepsFEEDBACK and Propname. These steps work in tandem to generate high quality outputs. Given an initial output generated by a model Propname, we pass it back to the same model Propname to get 
 feedback. Then, the feedback is passed back to the same model to refine the previously generated 
 draft. This process is repeated either for a specified number of iterations or until Propname determines that 
 no further refinement is necessary. We use few shot prompting to guide Propname to 
 both generate feedback and incorporate the feedback into an improved draft."," For example, when drafting an email to request a document from a colleague, an individual may initially write a direct 
 request such as Send me the data ASAP. Upon reflection, however, the writer recognizes the 
 potential impoliteness of the phrasing and revises it to Propname Propname, could you please send me the data 
 at your earliest convenience?. When writing code, a programmer may implement an initial quick 
 and dirty implementation, and then, upon reflection, refactor their code to a solution that is more 
 efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self refinement 
 without additional training, leading to higher quality outputs on a wide range of tasks. We present Propname Propname: an iterative self refinement Propname that alternates between two generative stepsFEEDBACK and Propname. These steps work in tandem to generate high quality outputs. Given an initial output generated by a model Propname, we pass it back to the same model Propname to get 
 feedback. Then, the feedback is passed back to the same model to refine the previously generated 
 draft. This process is repeated either for a specified number of iterations or until Propname determines that 
 no further refinement is necessary. We use few shot prompting to guide Propname to 
 both generate feedback and incorporate the feedback into an improved draft.", ADP NOUN PUNCT SCONJ VERB DET NOUN PART VERB DET NOUN ADP DET NOUN PUNCT DET NOUN AUX ADV VERB DET ADJ SPACE NOUN ADJ ADP VERB PRON DET NOUN ADV PUNCT SCONJ NOUN PUNCT ADV PUNCT DET NOUN VERB DET SPACE ADJ NOUN ADP DET NOUN CCONJ VERB PRON ADP PROPN PROPN PUNCT AUX PRON INTJ VERB PRON DET NOUN SPACE ADP PRON ADJ NOUN PUNCT PUNCT SCONJ VERB NOUN PUNCT DET NOUN AUX VERB DET ADJ ADJ SPACE CCONJ ADJ NOUN PUNCT CCONJ ADV PUNCT SCONJ NOUN PUNCT VERB PRON NOUN ADP DET NOUN PRON AUX ADV SPACE ADJ CCONJ ADJ PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ NOUN AUX VERB ADJ NOUN NOUN SPACE ADP ADJ NOUN PUNCT VERB ADP ADJ NOUN NOUN ADP DET ADJ NOUN ADP NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET ADJ NOUN NOUN PROPN SCONJ VERB ADP NUM ADJ NOUN CCONJ PROPN PUNCT DET NOUN VERB ADP NOUN PART VERB ADJ NOUN NOUN PUNCT VERB DET ADJ NOUN VERB ADP DET NOUN PROPN PUNCT PRON VERB PRON ADV ADP DET ADJ NOUN PROPN PART VERB SPACE NOUN PUNCT ADV PUNCT DET NOUN AUX VERB ADV ADP DET ADJ NOUN PART VERB DET ADV VERB SPACE NOUN PUNCT DET NOUN AUX VERB CCONJ ADP DET ADJ NOUN ADP NOUN CCONJ SCONJ PROPN VERB SCONJ SPACE DET ADJ NOUN AUX ADJ PUNCT PRON VERB ADJ NOUN VERB PART VERB PROPN PART SPACE PRON VERB NOUN CCONJ VERB DET NOUN ADP DET ADJ NOUN PUNCT,0.5867768595041323,24.2,4.7727272727272725
115,29,Aman Madaan,"[' Although large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement—i.e., iteratively mapping a candidate output to an improved\none—to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision. Iterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback.', 'For example, when drafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as “Send me the data ASAP”. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to “Hi Ashley, could you please send me the data\nat your earliest convenience?"". When writing code, a programmer may implement an initial “quick\nand dirty” implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks. We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft.', 'Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural\nlanguage and source-code generation. We show that SELF-REFINE outperforms direct generation\nfrom strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,\nSELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code\nmodels such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which\nis easily extensible to other LLMs. In essence, our results show that even when an LLM cannot\ngenerate an optimal output on its first try, the LLM can often provide useful feedback and improve\nits own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs\nfrom a single model without any additional training, via iterative (self-)feedback and refinement']",intro_chunked,"Figure 1 illustrates the
high-level idea, that SELF-REFINE uses the same underlying language model to generate feedback
and refine its outputs. We evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural
language and source-code generation. We show that SELF-REFINE outperforms direct generation
from strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang
et al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks,
SELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code
models such as Codex (code-davinci-002; Chen et al., 2021). We release all of our code, which
is easily extensible to other LLMs. In essence, our results show that even when an LLM cannot
generate an optimal output on its first try, the LLM can often provide useful feedback and improve
its own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs
from a single model without any additional training, via iterative (self-)feedback and refinement",37.65899408284025,30.71625077440773,115,0.6473262310028076," Figure 0 illustrates the 
 high level idea, that Propname Propname uses the same underlying language model to generate feedback 
 and refine its outputs. We evaluate Propname Propname on 0 generation tasks that span diverse domains, including natural 
 language and source code generation. We show that Propname REFINE outperforms direct generation 
 from strong LLMs like Propname 0.0 text davinci Propname and gpt 0.0 turbo; Propname; Propname 
 Propname Propname Propname, 0000 and Propname 0 by 0 00 absolute improvement. In code generation tasks, 
 Propname Propname improves the initial generation by up to absolute 00 when applied to strong code 
 models such as Propname. We release all of our code, which 
 is easily extensible to other LLMs. In essence, our results show that even when an Propname can not 
 generate an optimal output on its first try, the Propname can often provide useful feedback and improve 
 its own output accordingly. In turn, Propname Propname provides an effective way to obtain better outputs 
 from a single model without any additional training, via iterative feedback and refinement"," Figure 0 illustrates the 
 high level idea, that Propname Propname uses the same underlying language model to generate feedback 
 and refine its outputs. We evaluate Propname Propname on 0 generation tasks that span diverse domains, including natural 
 language and source code generation. We show that Propname REFINE outperforms direct generation 
 from strong LLMs like Propname 0.0 text davinci Propname and gpt 0.0 turbo; Propname; Propname 
 Propname Propname Propname, 0000 and Propname 0 by 0 00 absolute improvement. In code generation tasks, 
 Propname Propname improves the initial generation by up to absolute 00 when applied to strong code 
 models such as Propname. We release all of our code, which 
 is easily extensible to other LLMs. In essence, our results show that even when an Propname can not 
 generate an optimal output on its first try, the Propname can often provide useful feedback and improve 
 its own output accordingly. In turn, Propname Propname provides an effective way to obtain better outputs 
 from a single model without any additional training, via iterative feedback and refinement", NOUN NUM VERB DET SPACE ADJ NOUN NOUN PUNCT SCONJ PROPN PROPN VERB DET ADJ ADJ NOUN NOUN PART VERB NOUN SPACE CCONJ VERB PRON NOUN PUNCT PRON VERB PROPN PROPN ADP NUM NOUN NOUN PRON VERB ADJ NOUN PUNCT VERB ADJ SPACE NOUN CCONJ NOUN NOUN NOUN PUNCT PRON VERB SCONJ PROPN VERB VERB ADJ NOUN SPACE ADP ADJ NOUN ADP PROPN NUM NOUN NOUN PROPN CCONJ VERB NUM NOUN PUNCT PROPN PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM CCONJ PROPN NUM ADP NUM NUM ADJ NOUN PUNCT ADP NOUN NOUN NOUN PUNCT SPACE PROPN PROPN VERB DET ADJ NOUN ADP ADP PART VERB NUM SCONJ VERB ADP ADJ NOUN SPACE NOUN ADJ ADP PROPN PUNCT PRON VERB PRON ADP PRON NOUN PUNCT PRON SPACE AUX ADV ADJ ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB SCONJ ADV SCONJ DET PROPN AUX PART SPACE VERB DET ADJ NOUN ADP PRON ADJ NOUN PUNCT DET PROPN AUX ADV VERB ADJ NOUN CCONJ VERB SPACE PRON ADJ NOUN ADV PUNCT ADP NOUN PUNCT PROPN PROPN VERB DET ADJ NOUN PART VERB ADJ NOUN SPACE ADP DET ADJ NOUN ADP DET ADJ NOUN PUNCT ADP ADJ NOUN CCONJ NOUN,0.544973544973545,27.0,4.915343915343915
116,30,Aman Madaan,"[' Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited\nor the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing\ncompiler, which iteratively applies optimizations as an intermediate step in generating machine code (Aho et al.,\n2007). Despite the impressive progress in optimizing compilers, programmers are still generally responsible for\nnumerous performance considerations. Experienced programmers can often shave off microseconds from an\nalready optimized code. However, such improvements typically come after laborious consideration of factors\nsuch as algorithm selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models (LLMs) have shown great potential in a variety of software-related tasks ranging from\ncompetitive programming (Li et al., 2022), code completion/editing (Fried et al., 2022), to clone detection,\ndefect detection, and much more (Lu et al., 2021; Xu et al., 2022; Zhou et al., 2022; Austin et al., 2021). Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the\nefficiency of programs while operating at the level of high-level programming languages like Python or C++? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or\nvery non-trivial to expect from an optimizing compiler or search procedure.', 'If so, large language models may\nhave an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating\na dataset of Performance-Improving Edits, PIE. We collect trajectories of programs written by the same user,\nwhere we track a single programmer’s submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us\nwith pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize\nimpact on the program’s runtime. We show that PIE allows us to improve the efficacy of large language models for code optimization. Specifically,\nwe investigate the effects of using PIE for few-shot prompting on CODEX and fine-tuning models like CODEGEN. We see noticeable improvements in all experiments using PIE. Ultimately, these models can successfully\nperform substantial (up to 2.5ˆ) code optimization for Python and C++ for many examples (up to 50% of the\ntest set). We also demonstrate that CODEGEN and CODEGEN trained on PIE can, in some cases, match the perfromance of CODEX while being up to 10 X smaller in size than CODEX.']",intro_chunked," Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited
or the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing
compiler, which iteratively applies optimizations as an intermediate step in generating machine code (Aho et al.,
2007). Despite the impressive progress in optimizing compilers, programmers are still generally responsible for
numerous performance considerations. Experienced programmers can often shave off microseconds from an
already optimized code. However, such improvements typically come after laborious consideration of factors
such as algorithm selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models (LLMs) have shown great potential in a variety of software-related tasks ranging from
competitive programming (Li et al., 2022), code completion/editing (Fried et al., 2022), to clone detection,
defect detection, and much more (Lu et al., 2021; Xu et al., 2022; Zhou et al., 2022; Austin et al., 2021). Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the
efficiency of programs while operating at the level of high-level programming languages like Python or C++? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or
very non-trivial to expect from an optimizing compiler or search procedure.",20.066158256880755,30.71625077440773,116,0.45342034101486206," Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited 
 or the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing 
 compiler, which iteratively applies optimizations as an intermediate step in generating machine code Propname et Propname Propname, 
 0000. Despite the impressive progress in optimizing compilers, programmers are still generally responsible for 
 numerous performance considerations. Experienced programmers can often shave off microseconds from an 
 already optimized code. However, such improvements typically come after laborious consideration of factors 
 such as Propname selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models have shown great potential in a variety of software related tasks ranging from 
 competitive programming, code completionediting, to clone detection, 
 defect detection, and much more. Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the 
 efficiency of programs while operating at the level of high level programming languages like Propname or C? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or 
 very non trivial to expect from an optimizing compiler or search procedure."," Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited 
 or the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing 
 compiler, which iteratively applies optimizations as an intermediate step in generating machine code Propname et Propname Propname, 
 0000. Despite the impressive progress in optimizing compilers, programmers are still generally responsible for 
 numerous performance considerations. Experienced programmers can often shave off microseconds from an 
 already optimized code. However, such improvements typically come after laborious consideration of factors 
 such as Propname selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models have shown great potential in a variety of software related tasks ranging from 
 competitive programming, code completionediting, to clone detection, 
 defect detection, and much more. Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the 
 efficiency of programs while operating at the level of high level programming languages like Propname or C? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or 
 very non trivial to expect from an optimizing compiler or search procedure.", VERB NOUN AUX DET ADJ NOUN ADP NOUN PUNCT ADV SCONJ ADJ NOUN AUX VERB SPACE CCONJ DET NOUN AUX VERB ADP DET ADJ NOUN PUNCT DET ADJ NOUN ADP VERB NOUN NOUN AUX DET VERB SPACE NOUN PUNCT PRON ADV VERB NOUN ADP DET ADJ NOUN ADP VERB NOUN NOUN PROPN NOUN PROPN PROPN PUNCT SPACE NUM PUNCT SCONJ DET ADJ NOUN ADP VERB NOUN PUNCT NOUN AUX ADV ADV ADJ ADP SPACE ADJ NOUN NOUN PUNCT VERB NOUN AUX ADV VERB ADP NOUN ADP DET SPACE ADV VERB NOUN PUNCT ADV PUNCT ADJ NOUN ADV VERB ADP ADJ NOUN ADP NOUN SPACE ADJ ADP PROPN NOUN PUNCT NOUN NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ VERB NOUN NOUN PUNCT ADJ NOUN NOUN AUX VERB ADJ NOUN ADP DET NOUN ADP NOUN VERB NOUN VERB ADP SPACE ADJ NOUN PUNCT NOUN NOUN PUNCT PART VERB NOUN PUNCT SPACE NOUN NOUN PUNCT CCONJ ADV ADJ PUNCT VERB ADP DET NOUN PUNCT PRON VERB PART VERB CCONJ VERB DET ADJ NOUN PUNCT AUX NOUN VERB DET SPACE NOUN ADP NOUN SCONJ VERB ADP DET NOUN ADP ADJ NOUN NOUN NOUN ADP PROPN CCONJ NOUN PUNCT PRON VERB SCONJ ADJ NOUN NOUN VERB DET NOUN PART VERB NOUN PRON AUX AUX ADJ CCONJ SPACE ADV X ADJ PART VERB ADP DET VERB NOUN CCONJ NOUN NOUN PUNCT,0.6558139534883721,26.875,5.493023255813953
117,31,Aman Madaan,"[' Ensuring efficiency is a crucial aspect of programming, particularly when computational resources are limited\nor the program is utilized at a large scale. The traditional tool for improving program efficiency is the optimizing\ncompiler, which iteratively applies optimizations as an intermediate step in generating machine code (Aho et al.,\n2007). Despite the impressive progress in optimizing compilers, programmers are still generally responsible for\nnumerous performance considerations. Experienced programmers can often shave off microseconds from an\nalready optimized code. However, such improvements typically come after laborious consideration of factors\nsuch as algorithm selection, data structure choice, memory hierarchy, and expected runtime inputs. Large language models (LLMs) have shown great potential in a variety of software-related tasks ranging from\ncompetitive programming (Li et al., 2022), code completion/editing (Fried et al., 2022), to clone detection,\ndefect detection, and much more (Lu et al., 2021; Xu et al., 2022; Zhou et al., 2022; Austin et al., 2021). Motivated by these successes, we seek to ask and answer the following question: can LLMs improve the\nefficiency of programs while operating at the level of high-level programming languages like Python or C++? We hypothesize that large language models have the capacity to propose rewrites that would be impractical or\nvery non-trivial to expect from an optimizing compiler or search procedure.', 'If so, large language models may\nhave an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating\na dataset of Performance-Improving Edits, PIE. We collect trajectories of programs written by the same user,\nwhere we track a single programmer’s submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us\nwith pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize\nimpact on the program’s runtime. We show that PIE allows us to improve the efficacy of large language models for code optimization. Specifically,\nwe investigate the effects of using PIE for few-shot prompting on CODEX and fine-tuning models like CODEGEN. We see noticeable improvements in all experiments using PIE. Ultimately, these models can successfully\nperform substantial (up to 2.5ˆ) code optimization for Python and C++ for many examples (up to 50% of the\ntest set). We also demonstrate that CODEGEN and CODEGEN trained on PIE can, in some cases, match the perfromance of CODEX while being up to 10 X smaller in size than CODEX.']",intro_chunked,"If so, large language models may
have an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating
a dataset of Performance-Improving Edits, PIE. We collect trajectories of programs written by the same user,
where we track a single programmer’s submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us
with pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize
impact on the program’s runtime. We show that PIE allows us to improve the efficacy of large language models for code optimization. Specifically,
we investigate the effects of using PIE for few-shot prompting on CODEX and fine-tuning models like CODEGEN. We see noticeable improvements in all experiments using PIE. Ultimately, these models can successfully
perform substantial (up to 2.5ˆ) code optimization for Python and C++ for many examples (up to 50% of the
test set). We also demonstrate that CODEGEN and CODEGEN trained on PIE can, in some cases, match the perfromance of CODEX while being up to 10 X smaller in size than CODEX.",38.58843434343436,30.71625077440773,117,0.5130863189697266," If so, large language models may 
 have an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating 
 a dataset of Performance Improving Propname, Propname. We collect trajectories of programs written by the same user, 
 where we track a single programmers submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us 
 with pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize 
 impact on the programs runtime. We show that Propname allows us to improve the efficacy of large language models for code optimization. Specifically, 
 we investigate the effects of using PIE for few shot prompting on Propname and fine tuning models like Propname. We see noticeable improvements in all experiments using Propname. Ultimately, these models can successfully 
 perform substantial code optimization for Propname and C for many examples up to 00 of the 
 test set. We also demonstrate that Propname and Propname trained on Propname can, in some cases, match the perfromance of Propname while being up to 00 X smaller in size than Propname."," If so, large language models may 
 have an important role to play in assisting programmers in choosing more desirable refactorings. We set forth to evaluate and improve the capacity of large language models to improve programs by curating 
 a dataset of Performance Improving Propname, Propname. We collect trajectories of programs written by the same user, 
 where we track a single programmers submission, how it evolved over time, and its performance characteristics. Having programs that were written by the same user for the same problem is important because it provides us 
 with pairs of programs that are mostly identical, except for the few, and targeted differences that have an outsize 
 impact on the programs runtime. We show that Propname allows us to improve the efficacy of large language models for code optimization. Specifically, 
 we investigate the effects of using PIE for few shot prompting on Propname and fine tuning models like Propname. We see noticeable improvements in all experiments using Propname. Ultimately, these models can successfully 
 perform substantial code optimization for Propname and C for many examples up to 00 of the 
 test set. We also demonstrate that Propname and Propname trained on Propname can, in some cases, match the perfromance of Propname while being up to 00 X smaller in size than Propname.", SCONJ ADV PUNCT ADJ NOUN NOUN AUX SPACE VERB DET ADJ NOUN PART VERB ADP VERB NOUN ADP VERB ADV ADJ NOUN PUNCT PRON VERB ADV PART VERB CCONJ VERB DET NOUN ADP ADJ NOUN NOUN PART VERB NOUN ADP VERB SPACE DET NOUN ADP NOUN VERB PROPN PUNCT PROPN PUNCT PRON VERB NOUN ADP NOUN VERB ADP DET ADJ NOUN PUNCT SPACE SCONJ PRON VERB DET ADJ NOUN NOUN PUNCT SCONJ PRON VERB ADP NOUN PUNCT CCONJ PRON NOUN NOUN PUNCT VERB NOUN PRON AUX VERB ADP DET ADJ NOUN ADP DET ADJ NOUN AUX ADJ SCONJ PRON VERB PRON SPACE ADP NOUN ADP NOUN PRON AUX ADV ADJ PUNCT SCONJ ADP DET ADJ PUNCT CCONJ VERB NOUN PRON VERB DET NOUN SPACE NOUN ADP DET NOUN ADV PUNCT PRON VERB SCONJ PROPN VERB PRON PART VERB DET NOUN ADP ADJ NOUN NOUN ADP NOUN NOUN PUNCT ADV PUNCT SPACE PRON VERB DET NOUN ADP VERB NOUN ADP ADJ NOUN VERB ADP PROPN CCONJ ADJ NOUN NOUN ADP PROPN PUNCT PRON VERB ADJ NOUN ADP DET NOUN VERB PROPN PUNCT ADV PUNCT DET NOUN AUX ADV SPACE VERB ADJ NOUN NOUN ADP PROPN CCONJ NOUN ADP ADJ NOUN ADP ADP NUM ADP DET SPACE NOUN NOUN PUNCT PRON ADV VERB SCONJ PROPN CCONJ PROPN VERB ADP PROPN AUX PUNCT ADP DET NOUN PUNCT VERB DET NOUN ADP PROPN SCONJ AUX ADP ADP NUM NOUN ADJ ADP NOUN ADP PROPN PUNCT,0.5278969957081545,25.88888888888889,4.819742489270387
118,32,Aman Madaan,"[' The growing capabilities of large pre-trained language models (LLMs) for generating text have enabled their successful application in a variety of\ntasks, including summarization, translation, and\nquestion-answering (Wang et al., 2019; Raffel et al.,\n2019; Brown et al., 2020; Chowdhery et al., 2022). Nevertheless, while employing LLMs for natural language (NL) tasks is straightforward, a major remaining challenge is how to leverage LLMs\nfor structured commonsense reasoning, including\ntasks such as generating event graphs (Tandon et al.,\n2019), reasoning graphs (Madaan et al., 2021a),\nscripts (Sakaguchi et al., 2021), and argument explanation graphs (Saha et al., 2021). Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured\noutput given a natural language input. This family\nof tasks relies on the natural language knowledge\nlearned by the LLM, but it also requires complex\nstructured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format\nof a problem. Specifically, the structure to be generated (e.g., a graph or a table) is converted, or\n“serialized”, into text. Such conversions include\n“flattening” the graph into a list of node pairs (Figure 1d), or into a specification language such as\nDOT (Figure 1c; Gansner et al., 2006).', 'While converting the structured output into text\nhas shown promising results (Rajagopal et al.,\n2021; Madaan and Yang, 2021), LLMs struggle\nto generate these “unnatural” outputs: LMs are\nprimarily pre-trained on free-form text, and these\nserialized structured outputs strongly diverge from\nthe majority of the pre-training data. Further, for\nnatural language, semantically relevant words are\ntypically found within a small span, whereas neighboring nodes in a graph might be pushed farther\napart when representing a graph as a flat string. Thus, a language model which was trained on\nnatural language text is likely to fail to capture\nthe topology of the graph. Consequently, using\nLLMs for graph generation typically requires a\nlarge amount of task-specific training data, and\ntheir generated outputs show structural errors and\nsemantic inconsistencies, which need to be further fixed either manually or by using a secondary\ndownstream model (Madaan et al., 2021b). Despite these struggles, the recent success of\nlarge-language models of code (Code-LLMs; Chen\net al., 2021b; Xu et al., 2022) for tasks such as\ncode generation from natural language (Austin\net al., 2021; Nijkamp et al., 2022), code completion (Fried et al., 2022), and code translation (Wang et al.) show that Code-LLMs are able to perform complex reasoning on structured data such\nas programs.', 'Thus, instead of forcing LLMs of\nnatural language (NL-LLMs) to be fine-tuned on\nstructured commonsense data, an easier way to\nclose the discrepancy between the pre-training data\n(free-form text) and the task-specific data (commonsense reasoning graphs) is to adapt LLMs that\nwere pre-trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language\nmodels of code are good structured commonsense\nreasoners. Further, we show that Code-LLMs can\nbe even better structured reasoners than NL-LLMs,\nwhen converting the desired output graph into a format similar to that observed in the code pre-training\ndata. We call our method COCOGEN: models\nof Code for Commonsense Generation, and it is\ndemonstrated in Figure 1. Our contributions are as follows:\n1. We highlight the insight that Code-LLMs\nare better structured commonsense reasoners\nthan NL-LLMs, when representing the desired\ngraph prediction as code. 2. We propose COCOGEN: a method for\nleveraging LLMs of code for structured\ncommonsense generation. 3. We perform an extensive evaluation across\nthree structured commonsense generation\ntasks and demonstrate that COCOGEN vastly\noutperforms NL-LLMs, either fine-tuned or\nfew-shot tested, while controlling for the num-\nber of downstream task examples. 4. We perform a thorough ablation study, which\nshows the role of data formatting, model size,\nand the number of few-shot examples.']",intro_chunked," The growing capabilities of large pre-trained language models (LLMs) for generating text have enabled their successful application in a variety of
tasks, including summarization, translation, and
question-answering (Wang et al., 2019; Raffel et al.,
2019; Brown et al., 2020; Chowdhery et al., 2022). Nevertheless, while employing LLMs for natural language (NL) tasks is straightforward, a major remaining challenge is how to leverage LLMs
for structured commonsense reasoning, including
tasks such as generating event graphs (Tandon et al.,
2019), reasoning graphs (Madaan et al., 2021a),
scripts (Sakaguchi et al., 2021), and argument explanation graphs (Saha et al., 2021). Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured
output given a natural language input. This family
of tasks relies on the natural language knowledge
learned by the LLM, but it also requires complex
structured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format
of a problem. Specifically, the structure to be generated (e.g., a graph or a table) is converted, or
“serialized”, into text. Such conversions include
“flattening” the graph into a list of node pairs (Figure 1d), or into a specification language such as
DOT (Figure 1c; Gansner et al., 2006).",27.32785714285717,30.71625077440773,118,0.44585639238357544," The growing capabilities of large pre trained language models for generating text have enabled their successful application in a variety of 
 tasks, including summarization, translation, and 
 question answering Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. Nevertheless, while employing LLMs for natural language tasks is straightforward, a major remaining challenge is how to leverage LLMs 
 for structured commonsense reasoning, including 
 tasks such as generating event graphs Propname Propname Propname Propname, 
 0000, reasoning graphs, 
 scripts, and argument explanation graphs. Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured 
 output given a natural language input. This family 
 of tasks relies on the natural language knowledge 
 learned by the Propname, but it also requires complex 
 structured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format 
 of a problem. Specifically, the structure to be generated is converted, or 
 serialized, into text. Such conversions include 
 flattening the graph into a list of Propname pairs, or into a specification language such as 
 Propname."," The growing capabilities of large pre trained language models for generating text have enabled their successful application in a variety of 
 tasks, including summarization, translation, and 
 question answering Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. Nevertheless, while employing LLMs for natural language tasks is straightforward, a major remaining challenge is how to leverage LLMs 
 for structured commonsense reasoning, including 
 tasks such as generating event graphs Propname Propname Propname Propname, 
 0000, reasoning graphs, 
 scripts, and argument explanation graphs. Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured 
 output given a natural language input. This family 
 of tasks relies on the natural language knowledge 
 learned by the Propname, but it also requires complex 
 structured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format 
 of a problem. Specifically, the structure to be generated is converted, or 
 serialized, into text. Such conversions include 
 flattening the graph into a list of Propname pairs, or into a specification language such as 
 Propname.", DET VERB NOUN ADP ADJ ADJ VERB NOUN NOUN ADP NOUN NOUN AUX VERB PRON ADJ NOUN ADP DET NOUN ADP SPACE NOUN PUNCT VERB NOUN PUNCT NOUN PUNCT CCONJ SPACE NOUN VERB PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADV PUNCT SCONJ VERB NOUN ADP ADJ NOUN NOUN AUX ADJ PUNCT DET ADJ VERB NOUN AUX SCONJ PART VERB NOUN SPACE ADP ADJ NOUN NOUN PUNCT VERB SPACE NOUN ADJ ADP VERB NOUN NOUN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT NOUN NOUN PUNCT SPACE NOUN PUNCT CCONJ NOUN NOUN NOUN PUNCT ADP ADJ NOUN NOUN NOUN ADJ ADP VERB NOUN CCONJ NOUN NOUN PUNCT ADJ NOUN VERB PART VERB VERB SPACE NOUN VERB DET ADJ NOUN NOUN PUNCT DET NOUN SPACE ADP NOUN VERB ADP DET ADJ NOUN NOUN SPACE VERB ADP DET PROPN PUNCT CCONJ PRON ADV VERB ADJ SPACE ADJ NOUN CCONJ NOUN PUNCT PART VERB NOUN PUNCT VERB ADJ NOUN NOUN NOUN VERB DET NOUN NOUN SPACE ADP DET NOUN PUNCT ADV PUNCT DET NOUN PART AUX VERB AUX VERB PUNCT CCONJ SPACE VERB PUNCT ADP NOUN PUNCT ADJ NOUN VERB SPACE VERB DET NOUN ADP DET NOUN ADP PROPN NOUN PUNCT CCONJ ADP DET NOUN NOUN ADJ ADP SPACE PROPN PUNCT,0.46511627906976744,30.714285714285715,5.497674418604651
119,33,Aman Madaan,"[' The growing capabilities of large pre-trained language models (LLMs) for generating text have enabled their successful application in a variety of\ntasks, including summarization, translation, and\nquestion-answering (Wang et al., 2019; Raffel et al.,\n2019; Brown et al., 2020; Chowdhery et al., 2022). Nevertheless, while employing LLMs for natural language (NL) tasks is straightforward, a major remaining challenge is how to leverage LLMs\nfor structured commonsense reasoning, including\ntasks such as generating event graphs (Tandon et al.,\n2019), reasoning graphs (Madaan et al., 2021a),\nscripts (Sakaguchi et al., 2021), and argument explanation graphs (Saha et al., 2021). Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured\noutput given a natural language input. This family\nof tasks relies on the natural language knowledge\nlearned by the LLM, but it also requires complex\nstructured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format\nof a problem. Specifically, the structure to be generated (e.g., a graph or a table) is converted, or\n“serialized”, into text. Such conversions include\n“flattening” the graph into a list of node pairs (Figure 1d), or into a specification language such as\nDOT (Figure 1c; Gansner et al., 2006).', 'While converting the structured output into text\nhas shown promising results (Rajagopal et al.,\n2021; Madaan and Yang, 2021), LLMs struggle\nto generate these “unnatural” outputs: LMs are\nprimarily pre-trained on free-form text, and these\nserialized structured outputs strongly diverge from\nthe majority of the pre-training data. Further, for\nnatural language, semantically relevant words are\ntypically found within a small span, whereas neighboring nodes in a graph might be pushed farther\napart when representing a graph as a flat string. Thus, a language model which was trained on\nnatural language text is likely to fail to capture\nthe topology of the graph. Consequently, using\nLLMs for graph generation typically requires a\nlarge amount of task-specific training data, and\ntheir generated outputs show structural errors and\nsemantic inconsistencies, which need to be further fixed either manually or by using a secondary\ndownstream model (Madaan et al., 2021b). Despite these struggles, the recent success of\nlarge-language models of code (Code-LLMs; Chen\net al., 2021b; Xu et al., 2022) for tasks such as\ncode generation from natural language (Austin\net al., 2021; Nijkamp et al., 2022), code completion (Fried et al., 2022), and code translation (Wang et al.) show that Code-LLMs are able to perform complex reasoning on structured data such\nas programs.', 'Thus, instead of forcing LLMs of\nnatural language (NL-LLMs) to be fine-tuned on\nstructured commonsense data, an easier way to\nclose the discrepancy between the pre-training data\n(free-form text) and the task-specific data (commonsense reasoning graphs) is to adapt LLMs that\nwere pre-trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language\nmodels of code are good structured commonsense\nreasoners. Further, we show that Code-LLMs can\nbe even better structured reasoners than NL-LLMs,\nwhen converting the desired output graph into a format similar to that observed in the code pre-training\ndata. We call our method COCOGEN: models\nof Code for Commonsense Generation, and it is\ndemonstrated in Figure 1. Our contributions are as follows:\n1. We highlight the insight that Code-LLMs\nare better structured commonsense reasoners\nthan NL-LLMs, when representing the desired\ngraph prediction as code. 2. We propose COCOGEN: a method for\nleveraging LLMs of code for structured\ncommonsense generation. 3. We perform an extensive evaluation across\nthree structured commonsense generation\ntasks and demonstrate that COCOGEN vastly\noutperforms NL-LLMs, either fine-tuned or\nfew-shot tested, while controlling for the num-\nber of downstream task examples. 4. We perform a thorough ablation study, which\nshows the role of data formatting, model size,\nand the number of few-shot examples.']",intro_chunked,"While converting the structured output into text
has shown promising results (Rajagopal et al.,
2021; Madaan and Yang, 2021), LLMs struggle
to generate these “unnatural” outputs: LMs are
primarily pre-trained on free-form text, and these
serialized structured outputs strongly diverge from
the majority of the pre-training data. Further, for
natural language, semantically relevant words are
typically found within a small span, whereas neighboring nodes in a graph might be pushed farther
apart when representing a graph as a flat string. Thus, a language model which was trained on
natural language text is likely to fail to capture
the topology of the graph. Consequently, using
LLMs for graph generation typically requires a
large amount of task-specific training data, and
their generated outputs show structural errors and
semantic inconsistencies, which need to be further fixed either manually or by using a secondary
downstream model (Madaan et al., 2021b). Despite these struggles, the recent success of
large-language models of code (Code-LLMs; Chen
et al., 2021b; Xu et al., 2022) for tasks such as
code generation from natural language (Austin
et al., 2021; Nijkamp et al., 2022), code completion (Fried et al., 2022), and code translation (Wang et al.) show that Code-LLMs are able to perform complex reasoning on structured data such
as programs.",30.9027987421384,30.71625077440773,119,0.24742825329303741," While converting the structured output into text 
 has shown promising results Propname Propname Propname Propname, 
 0000; Propname and Propname, 0000, LLMs struggle 
 to generate these unnatural outputs: LMs are 
 primarily pre trained on free form text, and these 
 serialized structured outputs strongly diverge from 
 the majority of the pre training data. Further, for 
 natural language, semantically relevant words are 
 typically found within a small span, whereas neighboring nodes in a graph might be pushed farther 
 apart when representing a graph as a flat string. Thus, a language model which was trained on 
 natural language text is likely to fail to capture 
 the topology of the graph. Consequently, using 
 LLMs for graph generation typically requires a 
 large amount of task specific training data, and 
 their generated outputs show structural errors and 
 semantic inconsistencies, which need to be further fixed either manually or by using a secondary 
 downstream model. Despite these struggles, the recent success of 
 large language models of code Propname Propname; Propname 
 Propname Propname Propname, 0000b; Propname Propname Propname Propname, 0000 for tasks such as 
 code generation from natural language Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, code completion, and code translation show that Propname LLMs are able to perform complex reasoning on structured data such 
 as programs."," While converting the structured output into text 
 has shown promising results Propname Propname Propname Propname, 
 0000; Propname and Propname, 0000, LLMs struggle 
 to generate these unnatural outputs: LMs are 
 primarily pre trained on free form text, and these 
 serialized structured outputs strongly diverge from 
 the majority of the pre training data. Further, for 
 natural language, semantically relevant words are 
 typically found within a small span, whereas neighboring nodes in a graph might be pushed farther 
 apart when representing a graph as a flat string. Thus, a language model which was trained on 
 natural language text is likely to fail to capture 
 the topology of the graph. Consequently, using 
 LLMs for graph generation typically requires a 
 large amount of task specific training data, and 
 their generated outputs show structural errors and 
 semantic inconsistencies, which need to be further fixed either manually or by using a secondary 
 downstream model. Despite these struggles, the recent success of 
 large language models of code Propname Propname; Propname 
 Propname Propname Propname, 0000b; Propname Propname Propname Propname, 0000 for tasks such as 
 code generation from natural language Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, code completion, and code translation show that Propname LLMs are able to perform complex reasoning on structured data such 
 as programs.", SCONJ VERB DET ADJ NOUN ADP NOUN SPACE AUX VERB ADJ NOUN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN CCONJ PROPN PUNCT NUM PUNCT NOUN VERB SPACE PART VERB DET ADJ NOUN PUNCT NOUN AUX SPACE ADV VERB VERB ADP ADJ NOUN NOUN PUNCT CCONJ DET SPACE VERB ADJ NOUN ADV VERB ADP SPACE DET NOUN ADP DET ADJ NOUN NOUN PUNCT ADV PUNCT ADP SPACE ADJ NOUN PUNCT ADV ADJ NOUN AUX SPACE ADV VERB ADP DET ADJ NOUN PUNCT SCONJ NOUN NOUN ADP DET NOUN AUX AUX VERB ADV SPACE ADV SCONJ VERB DET NOUN ADP DET ADJ NOUN PUNCT ADV PUNCT DET NOUN NOUN PRON AUX VERB ADP SPACE ADJ NOUN NOUN AUX ADJ PART VERB PART VERB SPACE DET NOUN ADP DET NOUN PUNCT ADV PUNCT VERB SPACE NOUN ADP NOUN NOUN ADV VERB DET SPACE ADJ NOUN ADP NOUN ADJ NOUN NOUN PUNCT CCONJ SPACE PRON VERB NOUN VERB ADJ NOUN CCONJ SPACE ADJ NOUN PUNCT PRON VERB PART AUX ADV VERB CCONJ ADV CCONJ ADP VERB DET ADJ SPACE ADJ NOUN PUNCT SCONJ DET NOUN PUNCT DET ADJ NOUN ADP SPACE ADJ NOUN NOUN ADP NOUN PROPN PROPN PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM ADP NOUN ADJ ADP SPACE NOUN NOUN ADP ADJ NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT NOUN NOUN PUNCT CCONJ NOUN NOUN VERB SCONJ PROPN NOUN AUX ADJ PART VERB ADJ NOUN ADP ADJ NOUN ADJ SPACE ADP NOUN PUNCT,0.497907949790795,47.8,5.083682008368201
120,34,Aman Madaan,"[' The growing capabilities of large pre-trained language models (LLMs) for generating text have enabled their successful application in a variety of\ntasks, including summarization, translation, and\nquestion-answering (Wang et al., 2019; Raffel et al.,\n2019; Brown et al., 2020; Chowdhery et al., 2022). Nevertheless, while employing LLMs for natural language (NL) tasks is straightforward, a major remaining challenge is how to leverage LLMs\nfor structured commonsense reasoning, including\ntasks such as generating event graphs (Tandon et al.,\n2019), reasoning graphs (Madaan et al., 2021a),\nscripts (Sakaguchi et al., 2021), and argument explanation graphs (Saha et al., 2021). Unlike traditional commonsense reasoning tasks such as reading comprehension or question answering, structured commonsense aims to generate structured\noutput given a natural language input. This family\nof tasks relies on the natural language knowledge\nlearned by the LLM, but it also requires complex\nstructured prediction and generation. To leverage LLMs, existing structured commonsense generation models modify the output format\nof a problem. Specifically, the structure to be generated (e.g., a graph or a table) is converted, or\n“serialized”, into text. Such conversions include\n“flattening” the graph into a list of node pairs (Figure 1d), or into a specification language such as\nDOT (Figure 1c; Gansner et al., 2006).', 'While converting the structured output into text\nhas shown promising results (Rajagopal et al.,\n2021; Madaan and Yang, 2021), LLMs struggle\nto generate these “unnatural” outputs: LMs are\nprimarily pre-trained on free-form text, and these\nserialized structured outputs strongly diverge from\nthe majority of the pre-training data. Further, for\nnatural language, semantically relevant words are\ntypically found within a small span, whereas neighboring nodes in a graph might be pushed farther\napart when representing a graph as a flat string. Thus, a language model which was trained on\nnatural language text is likely to fail to capture\nthe topology of the graph. Consequently, using\nLLMs for graph generation typically requires a\nlarge amount of task-specific training data, and\ntheir generated outputs show structural errors and\nsemantic inconsistencies, which need to be further fixed either manually or by using a secondary\ndownstream model (Madaan et al., 2021b). Despite these struggles, the recent success of\nlarge-language models of code (Code-LLMs; Chen\net al., 2021b; Xu et al., 2022) for tasks such as\ncode generation from natural language (Austin\net al., 2021; Nijkamp et al., 2022), code completion (Fried et al., 2022), and code translation (Wang et al.) show that Code-LLMs are able to perform complex reasoning on structured data such\nas programs.', 'Thus, instead of forcing LLMs of\nnatural language (NL-LLMs) to be fine-tuned on\nstructured commonsense data, an easier way to\nclose the discrepancy between the pre-training data\n(free-form text) and the task-specific data (commonsense reasoning graphs) is to adapt LLMs that\nwere pre-trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language\nmodels of code are good structured commonsense\nreasoners. Further, we show that Code-LLMs can\nbe even better structured reasoners than NL-LLMs,\nwhen converting the desired output graph into a format similar to that observed in the code pre-training\ndata. We call our method COCOGEN: models\nof Code for Commonsense Generation, and it is\ndemonstrated in Figure 1. Our contributions are as follows:\n1. We highlight the insight that Code-LLMs\nare better structured commonsense reasoners\nthan NL-LLMs, when representing the desired\ngraph prediction as code. 2. We propose COCOGEN: a method for\nleveraging LLMs of code for structured\ncommonsense generation. 3. We perform an extensive evaluation across\nthree structured commonsense generation\ntasks and demonstrate that COCOGEN vastly\noutperforms NL-LLMs, either fine-tuned or\nfew-shot tested, while controlling for the num-\nber of downstream task examples. 4. We perform a thorough ablation study, which\nshows the role of data formatting, model size,\nand the number of few-shot examples.']",intro_chunked,"Thus, instead of forcing LLMs of
natural language (NL-LLMs) to be fine-tuned on
structured commonsense data, an easier way to
close the discrepancy between the pre-training data
(free-form text) and the task-specific data (commonsense reasoning graphs) is to adapt LLMs that
were pre-trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language
models of code are good structured commonsense
reasoners. Further, we show that Code-LLMs can
be even better structured reasoners than NL-LLMs,
when converting the desired output graph into a format similar to that observed in the code pre-training
data. We call our method COCOGEN: models
of Code for Commonsense Generation, and it is
demonstrated in Figure 1. Our contributions are as follows:
1. We highlight the insight that Code-LLMs
are better structured commonsense reasoners
than NL-LLMs, when representing the desired
graph prediction as code. 2. We propose COCOGEN: a method for
leveraging LLMs of code for structured
commonsense generation. 3. We perform an extensive evaluation across
three structured commonsense generation
tasks and demonstrate that COCOGEN vastly
outperforms NL-LLMs, either fine-tuned or
few-shot tested, while controlling for the num-
ber of downstream task examples. 4. We perform a thorough ablation study, which
shows the role of data formatting, model size,
and the number of few-shot examples.",40.6979554263566,30.71625077440773,120,0.7667973637580872," Thus, instead of forcing LLMs of 
 natural language to be fine tuned on 
 structured commonsense data, an easier way to 
 close the discrepancy between the pre training data and the task specific data is to adapt LLMs that 
 were pre trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language 
 models of code are good structured commonsense 
 reasoners. Further, we show that Propname LLMs can 
 be even better structured reasoners than Propname LLMs, 
 when converting the desired output graph into a format similar to that observed in the code pre training 
 data. We call our method COCOGEN: models 
 of Propname for Propname Propname, and it is 
 demonstrated in Propname 0. Our contributions are as follows: 
0. We highlight the insight that Propname LLMs 
 are better structured commonsense reasoners 
 than Propname LLMs, when representing the desired 
 graph prediction as code.0. We propose Propname: a method for 
 leveraging LLMs of code for structured 
 commonsense generation.0. We perform an extensive evaluation across 
 three structured commonsense generation 
 tasks and demonstrate that Propname vastly 
 outperforms Propname LLMs, either fine tuned or 
 few shot tested, while controlling for the num ber of downstream task examples.0. We perform a thorough ablation study, which 
 shows the role of data formatting, model size, 
 and the number of few shot examples."," Thus, instead of forcing LLMs of 
 natural language to be fine tuned on 
 structured commonsense data, an easier way to 
 close the discrepancy between the pre training data and the task specific data is to adapt LLMs that 
 were pre trained on code to structured commonsense reasoning in natural language. Thus, our main insight is that large language 
 models of code are good structured commonsense 
 reasoners. Further, we show that Propname LLMs can 
 be even better structured reasoners than Propname LLMs, 
 when converting the desired output graph into a format similar to that observed in the code pre training 
 data. We call our method COCOGEN: models 
 of Propname for Propname Propname, and it is 
 demonstrated in Propname 0. Our contributions are as follows: 
0. We highlight the insight that Propname LLMs 
 are better structured commonsense reasoners 
 than Propname LLMs, when representing the desired 
 graph prediction as code.0. We propose Propname: a method for 
 leveraging LLMs of code for structured 
 commonsense generation.0. We perform an extensive evaluation across 
 three structured commonsense generation 
 tasks and demonstrate that Propname vastly 
 outperforms Propname LLMs, either fine tuned or 
 few shot tested, while controlling for the num ber of downstream task examples.0. We perform a thorough ablation study, which 
 shows the role of data formatting, model size, 
 and the number of few shot examples.", ADV PUNCT ADV ADP VERB NOUN ADP SPACE ADJ NOUN PART AUX ADV VERB ADP SPACE ADJ NOUN NOUN PUNCT DET ADJ NOUN PART SPACE VERB DET NOUN ADP DET ADJ NOUN NOUN CCONJ DET NOUN ADJ NOUN AUX PART VERB NOUN PRON SPACE AUX VERB VERB ADP NOUN ADP ADJ NOUN NOUN ADP ADJ NOUN PUNCT ADV PUNCT PRON ADJ NOUN AUX SCONJ ADJ NOUN SPACE NOUN ADP NOUN AUX ADJ ADJ NOUN SPACE NOUN PUNCT ADV PUNCT PRON VERB SCONJ PROPN NOUN AUX SPACE AUX ADV ADV ADJ NOUN ADP PROPN NOUN PUNCT SPACE SCONJ VERB DET VERB NOUN NOUN ADP DET NOUN ADJ ADP PRON VERB ADP DET NOUN NOUN NOUN SPACE NOUN PUNCT PRON VERB PRON NOUN NOUN PUNCT NOUN SPACE ADP PROPN ADP PROPN PROPN PUNCT CCONJ PRON AUX SPACE VERB ADP PROPN NUM PUNCT PRON NOUN AUX SCONJ VERB PUNCT SPACE PUNCT PUNCT PRON VERB DET NOUN PRON PROPN NOUN SPACE AUX ADJ ADJ NOUN NOUN SPACE ADP PROPN NOUN PUNCT SCONJ VERB DET VERB SPACE NOUN NOUN ADP NOUN PUNCT PUNCT PUNCT PRON VERB PROPN PUNCT DET NOUN ADP SPACE VERB NOUN ADP NOUN ADP ADJ SPACE NOUN NOUN PUNCT PUNCT PUNCT PRON VERB DET ADJ NOUN ADP SPACE NUM ADJ NOUN NOUN SPACE NOUN CCONJ VERB SCONJ PROPN ADV SPACE VERB PROPN NOUN PUNCT CCONJ ADV VERB CCONJ SPACE ADJ NOUN VERB PUNCT SCONJ VERB ADP DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT PUNCT PUNCT PRON VERB DET ADJ NOUN NOUN PUNCT PRON SPACE VERB DET NOUN ADP NOUN ADJ PUNCT NOUN NOUN PUNCT SPACE CCONJ DET NOUN ADP ADJ NOUN NOUN PUNCT,0.4732510288065844,27.0,4.872427983539095
121,35,Aman Madaan,"[' The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.', '(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),\ndeliberately sketched as controlled studies. First, we identify key components of an example in few-shot\nprompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping\nall but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).', 'Finally, we elicit\nmeaningful findings via conducting a systematic and qualitative analysis of the performance divergence\nbetween different prompt queries. Our experiments on four diverse reasoning tasks and across three large\nlanguage models—PaLM, GPT-3, and CODEX, reveal several surprising findings:\n- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In\naddition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the\ncorrectness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute\nchiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain\ncorrect outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays\na vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense\nknowledge), and patterns help reinforce task understanding, enabling the language model to generate text\nthat helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this\ninterplay between text and patterns—COT helps a language model in imitating the prompt and generating\nthe right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated\nby applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key\nrole in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a\nset of key design principles is an important challenge. To this end, we distill our findings to create concise\nprompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without\nnegative repercussions on the task solve rate.']",intro_chunked," The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.",58.52096283783787,30.71625077440773,121,0.5116338729858398," The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence. This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few shot learning. Few shot learning has shown promising applications for a wide range of tasks. While beneficial, this setting requires meticulous design of prompts. Ling Propname Propname. pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Propname Propname Propname."," The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence. This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few shot learning. Few shot learning has shown promising applications for a wide range of tasks. While beneficial, this setting requires meticulous design of prompts. Ling Propname Propname. pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Propname Propname Propname.", DET NOUN PART VERB DET ADV ADJ NOUN ADP VERB DET ADJ NOUN AUX NUM ADP DET NOUN ADP ADJ NOUN PUNCT PRON AUX ADP ADJ NOUN ADP ADJ ADJ NOUN NOUN PUNCT PRON ADV VERB ADP DET ADJ VERB NOUN ADP NOUN PUNCT ADV PUNCT ADJ NOUN NOUN AUX VERB ADJ NOUN ADP VERB DET NOUN PART VERB DET NOUN PUNCT ADP DET ADJ NOUN PUNCT ADV VERB ADP ADJ NOUN NOUN PUNCT ADJ NOUN NOUN AUX VERB ADJ NOUN ADP DET ADJ NOUN ADP NOUN PUNCT SCONJ ADJ PUNCT DET NOUN VERB ADJ NOUN ADP NOUN PUNCT VERB PROPN PROPN PUNCT VERB DET NOUN ADP VERB ADJ NOUN NOUN ADP DET ADJ NOUN ADP NOUN PART VERB NOUN NOUN ADP ADJ NOUN PUNCT ADV PUNCT PROPN PROPN PROPN PUNCT,0.6564885496183206,16.375,5.106870229007634
122,36,Aman Madaan,"[' The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.', '(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),\ndeliberately sketched as controlled studies. First, we identify key components of an example in few-shot\nprompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping\nall but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).', 'Finally, we elicit\nmeaningful findings via conducting a systematic and qualitative analysis of the performance divergence\nbetween different prompt queries. Our experiments on four diverse reasoning tasks and across three large\nlanguage models—PaLM, GPT-3, and CODEX, reveal several surprising findings:\n- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In\naddition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the\ncorrectness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute\nchiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain\ncorrect outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays\na vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense\nknowledge), and patterns help reinforce task understanding, enabling the language model to generate text\nthat helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this\ninterplay between text and patterns—COT helps a language model in imitating the prompt and generating\nthe right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated\nby applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key\nrole in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a\nset of key design principles is an important challenge. To this end, we distill our findings to create concise\nprompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without\nnegative repercussions on the task solve rate.']",intro_chunked,"(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),
deliberately sketched as controlled studies. First, we identify key components of an example in few-shot
prompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping
all but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).",43.03080551523951,30.71625077440773,122,0.27030861377716064," proposed chain of thought prompting, showing that the few shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks. Despite its wide range usage, the rationale behind the success of Propname remains unclear. Recent work draws parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind Propname. Contributions and findings. We construct a series of tailored counterfactual prompts, 
 deliberately sketched as controlled studies. First, we identify key components of an example in few shot 
 prompting as follows: Propname, Propname, and Propname. Next, we perform counterfactual promptingkeeping 
 all but one component fixed with Greek alphabets."," proposed chain of thought prompting, showing that the few shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks. Despite its wide range usage, the rationale behind the success of Propname remains unclear. Recent work draws parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind Propname. Contributions and findings. We construct a series of tailored counterfactual prompts, 
 deliberately sketched as controlled studies. First, we identify key components of an example in few shot 
 prompting as follows: Propname, Propname, and Propname. Next, we perform counterfactual promptingkeeping 
 all but one component fixed with Greek alphabets.", VERB NOUN ADP NOUN VERB PUNCT VERB SCONJ DET ADJ NOUN NOUN ADP NOUN ADV VERB ADP ADJ ADJ NOUN NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT SCONJ PRON ADJ NOUN NOUN PUNCT DET NOUN ADP DET NOUN ADP PROPN VERB ADJ PUNCT ADJ NOUN VERB NOUN ADP ADJ NOUN PUNCT NOUN ADV VERB ADP DET NOUN ADP VERB DET NOUN PUNCT ADJ ADP DET NOUN PUNCT PRON AUX VERB SCONJ NOUN AUX ADV AUX ADJ PART VERB DET ADJ NOUN PUNCT SCONJ ADJ PUNCT ADJ ADJ ADJ NOUN VERB ADJ ADP VERB SCONJ PUNCT SCONJ PUNCT CCONJ SCONJ DET NOUN VERB PUNCT ADV PUNCT NOUN AUX VERB PART VERB DET ADJ ADJ NOUN ADP DET VERB NOUN PUNCT ADV PUNCT PRON VERB ADV DET ADJ NOUN ADP PRON NOUN CCONJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB ADJ NOUN ADP VERB DET NOUN ADP PROPN PUNCT NOUN CCONJ NOUN PUNCT PRON VERB DET NOUN ADP VERB ADJ NOUN PUNCT SPACE ADV VERB ADP VERB NOUN PUNCT ADV PUNCT PRON VERB ADJ NOUN ADP DET NOUN ADP ADJ NOUN SPACE NOUN SCONJ VERB PUNCT PROPN PUNCT PROPN PUNCT CCONJ PROPN PUNCT ADV PUNCT PRON VERB ADJ NOUN SPACE DET CCONJ NUM NOUN VERB ADP ADJ NOUN PUNCT,0.6666666666666666,15.923076923076923,5.091787439613527
123,37,Aman Madaan,"[' The ability to learn a previously unseen task by observing a few examples is one of the cornerstones of human intelligence (Lake et al., 2017). This is in stark contrast with modern deep learning methods, which typically rely on a substantial labeled corpus of data. Recently, large language models (LLMs) (Chowdhery et al., 2022; Brown et al., 2020; Chen et al., 2021a) have demonstrated remarkable performance in employing a prompt to perform a task, with no additional finetuning, commonly known as few-shot learning. Few-shot learning has shown promising applications for a wide range of tasks (Gehrmann et al., 2021; Wei et al., 2021; Sanh et al., 2021; Thoppilan et al., 2022; Liu et al., 2021a; Reif et al., 2021; Wang et al., 2020; Chen et al., 2021b; Lewkowycz et al., 2022; Wu et al., 2022). While beneficial, this setting requires meticulous design of prompts (Le Scao & Rush, 2021; Liu et al., 2021c; Mishra et al., 2021). Ling et al. (2017) pioneered the idea of using natural language rationales as the intermediate steps in prompts to help model performance for mathematical reasoning. Recently, Wei et al.', '(2022) proposed chain of thought (COT) prompting, showing that the few-shot setting in LLMs similarly benefits from intermediate natural language rationale across a range of complex reasoning tasks (Ling et al., 2017; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022). Despite its wide-range usage, the rationale behind the success of COT remains unclear. Recent work draws (Ling et al., 2017; Wei et al., 2022) parallels to human thinking. Humans often think about a problem before deducing a solution. Akin to this process, it is argued that models should also be able to employ a similar mechanism. While intuitive, such restrictive abstract explanations fall short in explaining why, when, and how these mechanisms operate. Ultimately, LLMs are trained to estimate the next token distribution for a given context. Therefore, there is presumably a systematic rationale behind their successes and failures. In this work, we undertake initial steps towards understanding the mechanism behind COT. Contributions and findings. We construct a series of tailored counterfactual prompts (Goyal et al., 2019),\ndeliberately sketched as controlled studies. First, we identify key components of an example in few-shot\nprompting as follows: Symbols, Patterns, and Text. Next, we perform counterfactual prompting—keeping\nall but one component fixed (e.g., replacing symbols (numbers) with Greek alphabets).', 'Finally, we elicit\nmeaningful findings via conducting a systematic and qualitative analysis of the performance divergence\nbetween different prompt queries. Our experiments on four diverse reasoning tasks and across three large\nlanguage models—PaLM, GPT-3, and CODEX, reveal several surprising findings:\n- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In\naddition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the\ncorrectness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute\nchiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain\ncorrect outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays\na vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense\nknowledge), and patterns help reinforce task understanding, enabling the language model to generate text\nthat helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this\ninterplay between text and patterns—COT helps a language model in imitating the prompt and generating\nthe right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated\nby applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key\nrole in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a\nset of key design principles is an important challenge. To this end, we distill our findings to create concise\nprompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without\nnegative repercussions on the task solve rate.']",intro_chunked,"Finally, we elicit
meaningful findings via conducting a systematic and qualitative analysis of the performance divergence
between different prompt queries. Our experiments on four diverse reasoning tasks and across three large
language models—PaLM, GPT-3, and CODEX, reveal several surprising findings:
- We find that the exact type of symbols in the prompt virtually does not affect the model performance. In
addition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the
correctness of symbols and patterns is immaterial to the task solve rate. - We learn that patterns contribute
chiefly as a venue to reinforce task understanding (Ouyang et al., 2022) and prompt the model to attain
correct outputs. -Most importantly, we find that text and patterns form a symbiotic relationship that plays
a vital role in the success of COT. Text helps generate useful patterns (e.g., by extracting commonsense
knowledge), and patterns help reinforce task understanding, enabling the language model to generate text
that helps solve the task. Overall, we argue that one of the primary reasons behind the success of COT is this
interplay between text and patterns—COT helps a language model in imitating the prompt and generating
the right tokens for the task—and is conceivably less related to their reasoning abilities. Finally, as indicated
by applications such as PaLM-SAYCAN (Ahn et al., 2022), we posit that techniques like COT will play a key
role in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a
set of key design principles is an important challenge. To this end, we distill our findings to create concise
prompting, dubbed CCOT. CCOT prunes the prompt (20% Ó) to only retain indispensable tokens without
negative repercussions on the task solve rate.",39.816479591836725,30.71625077440773,123,0.6141576766967773," Finally, we elicit 
 meaningful findings via conducting a systematic and qualitative analysis of the performance divergence 
 between different prompt queries. Our experiments on four diverse reasoning tasks and across three large 
 language Propname, Propname 0, and Propname, reveal several surprising findings: We find that the exact type of symbols in the prompt virtually does not affect the model performance. In 
 addition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the 
 correctness of symbols and patterns is immaterial to the task solve rate. We learn that patterns contribute 
 chiefly as a venue to reinforce task understanding and prompt the model to attain 
 correct outputs. Most importantly, we find that text and patterns form a symbiotic relationship that plays 
 a vital role in the success of Propname. Text helps generate useful patterns Propname, by extracting commonsense 
 knowledge, and patterns help reinforce task understanding, enabling the language model to generate text 
 that helps solve the task. Overall, we argue that one of the primary reasons behind the success of Propname is this 
 interplay between text and patternsCOT helps a language model in imitating the prompt and generating 
 the right tokens for the taskand is conceivably less related to their reasoning abilities. Finally, as indicated 
 by applications such as Propname Propname, we posit that techniques like Propname will play a key 
 role in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a 
 set of key design principles is an important challenge. To this end, we distill our findings to create concise 
 prompting, dubbed Propname. CCOT prunes the prompt to only retain indispensable tokens without 
 negative repercussions on the task solve rate."," Finally, we elicit 
 meaningful findings via conducting a systematic and qualitative analysis of the performance divergence 
 between different prompt queries. Our experiments on four diverse reasoning tasks and across three large 
 language Propname, Propname 0, and Propname, reveal several surprising findings: We find that the exact type of symbols in the prompt virtually does not affect the model performance. In 
 addition, our results and analysis demonstrate counterintuitive phenomena. For example, we identify that the 
 correctness of symbols and patterns is immaterial to the task solve rate. We learn that patterns contribute 
 chiefly as a venue to reinforce task understanding and prompt the model to attain 
 correct outputs. Most importantly, we find that text and patterns form a symbiotic relationship that plays 
 a vital role in the success of Propname. Text helps generate useful patterns Propname, by extracting commonsense 
 knowledge, and patterns help reinforce task understanding, enabling the language model to generate text 
 that helps solve the task. Overall, we argue that one of the primary reasons behind the success of Propname is this 
 interplay between text and patternsCOT helps a language model in imitating the prompt and generating 
 the right tokens for the taskand is conceivably less related to their reasoning abilities. Finally, as indicated 
 by applications such as Propname Propname, we posit that techniques like Propname will play a key 
 role in enabling the success of LLMs on diverse use cases. Thus, designing efficient prompts informed by a 
 set of key design principles is an important challenge. To this end, we distill our findings to create concise 
 prompting, dubbed Propname. CCOT prunes the prompt to only retain indispensable tokens without 
 negative repercussions on the task solve rate.", ADV PUNCT PRON VERB SPACE ADJ NOUN ADP VERB DET ADJ CCONJ ADJ NOUN ADP DET NOUN NOUN SPACE ADP ADJ ADJ NOUN PUNCT PRON NOUN ADP NUM ADJ NOUN NOUN CCONJ ADP NUM ADJ SPACE NOUN PROPN PUNCT PROPN NUM PUNCT CCONJ PROPN PUNCT VERB ADJ ADJ NOUN PUNCT PRON VERB SCONJ DET ADJ NOUN ADP NOUN ADP DET NOUN ADV AUX PART VERB DET NOUN NOUN PUNCT ADP SPACE NOUN PUNCT PRON NOUN CCONJ NOUN VERB ADJ NOUN PUNCT ADP NOUN PUNCT PRON VERB SCONJ DET SPACE NOUN ADP NOUN CCONJ NOUN AUX ADJ ADP DET NOUN NOUN NOUN PUNCT PRON VERB SCONJ NOUN VERB SPACE NOUN ADP DET NOUN PART VERB NOUN NOUN CCONJ VERB DET NOUN PART VERB SPACE ADJ NOUN PUNCT ADV ADV PUNCT PRON VERB SCONJ NOUN CCONJ NOUN VERB DET ADJ NOUN PRON VERB SPACE DET ADJ NOUN ADP DET NOUN ADP PROPN PUNCT NOUN VERB VERB ADJ NOUN PROPN PUNCT ADP VERB NOUN SPACE NOUN PUNCT CCONJ NOUN VERB VERB NOUN NOUN PUNCT VERB DET NOUN NOUN PART VERB NOUN SPACE PRON VERB VERB DET NOUN PUNCT ADV PUNCT PRON VERB SCONJ NUM ADP DET ADJ NOUN ADP DET NOUN ADP PROPN AUX DET SPACE NOUN ADP NOUN CCONJ NOUN VERB DET NOUN NOUN ADP VERB DET NOUN CCONJ VERB SPACE DET ADJ NOUN ADP DET NOUN AUX ADV ADV ADJ ADP PRON NOUN NOUN PUNCT ADV PUNCT SCONJ VERB SPACE ADP NOUN ADJ ADP PROPN PROPN PUNCT PRON VERB SCONJ NOUN ADP PROPN AUX VERB DET ADJ SPACE NOUN ADP VERB DET NOUN ADP NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT VERB ADJ NOUN VERB ADP DET SPACE NOUN ADP ADJ NOUN NOUN AUX DET ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PRON NOUN PART VERB NOUN SPACE NOUN PUNCT VERB PROPN PUNCT VERB VERB DET NOUN PART ADV VERB ADJ NOUN ADP SPACE ADJ NOUN ADP DET NOUN NOUN NOUN PUNCT,0.511400651465798,25.583333333333332,5.035830618892508
124,38,Aman Madaan,"[' Graphs provide a rich abstraction for a wide range of tasks\nincluding molecular design (De Cao & Kipf, 2018; Samanta\net al., 2019; Lim et al., 2020), temporal and commonsense\nreasoning (Madaan & Yang, 2021; Madaan et al., 2021;\nSakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout\ndesign (Mi et al., 2021). Developing generative models of\ngraphs is is therefore an important classical problem, which\nhas seen renewed interest with the success of deep learning\nmodels. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit\nmodels, implicit generative models do not explicitly model\nthe distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and\nhave recently shown state of the art results for generative\nmodeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of\ngraphs currently use identical model complexity and computational strength while generating graphs. However, since\nthese models are constructive by design (i.e., they build a\ngraph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of\nreasoning. For example, generating a 2-hop neighborhood\nfrequently seen during training might be easier than generating a novel 4-hop neighborhood.', 'Indeed, it has long been posited (Posner & Snyder, 1975;\nShiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;\nKahneman, 2003; Frankish, 2010) that humans frequently\nuse differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)\n203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different\nlevel of difficulty for a human solver. The answer to 2*2\nwill almost instinctively come to most, while solving 19*3\nwill require more careful thinking. Specifically, Stanovich\n(2000) propose to divide mental processing as being done\nby two metaphorical systems referred by them as System\n1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for\nSystems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a\ncombination of fast and slow reasoning systems in diverse\nareas of Machine Learning (Anthony et al., 2017; Mujika\net al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model\nthat is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into\nthe problem of learning to generate walks.', 'Generating\nwalks provides a setting where identifying the easy and\nchallenging portions is easier: starting from a given node,\nthe model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating\nsuch walks then gradually increases for two reasons. First,\nconditioning on increasingly longer contexts is required\nfor generating longer walks. Second, as the length of the\nwalks exceeds the length seen during training, a model is\nforced to create neighborhoods not seen during the training:\na task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty\nby dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1\nprovides an overview of our approach. FLOWGEN method\nachieves the same results as using the SLOW method alone\non three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer\nmodel, similar to the architectures used by the popular GPT2\nmodels. Using transformers allows us to easily instantiate\nfast and slow versions of the same model by varying the\nnumber of layers. In contrast to the state-of-the-art methods\nfor generative modeling of graphs that use either an implicit\nmodel (e.g., GANs as done by Bojchevski et al. (2018)),\nexplicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and\nleverage graph-aware decoding methods (You et al., 2018),\nour method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. ).']",intro_chunked," Graphs provide a rich abstraction for a wide range of tasks
including molecular design (De Cao & Kipf, 2018; Samanta
et al., 2019; Lim et al., 2020), temporal and commonsense
reasoning (Madaan & Yang, 2021; Madaan et al., 2021;
Sakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout
design (Mi et al., 2021). Developing generative models of
graphs is is therefore an important classical problem, which
has seen renewed interest with the success of deep learning
models. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit
models, implicit generative models do not explicitly model
the distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and
have recently shown state of the art results for generative
modeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of
graphs currently use identical model complexity and computational strength while generating graphs. However, since
these models are constructive by design (i.e., they build a
graph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of
reasoning. For example, generating a 2-hop neighborhood
frequently seen during training might be easier than generating a novel 4-hop neighborhood.",30.205000000000013,30.71625077440773,124,0.5497516393661499," Graphs provide a rich abstraction for a wide range of tasks 
 including molecular design Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, temporal and commonsense 
 reasoning Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, online user interaction modeling, and map layout 
 design. Developing generative models of 
 graphs is is therefore an important classical problem, which 
 has seen renewed interest with the success of deep learning 
 models. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit 
 models, implicit generative models do not explicitly model 
 the distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and 
 have recently shown state of the art results for generative 
 modeling of graphs. Like typical machine learning models, generative models of 
 graphs currently use identical model complexity and computational strength while generating graphs. However, since 
 these models are constructive by design ie, they build a 
 graph piece by piece, it is natural to expect that generating different parts of a graph requires different levels of 
 reasoning. For example, generating a 0 hop neighborhood 
 frequently seen during training might be easier than generating a novel 0 hop neighborhood."," Graphs provide a rich abstraction for a wide range of tasks 
 including molecular design Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, temporal and commonsense 
 reasoning Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, online user interaction modeling, and map layout 
 design. Developing generative models of 
 graphs is is therefore an important classical problem, which 
 has seen renewed interest with the success of deep learning 
 models. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit 
 models, implicit generative models do not explicitly model 
 the distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and 
 have recently shown state of the art results for generative 
 modeling of graphs. Like typical machine learning models, generative models of 
 graphs currently use identical model complexity and computational strength while generating graphs. However, since 
 these models are constructive by design ie, they build a 
 graph piece by piece, it is natural to expect that generating different parts of a graph requires different levels of 
 reasoning. For example, generating a 0 hop neighborhood 
 frequently seen during training might be easier than generating a novel 0 hop neighborhood.", NOUN VERB DET ADJ NOUN ADP DET ADJ NOUN ADP NOUN SPACE VERB ADJ NOUN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADJ CCONJ NOUN SPACE VERB PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADJ NOUN NOUN NOUN PUNCT CCONJ VERB ADJ SPACE NOUN PUNCT VERB ADJ NOUN ADP SPACE NOUN AUX AUX ADV DET ADJ ADJ NOUN PUNCT PRON SPACE AUX VERB VERB NOUN ADP DET NOUN ADP ADJ NOUN SPACE NOUN PUNCT ADV PUNCT ADJ ADJ NOUN AUX DET ADJ NOUN ADP NOUN ADJ NOUN PUNCT ADP ADJ SPACE NOUN PUNCT ADJ ADJ NOUN AUX PART ADV VERB SPACE DET NOUN ADP NOUN CCONJ ADV VERB VERB NOUN PUNCT DET ADJ NOUN ADP ADJ ADJ NOUN AUX NOUN PUNCT CCONJ SPACE AUX ADV VERB NOUN ADP DET NOUN NOUN ADP ADJ SPACE NOUN ADP NOUN PUNCT ADP ADJ NOUN NOUN NOUN PUNCT ADJ NOUN ADP SPACE NOUN ADV VERB ADJ NOUN NOUN CCONJ ADJ NOUN SCONJ VERB NOUN PUNCT ADV PUNCT SCONJ SPACE DET NOUN AUX ADJ ADP NOUN X PUNCT PRON VERB DET SPACE NOUN NOUN ADP NOUN PUNCT PRON AUX ADJ PART VERB SCONJ VERB ADJ NOUN ADP DET NOUN VERB ADJ NOUN ADP SPACE NOUN PUNCT ADP NOUN PUNCT VERB DET NUM NOUN NOUN SPACE ADV VERB ADP NOUN AUX AUX ADJ ADP VERB DET ADJ NUM NOUN NOUN PUNCT,0.4793388429752066,30.25,5.235537190082645
125,39,Aman Madaan,"[' Graphs provide a rich abstraction for a wide range of tasks\nincluding molecular design (De Cao & Kipf, 2018; Samanta\net al., 2019; Lim et al., 2020), temporal and commonsense\nreasoning (Madaan & Yang, 2021; Madaan et al., 2021;\nSakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout\ndesign (Mi et al., 2021). Developing generative models of\ngraphs is is therefore an important classical problem, which\nhas seen renewed interest with the success of deep learning\nmodels. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit\nmodels, implicit generative models do not explicitly model\nthe distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and\nhave recently shown state of the art results for generative\nmodeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of\ngraphs currently use identical model complexity and computational strength while generating graphs. However, since\nthese models are constructive by design (i.e., they build a\ngraph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of\nreasoning. For example, generating a 2-hop neighborhood\nfrequently seen during training might be easier than generating a novel 4-hop neighborhood.', 'Indeed, it has long been posited (Posner & Snyder, 1975;\nShiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;\nKahneman, 2003; Frankish, 2010) that humans frequently\nuse differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)\n203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different\nlevel of difficulty for a human solver. The answer to 2*2\nwill almost instinctively come to most, while solving 19*3\nwill require more careful thinking. Specifically, Stanovich\n(2000) propose to divide mental processing as being done\nby two metaphorical systems referred by them as System\n1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for\nSystems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a\ncombination of fast and slow reasoning systems in diverse\nareas of Machine Learning (Anthony et al., 2017; Mujika\net al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model\nthat is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into\nthe problem of learning to generate walks.', 'Generating\nwalks provides a setting where identifying the easy and\nchallenging portions is easier: starting from a given node,\nthe model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating\nsuch walks then gradually increases for two reasons. First,\nconditioning on increasingly longer contexts is required\nfor generating longer walks. Second, as the length of the\nwalks exceeds the length seen during training, a model is\nforced to create neighborhoods not seen during the training:\na task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty\nby dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1\nprovides an overview of our approach. FLOWGEN method\nachieves the same results as using the SLOW method alone\non three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer\nmodel, similar to the architectures used by the popular GPT2\nmodels. Using transformers allows us to easily instantiate\nfast and slow versions of the same model by varying the\nnumber of layers. In contrast to the state-of-the-art methods\nfor generative modeling of graphs that use either an implicit\nmodel (e.g., GANs as done by Bojchevski et al. (2018)),\nexplicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and\nleverage graph-aware decoding methods (You et al., 2018),\nour method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. ).']",intro_chunked,"Indeed, it has long been posited (Posner & Snyder, 1975;
Shiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;
Kahneman, 2003; Frankish, 2010) that humans frequently
use differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)
203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different
level of difficulty for a human solver. The answer to 2*2
will almost instinctively come to most, while solving 19*3
will require more careful thinking. Specifically, Stanovich
(2000) propose to divide mental processing as being done
by two metaphorical systems referred by them as System
1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for
Systems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a
combination of fast and slow reasoning systems in diverse
areas of Machine Learning (Anthony et al., 2017; Mujika
et al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model
that is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into
the problem of learning to generate walks.",45.54370588235295,30.71625077440773,125,0.4006766080856323," Indeed, it has long been posited Propname Propname, 0000; 
 Propname Propname, 0000; Propname, 0000; Propname, 0000; 
 Propname, 0000; Propname, 0000 that humans frequently 
 use differential reasoning based on the problem difficulty. For example, consider two problems: i0 0?, and Propname 
 000 000? Both these problems involve multiplication between two integers. Yet, they pose a very different 
 level of difficulty for a human solver. The answer to 00 
 will almost instinctively come to most, while solving 000 
 will require more careful thinking. Specifically, Propname propose to divide mental processing as being done 
 by two metaphorical systems referred by them as System 
 0 and Propname 0. The terms FAST and Propname for 
 Propname 0 and 0 were subsequently popularized by Propname. There is now a growing interest in utilizing a 
 combination of fast and slow reasoning systems in diverse 
 areas of Propname Propname Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000b. This paper introduces Propname, a generative graph model 
 that is inspired by the dual process theory of mind. Propname decomposes the problem of generating a graph into 
 the problem of learning to generate walks."," Indeed, it has long been posited Propname Propname, 0000; 
 Propname Propname, 0000; Propname, 0000; Propname, 0000; 
 Propname, 0000; Propname, 0000 that humans frequently 
 use differential reasoning based on the problem difficulty. For example, consider two problems: i0 0?, and Propname 
 000 000? Both these problems involve multiplication between two integers. Yet, they pose a very different 
 level of difficulty for a human solver. The answer to 00 
 will almost instinctively come to most, while solving 000 
 will require more careful thinking. Specifically, Propname propose to divide mental processing as being done 
 by two metaphorical systems referred by them as System 
 0 and Propname 0. The terms FAST and Propname for 
 Propname 0 and 0 were subsequently popularized by Propname. There is now a growing interest in utilizing a 
 combination of fast and slow reasoning systems in diverse 
 areas of Propname Propname Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000b. This paper introduces Propname, a generative graph model 
 that is inspired by the dual process theory of mind. Propname decomposes the problem of generating a graph into 
 the problem of learning to generate walks.", ADV PUNCT PRON AUX ADV AUX VERB PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PUNCT NUM PUNCT PROPN PUNCT NUM PUNCT PROPN PUNCT NUM PUNCT SPACE PROPN PUNCT NUM PUNCT PROPN PUNCT NUM SCONJ NOUN ADV SPACE VERB ADJ NOUN VERB ADP DET NOUN NOUN PUNCT ADP NOUN PUNCT VERB NUM NOUN PUNCT PRON PUNCT NUM PUNCT PUNCT CCONJ PROPN SPACE NUM NUM PUNCT CCONJ DET NOUN VERB NOUN ADP NUM NOUN PUNCT ADV PUNCT PRON VERB DET ADV ADJ SPACE NOUN ADP NOUN ADP DET ADJ NOUN PUNCT DET NOUN ADP NUM SPACE AUX ADV ADV VERB ADP ADJ PUNCT SCONJ VERB NUM SPACE AUX VERB ADV ADJ NOUN PUNCT ADV PUNCT PROPN VERB PART VERB ADJ NOUN ADP AUX VERB SPACE ADP NUM ADJ NOUN VERB ADP PRON ADP NOUN SPACE NUM CCONJ PROPN NUM PUNCT DET NOUN VERB CCONJ PROPN ADP SPACE PROPN NUM CCONJ NUM AUX ADV VERB ADP PROPN PUNCT PRON VERB ADV DET VERB NOUN ADP VERB DET SPACE NOUN ADP ADJ CCONJ ADJ NOUN NOUN ADP ADJ SPACE NOUN ADP PROPN PROPN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN NOUN SPACE PRON AUX VERB ADP DET ADJ NOUN NOUN ADP NOUN PUNCT PROPN VERB DET NOUN ADP VERB DET NOUN ADP SPACE DET NOUN ADP VERB PART VERB NOUN PUNCT,0.5066666666666667,22.5,4.728888888888889
126,40,Aman Madaan,"[' Graphs provide a rich abstraction for a wide range of tasks\nincluding molecular design (De Cao & Kipf, 2018; Samanta\net al., 2019; Lim et al., 2020), temporal and commonsense\nreasoning (Madaan & Yang, 2021; Madaan et al., 2021;\nSakaguchi et al., 2021; Saha et al., 2021), online user interaction modeling (Zhou et al., 2020a), and map layout\ndesign (Mi et al., 2021). Developing generative models of\ngraphs is is therefore an important classical problem, which\nhas seen renewed interest with the success of deep learning\nmodels. Specifically, implicit generative models are a popular choice for graph generative modeling. Unlike explicit\nmodels, implicit generative models do not explicitly model\nthe distribution of graphs but instead allow sampling graphs. A popular example of such implicit models are GANs, and\nhave recently shown state of the art results for generative\nmodeling of graphs (Bojchevski et al., 2018). Like typical machine learning models, generative models of\ngraphs currently use identical model complexity and computational strength while generating graphs. However, since\nthese models are constructive by design (i.e., they build a\ngraph piece-by-piece), it is natural to expect that generating different parts of a graph requires different levels of\nreasoning. For example, generating a 2-hop neighborhood\nfrequently seen during training might be easier than generating a novel 4-hop neighborhood.', 'Indeed, it has long been posited (Posner & Snyder, 1975;\nShiffrin & Schneider, 1977; Evans, 1984; Stanovich, 2000;\nKahneman, 2003; Frankish, 2010) that humans frequently\nuse differential reasoning based on the problem difficulty. For example, consider two problems: i) 2 * 2 = ?, and ii)\n203 * 197 = ? Both these problems involve multiplication between two integers. Yet, they pose a very different\nlevel of difficulty for a human solver. The answer to 2*2\nwill almost instinctively come to most, while solving 19*3\nwill require more careful thinking. Specifically, Stanovich\n(2000) propose to divide mental processing as being done\nby two metaphorical systems referred by them as System\n1 (instinctive, used for 2 * 2) and System 2 (analytical, planner, used for 203 * 197). The terms FAST and SLOW for\nSystems 1 and 2 were subsequently popularized by Kahneman (2011). There is now a growing interest in utilizing a\ncombination of fast and slow reasoning systems in diverse\nareas of Machine Learning (Anthony et al., 2017; Mujika\net al., 2017; Schwarzschild et al., 2021b). This paper introduces FLOWGEN, a generative graph model\nthat is inspired by the dual-process theory of mind. FLOWGEN decomposes the problem of generating a graph into\nthe problem of learning to generate walks.', 'Generating\nwalks provides a setting where identifying the easy and\nchallenging portions is easier: starting from a given node,\nthe model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating\nsuch walks then gradually increases for two reasons. First,\nconditioning on increasingly longer contexts is required\nfor generating longer walks. Second, as the length of the\nwalks exceeds the length seen during training, a model is\nforced to create neighborhoods not seen during the training:\na task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty\nby dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1\nprovides an overview of our approach. FLOWGEN method\nachieves the same results as using the SLOW method alone\non three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer\nmodel, similar to the architectures used by the popular GPT2\nmodels. Using transformers allows us to easily instantiate\nfast and slow versions of the same model by varying the\nnumber of layers. In contrast to the state-of-the-art methods\nfor generative modeling of graphs that use either an implicit\nmodel (e.g., GANs as done by Bojchevski et al. (2018)),\nexplicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and\nleverage graph-aware decoding methods (You et al., 2018),\nour method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. ).']",intro_chunked,"Generating
walks provides a setting where identifying the easy and
challenging portions is easier: starting from a given node,
the model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating
such walks then gradually increases for two reasons. First,
conditioning on increasingly longer contexts is required
for generating longer walks. Second, as the length of the
walks exceeds the length seen during training, a model is
forced to create neighborhoods not seen during the training:
a task that requires more robust generalization capabilities. FLOWGEN leverages this mismatch in problem difficulty
by dynamically switching from a small (FAST) model to a large (SLOW) model for efficient graph generation. Figure 1
provides an overview of our approach. FLOWGEN method
achieves the same results as using the SLOW method alone
on three different graphs, while taking up to 50% less time. The backbone of FLOWGEN is a decoder-only transformer
model, similar to the architectures used by the popular GPT2
models. Using transformers allows us to easily instantiate
fast and slow versions of the same model by varying the
number of layers. In contrast to the state-of-the-art methods
for generative modeling of graphs that use either an implicit
model (e.g., GANs as done by Bojchevski et al. (2018)),
explicit graph distributions (with no option to vary the parameterization), or generate an entire graph sequence and
leverage graph-aware decoding methods (You et al., 2018),
our method is simpler (based on a standard transformer language model) and not sensitive to hyper-parameters (an identical network setup achieves gains across different graphs. ).",35.816914758269746,30.71625077440773,126,0.3418042063713074," Generating 
 walks provides a setting where identifying the easy and 
 challenging portions is easier: starting from a given node, 
 the model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating 
 such walks then gradually increases for two reasons. First, 
 conditioning on increasingly longer Propname is required 
 for generating longer walks. Second, as the length of the 
 walks exceeds the length seen during training, a model is 
 forced to create neighborhoods not seen during the training: 
 a task that requires more robust generalization capabilities. Propname leverages this mismatch in problem difficulty 
 by dynamically switching from a small model to a large model for efficient graph generation. Figure 0 
 provides an overview of our approach. Propname method 
 achieves the same results as using the SLOW method alone 
 on three different graphs, while taking up to 00 less time. The backbone of Propname is a decoder only transformer 
 model, similar to the architectures used by the popular Propname 
 models. Using transformers allows us to easily instantiate 
 fast and slow versions of the same model by varying the 
 number of layers. In contrast to the state of the art methods 
 for generative modeling of graphs that use either an implicit 
 model, 
 explicit graph distributions, or generate an entire graph sequence and 
 leverage graph aware decoding methods, 
 our method is simpler and not sensitive to hyper parameters."," Generating 
 walks provides a setting where identifying the easy and 
 challenging portions is easier: starting from a given node, 
 the model begins by generating walks seen during the training in known neighborhoods. The difficulty of generating 
 such walks then gradually increases for two reasons. First, 
 conditioning on increasingly longer Propname is required 
 for generating longer walks. Second, as the length of the 
 walks exceeds the length seen during training, a model is 
 forced to create neighborhoods not seen during the training: 
 a task that requires more robust generalization capabilities. Propname leverages this mismatch in problem difficulty 
 by dynamically switching from a small model to a large model for efficient graph generation. Figure 0 
 provides an overview of our approach. Propname method 
 achieves the same results as using the SLOW method alone 
 on three different graphs, while taking up to 00 less time. The backbone of Propname is a decoder only transformer 
 model, similar to the architectures used by the popular Propname 
 models. Using transformers allows us to easily instantiate 
 fast and slow versions of the same model by varying the 
 number of layers. In contrast to the state of the art methods 
 for generative modeling of graphs that use either an implicit 
 model, 
 explicit graph distributions, or generate an entire graph sequence and 
 leverage graph aware decoding methods, 
 our method is simpler and not sensitive to hyper parameters.", NOUN SPACE NOUN VERB DET NOUN SCONJ VERB DET ADJ CCONJ SPACE VERB NOUN AUX ADJ PUNCT VERB ADP DET VERB NOUN PUNCT SPACE DET NOUN VERB ADP VERB NOUN VERB ADP DET NOUN ADP VERB NOUN PUNCT DET NOUN ADP VERB SPACE ADJ NOUN ADV ADV VERB ADP NUM NOUN PUNCT ADV PUNCT SPACE NOUN ADP ADV ADJ PROPN AUX VERB SPACE ADP VERB ADV VERB PUNCT ADV PUNCT SCONJ DET NOUN ADP DET SPACE NOUN VERB DET NOUN VERB ADP NOUN PUNCT DET NOUN AUX SPACE VERB PART VERB NOUN PART VERB ADP DET NOUN PUNCT SPACE DET NOUN PRON VERB ADJ ADJ NOUN NOUN PUNCT PROPN NOUN DET NOUN ADP NOUN NOUN SPACE ADP ADV VERB ADP DET ADJ NOUN ADP DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT NOUN NUM SPACE VERB DET NOUN ADP PRON NOUN PUNCT PROPN NOUN SPACE VERB DET ADJ NOUN ADP VERB DET ADJ NOUN ADV SPACE ADP NUM ADJ NOUN PUNCT SCONJ VERB ADP PART NUM ADJ NOUN PUNCT DET NOUN ADP PROPN AUX DET NOUN ADV ADJ SPACE NOUN PUNCT ADJ ADP DET NOUN VERB ADP DET ADJ PROPN SPACE NOUN PUNCT VERB NOUN VERB PRON PART ADV VERB SPACE ADJ CCONJ ADJ NOUN ADP DET ADJ NOUN ADP VERB DET SPACE NOUN ADP NOUN PUNCT ADP NOUN ADP DET NOUN ADP DET NOUN NOUN SPACE ADP ADJ NOUN ADP NOUN PRON VERB CCONJ DET ADJ SPACE NOUN PUNCT SPACE ADJ NOUN NOUN PUNCT CCONJ VERB DET ADJ NOUN NOUN CCONJ SPACE NOUN VERB ADJ VERB NOUN PUNCT SPACE PRON NOUN AUX ADJ CCONJ PART ADJ ADP ADJ NOUN PUNCT,0.5622489959839357,24.9,4.959839357429719
127,41,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked," Conditional set generation is the task of modeling
the distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several
NLP tasks are instances of set generation, including
open-entity typing (Choi et al., 2018; Dai et al.,
2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng
et al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning
paradigm have encouraged a formulation of set
generation as a SEQ2SEQ generation task (Vinyals
et al., 2016; Yang et al., 2018; Meng et al., 2019;
Ju et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not
explicitly account for two key properties of a set
output: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation
treats a set as a sequence, assuming an arbitrary
order between the elements it outputs. Similarly,
the cardinality of sets is ignored, as the number of
elements to be generated is typically not modeled.",29.567574487895712,30.71625077440773,127,0.20490118861198425," Conditional set generation is the task of modeling 
 the distribution of an output set given an input sequence of Propname. Several 
 Propname tasks are instances of set generation, including 
 open entity typing Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000, fine grained emotion classification, and keyphrase generation Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. The recent successes of the pretraining finetuning 
 paradigm have encouraged a formulation of set 
 generation as a Propname generation task Vinyals 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000. In this paper, we posit that modeling set generation as a vanilla Propname generation task is suboptimal, because the Propname formulations do not 
 explicitly account for two key properties of a set 
 output: order invariance and cardinality. Forgoing order invariance, vanilla Propname generation 
 treats a set as a sequence, assuming an arbitrary 
 order between the elements it outputs. Similarly, 
 the cardinality of sets is ignored, as the number of 
 elements to be generated is typically not modeled."," Conditional set generation is the task of modeling 
 the distribution of an output set given an input sequence of Propname. Several 
 Propname tasks are instances of set generation, including 
 open entity typing Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000, fine grained emotion classification, and keyphrase generation Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. The recent successes of the pretraining finetuning 
 paradigm have encouraged a formulation of set 
 generation as a Propname generation task Vinyals 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000. In this paper, we posit that modeling set generation as a vanilla Propname generation task is suboptimal, because the Propname formulations do not 
 explicitly account for two key properties of a set 
 output: order invariance and cardinality. Forgoing order invariance, vanilla Propname generation 
 treats a set as a sequence, assuming an arbitrary 
 order between the elements it outputs. Similarly, 
 the cardinality of sets is ignored, as the number of 
 elements to be generated is typically not modeled.", ADJ NOUN NOUN AUX DET NOUN ADP VERB SPACE DET NOUN ADP DET NOUN VERB VERB DET NOUN NOUN ADP PROPN PUNCT ADJ SPACE PROPN NOUN AUX NOUN ADP ADJ NOUN PUNCT VERB SPACE ADJ NOUN VERB PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT ADJ VERB NOUN NOUN PUNCT CCONJ NOUN NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT DET ADJ NOUN ADP DET VERB NOUN SPACE NOUN AUX VERB DET NOUN ADP ADJ SPACE NOUN ADP DET PROPN NOUN NOUN NOUN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ VERB VERB NOUN ADP DET NOUN PROPN NOUN NOUN AUX ADJ PUNCT SCONJ DET PROPN NOUN AUX PART SPACE ADV VERB ADP NUM ADJ NOUN ADP DET ADJ SPACE NOUN PUNCT NOUN NOUN CCONJ NOUN PUNCT NOUN NOUN NOUN PUNCT NOUN PROPN NOUN SPACE VERB DET NOUN ADP DET NOUN PUNCT VERB DET ADJ SPACE NOUN ADP DET NOUN PRON VERB PUNCT ADV PUNCT SPACE DET NOUN ADP NOUN AUX VERB PUNCT SCONJ DET NOUN ADP SPACE NOUN PART AUX VERB AUX ADV PART VERB PUNCT,0.39622641509433965,35.333333333333336,5.240566037735849
128,42,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked,"Prior work has highlighted the importance of
these two properties for set output through loss
functions that encourage order invariance (Ye et al.,
2021), exhaustive search over the label space
for finding an optimal order (Qin et al., 2019;
Rezatofighi et al., 2018; Vinyals et al., 2016), and
post-processing the output (Nag Chowdhury et al.,
2016). Despite the progress, several important gaps
remain. First, exhaustive search does not scale with
large output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is
still not explicitly modeled in the SEQ2SEQ setting
despite being an essential aspect for a set. Finally,
architectural modifications required for specialized
set-generation techniques might not be viable for
modern large-language models. We address these challenges with a novel data
augmentation strategy. Specifically, we take advantage of the auto-regressive factorization used
by SEQ2SEQ models and (i) impose an informative
order over the label space, and (ii) explicitly model
cardinality. First, the label sets are converted to
sequences using informative orders by grouping
labels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the
edges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.",31.07136363636363,30.71625077440773,128,0.31369251012802124," Prior work has highlighted the importance of 
 these two properties for set output through loss 
 functions that encourage order invariance Propname et Propname Propname, 
 0000, exhaustive search over the label space 
 for finding an optimal order Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, and 
 post processing the output Propname Propname Propname Propname Propname, 
 0000. Despite the progress, several important gaps 
 remain. First, exhaustive search does not scale with 
 large output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is 
 still not explicitly modeled in the Propname setting 
 despite being an essential aspect for a set. Finally, 
 architectural modifications required for specialized 
 set generation techniques might not be viable for 
 modern large language models. We address these challenges with a novel data 
 augmentation strategy. Specifically, we take advantage of the auto regressive factorization used 
 by Propname models and impose an informative 
 order over the label space, and explicitly model 
 cardinality. First, the label sets are converted to 
 sequences using informative orders by grouping 
 labels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the 
 edges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order invariance."," Prior work has highlighted the importance of 
 these two properties for set output through loss 
 functions that encourage order invariance Propname et Propname Propname, 
 0000, exhaustive search over the label space 
 for finding an optimal order Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, and 
 post processing the output Propname Propname Propname Propname Propname, 
 0000. Despite the progress, several important gaps 
 remain. First, exhaustive search does not scale with 
 large output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is 
 still not explicitly modeled in the Propname setting 
 despite being an essential aspect for a set. Finally, 
 architectural modifications required for specialized 
 set generation techniques might not be viable for 
 modern large language models. We address these challenges with a novel data 
 augmentation strategy. Specifically, we take advantage of the auto regressive factorization used 
 by Propname models and impose an informative 
 order over the label space, and explicitly model 
 cardinality. First, the label sets are converted to 
 sequences using informative orders by grouping 
 labels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the 
 edges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order invariance.", ADJ NOUN AUX VERB DET NOUN ADP SPACE DET NUM NOUN ADP ADJ NOUN ADP NOUN SPACE NOUN PRON VERB NOUN NOUN PROPN NOUN PROPN PROPN PUNCT SPACE NUM PUNCT ADJ NOUN ADP DET NOUN NOUN SPACE ADP VERB DET ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT CCONJ SPACE NOUN VERB DET NOUN PROPN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT SCONJ DET NOUN PUNCT ADJ ADJ NOUN SPACE VERB PUNCT ADV PUNCT ADJ NOUN AUX PART VERB ADP SPACE ADJ NOUN NOUN ADV VERB ADP NOUN NOUN PUNCT ADV VERB DET NOUN ADP DET ADJ NOUN NOUN ADP DET NOUN PUNCT ADJ PUNCT NOUN AUX SPACE ADV PART ADV VERB ADP DET PROPN VERB SPACE SCONJ AUX DET ADJ NOUN ADP DET NOUN PUNCT ADV PUNCT SPACE ADJ NOUN VERB ADP ADJ SPACE VERB NOUN NOUN AUX PART AUX ADJ ADP SPACE ADJ ADJ NOUN NOUN PUNCT PRON VERB DET NOUN ADP DET ADJ NOUN SPACE NOUN NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP DET NOUN ADJ NOUN VERB SPACE ADP PROPN NOUN CCONJ VERB DET ADJ SPACE NOUN ADP DET NOUN NOUN PUNCT CCONJ ADV NOUN SPACE NOUN PUNCT ADV PUNCT DET NOUN NOUN AUX VERB ADP SPACE NOUN VERB ADJ NOUN ADP VERB SPACE NOUN CCONJ VERB PRON NOUN NOUN PUNCT PRON NOUN VERB DET ADJ NOUN NOUN ADP NOUN NOUN SCONJ DET NOUN AUX DET NOUN PUNCT CCONJ DET SPACE NOUN VERB DET ADJ NOUN NOUN PUNCT DET NOUN VERB DET ADJ NOUN PART VERB ADJ NOUN SCONJ VERB NOUN NOUN PUNCT,0.531496062992126,25.4,5.307086614173229
129,43,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked,"Specifically, sequences obtained via topological
traversals of this graph allow independent labels to
appear at different locations in the sequence, while restricting order for dependent labels. Next, we
jointly model a set with its cardinality by simply
prepending the set size to the output sequence. This
strategy aligns with the current trend of very large
language models which do not lend themselves to
architectural modifications but increasingly rely on
the informativeness of the inputs (Yang et al., 2020;
Liu et al., 2021). Figure 1 illustrates the key intuitions behind our
method using sample task where given an input
x (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more
meaningful, consider a case where one of the emotions is joy, which leads to a more general emotion
of pride. After first generating joy, the model can
generate pride with certainty (joy leads to pride in
all samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]
is thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,
joy contains two sub-emotions, and love contains
one.",46.780351674641196,30.71625077440773,129,0.8118076324462891," Specifically, sequences obtained via topological 
 traversals of this graph allow independent labels to 
 appear at different locations in the sequence, while restricting order for dependent labels. Next, we 
 jointly model a set with its cardinality by simply 
 prepending the set size to the output sequence. This 
 strategy aligns with the current trend of very large 
 language models which do not lend themselves to 
 architectural modifications but increasingly rely on 
 the informativeness of the inputs Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000. Figure 0 illustrates the key intuitions behind our 
 method using sample task where given an input 
 x, the output is a set of emotions. To see why certain orders might be more 
 meaningful, consider a case where one of the emotions is joy, which leads to a more general emotion 
 of pride. After first generating joy, the model can 
 generate pride with certainty joy leads to pride in 
 all samples. In contrast, the reverse order still leaves room for multiple possible emotions. The order is thus more informative than. The cardinality of a set can also be helpful. In our example, 
 joy contains two sub emotions, and love contains 
 one."," Specifically, sequences obtained via topological 
 traversals of this graph allow independent labels to 
 appear at different locations in the sequence, while restricting order for dependent labels. Next, we 
 jointly model a set with its cardinality by simply 
 prepending the set size to the output sequence. This 
 strategy aligns with the current trend of very large 
 language models which do not lend themselves to 
 architectural modifications but increasingly rely on 
 the informativeness of the inputs Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000. Figure 0 illustrates the key intuitions behind our 
 method using sample task where given an input 
 x, the output is a set of emotions. To see why certain orders might be more 
 meaningful, consider a case where one of the emotions is joy, which leads to a more general emotion 
 of pride. After first generating joy, the model can 
 generate pride with certainty joy leads to pride in 
 all samples. In contrast, the reverse order still leaves room for multiple possible emotions. The order is thus more informative than. The cardinality of a set can also be helpful. In our example, 
 joy contains two sub emotions, and love contains 
 one.", ADV PUNCT NOUN VERB ADP ADJ SPACE NOUN ADP DET NOUN VERB ADJ NOUN PART SPACE VERB ADP ADJ NOUN ADP DET NOUN PUNCT SCONJ VERB NOUN ADP ADJ NOUN PUNCT ADV PUNCT PRON SPACE ADV VERB DET NOUN ADP PRON NOUN ADP ADV SPACE VERB DET VERB NOUN ADP DET NOUN NOUN PUNCT DET SPACE NOUN NOUN ADP DET ADJ NOUN ADP ADV ADJ SPACE NOUN NOUN PRON AUX PART VERB PRON ADP SPACE ADJ NOUN CCONJ ADV VERB ADP SPACE DET NOUN ADP DET NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT NOUN NUM VERB DET ADJ NOUN ADP PRON SPACE NOUN VERB NOUN NOUN SCONJ VERB DET NOUN SPACE NOUN PUNCT DET NOUN AUX DET NOUN ADP NOUN PUNCT PART VERB SCONJ ADJ NOUN AUX AUX ADV SPACE ADJ PUNCT VERB DET NOUN SCONJ NUM ADP DET NOUN AUX NOUN PUNCT PRON VERB ADP DET ADV ADJ NOUN SPACE ADP NOUN PUNCT ADP ADJ VERB NOUN PUNCT DET NOUN AUX SPACE VERB NOUN ADP ADJ NOUN VERB ADP NOUN ADP SPACE DET NOUN PUNCT ADP NOUN PUNCT DET ADJ NOUN ADV VERB NOUN ADP ADJ ADJ NOUN PUNCT DET NOUN AUX ADV ADV ADJ ADP PUNCT DET NOUN ADP DET NOUN AUX ADV AUX ADJ PUNCT ADP PRON NOUN PUNCT SPACE NOUN VERB NUM NOUN NOUN PUNCT CCONJ NOUN VERB SPACE NUM PUNCT,0.5944700460829493,21.7,4.682027649769585
130,44,Aman Madaan,"[' Conditional set generation is the task of modeling\nthe distribution of an output set given an input sequence of tokens (Kosiorek et al., 2020). Several\nNLP tasks are instances of set generation, including\nopen-entity typing (Choi et al., 2018; Dai et al.,\n2021), fine-grained emotion classification (Demszky et al., 2020), and keyphrase generation (Meng\net al., 2017; Yuan et al., 2020; Ye et al., 2021). The recent successes of the pretraining-finetuning\nparadigm have encouraged a formulation of set\ngeneration as a SEQ2SEQ generation task (Vinyals\net al., 2016; Yang et al., 2018; Meng et al., 2019;\nJu et al., 2020). In this paper, we posit that modeling set generation as a vanilla SEQ2SEQ generation task is suboptimal, because the SEQ2SEQ formulations do not\nexplicitly account for two key properties of a set\noutput: order-invariance and cardinality. Forgoing order-invariance, vanilla SEQ2SEQ generation\ntreats a set as a sequence, assuming an arbitrary\norder between the elements it outputs. Similarly,\nthe cardinality of sets is ignored, as the number of\nelements to be generated is typically not modeled.', 'Prior work has highlighted the importance of\nthese two properties for set output through loss\nfunctions that encourage order invariance (Ye et al.,\n2021), exhaustive search over the label space\nfor finding an optimal order (Qin et al., 2019;\nRezatofighi et al., 2018; Vinyals et al., 2016), and\npost-processing the output (Nag Chowdhury et al.,\n2016). Despite the progress, several important gaps\nremain. First, exhaustive search does not scale with\nlarge output spaces typically found in NLP problems, thus stressing the need for an optimal sampling strategy for the labels. Second, cardinality is\nstill not explicitly modeled in the SEQ2SEQ setting\ndespite being an essential aspect for a set. Finally,\narchitectural modifications required for specialized\nset-generation techniques might not be viable for\nmodern large-language models. We address these challenges with a novel data\naugmentation strategy. Specifically, we take advantage of the auto-regressive factorization used\nby SEQ2SEQ models and (i) impose an informative\norder over the label space, and (ii) explicitly model\ncardinality. First, the label sets are converted to\nsequences using informative orders by grouping\nlabels and leveraging their dependency structure. Our method induces a partial order graph over label space where the nodes are the labels, and the\nedges denote the conditional dependence relations. This graph provides a natural way to obtain informative orders while reinforcing order-invariance.', 'Specifically, sequences obtained via topological\ntraversals of this graph allow independent labels to\nappear at different locations in the sequence, while restricting order for dependent labels. Next, we\njointly model a set with its cardinality by simply\nprepending the set size to the output sequence. This\nstrategy aligns with the current trend of very large\nlanguage models which do not lend themselves to\narchitectural modifications but increasingly rely on\nthe informativeness of the inputs (Yang et al., 2020;\nLiu et al., 2021). Figure 1 illustrates the key intuitions behind our\nmethod using sample task where given an input\nx (say a conversation), the output is a set of emotions (Y). To see why certain orders might be more\nmeaningful, consider a case where one of the emotions is joy, which leads to a more general emotion\nof pride. After first generating joy, the model can\ngenerate pride with certainty (joy leads to pride in\nall samples). In contrast, the reverse order (generating pride first) still leaves room for multiple possible emotions (joy and love). The order [joy, pride]\nis thus more informative than [pride, joy]. The cardinality of a set can also be helpful. In our example,\njoy contains two sub-emotions, and love contains\none.', 'A model that first predicts the number of\nsub-emotions can be more precise and avoid overgeneration, a significant challenge with language\ngeneration models (Welleck et al., 2020; Fu et al.,\n2021). We efficiently sample such informative orders from the combinatorial space of all possible\norders and jointly model cardinality by leveraging\nthe auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly\nmodeling the cardinality and augmenting the\ntraining data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show\nthat our method serves as a better proposal\ndistribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with\nno additional annotations or architecture\nchanges.']",intro_chunked,"A model that first predicts the number of
sub-emotions can be more precise and avoid overgeneration, a significant challenge with language
generation models (Welleck et al., 2020; Fu et al.,
2021). We efficiently sample such informative orders from the combinatorial space of all possible
orders and jointly model cardinality by leveraging
the auto-regressive nature of SEQ2SEQ models. We show an efficient way to model sequenceto-set prediction as a SEQ2SEQ task by jointly
modeling the cardinality and augmenting the
training data with informative sequences using our novel SETAUG data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show
that our method serves as a better proposal
distribution in a variational inference framework. With our approach, SEQ2SEQ models of different sizes achieve a ∼20% relative improvement on four real-world tasks, with
no additional annotations or architecture
changes.",19.006589041095907,30.71625077440773,130,0.602085530757904," A model that first predicts the number of 
 sub emotions can be more precise and avoid overgeneration, a significant challenge with language 
 generation models Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000. We efficiently sample such informative orders from the combinatorial space of all possible 
 orders and jointly model cardinality by leveraging 
 the auto regressive nature of Propname models. We show an efficient way to model sequenceto set prediction as a Propname task by jointly 
 modeling the cardinality and augmenting the 
 training data with informative sequences using our novel Propname data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show 
 that our method serves as a better proposal 
 distribution in a variational inference framework. With our approach, Propname models of different sizes achieve a 00 relative improvement on four real world tasks, with 
 no additional annotations or architecture 
 changes."," A model that first predicts the number of 
 sub emotions can be more precise and avoid overgeneration, a significant challenge with language 
 generation models Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000. We efficiently sample such informative orders from the combinatorial space of all possible 
 orders and jointly model cardinality by leveraging 
 the auto regressive nature of Propname models. We show an efficient way to model sequenceto set prediction as a Propname task by jointly 
 modeling the cardinality and augmenting the 
 training data with informative sequences using our novel Propname data augmentation approach. We theoretically ground our approach: treating the order as a latent variable, we show 
 that our method serves as a better proposal 
 distribution in a variational inference framework. With our approach, Propname models of different sizes achieve a 00 relative improvement on four real world tasks, with 
 no additional annotations or architecture 
 changes.", DET NOUN PRON ADV VERB DET NOUN ADP SPACE NOUN NOUN AUX AUX ADV ADJ CCONJ VERB NOUN PUNCT DET ADJ NOUN ADP NOUN SPACE NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PRON ADV VERB ADJ ADJ NOUN ADP DET ADJ NOUN ADP DET ADJ SPACE NOUN CCONJ ADV NOUN NOUN ADP VERB SPACE DET NOUN ADJ NOUN ADP PROPN NOUN PUNCT PRON VERB DET ADJ NOUN PART VERB NOUN VERB NOUN ADP DET PROPN NOUN ADP ADV SPACE VERB DET NOUN CCONJ VERB DET SPACE NOUN NOUN ADP ADJ NOUN VERB PRON ADJ PROPN NOUN NOUN NOUN PUNCT PRON ADV VERB PRON NOUN PUNCT VERB DET NOUN ADP DET NOUN NOUN PUNCT PRON VERB SPACE SCONJ PRON NOUN VERB ADP DET ADJ NOUN SPACE NOUN ADP DET ADJ NOUN NOUN PUNCT ADP PRON NOUN PUNCT PROPN NOUN ADP ADJ NOUN VERB DET NUM ADJ NOUN ADP NUM ADJ NOUN NOUN PUNCT ADP SPACE DET ADJ NOUN CCONJ NOUN SPACE NOUN PUNCT,0.6335403726708074,32.2,5.366459627329193
131,45,Aman Madaan,"[' Language models are now better than ever before at\ngenerating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense\nis in misunderstanding a user’s intent. The typical\nremedy of retraining with more data is prohibitive\ndue to the cost and infrastructure requirements. In\nsuch cases, even if users repeatedly observe the\nmodel making a mistake, there are no avenues to\nprovide feedback to the model to make it more\naccurate and personalized over time. Our goal is to allow users to correct such errors\ndirectly through interaction, and without retraining by injecting the knowledge required to correct the\nmodel’s misunderstanding. Building upon the recent success of injecting commonsense in the input\n(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in\nthe input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with\na growing memory of cases where the model misunderstood user’s intent and was provided with\ncorrective feedback. This feedback is question dependent, and thus the prompt for each sample is\nedited to adapt to the input. In this sense, our\nwork can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing\nthe prompts.', 'Our work adds interactivity to prompt\nengineering as it involves dynamically updating the\nprompt for every instance. Figure 1 presents a sample interaction between a\nuser and GPT-3 that our setup enables. The model\nwas asked for a similar word. However, the model’s\n(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task\ninstruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,\nnote that such instructional correction is feasible\neven if the user does not know the correct answer to\ntheir question, as they are critiquing the model’s understanding of their intent, rather than the answers\nthemselves. Thus, our setup does not require the\nusers to be experts at tasks being solved, another\nadvantage of our approach. Further, it is desirable to have a system that can\nleverage past feedback on new, unseen examples\nfor prompt-editing. We maintain a memory M of\nsuch feedback as a set of key-value pairs, where the\nkey is a misunderstood question, and the value is\nthe user’s feedback to correct that misunderstanding. Given a new question, we check if the model\nhas made a mistake on a similar question earlier,\nby querying the memory for a similar question.', 'If\nfound, append the corresponding feedback to the\nquestion prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive\nreminding in psychology (Jacoby and Wahlheim,\n2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for\nthe system and provides representative implementations for each component. We then demonstrate\nthe system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure\n1), (2) word scrambling (e.g., anagrams), (3) ethical\nreasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)\nethics reasoning with user feedback being natural\nlanguage. We find that in all cases, GPT-3’s accuracy significantly increases with time, without\nretraining, as our approach enables it to use corrective feedback from earlier examples to avoid\nsimilar misunderstandings on future examples. In\nsummary, our contributions are:• We show that a large model like GPT-3 can be\nimproved after deployment, without retraining,\nthrough a memory-assisted architecture. • Our implementation, MemPrompt, is the first\ndemonstration that this is possible - this is an important step forward for real use of LMs, and the\npaper sets out a general architecture that others can\nbuild on, a specific implementation, and detailed\nevaluation on multiple tasks.']",intro_chunked," Language models are now better than ever before at
generating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense
is in misunderstanding a user’s intent. The typical
remedy of retraining with more data is prohibitive
due to the cost and infrastructure requirements. In
such cases, even if users repeatedly observe the
model making a mistake, there are no avenues to
provide feedback to the model to make it more
accurate and personalized over time. Our goal is to allow users to correct such errors
directly through interaction, and without retraining by injecting the knowledge required to correct the
model’s misunderstanding. Building upon the recent success of injecting commonsense in the input
(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in
the input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with
a growing memory of cases where the model misunderstood user’s intent and was provided with
corrective feedback. This feedback is question dependent, and thus the prompt for each sample is
edited to adapt to the input. In this sense, our
work can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing
the prompts.",45.12570707070711,30.71625077440773,131,0.16483959555625916," Language models are now better than ever before at 
 generating realistic content, but still lack commonsense. One failure mode due to a lack of commonsense 
 is in misunderstanding a users intent. The typical 
 remedy of retraining with more data is prohibitive 
 due to the cost and infrastructure requirements. In 
 such cases, even if users repeatedly observe the 
 model making a mistake, there are no avenues to 
 provide feedback to the model to make it more 
 accurate and personalized over time. Our goal is to allow users to correct such errors 
 directly through interaction, and without retraining by injecting the knowledge required to correct the 
 models misunderstanding. Building upon the recent success of injecting commonsense in the input 
, we propose a novel approach of injecting knowledge in 
 the input via interactive feedback from an end user. Our approach, Propname, pairs Propname 0 with 
 a growing memory of cases where the model misunderstood users intent and was provided with 
 corrective feedback. This feedback is question dependent, and thus the prompt for each sample is 
 edited to adapt to the input. In this sense, our 
 work can be seen as an instance of prompt engineering which involves editing 
 the prompts."," Language models are now better than ever before at 
 generating realistic content, but still lack commonsense. One failure mode due to a lack of commonsense 
 is in misunderstanding a users intent. The typical 
 remedy of retraining with more data is prohibitive 
 due to the cost and infrastructure requirements. In 
 such cases, even if users repeatedly observe the 
 model making a mistake, there are no avenues to 
 provide feedback to the model to make it more 
 accurate and personalized over time. Our goal is to allow users to correct such errors 
 directly through interaction, and without retraining by injecting the knowledge required to correct the 
 models misunderstanding. Building upon the recent success of injecting commonsense in the input 
, we propose a novel approach of injecting knowledge in 
 the input via interactive feedback from an end user. Our approach, Propname, pairs Propname 0 with 
 a growing memory of cases where the model misunderstood users intent and was provided with 
 corrective feedback. This feedback is question dependent, and thus the prompt for each sample is 
 edited to adapt to the input. In this sense, our 
 work can be seen as an instance of prompt engineering which involves editing 
 the prompts.", NOUN NOUN AUX ADV ADJ ADP ADV ADV ADP SPACE VERB ADJ NOUN PUNCT CCONJ ADV VERB NOUN PUNCT NUM NOUN NOUN ADP ADP DET NOUN ADP NOUN SPACE AUX ADP VERB DET NOUN NOUN PUNCT DET ADJ SPACE NOUN ADP VERB ADP ADJ NOUN AUX ADJ SPACE ADJ ADP DET NOUN CCONJ NOUN NOUN PUNCT ADP SPACE ADJ NOUN PUNCT ADV SCONJ NOUN ADV VERB DET SPACE NOUN VERB DET NOUN PUNCT PRON VERB DET NOUN PART SPACE VERB NOUN ADP DET NOUN PART VERB PRON ADJ SPACE ADJ CCONJ VERB ADP NOUN PUNCT PRON NOUN AUX PART VERB NOUN PART VERB ADJ NOUN SPACE ADV ADP NOUN PUNCT CCONJ ADP VERB ADP VERB DET NOUN VERB PART VERB DET SPACE NOUN VERB PUNCT VERB SCONJ DET ADJ NOUN ADP VERB NOUN ADP DET NOUN SPACE PUNCT PRON VERB DET ADJ NOUN ADP VERB NOUN ADP SPACE DET NOUN ADP ADJ NOUN ADP DET NOUN NOUN PUNCT PRON NOUN PUNCT PROPN PUNCT VERB PROPN NUM ADP SPACE DET VERB NOUN ADP NOUN SCONJ DET NOUN NOUN NOUN ADJ CCONJ AUX VERB ADP SPACE ADJ NOUN PUNCT DET NOUN AUX NOUN ADJ PUNCT CCONJ ADV DET NOUN ADP DET NOUN AUX SPACE VERB PART VERB ADP DET NOUN PUNCT ADP DET NOUN PUNCT PRON SPACE NOUN AUX AUX VERB ADP DET NOUN ADP ADJ NOUN PRON VERB VERB SPACE DET NOUN PUNCT,0.586046511627907,23.88888888888889,4.758139534883721
132,46,Aman Madaan,"[' Language models are now better than ever before at\ngenerating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense\nis in misunderstanding a user’s intent. The typical\nremedy of retraining with more data is prohibitive\ndue to the cost and infrastructure requirements. In\nsuch cases, even if users repeatedly observe the\nmodel making a mistake, there are no avenues to\nprovide feedback to the model to make it more\naccurate and personalized over time. Our goal is to allow users to correct such errors\ndirectly through interaction, and without retraining by injecting the knowledge required to correct the\nmodel’s misunderstanding. Building upon the recent success of injecting commonsense in the input\n(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in\nthe input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with\na growing memory of cases where the model misunderstood user’s intent and was provided with\ncorrective feedback. This feedback is question dependent, and thus the prompt for each sample is\nedited to adapt to the input. In this sense, our\nwork can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing\nthe prompts.', 'Our work adds interactivity to prompt\nengineering as it involves dynamically updating the\nprompt for every instance. Figure 1 presents a sample interaction between a\nuser and GPT-3 that our setup enables. The model\nwas asked for a similar word. However, the model’s\n(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task\ninstruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,\nnote that such instructional correction is feasible\neven if the user does not know the correct answer to\ntheir question, as they are critiquing the model’s understanding of their intent, rather than the answers\nthemselves. Thus, our setup does not require the\nusers to be experts at tasks being solved, another\nadvantage of our approach. Further, it is desirable to have a system that can\nleverage past feedback on new, unseen examples\nfor prompt-editing. We maintain a memory M of\nsuch feedback as a set of key-value pairs, where the\nkey is a misunderstood question, and the value is\nthe user’s feedback to correct that misunderstanding. Given a new question, we check if the model\nhas made a mistake on a similar question earlier,\nby querying the memory for a similar question.', 'If\nfound, append the corresponding feedback to the\nquestion prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive\nreminding in psychology (Jacoby and Wahlheim,\n2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for\nthe system and provides representative implementations for each component. We then demonstrate\nthe system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure\n1), (2) word scrambling (e.g., anagrams), (3) ethical\nreasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)\nethics reasoning with user feedback being natural\nlanguage. We find that in all cases, GPT-3’s accuracy significantly increases with time, without\nretraining, as our approach enables it to use corrective feedback from earlier examples to avoid\nsimilar misunderstandings on future examples. In\nsummary, our contributions are:• We show that a large model like GPT-3 can be\nimproved after deployment, without retraining,\nthrough a memory-assisted architecture. • Our implementation, MemPrompt, is the first\ndemonstration that this is possible - this is an important step forward for real use of LMs, and the\npaper sets out a general architecture that others can\nbuild on, a specific implementation, and detailed\nevaluation on multiple tasks.']",intro_chunked,"Our work adds interactivity to prompt
engineering as it involves dynamically updating the
prompt for every instance. Figure 1 presents a sample interaction between a
user and GPT-3 that our setup enables. The model
was asked for a similar word. However, the model’s
(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task
instruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,
note that such instructional correction is feasible
even if the user does not know the correct answer to
their question, as they are critiquing the model’s understanding of their intent, rather than the answers
themselves. Thus, our setup does not require the
users to be experts at tasks being solved, another
advantage of our approach. Further, it is desirable to have a system that can
leverage past feedback on new, unseen examples
for prompt-editing. We maintain a memory M of
such feedback as a set of key-value pairs, where the
key is a misunderstood question, and the value is
the user’s feedback to correct that misunderstanding. Given a new question, we check if the model
has made a mistake on a similar question earlier,
by querying the memory for a similar question.",43.43510526315791,30.71625077440773,132,0.21850548684597015," Our work adds interactivity to prompt 
 engineering as it involves dynamically updating the 
 prompt for every instance. Figure 0 presents a sample interaction between a 
 user and Propname 0 that our setup enables. The model 
 was asked for a similar word. However, the models task understanding was The homophone of good is. The user can detect such discrepancy between the intended and interpreted task 
 instruction, and can provide feedback as similar to means with a similar meaning, clarifying that they actually wanted a synonym. Crucially, 
 note that such instructional correction is feasible 
 even if the user does not know the correct answer to 
 their question, as they are critiquing the models understanding of their intent, rather than the answers 
 themselves. Thus, our setup does not require the 
 users to be experts at tasks being solved, another 
 advantage of our approach. Further, it is desirable to have a system that can 
 leverage past feedback on new, unseen examples 
 for prompt editing. We maintain a memory Propname of 
 such feedback as a set of key value pairs, where the 
 key is a misunderstood question, and the value is 
 the users feedback to correct that misunderstanding. Given a new question, we check if the model 
 has made a mistake on a similar question earlier, 
 by querying the memory for a similar question."," Our work adds interactivity to prompt 
 engineering as it involves dynamically updating the 
 prompt for every instance. Figure 0 presents a sample interaction between a 
 user and Propname 0 that our setup enables. The model 
 was asked for a similar word. However, the models task understanding was The homophone of good is. The user can detect such discrepancy between the intended and interpreted task 
 instruction, and can provide feedback as similar to means with a similar meaning, clarifying that they actually wanted a synonym. Crucially, 
 note that such instructional correction is feasible 
 even if the user does not know the correct answer to 
 their question, as they are critiquing the models understanding of their intent, rather than the answers 
 themselves. Thus, our setup does not require the 
 users to be experts at tasks being solved, another 
 advantage of our approach. Further, it is desirable to have a system that can 
 leverage past feedback on new, unseen examples 
 for prompt editing. We maintain a memory Propname of 
 such feedback as a set of key value pairs, where the 
 key is a misunderstood question, and the value is 
 the users feedback to correct that misunderstanding. Given a new question, we check if the model 
 has made a mistake on a similar question earlier, 
 by querying the memory for a similar question.", PRON NOUN VERB NOUN PART VERB SPACE NOUN SCONJ PRON VERB ADV VERB DET SPACE NOUN ADP DET NOUN PUNCT NOUN NUM VERB DET NOUN NOUN ADP DET SPACE NOUN CCONJ PROPN NUM SCONJ PRON NOUN VERB PUNCT DET NOUN SPACE AUX VERB ADP DET ADJ NOUN PUNCT ADV PUNCT DET NOUN NOUN NOUN AUX DET NOUN ADP ADJ AUX PUNCT DET NOUN AUX VERB ADJ NOUN ADP DET VERB CCONJ VERB NOUN SPACE NOUN PUNCT CCONJ AUX VERB NOUN ADV ADJ ADP NOUN ADP DET ADJ NOUN PUNCT VERB SCONJ PRON ADV VERB DET NOUN PUNCT ADV PUNCT SPACE VERB SCONJ ADJ ADJ NOUN AUX ADJ SPACE ADV SCONJ DET NOUN AUX PART VERB DET ADJ NOUN ADP SPACE PRON NOUN PUNCT SCONJ PRON AUX VERB DET NOUN NOUN ADP PRON NOUN PUNCT ADV ADP DET NOUN SPACE PRON PUNCT ADV PUNCT PRON NOUN AUX PART VERB DET SPACE NOUN PART AUX NOUN ADP NOUN AUX VERB PUNCT DET SPACE NOUN ADP PRON NOUN PUNCT ADV PUNCT PRON AUX ADJ PART VERB DET NOUN PRON AUX SPACE VERB ADP NOUN ADP ADJ PUNCT ADJ NOUN SPACE ADP ADJ NOUN PUNCT PRON VERB DET NOUN PROPN ADP SPACE ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT SCONJ DET SPACE NOUN AUX DET NOUN NOUN PUNCT CCONJ DET NOUN AUX SPACE DET NOUN VERB PART VERB DET NOUN PUNCT VERB DET ADJ NOUN PUNCT PRON VERB SCONJ DET NOUN SPACE AUX VERB DET NOUN ADP DET ADJ NOUN ADV PUNCT SPACE ADP VERB DET NOUN ADP DET ADJ NOUN PUNCT,0.5185185185185185,24.3,4.5473251028806585
133,47,Aman Madaan,"[' Language models are now better than ever before at\ngenerating realistic content, but still lack commonsense (Bender and Koller, 2020; Marcus, 2021). One failure mode due to a lack of commonsense\nis in misunderstanding a user’s intent. The typical\nremedy of retraining with more data is prohibitive\ndue to the cost and infrastructure requirements. In\nsuch cases, even if users repeatedly observe the\nmodel making a mistake, there are no avenues to\nprovide feedback to the model to make it more\naccurate and personalized over time. Our goal is to allow users to correct such errors\ndirectly through interaction, and without retraining by injecting the knowledge required to correct the\nmodel’s misunderstanding. Building upon the recent success of injecting commonsense in the input\n(Lewis et al., 2020; Talmor et al., 2020), we propose a novel approach of injecting knowledge in\nthe input via interactive feedback from an end-user. Our approach, MemPrompt, pairs GPT-3 with\na growing memory of cases where the model misunderstood user’s intent and was provided with\ncorrective feedback. This feedback is question dependent, and thus the prompt for each sample is\nedited to adapt to the input. In this sense, our\nwork can be seen as an instance of prompt engineering (Liu et al., 2021b) which involves editing\nthe prompts.', 'Our work adds interactivity to prompt\nengineering as it involves dynamically updating the\nprompt for every instance. Figure 1 presents a sample interaction between a\nuser and GPT-3 that our setup enables. The model\nwas asked for a similar word. However, the model’s\n(incorrect) task understanding was “The homophone of good is”. The user can detect such discrepancy between the intended and interpreted task\ninstruction, and can provide feedback as ""similar to means with a similar meaning"", clarifying that they actually wanted a synonym. Crucially,\nnote that such instructional correction is feasible\neven if the user does not know the correct answer to\ntheir question, as they are critiquing the model’s understanding of their intent, rather than the answers\nthemselves. Thus, our setup does not require the\nusers to be experts at tasks being solved, another\nadvantage of our approach. Further, it is desirable to have a system that can\nleverage past feedback on new, unseen examples\nfor prompt-editing. We maintain a memory M of\nsuch feedback as a set of key-value pairs, where the\nkey is a misunderstood question, and the value is\nthe user’s feedback to correct that misunderstanding. Given a new question, we check if the model\nhas made a mistake on a similar question earlier,\nby querying the memory for a similar question.', 'If\nfound, append the corresponding feedback to the\nquestion prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive\nreminding in psychology (Jacoby and Wahlheim,\n2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for\nthe system and provides representative implementations for each component. We then demonstrate\nthe system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure\n1), (2) word scrambling (e.g., anagrams), (3) ethical\nreasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)\nethics reasoning with user feedback being natural\nlanguage. We find that in all cases, GPT-3’s accuracy significantly increases with time, without\nretraining, as our approach enables it to use corrective feedback from earlier examples to avoid\nsimilar misunderstandings on future examples. In\nsummary, our contributions are:• We show that a large model like GPT-3 can be\nimproved after deployment, without retraining,\nthrough a memory-assisted architecture. • Our implementation, MemPrompt, is the first\ndemonstration that this is possible - this is an important step forward for real use of LMs, and the\npaper sets out a general architecture that others can\nbuild on, a specific implementation, and detailed\nevaluation on multiple tasks.']",intro_chunked,"If
found, append the corresponding feedback to the
question prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure-driven reminding mechanism draws inspiration from the theory of recursive
reminding in psychology (Jacoby and Wahlheim,
2013), which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for
the system and provides representative implementations for each component. We then demonstrate
the system on four tasks, using simulated user feedback: (1) lexical relations (e.g., antonyms, Figure
1), (2) word scrambling (e.g., anagrams), (3) ethical
reasoning with user feedback being the appropriate class of ethical consideration, e.g., “it is about cheating”, using a small set of categories, and (4)
ethics reasoning with user feedback being natural
language. We find that in all cases, GPT-3’s accuracy significantly increases with time, without
retraining, as our approach enables it to use corrective feedback from earlier examples to avoid
similar misunderstandings on future examples. In
summary, our contributions are:• We show that a large model like GPT-3 can be
improved after deployment, without retraining,
through a memory-assisted architecture. • Our implementation, MemPrompt, is the first
demonstration that this is possible - this is an important step forward for real use of LMs, and the
paper sets out a general architecture that others can
build on, a specific implementation, and detailed
evaluation on multiple tasks.",23.8636342592593,30.71625077440773,133,0.4682559669017792," If 
 found, append the corresponding feedback to the 
 question prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure driven reminding mechanism draws inspiration from the theory of recursive 
 reminding in psychology Propname and Propname, 
 0000, which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for 
 the system and provides representative implementations for each component. We then demonstrate 
 the system on four tasks, using simulated user feedback: lexical relations Propname, antonyms, Propname 
 0, word scrambling, ethical 
 reasoning with user feedback being the appropriate class of ethical consideration, Propname, it is about cheating, using a small set of categories, and ethics reasoning with user feedback being natural 
 language. We find that in all cases, Propname 0s accuracy significantly increases with time, without 
 retraining, as our approach enables it to use corrective feedback from earlier examples to avoid 
 similar misunderstandings on future examples. In 
 summary, our contributions are: We show that a large model like Propname 0 can be 
 improved after deployment, without retraining, 
 through a memory assisted architecture. Our implementation, Propname, is the first 
 demonstration that this is possible this is an important step forward for real use of LMs, and the 
 paper sets out a general architecture that others can 
 build on, a specific implementation, and detailed 
 evaluation on multiple tasks."," If 
 found, append the corresponding feedback to the 
 question prompt. This mechanism aims to prevent the model from making the same type of mistake twice. This failure driven reminding mechanism draws inspiration from the theory of recursive 
 reminding in psychology Propname and Propname, 
 0000, which suggests humans index error corrections in the context in which those errors occurred. This paper presents the general architecture for 
 the system and provides representative implementations for each component. We then demonstrate 
 the system on four tasks, using simulated user feedback: lexical relations Propname, antonyms, Propname 
 0, word scrambling, ethical 
 reasoning with user feedback being the appropriate class of ethical consideration, Propname, it is about cheating, using a small set of categories, and ethics reasoning with user feedback being natural 
 language. We find that in all cases, Propname 0s accuracy significantly increases with time, without 
 retraining, as our approach enables it to use corrective feedback from earlier examples to avoid 
 similar misunderstandings on future examples. In 
 summary, our contributions are: We show that a large model like Propname 0 can be 
 improved after deployment, without retraining, 
 through a memory assisted architecture. Our implementation, Propname, is the first 
 demonstration that this is possible this is an important step forward for real use of LMs, and the 
 paper sets out a general architecture that others can 
 build on, a specific implementation, and detailed 
 evaluation on multiple tasks.", SCONJ SPACE VERB PUNCT VERB DET ADJ NOUN ADP DET SPACE NOUN NOUN PUNCT DET NOUN VERB PART VERB DET NOUN ADP VERB DET ADJ NOUN ADP NOUN ADV PUNCT DET NOUN VERB NOUN NOUN VERB NOUN ADP DET NOUN ADP ADJ SPACE NOUN ADP NOUN PROPN CCONJ PROPN PUNCT SPACE NUM PUNCT PRON VERB NOUN NOUN NOUN NOUN ADP DET NOUN ADP PRON DET NOUN VERB PUNCT DET NOUN VERB DET ADJ NOUN ADP SPACE DET NOUN CCONJ VERB ADJ NOUN ADP DET NOUN PUNCT PRON ADV VERB SPACE DET NOUN ADP NUM NOUN PUNCT VERB VERB NOUN NOUN PUNCT ADJ NOUN PROPN PUNCT NOUN PUNCT PROPN SPACE NUM PUNCT NOUN NOUN PUNCT ADJ SPACE NOUN ADP NOUN NOUN AUX DET ADJ NOUN ADP ADJ NOUN PUNCT PROPN PUNCT PRON AUX ADP VERB PUNCT VERB DET ADJ NOUN ADP NOUN PUNCT CCONJ NOUN VERB ADP NOUN NOUN AUX ADJ SPACE NOUN PUNCT PRON VERB SCONJ ADP DET NOUN PUNCT PROPN NUM NOUN ADV VERB ADP NOUN PUNCT ADP SPACE VERB PUNCT SCONJ PRON NOUN VERB PRON PART VERB ADJ NOUN ADP ADJ NOUN PART VERB SPACE ADJ NOUN ADP ADJ NOUN PUNCT ADP SPACE NOUN PUNCT PRON NOUN AUX PUNCT PRON VERB SCONJ DET ADJ NOUN ADP PROPN NUM AUX AUX SPACE VERB ADP NOUN PUNCT ADP VERB PUNCT SPACE ADP DET NOUN VERB NOUN PUNCT PRON NOUN PUNCT PROPN PUNCT AUX DET ADJ SPACE NOUN SCONJ PRON AUX ADJ PRON AUX DET ADJ NOUN ADV ADP ADJ NOUN ADP NOUN PUNCT CCONJ DET SPACE NOUN NOUN ADP DET ADJ NOUN PRON NOUN AUX SPACE VERB ADP PUNCT DET ADJ NOUN PUNCT CCONJ VERB SPACE NOUN ADP ADJ NOUN PUNCT,0.5627376425855514,32.875,5.0418250950570345
134,48,Aman Madaan,"[' Defeasible inference is a mode of reasoning where\nadditional information can modify conclusions\n(Koons, 2017). Here we consider the specific formulation and challenge in Rudinger et al. (2020):\nGiven that some premise P plausibly implies a hypothesis H, does new information that the situation\nis S weaken or strengthen the conclusion H? For\nexample, consider the premise “The drinking glass\nfell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow”\nhere weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans\nwith an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan et al.,\n2021), which we use in this paper, draws connections between the P, H, and S through mediating events. This can be seen as a mental model of the\nquestion scenario before answering the question\n(Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario\nwith inference graphs help machines in defeasible\nreasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then,\nuse that graph as an additional input when answering the defeasible reasoning query. Our proposed\nsystem, CURIOUS, comprises a graph generation\nmodule and a graph encoding module to use the\ngenerated graph for the query (Figure 2).', 'To generate inference graphs, we build upon past\nwork that uses a sequence to sequence approach\n(Madaan et al., 2021). However, our analysis revealed that the graphs can often be erroneous, and\nCURIOUS also includes an error correction module\nto generate higher quality inference graphs. This\nwas important because we found that better graphs\nare more helpful in the downstream QA task. The generated inference graph is then used for\nthe QA task on three existing defeasible inference\ndatasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015),\nδ-SOCIAL (reasoning about social norms) (Forbes\net al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way\nthe graph is encoded for input is important. If we\nsimply augment the question with the generated\ngraphs, there are some gains on all datasets. However, the accuracy improves substantially across\nall datasets with a more judicious encoding of the\ngraph-augmented question that accounts for interactions between the graph nodes. To achieve this,\nwe use the mixture of experts approach (Jacobs\net al., 1991) to include a mixture of experts layers\nduring encoding, enabling the ability to attend to\nspecific nodes while capturing their interactions\nselectively. In summary, our contributiion is in drawing on\nthe idea of an inference graph from cognitive science to show benefits in a defeasible inference QA\ntask. Using an error correction module in the graph\ngeneration process, and a judicious encoding of the\ngraph augmented question, CURIOUS achieves a\nnew state-of-the-art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to “think about""\na question before answering can improve performance.']",intro_chunked," Defeasible inference is a mode of reasoning where
additional information can modify conclusions
(Koons, 2017). Here we consider the specific formulation and challenge in Rudinger et al. (2020):
Given that some premise P plausibly implies a hypothesis H, does new information that the situation
is S weaken or strengthen the conclusion H? For
example, consider the premise “The drinking glass
fell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow”
here weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans
with an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan et al.,
2021), which we use in this paper, draws connections between the P, H, and S through mediating events. This can be seen as a mental model of the
question scenario before answering the question
(Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario
with inference graphs help machines in defeasible
reasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then,
use that graph as an additional input when answering the defeasible reasoning query. Our proposed
system, CURIOUS, comprises a graph generation
module and a graph encoding module to use the
generated graph for the query (Figure 2).",43.95382943143815,30.71625077440773,134,0.19608403742313385," Defeasible inference is a mode of reasoning where 
 additional information can modify conclusions 
. Here we consider the specific formulation and challenge in Propname Propname Propname.: 
 Given that some premise P plausibly implies a hypothesis H, does new information that the situation 
 is Propname weaken or strengthen the conclusion H? For 
 example, consider the premise The drinking glass 
 fell with a possible implication The glass broke. New information that The glass fell on a pillow 
 here weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans 
 with an inference graph. Inference graphs formulation in Propname Propname Propname Propname, 
 0000, which we use in this paper, draws connections between the Propname, Propname, and Propname through mediating events. This can be seen as a mental model of the 
 question scenario before answering the question 
. This paper asks the natural question: can modeling the question scenario 
 with inference graphs help machines in defeasible 
 reasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then, 
 use that graph as an additional input when answering the defeasible reasoning query. Our proposed 
 system, Propname, comprises a graph generation 
 module and a graph encoding module to use the 
 generated graph for the query."," Defeasible inference is a mode of reasoning where 
 additional information can modify conclusions 
. Here we consider the specific formulation and challenge in Propname Propname Propname.: 
 Given that some premise P plausibly implies a hypothesis H, does new information that the situation 
 is Propname weaken or strengthen the conclusion H? For 
 example, consider the premise The drinking glass 
 fell with a possible implication The glass broke. New information that The glass fell on a pillow 
 here weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans 
 with an inference graph. Inference graphs formulation in Propname Propname Propname Propname, 
 0000, which we use in this paper, draws connections between the Propname, Propname, and Propname through mediating events. This can be seen as a mental model of the 
 question scenario before answering the question 
. This paper asks the natural question: can modeling the question scenario 
 with inference graphs help machines in defeasible 
 reasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then, 
 use that graph as an additional input when answering the defeasible reasoning query. Our proposed 
 system, Propname, comprises a graph generation 
 module and a graph encoding module to use the 
 generated graph for the query.", ADJ NOUN AUX DET NOUN ADP NOUN SCONJ SPACE ADJ NOUN AUX VERB NOUN SPACE PUNCT ADV PRON VERB DET ADJ NOUN CCONJ NOUN ADP PROPN PROPN PROPN PUNCT PUNCT SPACE VERB SCONJ DET NOUN NOUN ADV VERB DET NOUN NOUN PUNCT VERB ADJ NOUN SCONJ DET NOUN SPACE AUX PROPN VERB CCONJ VERB DET NOUN NOUN PUNCT ADP SPACE NOUN PUNCT VERB DET NOUN DET NOUN NOUN SPACE VERB ADP DET ADJ NOUN DET NOUN VERB PUNCT ADJ NOUN SCONJ DET NOUN VERB ADP DET NOUN SPACE ADV VERB DET NOUN PUNCT PRON VERB NOUN ADP DET ADJ NOUN NOUN PRON VERB ADJ NOUN ADP NOUN SPACE ADP DET NOUN NOUN PUNCT NOUN NOUN NOUN ADP PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PRON PRON VERB ADP DET NOUN PUNCT VERB NOUN ADP DET PROPN PUNCT PROPN PUNCT CCONJ PROPN ADP VERB NOUN PUNCT PRON AUX AUX VERB ADP DET ADJ NOUN ADP DET SPACE NOUN NOUN ADP VERB DET NOUN SPACE PUNCT DET NOUN VERB DET ADJ NOUN PUNCT AUX VERB DET NOUN NOUN SPACE ADP NOUN NOUN VERB NOUN ADP ADJ SPACE NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT ADV PUNCT VERB DET NOUN PUNCT VERB DET NOUN NOUN VERB ADJ NOUN ADP NOUN NOUN PUNCT ADV PUNCT SPACE VERB DET NOUN ADP DET ADJ NOUN SCONJ VERB DET ADJ NOUN NOUN PUNCT PRON VERB SPACE NOUN PUNCT PROPN PUNCT VERB DET NOUN NOUN SPACE NOUN CCONJ DET NOUN VERB NOUN PART VERB DET SPACE VERB NOUN ADP DET NOUN PUNCT,0.5062761506276151,18.384615384615383,5.083682008368201
135,49,Aman Madaan,"[' Defeasible inference is a mode of reasoning where\nadditional information can modify conclusions\n(Koons, 2017). Here we consider the specific formulation and challenge in Rudinger et al. (2020):\nGiven that some premise P plausibly implies a hypothesis H, does new information that the situation\nis S weaken or strengthen the conclusion H? For\nexample, consider the premise “The drinking glass\nfell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow”\nhere weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans\nwith an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan et al.,\n2021), which we use in this paper, draws connections between the P, H, and S through mediating events. This can be seen as a mental model of the\nquestion scenario before answering the question\n(Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario\nwith inference graphs help machines in defeasible\nreasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then,\nuse that graph as an additional input when answering the defeasible reasoning query. Our proposed\nsystem, CURIOUS, comprises a graph generation\nmodule and a graph encoding module to use the\ngenerated graph for the query (Figure 2).', 'To generate inference graphs, we build upon past\nwork that uses a sequence to sequence approach\n(Madaan et al., 2021). However, our analysis revealed that the graphs can often be erroneous, and\nCURIOUS also includes an error correction module\nto generate higher quality inference graphs. This\nwas important because we found that better graphs\nare more helpful in the downstream QA task. The generated inference graph is then used for\nthe QA task on three existing defeasible inference\ndatasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015),\nδ-SOCIAL (reasoning about social norms) (Forbes\net al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way\nthe graph is encoded for input is important. If we\nsimply augment the question with the generated\ngraphs, there are some gains on all datasets. However, the accuracy improves substantially across\nall datasets with a more judicious encoding of the\ngraph-augmented question that accounts for interactions between the graph nodes. To achieve this,\nwe use the mixture of experts approach (Jacobs\net al., 1991) to include a mixture of experts layers\nduring encoding, enabling the ability to attend to\nspecific nodes while capturing their interactions\nselectively. In summary, our contributiion is in drawing on\nthe idea of an inference graph from cognitive science to show benefits in a defeasible inference QA\ntask. Using an error correction module in the graph\ngeneration process, and a judicious encoding of the\ngraph augmented question, CURIOUS achieves a\nnew state-of-the-art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to “think about""\na question before answering can improve performance.']",intro_chunked,"To generate inference graphs, we build upon past
work that uses a sequence to sequence approach
(Madaan et al., 2021). However, our analysis revealed that the graphs can often be erroneous, and
CURIOUS also includes an error correction module
to generate higher quality inference graphs. This
was important because we found that better graphs
are more helpful in the downstream QA task. The generated inference graph is then used for
the QA task on three existing defeasible inference
datasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015),
δ-SOCIAL (reasoning about social norms) (Forbes
et al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way
the graph is encoded for input is important. If we
simply augment the question with the generated
graphs, there are some gains on all datasets. However, the accuracy improves substantially across
all datasets with a more judicious encoding of the
graph-augmented question that accounts for interactions between the graph nodes. To achieve this,
we use the mixture of experts approach (Jacobs
et al., 1991) to include a mixture of experts layers
during encoding, enabling the ability to attend to
specific nodes while capturing their interactions
selectively. In summary, our contributiion is in drawing on
the idea of an inference graph from cognitive science to show benefits in a defeasible inference QA
task. Using an error correction module in the graph
generation process, and a judicious encoding of the
graph augmented question, CURIOUS achieves a
new state-of-the-art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to “think about""
a question before answering can improve performance.",33.285340091563114,30.71625077440773,135,0.6046884059906006," To generate inference graphs, we build upon past 
 work that uses a sequence to sequence approach 
. However, our analysis revealed that the graphs can often be erroneous, and 
 Propname also includes an error correction module 
 to generate higher quality inference graphs. This 
 was important because we found that better graphs 
 are more helpful in the downstream Propname task. The generated inference graph is then used for 
 the Propname task on three existing defeasible inference 
 datasets from diverse domains, Propname Propname, Propname, Propname Propname 
 Propname Propname Propname, 0000, and Propname. We show that the way 
 the graph is encoded for input is important. If we 
 simply augment the question with the generated 
 graphs, there are some gains on all datasets. However, the accuracy improves substantially across 
 all datasets with a more judicious encoding of the 
 graph augmented question that accounts for interactions between the graph Propname. To achieve this, 
 we use the mixture of experts approach Propname 
 Propname Propname Propname, 0000 to include a mixture of experts layers 
 during encoding, enabling the ability to attend to 
 specific nodes while capturing their interactions 
 selectively. In summary, our contributiion is in drawing on 
 the idea of an inference graph from cognitive science to show benefits in a defeasible inference QA 
 task. Using an error correction module in the graph 
 generation process, and a judicious encoding of the 
 graph augmented question, Propname achieves a 
 new state of the art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to think about 
 a question before answering can improve performance."," To generate inference graphs, we build upon past 
 work that uses a sequence to sequence approach 
. However, our analysis revealed that the graphs can often be erroneous, and 
 Propname also includes an error correction module 
 to generate higher quality inference graphs. This 
 was important because we found that better graphs 
 are more helpful in the downstream Propname task. The generated inference graph is then used for 
 the Propname task on three existing defeasible inference 
 datasets from diverse domains, Propname Propname, Propname, Propname Propname 
 Propname Propname Propname, 0000, and Propname. We show that the way 
 the graph is encoded for input is important. If we 
 simply augment the question with the generated 
 graphs, there are some gains on all datasets. However, the accuracy improves substantially across 
 all datasets with a more judicious encoding of the 
 graph augmented question that accounts for interactions between the graph Propname. To achieve this, 
 we use the mixture of experts approach Propname 
 Propname Propname Propname, 0000 to include a mixture of experts layers 
 during encoding, enabling the ability to attend to 
 specific nodes while capturing their interactions 
 selectively. In summary, our contributiion is in drawing on 
 the idea of an inference graph from cognitive science to show benefits in a defeasible inference QA 
 task. Using an error correction module in the graph 
 generation process, and a judicious encoding of the 
 graph augmented question, Propname achieves a 
 new state of the art over three defeasible datasets. This result is significant also because our work illustrates that guiding a system to think about 
 a question before answering can improve performance.", PART VERB NOUN NOUN PUNCT PRON VERB SCONJ ADJ SPACE NOUN PRON VERB DET NOUN PART NOUN NOUN SPACE PUNCT ADV PUNCT PRON NOUN VERB SCONJ DET NOUN AUX ADV AUX ADJ PUNCT CCONJ SPACE PROPN ADV VERB DET NOUN NOUN NOUN SPACE PART VERB ADJ NOUN NOUN NOUN PUNCT PRON SPACE AUX ADJ SCONJ PRON VERB SCONJ ADJ NOUN SPACE AUX ADV ADJ ADP DET ADJ PROPN NOUN PUNCT DET VERB NOUN NOUN AUX ADV VERB ADP SPACE DET PROPN NOUN ADP NUM VERB ADJ NOUN SPACE NOUN ADP ADJ NOUN PUNCT PROPN PROPN PUNCT PROPN PUNCT PROPN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT CCONJ PROPN PUNCT PRON VERB SCONJ DET NOUN SPACE DET NOUN AUX VERB ADP NOUN AUX ADJ PUNCT SCONJ PRON SPACE ADV VERB DET NOUN ADP DET VERB SPACE NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN PUNCT ADV PUNCT DET NOUN VERB ADV ADP SPACE DET NOUN ADP DET ADV ADJ NOUN ADP DET SPACE NOUN VERB NOUN PRON VERB ADP NOUN ADP DET NOUN PROPN PUNCT PART VERB PRON PUNCT SPACE PRON VERB DET NOUN ADP NOUN VERB PROPN SPACE PROPN PROPN PROPN PUNCT NUM PART VERB DET NOUN ADP NOUN NOUN SPACE ADP NOUN PUNCT VERB DET NOUN PART VERB ADP SPACE ADJ NOUN SCONJ VERB PRON NOUN SPACE ADV PUNCT ADP NOUN PUNCT PRON NOUN AUX ADP VERB ADP SPACE DET NOUN ADP DET NOUN NOUN ADP ADJ NOUN PART VERB NOUN ADP DET ADJ NOUN NOUN SPACE NOUN PUNCT VERB DET NOUN NOUN NOUN ADP DET NOUN SPACE NOUN NOUN PUNCT CCONJ DET ADJ NOUN ADP DET SPACE NOUN VERB NOUN PUNCT PROPN VERB DET SPACE ADJ NOUN ADP DET NOUN ADP NUM ADJ NOUN PUNCT DET NOUN AUX ADJ ADV SCONJ PRON NOUN VERB SCONJ VERB DET NOUN PART VERB ADP SPACE DET NOUN ADP VERB AUX VERB NOUN PUNCT,0.46366782006920415,26.272727272727273,4.996539792387543
136,50,Aman Madaan,"[' Defeasible inference (Rudinger et al., 2020) is a\nmode of reasoning in which given a premise P\n(Rob went for a hike), a hypothesis H (Rob saw\nan elephant, it was pink) may be weakened or\noverturned in light of new evidence i.e., an update U (Rob often has hallucinations). Given the\nnon-monotonic nature of this reasoning, humans\nfind it challenging to master this task (Morgan,\n2004). This problem has been widely studied in\nclassical AI through logic (Israel, 1980; McCarthy,\n1981), and in cognitive science through argumentative models (Pollock, 1987). A prominent approach\nis to support defeasible inference through argumentations by constructing an inference graph (Pollock,\n2009). Despite their prominence (Bentahar et al., 2010),\nargumentative models are not scalable because an\ninference graph needs to be handcrafted for every\nexample. Recently, Rudinger et al.', '(2020) proposed\ntwo auxiliary tasks related to defeasible inference:\n(i) an NLI task to predict whether an update U\nwould weaken or strengthen a hypothesis H, and\n(ii) a generative task to generate an update U given\na premise P and a hypothesis H. However, this\nonly addresses a part of the problem because their\ninference is still not supported by the line of reasoning that a human typically uses to solve this\ntask, namely mediators (e.g., hallucinations can be\ndeceptive) and contextualizers (some elephants can\nhave mutated gene which makes them look different) that are inherently embedded in an inference\ngraph, limiting their utility for humans (figure 1).In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive\nscience and provide a computational model to make\ntheir generation scalable. Training such a model\nwould require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in NLP (Tandon et al., 2019),\nwhere the reasoning is supported by a graph that we\nfind has similarities with the kind of reasoning that\nan inference graph supports. We train a model that\ncan learn from the NLP task and effectively transfer\nit to generate inference graphs. Such transfer learning is made possible due to the powerful seq-to-seq\nneural language models that did not exist before. The contributions of this paper are the answers\nto the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In\n§2, we show that we can effectively construct\nmeaningful graphs using transfer learning. Can our generated graphs help improve human performance? In §3, we show that humans\nleverage generated graphs to improve their performance on a previously reported benchmark.']",intro_chunked," Defeasible inference (Rudinger et al., 2020) is a
mode of reasoning in which given a premise P
(Rob went for a hike), a hypothesis H (Rob saw
an elephant, it was pink) may be weakened or
overturned in light of new evidence i.e., an update U (Rob often has hallucinations). Given the
non-monotonic nature of this reasoning, humans
find it challenging to master this task (Morgan,
2004). This problem has been widely studied in
classical AI through logic (Israel, 1980; McCarthy,
1981), and in cognitive science through argumentative models (Pollock, 1987). A prominent approach
is to support defeasible inference through argumentations by constructing an inference graph (Pollock,
2009). Despite their prominence (Bentahar et al., 2010),
argumentative models are not scalable because an
inference graph needs to be handcrafted for every
example. Recently, Rudinger et al.",40.13274509803924,30.71625077440773,136,0.31190165877342224," Defeasible inference is a 
 mode of reasoning in which given a premise P 
, a hypothesis Propname Propname saw 
 an elephant, it was pink may be weakened or 
 overturned in light of new evidence ie, an update U. Given the 
 non monotonic nature of this reasoning, humans 
 find it challenging to master this task Propname, 
 0000. This problem has been widely studied in 
 classical Propname through logic Propname, 0000; Propname, 
 0000, and in cognitive science through argumentative models. A prominent approach 
 is to support defeasible inference through argumentations by constructing an inference graph Propname, 
 0000. Despite their prominence, 
 argumentative models are not scalable because an 
 inference graph needs to be handcrafted for every 
 example. Recently, Propname Propname Propname."," Defeasible inference is a 
 mode of reasoning in which given a premise P 
, a hypothesis Propname Propname saw 
 an elephant, it was pink may be weakened or 
 overturned in light of new evidence ie, an update U. Given the 
 non monotonic nature of this reasoning, humans 
 find it challenging to master this task Propname, 
 0000. This problem has been widely studied in 
 classical Propname through logic Propname, 0000; Propname, 
 0000, and in cognitive science through argumentative models. A prominent approach 
 is to support defeasible inference through argumentations by constructing an inference graph Propname, 
 0000. Despite their prominence, 
 argumentative models are not scalable because an 
 inference graph needs to be handcrafted for every 
 example. Recently, Propname Propname Propname.", ADJ NOUN AUX DET SPACE NOUN ADP NOUN ADP PRON VERB DET NOUN NOUN SPACE PUNCT DET NOUN PROPN PROPN VERB SPACE DET NOUN PUNCT PRON AUX ADJ AUX AUX VERB CCONJ SPACE VERB ADP NOUN ADP ADJ NOUN ADV PUNCT DET NOUN NOUN PUNCT VERB DET SPACE ADJ ADJ NOUN ADP DET NOUN PUNCT NOUN SPACE VERB PRON VERB PART VERB DET NOUN PROPN PUNCT SPACE NUM PUNCT DET NOUN AUX AUX ADV VERB ADP SPACE ADJ PROPN ADP NOUN PROPN PUNCT NUM PUNCT PROPN PUNCT SPACE NUM PUNCT CCONJ ADP ADJ NOUN ADP ADJ NOUN PUNCT DET ADJ NOUN SPACE AUX PART VERB ADJ NOUN ADP NOUN ADP VERB DET NOUN NOUN PROPN PUNCT SPACE NUM PUNCT SCONJ PRON NOUN PUNCT SPACE ADJ NOUN AUX PART ADJ SCONJ DET SPACE NOUN NOUN VERB PART AUX VERB ADP DET SPACE NOUN PUNCT ADV PUNCT PROPN PROPN PROPN PUNCT,0.6148148148148148,22.5,4.9037037037037035
137,51,Aman Madaan,"[' Defeasible inference (Rudinger et al., 2020) is a\nmode of reasoning in which given a premise P\n(Rob went for a hike), a hypothesis H (Rob saw\nan elephant, it was pink) may be weakened or\noverturned in light of new evidence i.e., an update U (Rob often has hallucinations). Given the\nnon-monotonic nature of this reasoning, humans\nfind it challenging to master this task (Morgan,\n2004). This problem has been widely studied in\nclassical AI through logic (Israel, 1980; McCarthy,\n1981), and in cognitive science through argumentative models (Pollock, 1987). A prominent approach\nis to support defeasible inference through argumentations by constructing an inference graph (Pollock,\n2009). Despite their prominence (Bentahar et al., 2010),\nargumentative models are not scalable because an\ninference graph needs to be handcrafted for every\nexample. Recently, Rudinger et al.', '(2020) proposed\ntwo auxiliary tasks related to defeasible inference:\n(i) an NLI task to predict whether an update U\nwould weaken or strengthen a hypothesis H, and\n(ii) a generative task to generate an update U given\na premise P and a hypothesis H. However, this\nonly addresses a part of the problem because their\ninference is still not supported by the line of reasoning that a human typically uses to solve this\ntask, namely mediators (e.g., hallucinations can be\ndeceptive) and contextualizers (some elephants can\nhave mutated gene which makes them look different) that are inherently embedded in an inference\ngraph, limiting their utility for humans (figure 1).In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive\nscience and provide a computational model to make\ntheir generation scalable. Training such a model\nwould require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in NLP (Tandon et al., 2019),\nwhere the reasoning is supported by a graph that we\nfind has similarities with the kind of reasoning that\nan inference graph supports. We train a model that\ncan learn from the NLP task and effectively transfer\nit to generate inference graphs. Such transfer learning is made possible due to the powerful seq-to-seq\nneural language models that did not exist before. The contributions of this paper are the answers\nto the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In\n§2, we show that we can effectively construct\nmeaningful graphs using transfer learning. Can our generated graphs help improve human performance? In §3, we show that humans\nleverage generated graphs to improve their performance on a previously reported benchmark.']",intro_chunked,"(2020) proposed
two auxiliary tasks related to defeasible inference:
(i) an NLI task to predict whether an update U
would weaken or strengthen a hypothesis H, and
(ii) a generative task to generate an update U given
a premise P and a hypothesis H. However, this
only addresses a part of the problem because their
inference is still not supported by the line of reasoning that a human typically uses to solve this
task, namely mediators (e.g., hallucinations can be
deceptive) and contextualizers (some elephants can
have mutated gene which makes them look different) that are inherently embedded in an inference
graph, limiting their utility for humans (figure 1).In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive
science and provide a computational model to make
their generation scalable. Training such a model
would require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in NLP (Tandon et al., 2019),
where the reasoning is supported by a graph that we
find has similarities with the kind of reasoning that
an inference graph supports. We train a model that
can learn from the NLP task and effectively transfer
it to generate inference graphs. Such transfer learning is made possible due to the powerful seq-to-seq
neural language models that did not exist before. The contributions of this paper are the answers
to the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In
§2, we show that we can effectively construct
meaningful graphs using transfer learning. Can our generated graphs help improve human performance? In §3, we show that humans
leverage generated graphs to improve their performance on a previously reported benchmark.",32.28776470588238,30.71625077440773,137,0.32333141565322876," proposed 
 two auxiliary tasks related to defeasible inference: an Propname task to predict whether an update U 
 would weaken or strengthen a hypothesis H, and a generative task to generate an update U given 
 a premise P and a hypothesis Propname However, this 
 only addresses a part of the problem because their 
 inference is still not supported by the line of reasoning that a human typically uses to solve this 
 task, namely mediators Propname, hallucinations can be 
 deceptive and contextualizers some elephants can 
 have mutated gene which makes them look different that are inherently embedded in an inference 
 graph, limiting their utility for humans.In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive 
 science and provide a computational model to make 
 their generation scalable. Training such a model 
 would require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in Propname, 
 where the reasoning is supported by a graph that we 
 find has similarities with the kind of reasoning that 
 an inference graph supports. We train a model that 
 can learn from the Propname task and effectively transfer 
 it to generate inference graphs. Such transfer learning is made possible due to the powerful seq to seq 
 neural language models that did not exist before. The contributions of this paper are the answers 
 to the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In 
 0, we show that we can effectively construct 
 meaningful graphs using transfer learning. Can our generated graphs help improve human performance? In 0, we show that humans 
 leverage generated graphs to improve their performance on a previously reported benchmark."," proposed 
 two auxiliary tasks related to defeasible inference: an Propname task to predict whether an update U 
 would weaken or strengthen a hypothesis H, and a generative task to generate an update U given 
 a premise P and a hypothesis Propname However, this 
 only addresses a part of the problem because their 
 inference is still not supported by the line of reasoning that a human typically uses to solve this 
 task, namely mediators Propname, hallucinations can be 
 deceptive and contextualizers some elephants can 
 have mutated gene which makes them look different that are inherently embedded in an inference 
 graph, limiting their utility for humans.In this paper, we adopt the concept of an inference graph for defeasible reasoning from cognitive 
 science and provide a computational model to make 
 their generation scalable. Training such a model 
 would require a large amount of annotated inference graphs, which will be too expensive to obtain. Instead, our solution is to draw a parallel to a related reasoning task in Propname, 
 where the reasoning is supported by a graph that we 
 find has similarities with the kind of reasoning that 
 an inference graph supports. We train a model that 
 can learn from the Propname task and effectively transfer 
 it to generate inference graphs. Such transfer learning is made possible due to the powerful seq to seq 
 neural language models that did not exist before. The contributions of this paper are the answers 
 to the following two research questions. Can we automate the construction of the argumentation supporting inference graphs? In 
 0, we show that we can effectively construct 
 meaningful graphs using transfer learning. Can our generated graphs help improve human performance? In 0, we show that humans 
 leverage generated graphs to improve their performance on a previously reported benchmark.", VERB SPACE NUM ADJ NOUN VERB ADP ADJ NOUN PUNCT DET PROPN NOUN PART VERB SCONJ DET NOUN NOUN SPACE AUX VERB CCONJ VERB DET NOUN NOUN PUNCT CCONJ DET ADJ NOUN PART VERB DET NOUN NOUN VERB SPACE DET NOUN NOUN CCONJ DET NOUN PROPN ADV PUNCT PRON SPACE ADV VERB DET NOUN ADP DET NOUN SCONJ PRON SPACE NOUN AUX ADV PART VERB ADP DET NOUN ADP NOUN SCONJ DET NOUN ADV VERB PART VERB DET SPACE NOUN PUNCT ADV NOUN PROPN PUNCT NOUN AUX AUX SPACE ADJ CCONJ VERB DET NOUN AUX SPACE AUX VERB NOUN PRON VERB PRON VERB ADJ PRON AUX ADV VERB ADP DET NOUN SPACE NOUN PUNCT VERB PRON NOUN ADP NOUN PUNCT DET NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN NOUN ADP ADJ NOUN ADP ADJ SPACE NOUN CCONJ VERB DET ADJ NOUN PART VERB SPACE PRON NOUN ADJ PUNCT VERB DET DET NOUN SPACE AUX VERB DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT PRON AUX AUX ADV ADJ PART VERB PUNCT ADV PUNCT PRON NOUN AUX PART VERB DET NOUN ADP DET ADJ NOUN NOUN ADP PROPN PUNCT SPACE SCONJ DET NOUN AUX VERB ADP DET NOUN PRON PRON SPACE VERB VERB NOUN ADP DET NOUN ADP NOUN SCONJ SPACE DET NOUN NOUN NOUN PUNCT PRON VERB DET NOUN PRON SPACE AUX VERB ADP DET PROPN NOUN CCONJ ADV VERB SPACE PRON PART VERB NOUN NOUN PUNCT ADJ NOUN NOUN AUX VERB ADJ ADP ADP DET ADJ NOUN ADP VERB SPACE ADJ NOUN NOUN PRON AUX PART VERB ADV PUNCT DET NOUN ADP DET NOUN AUX DET NOUN SPACE ADP DET VERB NUM NOUN NOUN PUNCT AUX PRON VERB DET NOUN ADP DET NOUN VERB NOUN NOUN PUNCT ADP SPACE NUM PUNCT PRON VERB SCONJ PRON AUX ADV VERB SPACE ADJ NOUN VERB NOUN NOUN PUNCT AUX PRON VERB NOUN VERB VERB ADJ NOUN PUNCT ADP NUM PUNCT PRON VERB SCONJ NOUN SPACE NOUN VERB NOUN PART VERB PRON NOUN ADP DET ADV VERB NOUN PUNCT,0.5142857142857142,31.5,4.895238095238096
138,52,Aman Madaan,"[' Interactive Machine Learning allows humans to give feedback to the models, often leading to improved accuracy (Fails and Olsen, 2003; Raghavan, 2006; Settles, 2011). Interactive systems for NLP have used human-in-the-loop style interactions for helping refugee settlement (Brown and Grinter, 2016), aligning topic models (Yuan et al., 2018) and enhancing bilingual word embeddings (Yuan et al., 2020). Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback (e.g., on its explanation) without retraining. One line of prior approaches (interactive semantic parsing approach) (Elgohary et al., 2021; Wang et al., 2016) parse natural language user feedback into a set of edit operations, which can then be ∗ authors contributed equally to this work. Ordering determined by dice rolling. 1We release a dataset of over 450k graphs for defeasible reasoning generated by our system at https://tinyurl. com/mercurie. executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is specific to a semantic parsing schema and has to be specialized, i.e., directly mapped to specific instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a specific set of edit operations expressed in natural language.', 'However, real-world human feedback is often imprecise and not directly actionable. Another line of prior approaches (interactive reasoning approach) (Talmor et al., 2020) explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box – so the human does not know what the model’s internal belief is and how it will change based on the feedback. These two lines of prior approaches inspire this paper – we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation (similar to the interactive semantic parsing approach). We do this while relaxing the assumptions of the parsing approach – our feedback does not have a task-specific structure, and it is not assumed to be actionable (similar to the interactive reasoning approach). We introduce MERCURIE, a pipeline system with two components, a previously trained neural model M and a graph corrector G. It takes as input any previously trained neural model M capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure (for example, that some nodes are inconsistent with the rest of the graph). As output, it produces a better explanation structure. The contributions of this work are: • We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that MERCURIE can improve the consistency of explanation structures by up to 40% (§4). • We also show downstream task (defeasible inference (Rudinger et al., 2020)) improvement for all domains by at least 1.2 points on accuracy (§6).']",intro_chunked," Interactive Machine Learning allows humans to give feedback to the models, often leading to improved accuracy (Fails and Olsen, 2003; Raghavan, 2006; Settles, 2011). Interactive systems for NLP have used human-in-the-loop style interactions for helping refugee settlement (Brown and Grinter, 2016), aligning topic models (Yuan et al., 2018) and enhancing bilingual word embeddings (Yuan et al., 2020). Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback (e.g., on its explanation) without retraining. One line of prior approaches (interactive semantic parsing approach) (Elgohary et al., 2021; Wang et al., 2016) parse natural language user feedback into a set of edit operations, which can then be ∗ authors contributed equally to this work. Ordering determined by dice rolling. 1We release a dataset of over 450k graphs for defeasible reasoning generated by our system at https://tinyurl. com/mercurie. executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is specific to a semantic parsing schema and has to be specialized, i.e., directly mapped to specific instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a specific set of edit operations expressed in natural language.",29.134870129870137,30.71625077440773,138,0.5217647552490234," Interactive Propname Propname allows humans to give feedback to the models, often leading to improved accuracy. Interactive systems for Propname have used human in the loop style interactions for helping refugee settlement, aligning topic models and enhancing bilingual word embeddings. Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback without retraining. One line of prior approaches parse natural language user feedback into a set of edit operations, which can then be authors contributed equally to this work. Ordering determined by dice rolling. Propname release a dataset of over 000k graphs for defeasible reasoning generated by our system at commercurie. executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is specific to a semantic parsing schema and has to be specialized, ie, directly mapped to specific instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a specific set of edit operations expressed in natural language."," Interactive Propname Propname allows humans to give feedback to the models, often leading to improved accuracy. Interactive systems for Propname have used human in the loop style interactions for helping refugee settlement, aligning topic models and enhancing bilingual word embeddings. Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback without retraining. One line of prior approaches parse natural language user feedback into a set of edit operations, which can then be authors contributed equally to this work. Ordering determined by dice rolling. Propname release a dataset of over 000k graphs for defeasible reasoning generated by our system at commercurie. executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is specific to a semantic parsing schema and has to be specialized, ie, directly mapped to specific instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a specific set of edit operations expressed in natural language.", ADJ PROPN PROPN VERB NOUN PART VERB NOUN ADP DET NOUN PUNCT ADV VERB ADP ADJ NOUN PUNCT ADJ NOUN ADP PROPN AUX VERB NOUN ADP DET NOUN NOUN NOUN ADP VERB NOUN NOUN PUNCT VERB NOUN NOUN CCONJ VERB ADJ NOUN NOUN PUNCT ADJ NOUN AUX VERB NOUN ADP NOUN NOUN CCONJ AUX ADJ PART VERB PUNCT DET NOUN VERB PART VERB DET NOUN NOUN ADP ADJ NOUN NOUN ADP VERB PUNCT NUM NOUN ADP ADJ NOUN VERB ADJ NOUN NOUN NOUN ADP DET NOUN ADP NOUN NOUN PUNCT PRON AUX ADV AUX NOUN VERB ADV ADP DET NOUN PUNCT VERB VERB ADP NOUN NOUN PUNCT PROPN VERB DET NOUN ADP ADP NUM NOUN ADP ADJ NOUN VERB ADP PRON NOUN ADP NOUN PUNCT VERB ADP DET ADJ NOUN NOUN PUNCT ADV VERB DET NOUN PUNCT ADP DET NOUN PUNCT DET NOUN AUX ADJ ADP DET ADJ VERB NOUN CCONJ VERB PART AUX VERB PUNCT ADV PUNCT ADV VERB ADP ADJ NOUN CCONJ NOUN PUNCT VERB PRON NOUN PUNCT ADV PUNCT DET NOUN AUX VERB PART AUX ADJ PUNCT VERB DET ADJ NOUN ADP NOUN NOUN VERB ADP ADJ NOUN PUNCT,0.6302083333333334,19.2,5.296875
139,53,Aman Madaan,"[' Interactive Machine Learning allows humans to give feedback to the models, often leading to improved accuracy (Fails and Olsen, 2003; Raghavan, 2006; Settles, 2011). Interactive systems for NLP have used human-in-the-loop style interactions for helping refugee settlement (Brown and Grinter, 2016), aligning topic models (Yuan et al., 2018) and enhancing bilingual word embeddings (Yuan et al., 2020). Neural models have made advancements in explanation generation but are expensive to retrain. This paper aims to improve the model output through natural language feedback (e.g., on its explanation) without retraining. One line of prior approaches (interactive semantic parsing approach) (Elgohary et al., 2021; Wang et al., 2016) parse natural language user feedback into a set of edit operations, which can then be ∗ authors contributed equally to this work. Ordering determined by dice rolling. 1We release a dataset of over 450k graphs for defeasible reasoning generated by our system at https://tinyurl. com/mercurie. executed on the incorrect explanation structure, thereby correcting the explanation. In these approaches, the feedback is specific to a semantic parsing schema and has to be specialized, i.e., directly mapped to specific instructions or literals, limiting its generalizability. Moreover, the feedback is expected to be actionable, containing a specific set of edit operations expressed in natural language.', 'However, real-world human feedback is often imprecise and not directly actionable. Another line of prior approaches (interactive reasoning approach) (Talmor et al., 2020) explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box – so the human does not know what the model’s internal belief is and how it will change based on the feedback. These two lines of prior approaches inspire this paper – we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation (similar to the interactive semantic parsing approach). We do this while relaxing the assumptions of the parsing approach – our feedback does not have a task-specific structure, and it is not assumed to be actionable (similar to the interactive reasoning approach). We introduce MERCURIE, a pipeline system with two components, a previously trained neural model M and a graph corrector G. It takes as input any previously trained neural model M capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure (for example, that some nodes are inconsistent with the rest of the graph). As output, it produces a better explanation structure. The contributions of this work are: • We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that MERCURIE can improve the consistency of explanation structures by up to 40% (§4). • We also show downstream task (defeasible inference (Rudinger et al., 2020)) improvement for all domains by at least 1.2 points on accuracy (§6).']",intro_chunked,"However, real-world human feedback is often imprecise and not directly actionable. Another line of prior approaches (interactive reasoning approach) (Talmor et al., 2020) explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box – so the human does not know what the model’s internal belief is and how it will change based on the feedback. These two lines of prior approaches inspire this paper – we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation (similar to the interactive semantic parsing approach). We do this while relaxing the assumptions of the parsing approach – our feedback does not have a task-specific structure, and it is not assumed to be actionable (similar to the interactive reasoning approach). We introduce MERCURIE, a pipeline system with two components, a previously trained neural model M and a graph corrector G. It takes as input any previously trained neural model M capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure (for example, that some nodes are inconsistent with the rest of the graph). As output, it produces a better explanation structure. The contributions of this work are: • We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that MERCURIE can improve the consistency of explanation structures by up to 40% (§4). • We also show downstream task (defeasible inference (Rudinger et al., 2020)) improvement for all domains by at least 1.2 points on accuracy (§6).",34.19411483253592,30.71625077440773,139,0.4009321928024292," However, real world human feedback is often imprecise and not directly actionable. Another line of prior approaches explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box so the human does not know what the models internal belief is and how it will change based on the feedback. These two lines of prior approaches inspire this paper we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation. We do this while relaxing the assumptions of the parsing approach our feedback does not have a task specific structure, and it is not assumed to be actionable. We introduce Propname, a pipeline system with two components, a previously trained neural model Propname and a graph corrector Propname It takes as input any previously trained neural model Propname capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure. As output, it produces a better explanation structure. The contributions of this work are: We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that Propname can improve the consistency of explanation structures by up to 00. We also show downstream task improvement for all domains by at least 0.0 points on accuracy."," However, real world human feedback is often imprecise and not directly actionable. Another line of prior approaches explore interactivity by enriching the context of an input sample through human feedback. However, for the human giving the feedback, the model is a black box so the human does not know what the models internal belief is and how it will change based on the feedback. These two lines of prior approaches inspire this paper we provide more transparency to the human than the interactive reasoning approach as the model receives feedback on the explanation. We do this while relaxing the assumptions of the parsing approach our feedback does not have a task specific structure, and it is not assumed to be actionable. We introduce Propname, a pipeline system with two components, a previously trained neural model Propname and a graph corrector Propname It takes as input any previously trained neural model Propname capable of generating an explanation structure. The second input is a natural language human feedback on the generated explanation structure. As output, it produces a better explanation structure. The contributions of this work are: We demonstrate a system that shows that an explainable NLP model output can be improved through natural feedback on their explanations. Experiments show that Propname can improve the consistency of explanation structures by up to 00. We also show downstream task improvement for all domains by at least 0.0 points on accuracy.", ADV PUNCT ADJ NOUN ADJ NOUN AUX ADV NOUN CCONJ PART ADV ADJ PUNCT DET NOUN ADP ADJ NOUN VERB NOUN ADP VERB DET NOUN ADP DET NOUN NOUN ADP ADJ NOUN PUNCT ADV PUNCT ADP DET NOUN VERB DET NOUN PUNCT DET NOUN AUX DET ADJ NOUN SCONJ DET NOUN AUX PART VERB PRON DET NOUN ADJ NOUN AUX CCONJ SCONJ PRON AUX VERB VERB ADP DET NOUN PUNCT DET NUM NOUN ADP ADJ NOUN VERB DET NOUN PRON VERB ADJ NOUN ADP DET NOUN ADP DET ADJ NOUN NOUN SCONJ DET NOUN VERB NOUN ADP DET NOUN PUNCT PRON VERB PRON SCONJ VERB DET NOUN ADP DET VERB NOUN PRON NOUN AUX PART VERB DET NOUN ADJ NOUN PUNCT CCONJ PRON AUX PART VERB PART AUX ADJ PUNCT PRON VERB PROPN PUNCT DET NOUN NOUN ADP NUM NOUN PUNCT DET ADV VERB ADJ NOUN PROPN CCONJ DET NOUN NOUN PROPN PRON VERB ADP NOUN DET ADV VERB ADJ NOUN PROPN ADJ ADP VERB DET NOUN NOUN PUNCT DET ADJ NOUN AUX DET ADJ NOUN ADJ NOUN ADP DET VERB NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NOUN NOUN PUNCT DET NOUN ADP DET NOUN AUX PUNCT PRON VERB DET NOUN PRON VERB SCONJ DET ADJ NOUN NOUN NOUN AUX AUX VERB ADP ADJ NOUN ADP PRON NOUN PUNCT NOUN VERB SCONJ PROPN AUX VERB DET NOUN ADP NOUN NOUN ADP ADP PART NUM PUNCT PRON ADV VERB ADJ NOUN NOUN ADP DET NOUN ADP ADV ADV NUM NOUN ADP NOUN PUNCT,0.5234375,23.272727272727273,4.89453125
140,54,Aman Madaan,"[' Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.', 'The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1.', 'In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning']",intro_chunked," Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.",38.43567636986302,30.71625077440773,140,0.44773319363594055," Humans are adept at anticipating and reasoning about events and their causal effects on other events. Consider these questions Would it rain more if we plant more trees?, What would help the water in boiling faster? answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering, process tracking, reasoning about qualitative relationships, and physical commonsense reasoning. authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction, temporal event reasoning, and Propname. However, these systems are primarily extractive they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events."," Humans are adept at anticipating and reasoning about events and their causal effects on other events. Consider these questions Would it rain more if we plant more trees?, What would help the water in boiling faster? answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering, process tracking, reasoning about qualitative relationships, and physical commonsense reasoning. authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction, temporal event reasoning, and Propname. However, these systems are primarily extractive they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.", NOUN AUX ADJ ADP VERB CCONJ VERB ADP NOUN CCONJ PRON ADJ NOUN ADP ADJ NOUN PUNCT VERB DET NOUN AUX PRON VERB ADJ SCONJ PRON VERB ADJ NOUN PUNCT PUNCT PRON AUX VERB DET NOUN ADP VERB ADV PUNCT VERB DET NOUN VERB DET NOUN PART VERB DET ADJ NOUN ADP NOUN NOUN CCONJ NOUN NOUN CCONJ DET NOUN PART VERB ADP SCONJ ADJ NOUN VERB DET ADJ ADP DET NOUN PRON AUX ADV ADJ ADP NOUN PUNCT ADV PUNCT VERB ADP NOUN CCONJ NOUN VERB DET ADJ NOUN ADP NOUN PUNCT VERB ADJ NOUN CCONJ VERB PRON NOUN NOUN AUX ADJ ADP NOUN NOUN ADP NOUN NOUN PUNCT NOUN NOUN PUNCT VERB ADP ADJ NOUN PUNCT CCONJ ADJ NOUN NOUN PUNCT NOUN VERB ADV ADP DET NOUN PUNCT ADJ NOUN AUX VERB NOUN NOUN ADP DET NOUN ADP NOUN NOUN PUNCT ADJ NOUN NOUN PUNCT CCONJ PROPN PUNCT ADV PUNCT DET NOUN AUX ADV ADJ PRON NOUN ADP NOUN ADV VERB ADP DET NOUN PUNCT VERB PRON NOUN PART AUX VERB ADP ADJ NOUN PRON VERB ADJ NOUN ADP NOUN PUNCT,0.5792349726775956,22.875,5.3497267759562845
141,55,Aman Madaan,"[' Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.', 'The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1.', 'In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning']",intro_chunked,"The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1.",34.79116279069771,30.71625077440773,141,0.5664810538291931," The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models to encode a wide range of knowledge from their pretraining corpus, enabling their successful adaptation in downstream tasks. Motivated by these successes, we investigate whether we can adapt Propname for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i lack of large scale stand alone datasets to study event influences, and ii a framework to leverage Propname to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on Propname dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, Propname, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Propname 0."," The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models to encode a wide range of knowledge from their pretraining corpus, enabling their successful adaptation in downstream tasks. Motivated by these successes, we investigate whether we can adapt Propname for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i lack of large scale stand alone datasets to study event influences, and ii a framework to leverage Propname to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on Propname dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, Propname, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Propname 0.", DET NOUN ADP VERB NOUN NOUN NOUN ADP ADJ NOUN AUX ADV DET ADJ NOUN PUNCT ADV PUNCT VERB NOUN ADP ADJ NOUN VERB ADP DET NOUN ADP VERB NOUN NOUN PART VERB DET ADJ NOUN ADP NOUN ADP PRON VERB NOUN PUNCT VERB PRON ADJ NOUN ADP ADJ NOUN PUNCT VERB ADP DET NOUN PUNCT PRON VERB SCONJ PRON AUX VERB PROPN ADP DET ADJ NOUN ADP NOUN NOUN NOUN CCONJ VERB ADV SCONJ DET VERB NOUN NOUN VERB ADP ADJ NOUN NOUN PUNCT DET DET NOUN VERB NUM ADJ NOUN PUNCT PRON VERB ADP ADJ NOUN VERB ADV NOUN PART VERB NOUN NOUN PUNCT CCONJ VERB DET NOUN AUX NOUN PROPN PART VERB PRON ADP NOUN NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP ADV VERB DET ADJ NOUN VERB ADP PROPN NOUN PRON AUX AUX VERB ADP DET NOUN ADP NOUN NOUN VERB ADP NOUN PUNCT NOUN ADP DET NOUN PUNCT CCONJ DET NOUN ADP PRON ADP DET NOUN NOUN PUNCT ADV PUNCT PRON VERB PRON NOUN PUNCT PROPN PUNCT PRON VERB DET NOUN CCONJ DET NOUN PUNCT CCONJ VERB PRON NOUN CCONJ ADP ADV CCONJ ADJ NOUN PUNCT DET NOUN NOUN ADP PRON NOUN AUX VERB ADP PROPN NUM PUNCT,0.5576923076923077,29.714285714285715,4.951923076923077
142,56,Aman Madaan,"[' Humans are adept at anticipating and reasoning about events and their causal effects (influences) on other events. Consider these questions - Would it rain more if we plant more trees?, What would help the water in boiling faster? - answering these questions requires the ability to comprehend the complex processes of plant growth and water boiling and the capacity to reason about how various events influence each other in these processes that are typically implicit in text. Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering (QA) (Tandon et al., 2019), process tracking (Dalvi et al., 2018), reasoning about qualitative relationships (Tafjord et al., 2019), and physical commonsense reasoning (Sap et al., 2019; Bisk et al., 2020). ∗ authors contributed equally to this work. Previous approaches have studied event understanding in the context of event extraction (Chambers and Jurafsky, 2008; Yang et al., 2019; Wang et al., 2019), temporal event reasoning (Ning et al., 2018; Vashishtha et al., 2019), and QuestionAnswering (Tandon et al., 2019; Dalvi et al., 2018). However, these systems are primarily extractive — they reason about events already mentioned in the text, limiting their ability to be integrated to downstream tasks that require implicit reasoning about events.', 'The task of generating novel event influence in unseen contexts is still an open challenge. Meanwhile, promising evidence from recent work attests to the ability of pretrained language models (PLM) to encode a wide-range of knowledge from their pretraining corpus (Bosselut et al., 2019; Petroni et al., 2019; Davison et al., 2019), enabling their successful adaptation in downstream tasks (Yang et al., 2020; Kumar et al., 2020; Guan et al., 2020). Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation. In this work, we address these challenges by first deriving a large corpus based on WIQA (Tandon et al., 2019) dataset that can be used for the generation of event influences conditioned on context, relationship between the events, and the distance between them in a reasoning chain. Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions. An example use of our framework is shown in Figure 1.', 'In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning']",intro_chunked,"In the figure, nodes represent the event influences and the edges represent the nature of the influence (relation) between them. These relations can either be positive (when one event helps the occurrence of another) or negative (when one event hurts the occurrence of another). The distance between any given pair of nodes (in terms of number of edges traversed) is denoted by hop. EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics (by ∼ 10 ROUGE) and human metrics — relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 1. We propose the task of event influence generation and derive a large-scale dataset for the same. 2. We propose EIGEN, a framework to generate targeted influence nodes for an event. Our experiments show that EIGEN outperforms strong baselines in both automated and human evaluation. 3. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 3% in overall accuracy, and by 8% on the subset of questions that require implicit eventinfluence reasoning",40.829200000000014,30.71625077440773,142,0.6281325221061707," In the figure, nodes represent the event influences and the edges represent the nature of the influence between them. These relations can either be positive or negative. The distance between any given pair of nodes is denoted by hop. EIGEN fine tunes a Propname to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics and human metrics relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from Propname can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 0. We propose the task of event influence generation and derive a large scale dataset for the same.0. We propose Propname, a framework to generate targeted influence nodes for an event. Our experiments show that Propname outperforms strong baselines in both automated and human evaluation.0. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 0 in overall accuracy, and by 0 on the subset of questions that require implicit eventinfluence reasoning"," In the figure, nodes represent the event influences and the edges represent the nature of the influence between them. These relations can either be positive or negative. The distance between any given pair of nodes is denoted by hop. EIGEN fine tunes a Propname to generate novel event influences for unseen contexts using masked language modeling. We show empirically that our framework generates high quality influences for an event, both in terms of automated metrics and human metrics relevance and proximity to the reference text. Together, the overall framework can be seamlessly integrated into any downstream task. In one such instance, we show how the event influences generated from Propname can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture. In summary, our contributions are: 0. We propose the task of event influence generation and derive a large scale dataset for the same.0. We propose Propname, a framework to generate targeted influence nodes for an event. Our experiments show that Propname outperforms strong baselines in both automated and human evaluation.0. We also validate our approach by augmenting generated influences to a downstream QA dataset, improving over the state of the art by 0 in overall accuracy, and by 0 on the subset of questions that require implicit eventinfluence reasoning", ADP DET NOUN PUNCT NOUN VERB DET NOUN NOUN CCONJ DET NOUN VERB DET NOUN ADP DET NOUN ADP PRON PUNCT DET NOUN AUX CCONJ AUX ADJ CCONJ ADJ PUNCT DET NOUN ADP DET VERB NOUN ADP NOUN AUX VERB ADP NOUN PUNCT NOUN ADJ NOUN DET PROPN PART VERB ADJ NOUN NOUN ADP ADJ NOUN VERB VERB NOUN NOUN PUNCT PRON VERB ADV SCONJ PRON NOUN VERB ADJ NOUN NOUN ADP DET NOUN PUNCT CCONJ ADP NOUN ADP VERB NOUN CCONJ ADJ NOUN NOUN CCONJ NOUN ADP DET NOUN NOUN PUNCT ADV PUNCT DET ADJ NOUN AUX AUX ADV VERB ADP DET ADJ NOUN PUNCT ADP NUM ADJ NOUN PUNCT PRON VERB SCONJ DET NOUN NOUN VERB ADP PROPN AUX AUX ADV VERB ADP DET ADJ NOUN NOUN CCONJ VERB PRON NOUN ADP DET NOUN ADP VERB DET ADJ NOUN NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX PUNCT NUM PUNCT PRON VERB DET NOUN ADP NOUN NOUN NOUN CCONJ VERB DET ADJ NOUN NOUN ADP DET ADJ PUNCT PUNCT PUNCT PRON VERB PROPN PUNCT DET NOUN PART VERB VERB NOUN NOUN ADP DET NOUN PUNCT PRON NOUN VERB SCONJ PROPN VERB ADJ NOUN ADP DET VERB CCONJ ADJ NOUN PUNCT PUNCT PUNCT PRON ADV VERB PRON NOUN ADP VERB VERB NOUN ADP DET ADJ NOUN NOUN PUNCT VERB ADP DET NOUN ADP DET NOUN ADP NUM ADP ADJ NOUN PUNCT CCONJ ADP NUM ADP DET NOUN ADP NOUN PRON VERB ADJ NOUN NOUN,0.5352697095435685,20.083333333333332,4.929460580912863
143,57,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked," Temporal reasoning is crucial for analyzing the interactions among complex events and producing
coherent interpretations of text data (Duran et al.,
2007). There is a rich body of research on the
use of temporal information in a variety of important application domains, including topic detection
and tracking (Makkonen et al., 2003), information
extraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://
github.com/madaan/temporal-graph-gen
sis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the
temporal ordering among events, where the nodes
are the individual events, and the edges capture
temporal relationships such as “before”, “after” or
“simultaneous”. Representative work on automated
extraction of such graphs from textual documents
includes the early work by Chambers and Jurafsky
(2009), where the focus is on the construction of
event chains from a collection of documents, and
the more recent CAEVO (Chambers et al., 2014) and
Cogcomptime (Ning et al., 2018), which extract
a graph for each input document instead. These
methods focus on rule-based and statistical submodules to extract verb-centered events and the
temporal relations among them.",17.15078217821784,30.71625077440773,143,0.46954765915870667," Temporal reasoning is crucial for analyzing the interactions among complex events and producing 
 coherent interpretations of text data Propname Propname Propname Propname, 
 0000. There is a rich body of research on the 
 use of temporal information in a variety of important application domains, including topic detection 
 and tracking, information 
 extraction, parsing of clinical records, discourse Propname and pre trained models available at https: 
 github.commadaantemporal Propname Propname 
 sis, and question answering. Graphs are a natural choice for representing the 
 temporal ordering among events, where the nodes 
 are the individual events, and the edges capture 
 temporal relationships such as before, after or 
 simultaneous. Representative work on automated 
 extraction of such graphs from textual documents 
 includes the early work by Propname and Propname 
, where the focus is on the construction of 
 event chains from a collection of documents, and 
 the more recent Propname and 
 Propname, which extract 
 a graph for each input document instead. These 
 methods focus on rule based and statistical submodules to extract verb centered events and the 
 temporal relations among them."," Temporal reasoning is crucial for analyzing the interactions among complex events and producing 
 coherent interpretations of text data Propname Propname Propname Propname, 
 0000. There is a rich body of research on the 
 use of temporal information in a variety of important application domains, including topic detection 
 and tracking, information 
 extraction, parsing of clinical records, discourse Propname and pre trained models available at https: 
 github.commadaantemporal Propname Propname 
 sis, and question answering. Graphs are a natural choice for representing the 
 temporal ordering among events, where the nodes 
 are the individual events, and the edges capture 
 temporal relationships such as before, after or 
 simultaneous. Representative work on automated 
 extraction of such graphs from textual documents 
 includes the early work by Propname and Propname 
, where the focus is on the construction of 
 event chains from a collection of documents, and 
 the more recent Propname and 
 Propname, which extract 
 a graph for each input document instead. These 
 methods focus on rule based and statistical submodules to extract verb centered events and the 
 temporal relations among them.", ADJ NOUN AUX ADJ ADP VERB DET NOUN ADP ADJ NOUN CCONJ VERB SPACE ADJ NOUN ADP NOUN NOUN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PRON VERB DET ADJ NOUN ADP NOUN ADP DET SPACE NOUN ADP ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT VERB NOUN NOUN SPACE CCONJ NOUN PUNCT NOUN SPACE NOUN PUNCT VERB ADP ADJ NOUN PUNCT VERB PROPN CCONJ VERB VERB NOUN ADJ ADP NOUN PUNCT SPACE ADJ PROPN PROPN SPACE NOUN PUNCT CCONJ NOUN VERB PUNCT NOUN AUX DET ADJ NOUN ADP VERB DET SPACE ADJ NOUN ADP NOUN PUNCT SCONJ DET NOUN SPACE AUX DET ADJ NOUN PUNCT CCONJ DET NOUN VERB SPACE ADJ NOUN ADJ ADP ADV PUNCT ADP CCONJ SPACE ADJ PUNCT ADJ NOUN ADP VERB SPACE NOUN ADP ADJ NOUN ADP ADJ NOUN SPACE VERB DET ADJ NOUN ADP PROPN CCONJ PROPN SPACE PUNCT SCONJ DET NOUN AUX ADP DET NOUN ADP SPACE NOUN NOUN ADP DET NOUN ADP NOUN PUNCT CCONJ SPACE DET ADV ADJ PROPN CCONJ SPACE PROPN PUNCT PRON VERB SPACE DET NOUN ADP DET NOUN NOUN ADV PUNCT DET SPACE NOUN VERB ADP NOUN VERB CCONJ ADJ NOUN PART VERB VERB VERB NOUN CCONJ DET SPACE ADJ NOUN ADP PRON PUNCT,0.5767195767195767,37.8,5.317460317460317
144,58,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked,"As an emerging
area of NLP, large scale pre-trained language models have made strides in addressing challenging
tasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog
generation (Budzianowski and Vulic´, 2019). These
systems typically fine-tune large language models
on a corpus of a task-specific dataset. However,
these techniques have not been investigated for
temporal graph extraction. This paper focuses on the problem of generation
of an event-level temporal graph for each document, and we refer to this task as contextualized
graph generation. We address this open challenge
by proposing a novel reformulation of the task as a
sequence-to-sequence mapping problem (Sutskever
et al., 2014), which enables us to leverage large pretrained models for our task. Further, different from
existing methods, our proposed approach is completely end-to-end and eliminates the need for a
pipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which
is a prerequisite to our main goal: the difficulty of
obtaining a large quantity of training graphs with human-annotated events and temporal relations. To
this end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps
for pruning and noise reduction.",21.881345443349772,30.71625077440773,144,0.28581759333610535," As an emerging 
 area of Propname, large scale pre trained language models have made strides in addressing challenging 
 tasks like Propname knowledge graph completion and task oriented dialog 
 generation. These 
 systems typically fine tune large language models 
 on a corpus of a task specific dataset. However, 
 these techniques have not been investigated for 
 temporal graph extraction. This paper focuses on the problem of generation 
 of an event level temporal graph for each document, and we refer to this task as contextualized 
 graph generation. We address this open challenge 
 by proposing a novel reformulation of the task as a 
 sequence to sequence mapping problem Propname 
 Propname Propname Propname, 0000, which enables us to leverage large pretrained models for our task. Further, different from 
 existing methods, our proposed approach is completely end to end and eliminates the need for a 
 pipeline of sub systems commonly used by traditional methods. We also address a related open challenge, which 
 is a prerequisite to our main goal: the difficulty of 
 obtaining a large quantity of training graphs with human annotated events and temporal relations. To 
 this end, we automatically produce a large collection of document graph pairs by using Propname, followed by a few rule based post processing steps 
 for pruning and noise reduction."," As an emerging 
 area of Propname, large scale pre trained language models have made strides in addressing challenging 
 tasks like Propname knowledge graph completion and task oriented dialog 
 generation. These 
 systems typically fine tune large language models 
 on a corpus of a task specific dataset. However, 
 these techniques have not been investigated for 
 temporal graph extraction. This paper focuses on the problem of generation 
 of an event level temporal graph for each document, and we refer to this task as contextualized 
 graph generation. We address this open challenge 
 by proposing a novel reformulation of the task as a 
 sequence to sequence mapping problem Propname 
 Propname Propname Propname, 0000, which enables us to leverage large pretrained models for our task. Further, different from 
 existing methods, our proposed approach is completely end to end and eliminates the need for a 
 pipeline of sub systems commonly used by traditional methods. We also address a related open challenge, which 
 is a prerequisite to our main goal: the difficulty of 
 obtaining a large quantity of training graphs with human annotated events and temporal relations. To 
 this end, we automatically produce a large collection of document graph pairs by using Propname, followed by a few rule based post processing steps 
 for pruning and noise reduction.", ADP DET VERB SPACE NOUN ADP PROPN PUNCT ADJ NOUN VERB VERB NOUN NOUN AUX VERB NOUN ADP VERB VERB SPACE NOUN ADP PROPN NOUN NOUN NOUN CCONJ NOUN VERB NOUN SPACE NOUN PUNCT DET SPACE NOUN ADV ADJ NOUN ADJ NOUN NOUN SPACE ADP DET NOUN ADP DET NOUN ADJ NOUN PUNCT ADV PUNCT SPACE DET NOUN AUX PART AUX VERB ADP SPACE ADJ NOUN NOUN PUNCT DET NOUN VERB ADP DET NOUN ADP NOUN SPACE ADP DET NOUN NOUN ADJ NOUN ADP DET NOUN PUNCT CCONJ PRON VERB ADP DET NOUN ADP VERB SPACE NOUN NOUN PUNCT PRON VERB DET ADJ NOUN SPACE ADP VERB DET ADJ NOUN ADP DET NOUN ADP DET SPACE NOUN PART VERB NOUN NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PRON VERB PRON PART VERB ADJ VERB NOUN ADP PRON NOUN PUNCT ADV PUNCT ADJ ADP SPACE VERB NOUN PUNCT PRON VERB NOUN AUX ADV ADJ ADP NOUN CCONJ VERB DET NOUN ADP DET SPACE NOUN ADP NOUN NOUN ADV VERB ADP ADJ NOUN PUNCT PRON ADV VERB DET ADJ ADJ NOUN PUNCT PRON SPACE AUX DET NOUN ADP PRON ADJ NOUN PUNCT DET NOUN ADP SPACE VERB DET ADJ NOUN ADP NOUN NOUN ADP ADJ ADJ NOUN CCONJ ADJ NOUN PUNCT ADP SPACE DET NOUN PUNCT PRON ADV VERB DET ADJ NOUN ADP NOUN NOUN NOUN ADP VERB PROPN PUNCT VERB ADP DET ADJ NOUN VERB NOUN NOUN NOUN SPACE ADP NOUN CCONJ NOUN NOUN PUNCT,0.5701754385964912,28.5,4.951754385964913
145,59,Aman Madaan,"[' Temporal reasoning is crucial for analyzing the interactions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of important application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clinical records (Lin et al., 2016), discourse analy1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question answering (Ning et al., 2020). Graphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical submodules to extract verb-centered events and the\ntemporal relations among them.', 'As an emerging\narea of NLP, large scale pre-trained language models have made strides in addressing challenging\ntasks like commonsense knowledge graph completion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vulic´, 2019). These\nsystems typically fine-tune large language models\non a corpus of a task-specific dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction. This paper focuses on the problem of generation\nof an event-level temporal graph for each document, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pretrained models for our task. Further, different from\nexisting methods, our proposed approach is completely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by traditional methods. We also address a related open challenge, which\nis a prerequisite to our main goal: the difficulty of\nobtaining a large quantity of training graphs with human-annotated events and temporal relations. To\nthis end, we automatically produce a large collection of document-graph pairs by using CAEVO, followed by a few rule-based post-processing steps\nfor pruning and noise reduction.', 'We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We fine-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document\nand the generated graph by our system. In summary, our main contributions are:\n1. We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective fine-tuning of pretrained models, by automatically producing a\ncollection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advantage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).']",intro_chunked,"We then encode
the graph in each training pair as a string in the
graph representation format DOT, transforming the
text-to-graph mapping into sequence-to-sequence
mapping. We fine-tune GPT-2 on this dataset of
document-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense (Cassidy et al., 2014) on multiple metrics. Figure 1 shows an example of the input document
and the generated graph by our system. In summary, our main contributions are:
1. We present the first investigation on using
large pre-trained language models for contextualized temporal event graph generation by
proposing a new formulation of the problem
as a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large
collection of human-annotated graphs, which
is crucial for effective fine-tuning of pretrained models, by automatically producing a
collection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set (which allows us to compare the relative performance of different
models) and a hand-labeled, out-of-domain
dataset (TimeBank-Dense), show the advantage of our proposed approach over strong
baselines. Further, we show that our approach
can help in generating plausible answers for
open ended-temporal questions in a reading
comprehension dataset, Torque (Ning et al.,
2020).",26.433195652173936,30.71625077440773,145,0.7263843417167664," We then encode 
 the graph in each training pair as a string in the 
 graph representation format Propname, transforming the 
 text to graph mapping into sequence to sequence 
 mapping. We fine tune Propname 0 on this dataset of 
 document graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms Propname on TimeBankDense on multiple metrics. Figure 0 shows an example of the input document 
 and the generated graph by our system. In summary, our main contributions are: 
0. We present the first investigation on using 
 large pre trained language models for contextualized temporal event graph generation by 
 proposing a new formulation of the problem 
 as a sequence to sequence mapping task.0. We address the difficulty of obtaining a large 
 collection of human annotated graphs, which 
 is crucial for effective fine tuning of pretrained models, by automatically producing a 
 collection of 00,000 document graph pairs.0. Our experimental results on both the systemgenerated test set which allows us to compare the relative performance of different 
 models and a hand labeled, out of domain 
 dataset, show the advantage of our proposed approach over strong 
 baselines. Further, we show that our approach 
 can help in generating plausible answers for 
 open ended temporal questions in a reading 
 comprehension dataset, Propname Propname Propname Propname Propname, 
 0000."," We then encode 
 the graph in each training pair as a string in the 
 graph representation format Propname, transforming the 
 text to graph mapping into sequence to sequence 
 mapping. We fine tune Propname 0 on this dataset of 
 document graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms Propname on TimeBankDense on multiple metrics. Figure 0 shows an example of the input document 
 and the generated graph by our system. In summary, our main contributions are: 
0. We present the first investigation on using 
 large pre trained language models for contextualized temporal event graph generation by 
 proposing a new formulation of the problem 
 as a sequence to sequence mapping task.0. We address the difficulty of obtaining a large 
 collection of human annotated graphs, which 
 is crucial for effective fine tuning of pretrained models, by automatically producing a 
 collection of 00,000 document graph pairs.0. Our experimental results on both the systemgenerated test set which allows us to compare the relative performance of different 
 models and a hand labeled, out of domain 
 dataset, show the advantage of our proposed approach over strong 
 baselines. Further, we show that our approach 
 can help in generating plausible answers for 
 open ended temporal questions in a reading 
 comprehension dataset, Propname Propname Propname Propname Propname, 
 0000.", PRON ADV VERB SPACE DET NOUN ADP DET NOUN NOUN ADP DET NOUN ADP DET SPACE NOUN NOUN NOUN PROPN PUNCT VERB DET SPACE NOUN PART VERB NOUN ADP NOUN PART VERB SPACE NOUN PUNCT PRON ADJ NOUN PROPN NUM ADP DET NOUN ADP SPACE NOUN NOUN NOUN PUNCT PRON VERB ADJ NOUN NOUN ADP ADJ NOUN ADP NOUN VERB NOUN VERB CCONJ VERB PROPN ADP NOUN ADP ADJ NOUN PUNCT NOUN NUM VERB DET NOUN ADP DET NOUN NOUN SPACE CCONJ DET VERB NOUN ADP PRON NOUN PUNCT ADP NOUN PUNCT PRON ADJ NOUN AUX PUNCT SPACE PUNCT PUNCT PRON VERB DET ADJ NOUN ADP VERB SPACE ADJ ADJ VERB NOUN NOUN ADP VERB ADJ NOUN NOUN NOUN ADP SPACE VERB DET ADJ NOUN ADP DET NOUN SPACE ADP DET NOUN PART VERB NOUN NOUN PUNCT PUNCT PUNCT PRON VERB DET NOUN ADP VERB DET ADJ SPACE NOUN ADP ADJ ADJ NOUN PUNCT PRON SPACE AUX ADJ ADP ADJ ADJ NOUN ADP VERB NOUN PUNCT ADP ADV VERB DET SPACE NOUN ADP NUM NOUN NOUN NOUN PUNCT PUNCT PUNCT PRON ADJ NOUN ADP CCONJ DET ADJ NOUN NOUN PRON VERB PRON PART VERB DET ADJ NOUN ADP ADJ SPACE NOUN CCONJ DET NOUN VERB PUNCT ADP ADP NOUN SPACE NOUN PUNCT VERB DET NOUN ADP PRON VERB NOUN ADP ADJ SPACE NOUN PUNCT ADV PUNCT PRON VERB SCONJ PRON NOUN SPACE AUX VERB ADP VERB ADJ NOUN ADP SPACE ADJ VERB ADJ NOUN ADP DET NOUN SPACE NOUN NOUN PUNCT PROPN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT,0.5423728813559322,29.5,5.0423728813559325
146,60,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked," Politeness plays a crucial role in social interaction,
and is closely tied with power dynamics, social
distance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative
to use the appropriate level of politeness for smooth
communication in conversations (Coppock, 2005),
organizational settings like emails (Peterson et al.,
2011), memos, official documents, and many other
settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we
study the task of converting non-polite sentences
to polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,
2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a
style transfer task, and we argue that defining it is
cumbersome. While native speakers of a language
and cohabitants of a region have a good working
understanding of the phenomenon of politeness
for everyday conversation, pinning it down as a
definition is non-trivial (Meier, 1995). There are
primarily two reasons for this complexity. First, as
noted by (Brown et al., 1987), the phenomenon of
politeness is rich and multifaceted.",31.60524709302328,30.71625077440773,146,0.3552844822406769," Politeness plays a crucial role in social interaction, 
 and is closely tied with power dynamics, social 
 distance between the participants of a conversation, and gender. It is also imperative 
 to use the appropriate level of politeness for smooth 
 communication in conversations, 
 organizational settings like emails Propname Propname Propname Propname, 
 0000, memos, official documents, and many other 
 settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content. Motivated by its central importance, in this paper we 
 study the task of converting non polite sentences 
 to polite sentences while preserving the meaning. Prior work on text style transfer Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000 has not focused on politeness as a 
 style transfer task, and we argue that defining it is 
 cumbersome. While native speakers of a language 
 and cohabitants of a region have a good working 
 understanding of the phenomenon of politeness 
 for everyday conversation, pinning it down as a 
 definition is non trivial. There are 
 primarily two reasons for this complexity. First, as 
 noted by, the phenomenon of 
 politeness is rich and multifaceted."," Politeness plays a crucial role in social interaction, 
 and is closely tied with power dynamics, social 
 distance between the participants of a conversation, and gender. It is also imperative 
 to use the appropriate level of politeness for smooth 
 communication in conversations, 
 organizational settings like emails Propname Propname Propname Propname, 
 0000, memos, official documents, and many other 
 settings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content. Motivated by its central importance, in this paper we 
 study the task of converting non polite sentences 
 to polite sentences while preserving the meaning. Prior work on text style transfer Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000 has not focused on politeness as a 
 style transfer task, and we argue that defining it is 
 cumbersome. While native speakers of a language 
 and cohabitants of a region have a good working 
 understanding of the phenomenon of politeness 
 for everyday conversation, pinning it down as a 
 definition is non trivial. There are 
 primarily two reasons for this complexity. First, as 
 noted by, the phenomenon of 
 politeness is rich and multifaceted.", NOUN VERB DET ADJ NOUN ADP ADJ NOUN PUNCT SPACE CCONJ AUX ADV VERB ADP NOUN NOUN PUNCT ADJ SPACE NOUN ADP DET NOUN ADP DET NOUN PUNCT CCONJ NOUN PUNCT PRON AUX ADV ADJ SPACE PART VERB DET ADJ NOUN ADP NOUN ADP ADJ SPACE NOUN ADP NOUN PUNCT SPACE ADJ NOUN ADP NOUN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT NOUN PUNCT ADJ NOUN PUNCT CCONJ ADJ ADJ SPACE NOUN PUNCT ADV PUNCT NOUN AUX ADV AUX VERB ADP DET ADJ NOUN PRON AUX AUX VERB ADP NOUN PUNCT VERB ADP PRON ADJ NOUN PUNCT ADP DET NOUN PRON SPACE VERB DET NOUN ADP VERB ADJ ADJ NOUN SPACE ADP ADJ NOUN SCONJ VERB DET NOUN PUNCT ADV VERB ADP NOUN NOUN NOUN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM AUX PART VERB ADP NOUN ADP DET SPACE NOUN NOUN NOUN PUNCT CCONJ PRON VERB SCONJ VERB PRON AUX SPACE ADJ PUNCT SCONJ ADJ NOUN ADP DET NOUN SPACE CCONJ NOUN ADP DET NOUN VERB DET ADJ NOUN SPACE NOUN ADP DET NOUN ADP NOUN SPACE ADP ADJ NOUN PUNCT VERB PRON ADP ADP DET SPACE NOUN AUX ADV ADJ PUNCT PRON VERB SPACE ADV NUM NOUN ADP DET NOUN PUNCT ADV PUNCT SCONJ SPACE VERB ADP PUNCT DET NOUN ADP SPACE NOUN AUX ADJ CCONJ ADJ PUNCT,0.4957627118644068,29.5,4.987288135593221
147,61,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked,"Second, politeness of a sentence depends on the culture, language,
and social structure of both the speaker and the addressed person. For instance, while using “please”
in requests made to the closest friends is common
amongst the native speakers of North American
English, such an act would be considered awkward,
if not rude, in the Arab culture (Kad´ ar and Mills ´ ,
2011). We circumscribe the scope of politeness for the
purpose of this study as follows: First, we adopt
the data driven definition of politeness proposed by
(Danescu-Niculescu-Mizil et al., 2013). Second,
we base our experiments on a dataset derived from
the Enron corpus (Klimt and Yang, 2004) which
consists of email exchanges in an American corporation. Thus, we restrict our attention to the notion
of politeness as widely accepted by the speakers of
North American English in a formal setting. Even after framing politeness transfer as a task,
there are additional challenges involved that differentiate politeness from other styles. Consider a
common directive in formal communication, “send
me the data”. While the sentence is not impolite, a rephrasing “could you please send me the
data” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings
out a distinct characteristic of politeness.",36.827853192559104,30.71625077440773,147,0.2907685935497284," Second, politeness of a sentence depends on the culture, language, 
 and social structure of both the speaker and the addressed person. For instance, while using please 
 in requests made to the closest friends is common 
 amongst the native speakers of Propname Propname 
 Propname, such an act would be considered awkward, 
 if not rude, in the Arab culture Propname Propname and Propname, 
 0000. We circumscribe the scope of politeness for the 
 purpose of this study as follows: First, we adopt 
 the data driven definition of politeness proposed by 
. Second, 
 we base our experiments on a dataset derived from 
 the Propname corpus which 
 consists of email exchanges in an American corporation. Thus, we restrict our attention to the notion 
 of politeness as widely accepted by the speakers of 
 North American Propname in a formal setting. Even after framing politeness transfer as a task, 
 there are additional challenges involved that differentiate politeness from other styles. Consider a 
 common directive in formal communication, send 
 me the data. While the sentence is not impolite, a rephrasing could you please send me the 
 data would largely be accepted as a more polite way of phrasing the same statement. This example brings 
 out a distinct characteristic of politeness."," Second, politeness of a sentence depends on the culture, language, 
 and social structure of both the speaker and the addressed person. For instance, while using please 
 in requests made to the closest friends is common 
 amongst the native speakers of Propname Propname 
 Propname, such an act would be considered awkward, 
 if not rude, in the Arab culture Propname Propname and Propname, 
 0000. We circumscribe the scope of politeness for the 
 purpose of this study as follows: First, we adopt 
 the data driven definition of politeness proposed by 
. Second, 
 we base our experiments on a dataset derived from 
 the Propname corpus which 
 consists of email exchanges in an American corporation. Thus, we restrict our attention to the notion 
 of politeness as widely accepted by the speakers of 
 North American Propname in a formal setting. Even after framing politeness transfer as a task, 
 there are additional challenges involved that differentiate politeness from other styles. Consider a 
 common directive in formal communication, send 
 me the data. While the sentence is not impolite, a rephrasing could you please send me the 
 data would largely be accepted as a more polite way of phrasing the same statement. This example brings 
 out a distinct characteristic of politeness.", ADV PUNCT NOUN ADP DET NOUN VERB ADP DET NOUN PUNCT NOUN PUNCT SPACE CCONJ ADJ NOUN ADP CCONJ DET NOUN CCONJ DET VERB NOUN PUNCT ADP NOUN PUNCT SCONJ VERB INTJ SPACE ADP NOUN VERB ADP DET ADJ NOUN AUX ADJ SPACE ADP DET ADJ NOUN ADP PROPN PROPN SPACE PROPN PUNCT DET DET NOUN AUX AUX VERB ADJ PUNCT SPACE SCONJ PART ADJ PUNCT ADP DET ADJ NOUN PROPN PROPN CCONJ PROPN PUNCT SPACE NUM PUNCT PRON VERB DET NOUN ADP NOUN ADP DET SPACE NOUN ADP DET NOUN SCONJ VERB PUNCT ADV PUNCT PRON VERB SPACE DET NOUN VERB NOUN ADP NOUN VERB ADP SPACE PUNCT ADV PUNCT SPACE PRON VERB PRON NOUN ADP DET NOUN VERB ADP SPACE DET PROPN X PRON SPACE VERB ADP NOUN NOUN ADP DET ADJ NOUN PUNCT ADV PUNCT PRON VERB PRON NOUN ADP DET NOUN SPACE ADP NOUN SCONJ ADV VERB ADP DET NOUN ADP SPACE ADJ ADJ PROPN ADP DET ADJ NOUN PUNCT ADV ADP VERB NOUN NOUN ADP DET NOUN PUNCT SPACE PRON VERB ADJ NOUN VERB SCONJ ADJ NOUN ADP ADJ NOUN PUNCT VERB DET SPACE ADJ NOUN ADP ADJ NOUN PUNCT VERB SPACE PRON DET NOUN PUNCT SCONJ DET NOUN AUX PART ADJ PUNCT DET NOUN AUX PRON INTJ VERB PRON DET SPACE NOUN AUX ADV AUX VERB ADP DET ADV ADJ NOUN ADP VERB DET ADJ NOUN PUNCT DET NOUN VERB SPACE ADP DET ADJ NOUN ADP NOUN PUNCT,0.5555555555555556,25.0,4.711111111111111
148,62,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked,"It is easy
to pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite
sentences, are typically riddled with curse words
and insulting phrases. While interesting, such cases
can typically be neutralized using lexicons. For
our study, we focus on the task of transferring the
non-polite sentences to polite sentences, where we
simply define non-politeness to be the absence of
both politeness and impoliteness. Note that this
is in stark contrast with the standard style transfer
tasks, which involve transferring a sentence from a
well-defined style polarity to the other (like positive
to negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the
words or phrases which belong to the original style
and replaces them with a tag token. If the sentence
has no style attributes, as in the case for politeness
transfer, the tagger adds the tag token in positions
where phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.",38.486461538461555,30.71625077440773,148,0.1615036129951477," It is easy 
 to pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality, do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite 
 sentences, are typically riddled with curse words 
 and insulting phrases. While interesting, such cases 
 can typically be neutralized using lexicons. For 
 our study, we focus on the task of transferring the 
 non polite sentences to polite sentences, where we 
 simply define non politeness to be the absence of 
 both politeness and impoliteness. Note that this 
 is in stark contrast with the standard style transfer 
 tasks, which involve transferring a sentence from a 
 well defined style polarity to the other like positive 
 to negative sentiment. We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the 
 words or phrases which belong to the original style 
 and replaces them with a tag token. If the sentence 
 has no style attributes, as in the case for politeness 
 transfer, the tagger adds the tag token in positions 
 where phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style."," It is easy 
 to pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality, do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite 
 sentences, are typically riddled with curse words 
 and insulting phrases. While interesting, such cases 
 can typically be neutralized using lexicons. For 
 our study, we focus on the task of transferring the 
 non polite sentences to polite sentences, where we 
 simply define non politeness to be the absence of 
 both politeness and impoliteness. Note that this 
 is in stark contrast with the standard style transfer 
 tasks, which involve transferring a sentence from a 
 well defined style polarity to the other like positive 
 to negative sentiment. We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the 
 words or phrases which belong to the original style 
 and replaces them with a tag token. If the sentence 
 has no style attributes, as in the case for politeness 
 transfer, the tagger adds the tag token in positions 
 where phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.", PRON AUX ADJ SPACE PART VERB DET NOUN ADP NOUN PUNCT ADV PUNCT VERB PRON VERB DET NOUN ADP NOUN PUNCT ADP ADJ NOUN PUNCT NOUN CCONJ NOUN PUNCT AUX PART ADV VERB ADP DET NOUN PUNCT CCONJ AUX ADV ADJ PART VERB PUNCT ADV PUNCT DET ADJ NOUN ADP NOUN PUNCT ADJ SPACE NOUN PUNCT AUX ADV VERB ADP NOUN NOUN SPACE CCONJ VERB NOUN PUNCT SCONJ ADJ PUNCT ADJ NOUN SPACE AUX ADV AUX VERB VERB NOUN PUNCT ADP SPACE PRON NOUN PUNCT PRON VERB ADP DET NOUN ADP VERB DET SPACE X ADJ NOUN ADP ADJ NOUN PUNCT SCONJ PRON SPACE ADV VERB ADJ NOUN PART AUX DET NOUN ADP SPACE CCONJ NOUN CCONJ NOUN PUNCT VERB SCONJ PRON SPACE AUX ADP ADJ NOUN ADP DET ADJ NOUN NOUN SPACE NOUN PUNCT PRON VERB VERB DET NOUN ADP DET SPACE ADV VERB NOUN NOUN ADP DET ADJ ADP ADJ SPACE ADP ADJ NOUN PUNCT PRON VERB DET NOUN CCONJ VERB NOUN PART VERB DET NOUN PUNCT DET NOUN VERB DET SPACE NOUN CCONJ NOUN PRON VERB ADP DET ADJ NOUN SPACE CCONJ VERB PRON ADP DET NOUN ADJ PUNCT SCONJ DET NOUN SPACE VERB DET NOUN NOUN PUNCT ADP ADP DET NOUN ADP NOUN SPACE NOUN PUNCT DET NOUN VERB DET NOUN VERB ADP NOUN SPACE SCONJ NOUN ADP DET NOUN NOUN AUX AUX VERB PUNCT DET NOUN VERB SCONJ NOUN DET NOUN ADP DET NOUN CCONJ VERB DET NOUN ADP DET NOUN NOUN PUNCT,0.5151515151515151,23.1,4.6147186147186146
149,63,Aman Madaan,"[' Politeness plays a crucial role in social interaction,\nand is closely tied with power dynamics, social\ndistance between the participants of a conversation, and gender (Brown et al., 1987; DanescuNiculescu-Mizil et al., 2013). It is also imperative\nto use the appropriate level of politeness for smooth\ncommunication in conversations (Coppock, 2005),\norganizational settings like emails (Peterson et al.,\n2011), memos, official documents, and many other\nsettings. Notably, politeness has also been identified as an interpersonal style which can be decoupled from content (Kang and Hovy, 2019). Motivated by its central importance, in this paper we\nstudy the task of converting non-polite sentences\nto polite sentences while preserving the meaning. Prior work on text style transfer (Shen et al.,\n2017; Li et al., 2018; Prabhumoye et al., 2018; Rao and Tetreault, 2018; Xu et al., 2012; Jhamtani et al., 2017) has not focused on politeness as a\nstyle transfer task, and we argue that defining it is\ncumbersome. While native speakers of a language\nand cohabitants of a region have a good working\nunderstanding of the phenomenon of politeness\nfor everyday conversation, pinning it down as a\ndefinition is non-trivial (Meier, 1995). There are\nprimarily two reasons for this complexity. First, as\nnoted by (Brown et al., 1987), the phenomenon of\npoliteness is rich and multifaceted.', 'Second, politeness of a sentence depends on the culture, language,\nand social structure of both the speaker and the addressed person. For instance, while using “please”\nin requests made to the closest friends is common\namongst the native speakers of North American\nEnglish, such an act would be considered awkward,\nif not rude, in the Arab culture (Kad´ ar and Mills ´ ,\n2011). We circumscribe the scope of politeness for the\npurpose of this study as follows: First, we adopt\nthe data driven definition of politeness proposed by\n(Danescu-Niculescu-Mizil et al., 2013). Second,\nwe base our experiments on a dataset derived from\nthe Enron corpus (Klimt and Yang, 2004) which\nconsists of email exchanges in an American corporation. Thus, we restrict our attention to the notion\nof politeness as widely accepted by the speakers of\nNorth American English in a formal setting. Even after framing politeness transfer as a task,\nthere are additional challenges involved that differentiate politeness from other styles. Consider a\ncommon directive in formal communication, “send\nme the data”. While the sentence is not impolite, a rephrasing “could you please send me the\ndata” would largely be accepted as a more polite way of phrasing the same statement (DanescuNiculescu-Mizil et al., 2013). This example brings\nout a distinct characteristic of politeness.', 'It is easy\nto pinpoint the signals for politeness. However, cues that signal the absence of politeness, like direct questions, statements and factuality (DanescuNiculescu-Mizil et al., 2013), do not explicitly appear in a sentence, and are thus hard to objectify. Further, the other extreme of politeness, impolite\nsentences, are typically riddled with curse words\nand insulting phrases. While interesting, such cases\ncan typically be neutralized using lexicons. For\nour study, we focus on the task of transferring the\nnon-polite sentences to polite sentences, where we\nsimply define non-politeness to be the absence of\nboth politeness and impoliteness. Note that this\nis in stark contrast with the standard style transfer\ntasks, which involve transferring a sentence from a\nwell-defined style polarity to the other (like positive\nto negative sentiment). We propose a tag and generate pipeline to overcome these challenges. The tagger identifies the\nwords or phrases which belong to the original style\nand replaces them with a tag token. If the sentence\nhas no style attributes, as in the case for politeness\ntransfer, the tagger adds the tag token in positions\nwhere phrases in the target style can be inserted. The generator takes as input the output of the tagger and generates a sentence in the target style.', 'Additionally, unlike previous systems, the outputs\nof the intermediate steps in our system are fully\nrealized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target\nstyle, our model won’t add any stylistic markers\nand thus would allow the input to flow as is. We evaluate our model on politeness transfer as\nwell as 5 additional tasks described in prior work\n(Shen et al., 2017; Prabhumoye et al., 2018; Li\net al., 2018) on content preservation, fluency and\nstyle transfer accuracy. Both automatic and human\nevaluations show that our model beats the stateof-the-art methods in content preservation, while\neither matching or improving the transfer accuracy\nacross six different style transfer tasks(§5). The\nresults show that our technique is effective across a\nbroad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves\nupon several of its limitations as described in (§2). Our main contribution is the design of politeness\ntransfer task. To this end, we provide a large dataset\nof nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/\npoliteness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)\nwhich are annotated as requests. To the best of our knowledge, we are the first to undertake politeness\nas a style transfer task. In the process, we highlight an important class of problems wherein the\ntransfer involves going from a neutral style to the\ntarget style. Finally, we design a “tag and generate”\npipeline that is particularly well suited for tasks like\npoliteness, while being general enough to match\nor beat the performance of the existing systems on\npopular style transfer tasks.']",intro_chunked,"Additionally, unlike previous systems, the outputs
of the intermediate steps in our system are fully
realized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target
style, our model won’t add any stylistic markers
and thus would allow the input to flow as is. We evaluate our model on politeness transfer as
well as 5 additional tasks described in prior work
(Shen et al., 2017; Prabhumoye et al., 2018; Li
et al., 2018) on content preservation, fluency and
style transfer accuracy. Both automatic and human
evaluations show that our model beats the stateof-the-art methods in content preservation, while
either matching or improving the transfer accuracy
across six different style transfer tasks(§5). The
results show that our technique is effective across a
broad spectrum of style transfer tasks. Our methodology is inspired by Li et al. (2018) and improves
upon several of its limitations as described in (§2). Our main contribution is the design of politeness
transfer task. To this end, we provide a large dataset
of nearly 1.39 million sentences labeled for politeness (https://github.com/tag-and-generate/
politeness-dataset). Additionally, we hand curate a test set of 800 samples (from Enron emails)
which are annotated as requests. To the best of our knowledge, we are the first to undertake politeness
as a style transfer task. In the process, we highlight an important class of problems wherein the
transfer involves going from a neutral style to the
target style. Finally, we design a “tag and generate”
pipeline that is particularly well suited for tasks like
politeness, while being general enough to match
or beat the performance of the existing systems on
popular style transfer tasks.",38.71730769230771,30.71625077440773,149,0.6511602997779846," Additionally, unlike previous systems, the outputs 
 of the intermediate steps in our system are fully 
 realized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target 
 style, our model wo nt add any stylistic markers 
 and thus would allow the input to flow as is. We evaluate our model on politeness transfer as 
 well as 0 additional tasks described in prior work 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000 on content preservation, fluency and 
 style transfer accuracy. Both automatic and human 
 evaluations show that our model beats the stateof the art methods in content preservation, while 
 either matching or improving the transfer accuracy 
 across six different style transfer tasks. The 
 results show that our technique is effective across a 
 broad spectrum of style transfer tasks. Our methodology is inspired by Propname Propname Propname. and improves 
 upon several of its limitations as described in. Our main contribution is the design of politeness 
 transfer task. To this end, we provide a large dataset 
 of nearly 0.00 million sentences labeled for politeness and generate 
 politeness dataset. Additionally, we hand curate a test set of 000 samples which are annotated as requests. To the best of our knowledge, we are the first to undertake politeness 
 as a style transfer task. In the process, we highlight an important class of problems wherein the 
 transfer involves going from a neutral style to the 
 target style. Finally, we design a tag and generate 
 pipeline that is particularly well suited for tasks like 
 politeness, while being general enough to match 
 or beat the performance of the existing systems on 
 popular style transfer tasks."," Additionally, unlike previous systems, the outputs 
 of the intermediate steps in our system are fully 
 realized, making the whole pipeline interpretable. Finally, if the input sentence is already in the target 
 style, our model wo nt add any stylistic markers 
 and thus would allow the input to flow as is. We evaluate our model on politeness transfer as 
 well as 0 additional tasks described in prior work 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000 on content preservation, fluency and 
 style transfer accuracy. Both automatic and human 
 evaluations show that our model beats the stateof the art methods in content preservation, while 
 either matching or improving the transfer accuracy 
 across six different style transfer tasks. The 
 results show that our technique is effective across a 
 broad spectrum of style transfer tasks. Our methodology is inspired by Propname Propname Propname. and improves 
 upon several of its limitations as described in. Our main contribution is the design of politeness 
 transfer task. To this end, we provide a large dataset 
 of nearly 0.00 million sentences labeled for politeness and generate 
 politeness dataset. Additionally, we hand curate a test set of 000 samples which are annotated as requests. To the best of our knowledge, we are the first to undertake politeness 
 as a style transfer task. In the process, we highlight an important class of problems wherein the 
 transfer involves going from a neutral style to the 
 target style. Finally, we design a tag and generate 
 pipeline that is particularly well suited for tasks like 
 politeness, while being general enough to match 
 or beat the performance of the existing systems on 
 popular style transfer tasks.", ADV PUNCT ADP ADJ NOUN PUNCT DET NOUN SPACE ADP DET ADJ NOUN ADP PRON NOUN AUX ADV SPACE VERB PUNCT VERB DET ADJ NOUN ADJ PUNCT ADV PUNCT SCONJ DET NOUN NOUN AUX ADV ADP DET NOUN SPACE NOUN PUNCT PRON NOUN AUX PART VERB DET ADJ NOUN SPACE CCONJ ADV AUX VERB DET NOUN PART VERB SCONJ AUX PUNCT PRON VERB PRON NOUN ADP NOUN NOUN ADV SPACE ADV ADP NUM ADJ NOUN VERB ADP ADJ NOUN SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM ADP NOUN NOUN PUNCT NOUN CCONJ SPACE NOUN NOUN NOUN PUNCT CCONJ ADJ CCONJ ADJ SPACE NOUN VERB SCONJ PRON NOUN VERB DET ADJ DET NOUN NOUN ADP NOUN NOUN PUNCT SCONJ SPACE CCONJ VERB CCONJ VERB DET NOUN NOUN SPACE ADP NUM ADJ NOUN NOUN NOUN PUNCT DET SPACE NOUN VERB SCONJ PRON NOUN AUX ADJ ADP DET SPACE ADJ NOUN ADP NOUN NOUN NOUN PUNCT PRON NOUN AUX VERB ADP PROPN PROPN PROPN PUNCT CCONJ VERB SPACE SCONJ ADJ ADP PRON NOUN SCONJ VERB ADP PUNCT PRON ADJ NOUN AUX DET NOUN ADP NOUN SPACE NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN SPACE ADP ADV NUM NUM NOUN VERB ADP NOUN CCONJ VERB SPACE NOUN NOUN PUNCT ADV PUNCT PRON AUX VERB DET NOUN NOUN ADP NUM NOUN PRON AUX VERB ADP NOUN PUNCT ADP DET ADJ ADP PRON NOUN PUNCT PRON AUX DET ADJ PART VERB NOUN SPACE ADP DET NOUN NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP NOUN SCONJ DET SPACE NOUN VERB VERB ADP DET ADJ NOUN ADP DET SPACE NOUN NOUN PUNCT ADV PUNCT PRON VERB DET NOUN CCONJ VERB SPACE NOUN PRON AUX ADV ADV ADJ ADP NOUN ADP SPACE NOUN PUNCT SCONJ AUX ADJ ADV PART VERB SPACE CCONJ VERB DET NOUN ADP DET VERB NOUN ADP SPACE ADJ NOUN NOUN NOUN PUNCT,0.4886731391585761,23.76923076923077,4.82200647249191
150,64,Aman Madaan,"[' Machine translation (MT) is a natural language processing task that aims to automatically\ntranslate text from a source language into a target language. Current state-of-the-art methods for MT are based on neural network architectures (Barrault et al., 2019), and often\nrequire large parallel corpora (i.e. the same text in two or more languages) for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which\ntranslators are readily available, for many low-resource languages pairs it is challenging to\nfind any speakers sufficiently proficient in both the source and target languages. We propose\na method to create data for training machine translation systems in such situations. Our\nmethod relies on obtaining captions for images depicting concepts that are relevant in most\nlanguages in the world. The captions are collected individually in each of the source and\ntarget languages, removing the requirement for the human annotators to be well-versed in\nboth languages. The captions for the same image are then paired to create training data.', 'Unlike existing methods (Hewitt et al., 2018; Bergsma & Van Durme, 2011; Singhal et al.,\n2019; Hitschler et al., 2016; Chen et al., 2019) aimed at leveraging multimodal information\nfor creating comparable corpus, our method does not rely on the existence of large resources\navailable in both the languages. Our goal is to propose a solution for the cold start scenario:\none in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates\ndata that are comparable, rather than strictly parallel. Nevertheless, comparable corpora\nhave been proven useful for machine translation and related applications (Munteanu et al.,\n2004; Abdul-Rauf & Schwenk, 2009; Irvine & Callison-Burch, 2013). In this work, we\nevaluate the utility of our collected data as a replacement for parallel corpora in two ways: • We show that bilingual speakers (of the source and target languages) judge the\ndataset as containing over 81% acceptable translation pairs. • We demonstrate that the data collected via our method has the potential of being\nused for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of parallel corpus creation\nand show that it is significantly more cost-efficient.', 'We apply our proposed method and evaluation techniques to a specific language pair: Hindi\nas the source language and English the target. Hindi is chosen as a testbed because, although it has over 520 million speakers (Chandramouli & General, 2011), far fewer parallel\ncorpora exist as compared to other widely spoken languages like French, German, Spanish,\netc. (Kunchukuttan et al., 2017). We note that our annotation tools and methodology are\nnot language-specific and can be easily adapted to any very low-resource setting with two\nassumptions: i) availability of speakers, and ii) a writing system. If the language uses a\nnovel character set not present in unicode, we can create a mapping from existing unicode\nsymbols to those in the language. If (digitally) literate speakers are not available, or if the\nlanguage lacks a working orthography (most of the world’s languages are indeed oral (Simons & Fennig, 2017)) we could instead collect spoken input, to be transcribed to a phonetic\ntranscription, with tools such as Adams et al. (2019) or Li et al. (2020).']",intro_chunked," Machine translation (MT) is a natural language processing task that aims to automatically
translate text from a source language into a target language. Current state-of-the-art methods for MT are based on neural network architectures (Barrault et al., 2019), and often
require large parallel corpora (i.e. the same text in two or more languages) for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which
translators are readily available, for many low-resource languages pairs it is challenging to
find any speakers sufficiently proficient in both the source and target languages. We propose
a method to create data for training machine translation systems in such situations. Our
method relies on obtaining captions for images depicting concepts that are relevant in most
languages in the world. The captions are collected individually in each of the source and
target languages, removing the requirement for the human annotators to be well-versed in
both languages. The captions for the same image are then paired to create training data.",44.069884004884045,30.71625077440773,150,0.47037386894226074," Machine translation is a natural language processing task that aims to automatically 
 translate text from a source language into a target language. Current state of the art methods for Propname are based on neural network architectures, and often 
 require large parallel Propname for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which 
 translators are readily available, for many low resource languages pairs it is challenging to 
 find any speakers sufficiently proficient in both the source and target languages. We propose 
 a method to create data for training machine translation systems in such situations. Our 
 method relies on obtaining captions for images depicting concepts that are relevant in most 
 languages in the world. The captions are collected individually in each of the source and 
 target languages, removing the requirement for the human annotators to be well versed in 
 both languages. The captions for the same image are then paired to create training data."," Machine translation is a natural language processing task that aims to automatically 
 translate text from a source language into a target language. Current state of the art methods for Propname are based on neural network architectures, and often 
 require large parallel Propname for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which 
 translators are readily available, for many low resource languages pairs it is challenging to 
 find any speakers sufficiently proficient in both the source and target languages. We propose 
 a method to create data for training machine translation systems in such situations. Our 
 method relies on obtaining captions for images depicting concepts that are relevant in most 
 languages in the world. The captions are collected individually in each of the source and 
 target languages, removing the requirement for the human annotators to be well versed in 
 both languages. The captions for the same image are then paired to create training data.", NOUN NOUN AUX DET ADJ NOUN NOUN NOUN PRON VERB PART ADV SPACE VERB NOUN ADP DET NOUN NOUN ADP DET NOUN NOUN PUNCT ADJ NOUN ADP DET NOUN NOUN ADP PROPN AUX VERB ADP ADJ NOUN NOUN PUNCT CCONJ ADV SPACE VERB ADJ NOUN PROPN ADP NOUN PUNCT VERB ADJ NOUN ADP DET NOUN CCONJ NOUN NOUN ADV VERB ADJ NOUN PRON AUX ADJ ADP DET NOUN PUNCT SCONJ PRON VERB DET ADJ NOUN NOUN ADP PRON SPACE NOUN AUX ADV ADJ PUNCT SCONJ ADJ ADJ NOUN NOUN VERB PRON AUX VERB PART SPACE VERB DET NOUN ADV ADJ ADP CCONJ DET NOUN CCONJ NOUN NOUN PUNCT PRON VERB SPACE DET NOUN PART VERB NOUN ADP NOUN NOUN NOUN NOUN ADP ADJ NOUN PUNCT PRON SPACE NOUN VERB ADP VERB NOUN ADP NOUN VERB NOUN PRON AUX ADJ ADP ADJ SPACE NOUN ADP DET NOUN PUNCT DET NOUN AUX VERB ADV ADP PRON ADP DET NOUN CCONJ SPACE NOUN NOUN PUNCT VERB DET NOUN SCONJ DET ADJ NOUN PART AUX ADV VERB ADP SPACE DET NOUN PUNCT DET NOUN ADP DET ADJ NOUN AUX ADV VERB PART VERB NOUN NOUN PUNCT,0.5573770491803278,22.875,5.120218579234972
151,65,Aman Madaan,"[' Machine translation (MT) is a natural language processing task that aims to automatically\ntranslate text from a source language into a target language. Current state-of-the-art methods for MT are based on neural network architectures (Barrault et al., 2019), and often\nrequire large parallel corpora (i.e. the same text in two or more languages) for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which\ntranslators are readily available, for many low-resource languages pairs it is challenging to\nfind any speakers sufficiently proficient in both the source and target languages. We propose\na method to create data for training machine translation systems in such situations. Our\nmethod relies on obtaining captions for images depicting concepts that are relevant in most\nlanguages in the world. The captions are collected individually in each of the source and\ntarget languages, removing the requirement for the human annotators to be well-versed in\nboth languages. The captions for the same image are then paired to create training data.', 'Unlike existing methods (Hewitt et al., 2018; Bergsma & Van Durme, 2011; Singhal et al.,\n2019; Hitschler et al., 2016; Chen et al., 2019) aimed at leveraging multimodal information\nfor creating comparable corpus, our method does not rely on the existence of large resources\navailable in both the languages. Our goal is to propose a solution for the cold start scenario:\none in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates\ndata that are comparable, rather than strictly parallel. Nevertheless, comparable corpora\nhave been proven useful for machine translation and related applications (Munteanu et al.,\n2004; Abdul-Rauf & Schwenk, 2009; Irvine & Callison-Burch, 2013). In this work, we\nevaluate the utility of our collected data as a replacement for parallel corpora in two ways: • We show that bilingual speakers (of the source and target languages) judge the\ndataset as containing over 81% acceptable translation pairs. • We demonstrate that the data collected via our method has the potential of being\nused for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of parallel corpus creation\nand show that it is significantly more cost-efficient.', 'We apply our proposed method and evaluation techniques to a specific language pair: Hindi\nas the source language and English the target. Hindi is chosen as a testbed because, although it has over 520 million speakers (Chandramouli & General, 2011), far fewer parallel\ncorpora exist as compared to other widely spoken languages like French, German, Spanish,\netc. (Kunchukuttan et al., 2017). We note that our annotation tools and methodology are\nnot language-specific and can be easily adapted to any very low-resource setting with two\nassumptions: i) availability of speakers, and ii) a writing system. If the language uses a\nnovel character set not present in unicode, we can create a mapping from existing unicode\nsymbols to those in the language. If (digitally) literate speakers are not available, or if the\nlanguage lacks a working orthography (most of the world’s languages are indeed oral (Simons & Fennig, 2017)) we could instead collect spoken input, to be transcribed to a phonetic\ntranscription, with tools such as Adams et al. (2019) or Li et al. (2020).']",intro_chunked,"Unlike existing methods (Hewitt et al., 2018; Bergsma & Van Durme, 2011; Singhal et al.,
2019; Hitschler et al., 2016; Chen et al., 2019) aimed at leveraging multimodal information
for creating comparable corpus, our method does not rely on the existence of large resources
available in both the languages. Our goal is to propose a solution for the cold start scenario:
one in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates
data that are comparable, rather than strictly parallel. Nevertheless, comparable corpora
have been proven useful for machine translation and related applications (Munteanu et al.,
2004; Abdul-Rauf & Schwenk, 2009; Irvine & Callison-Burch, 2013). In this work, we
evaluate the utility of our collected data as a replacement for parallel corpora in two ways: • We show that bilingual speakers (of the source and target languages) judge the
dataset as containing over 81% acceptable translation pairs. • We demonstrate that the data collected via our method has the potential of being
used for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of parallel corpus creation
and show that it is significantly more cost-efficient.",28.62500000000003,30.71625077440773,151,0.31360191106796265," Unlike existing methods Propname Propname Propname Propname, 0000; Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000 aimed at leveraging multimodal information 
 for creating comparable Propname, our method does not rely on the existence of large resources 
 available in both the languages. Our goal is to propose a solution for the cold start scenario: 
 one in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates 
 data that are comparable, rather than strictly parallel. Nevertheless, comparable Propname 
 have been proven useful for machine translation and related applications Propname Propname Propname Propname, 
 0000; Propname Propname Propname, 0000; Propname Propname Propname, 0000. In this work, we 
 evaluate the utility of our collected data as a replacement for parallel Propname in two ways: We show that bilingual speakers judge the 
 dataset as containing over 00 acceptable translation pairs. We demonstrate that the data collected via our method has the potential of being 
 used for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of Propname corpus creation 
 and show that it is significantly more cost efficient."," Unlike existing methods Propname Propname Propname Propname, 0000; Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000 aimed at leveraging multimodal information 
 for creating comparable Propname, our method does not rely on the existence of large resources 
 available in both the languages. Our goal is to propose a solution for the cold start scenario: 
 one in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates 
 data that are comparable, rather than strictly parallel. Nevertheless, comparable Propname 
 have been proven useful for machine translation and related applications Propname Propname Propname Propname, 
 0000; Propname Propname Propname, 0000; Propname Propname Propname, 0000. In this work, we 
 evaluate the utility of our collected data as a replacement for parallel Propname in two ways: We show that bilingual speakers judge the 
 dataset as containing over 00 acceptable translation pairs. We demonstrate that the data collected via our method has the potential of being 
 used for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of Propname corpus creation 
 and show that it is significantly more cost efficient.", ADP VERB NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM VERB ADP VERB ADJ NOUN SPACE ADP VERB ADJ PROPN PUNCT PRON NOUN AUX PART VERB ADP DET NOUN ADP ADJ NOUN SPACE ADJ ADP CCONJ DET NOUN PUNCT PRON NOUN AUX PART VERB DET NOUN ADP DET ADJ NOUN NOUN PUNCT SPACE NUM ADP PRON PRON VERB ADV DET ADJ NOUN ADJ ADP DET NOUN NOUN PUNCT ADV PUNCT SCONJ DET NOUN AUX VERB ADV ADP DET NOUN PUNCT PRON NOUN VERB SPACE NOUN PRON AUX ADJ PUNCT ADV ADP ADV ADJ PUNCT ADV PUNCT ADJ PROPN SPACE AUX AUX VERB ADJ ADP NOUN NOUN CCONJ ADJ NOUN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET NOUN PUNCT PRON SPACE VERB DET NOUN ADP PRON VERB NOUN ADP DET NOUN ADP ADJ PROPN ADP NUM NOUN PUNCT PRON VERB SCONJ ADJ NOUN VERB DET SPACE NOUN ADP VERB ADP NUM ADJ NOUN NOUN PUNCT PRON VERB SCONJ DET NOUN VERB ADP PRON NOUN VERB DET NOUN ADP AUX SPACE VERB ADP ADJ NOUN ADJ ADP NOUN NOUN CCONJ ADJ NOUN PUNCT ADV PUNCT PRON ADV VERB PRON NOUN ADP DET ADJ NOUN ADP PROPN NOUN NOUN SPACE CCONJ VERB SCONJ PRON AUX ADV ADJ NOUN ADJ PUNCT,0.49159663865546216,34.0,5.1722689075630255
152,66,Aman Madaan,"[' Machine translation (MT) is a natural language processing task that aims to automatically\ntranslate text from a source language into a target language. Current state-of-the-art methods for MT are based on neural network architectures (Barrault et al., 2019), and often\nrequire large parallel corpora (i.e. the same text in two or more languages) for training. Creating parallel data between a source and target language usually requires bilingual translators who are fluent in both languages. While there are a few language pairs for which\ntranslators are readily available, for many low-resource languages pairs it is challenging to\nfind any speakers sufficiently proficient in both the source and target languages. We propose\na method to create data for training machine translation systems in such situations. Our\nmethod relies on obtaining captions for images depicting concepts that are relevant in most\nlanguages in the world. The captions are collected individually in each of the source and\ntarget languages, removing the requirement for the human annotators to be well-versed in\nboth languages. The captions for the same image are then paired to create training data.', 'Unlike existing methods (Hewitt et al., 2018; Bergsma & Van Durme, 2011; Singhal et al.,\n2019; Hitschler et al., 2016; Chen et al., 2019) aimed at leveraging multimodal information\nfor creating comparable corpus, our method does not rely on the existence of large resources\navailable in both the languages. Our goal is to propose a solution for the cold start scenario:\none in which there is absolutely no comparable data available for the source language. Notably, as the captions are developed independently for each language, our method creates\ndata that are comparable, rather than strictly parallel. Nevertheless, comparable corpora\nhave been proven useful for machine translation and related applications (Munteanu et al.,\n2004; Abdul-Rauf & Schwenk, 2009; Irvine & Callison-Burch, 2013). In this work, we\nevaluate the utility of our collected data as a replacement for parallel corpora in two ways: • We show that bilingual speakers (of the source and target languages) judge the\ndataset as containing over 81% acceptable translation pairs. • We demonstrate that the data collected via our method has the potential of being\nused for downstream tasks such as machine translation and dictionary extraction. Moreover, we also compare our method to the traditional process of parallel corpus creation\nand show that it is significantly more cost-efficient.', 'We apply our proposed method and evaluation techniques to a specific language pair: Hindi\nas the source language and English the target. Hindi is chosen as a testbed because, although it has over 520 million speakers (Chandramouli & General, 2011), far fewer parallel\ncorpora exist as compared to other widely spoken languages like French, German, Spanish,\netc. (Kunchukuttan et al., 2017). We note that our annotation tools and methodology are\nnot language-specific and can be easily adapted to any very low-resource setting with two\nassumptions: i) availability of speakers, and ii) a writing system. If the language uses a\nnovel character set not present in unicode, we can create a mapping from existing unicode\nsymbols to those in the language. If (digitally) literate speakers are not available, or if the\nlanguage lacks a working orthography (most of the world’s languages are indeed oral (Simons & Fennig, 2017)) we could instead collect spoken input, to be transcribed to a phonetic\ntranscription, with tools such as Adams et al. (2019) or Li et al. (2020).']",intro_chunked,"We apply our proposed method and evaluation techniques to a specific language pair: Hindi
as the source language and English the target. Hindi is chosen as a testbed because, although it has over 520 million speakers (Chandramouli & General, 2011), far fewer parallel
corpora exist as compared to other widely spoken languages like French, German, Spanish,
etc. (Kunchukuttan et al., 2017). We note that our annotation tools and methodology are
not language-specific and can be easily adapted to any very low-resource setting with two
assumptions: i) availability of speakers, and ii) a writing system. If the language uses a
novel character set not present in unicode, we can create a mapping from existing unicode
symbols to those in the language. If (digitally) literate speakers are not available, or if the
language lacks a working orthography (most of the world’s languages are indeed oral (Simons & Fennig, 2017)) we could instead collect spoken input, to be transcribed to a phonetic
transcription, with tools such as Adams et al. (2019) or Li et al. (2020).",43.55961343930636,30.71625077440773,152,0.21651926636695862," We apply our proposed method and evaluation techniques to a specific language pair: Propname 
 as the source language and Propname the target. Propname is chosen as a testbed because, although it has over 000 million speakers, far fewer parallel 
 Propname exist as compared to other widely spoken languages like Propname, German, Spanish, 
 etc .. We note that our annotation tools and methodology are 
 not language specific and can be easily adapted to any very low resource setting with two 
 assumptions: i availability of speakers, and Propname a writing system. If the language uses a 
 novel character set not present in unicode, we can create a mapping from existing unicode 
 symbols to those in the language. If literate speakers are not available, or if the 
 language lacks a working orthography we could instead collect spoken input, to be transcribed to a phonetic 
 transcription, with tools such as Propname Propname Propname. or Propname Propname Propname.."," We apply our proposed method and evaluation techniques to a specific language pair: Propname 
 as the source language and Propname the target. Propname is chosen as a testbed because, although it has over 000 million speakers, far fewer parallel 
 Propname exist as compared to other widely spoken languages like Propname, German, Spanish, 
 etc .. We note that our annotation tools and methodology are 
 not language specific and can be easily adapted to any very low resource setting with two 
 assumptions: i availability of speakers, and Propname a writing system. If the language uses a 
 novel character set not present in unicode, we can create a mapping from existing unicode 
 symbols to those in the language. If literate speakers are not available, or if the 
 language lacks a working orthography we could instead collect spoken input, to be transcribed to a phonetic 
 transcription, with tools such as Propname Propname Propname. or Propname Propname Propname..", PRON VERB PRON VERB NOUN CCONJ NOUN NOUN ADP DET ADJ NOUN NOUN PUNCT PROPN SPACE ADP DET NOUN NOUN CCONJ PROPN DET NOUN PUNCT PROPN AUX VERB ADP DET VERB SCONJ PUNCT SCONJ PRON VERB ADP NUM NUM NOUN PUNCT ADV ADJ NOUN SPACE PROPN VERB SCONJ VERB ADP ADJ ADV VERB NOUN ADP PROPN PUNCT ADJ PUNCT ADJ PUNCT SPACE X X PUNCT PRON VERB SCONJ PRON NOUN NOUN CCONJ NOUN AUX SPACE PART NOUN ADJ CCONJ AUX AUX ADV VERB ADP DET ADV ADJ NOUN VERB ADP NUM SPACE NOUN PUNCT PRON NOUN ADP NOUN PUNCT CCONJ PROPN DET NOUN NOUN PUNCT SCONJ DET NOUN VERB DET SPACE ADJ NOUN VERB PART VERB ADP NOUN PUNCT PRON AUX VERB DET NOUN ADP VERB ADJ SPACE NOUN ADP PRON ADP DET NOUN PUNCT SCONJ ADJ NOUN AUX PART ADJ PUNCT CCONJ SCONJ DET SPACE NOUN VERB DET VERB NOUN PRON AUX ADV VERB ADJ NOUN PUNCT PART AUX VERB ADP DET ADJ SPACE NOUN PUNCT ADP NOUN ADJ ADP PROPN PROPN PROPN PUNCT CCONJ PROPN PROPN PROPN PUNCT PUNCT,0.5882352941176471,34.0,4.735294117647059
153,67,Aman Madaan,"[' While there is a long history of relation extraction systems\nin the NLP literature (e.g., (ARPA 1991; Soderland 1999;\nHoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which\nthe arguments are non-numerical. These include real world\nentities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression\ntypes such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,\ni.e., relations involving general numeric arguments such as\npopulation, area, atomic number, inflation rate, or boiling\npoint. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information\nfrom text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present\nseveral peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,\nin which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that\nsentence.', 'The signal from distant supervision becomes much\nweaker for numerical relations since there can be a much\nlarger number of reasons why a certain number is present\nin the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),\na state-of-the-art IE system, obtained an F-score of under\n20, hardly acceptable for real tasks. Secondly, numbers have\nunits and their semantics is important. Thirdly, numbers may\nbe written at different rounding levels necessitating partial\nmatching techniques. Lastly, numerical relations allow for\nsentences which describe the change in the argument value\nfrom the last measurement, instead of the argument value\nitself. In response, we develop two numerical relation extractors\nthat incorporate these observations . Both extractors expect\nminimal human supervision in the form of the unit of the\nrelation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that\nlooks for occurrences of specific numerical relation based\npatterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords\nto learn new keywords and patterns and can also leverage\nany existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries.', 'We\ncompile a knowledge-base using geopolitical data from\nWorld Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a\nslightly higher precision as compared to NumberRule. Both\nsystems massively outperform MultiR model (and its simple\nextensions) obtaining 17–25 point F-score improvements. We release our code1\nand other resources for further research. Overall, we make the following contributions in this\npaper:\n• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this\ntask compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract\na numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns\nwhile also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher\nrecall and F-score than NumberRule, and both systems\noutperform the MultiR model as well as a recall oriented\nbaseline by wide margins.']",intro_chunked," While there is a long history of relation extraction systems
in the NLP literature (e.g., (ARPA 1991; Soderland 1999;
Hoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which
the arguments are non-numerical. These include real world
entities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression
types such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,
i.e., relations involving general numeric arguments such as
population, area, atomic number, inflation rate, or boiling
point. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information
from text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present
several peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,
in which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that
sentence.",23.076408582089556,30.71625077440773,153,0.2113730013370514," While there is a long history of relation extraction systems 
 in the Propname Propname Propname, Propname 0000; Propname 0000; 
 Propname Propname Propname. 0000; Propname Propname Propname. 0000, almost all information extractors have concentrated on relations in which 
 the arguments are non numerical. These include real world 
 entities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression 
 types such as dates, while some extract the age of individuals, but almost none have focused on numerical relations, 
 ie, relations involving general numeric arguments such as 
 population, area, atomic number, inflation rate, or boiling 
 point. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information 
 from text is an important Propname Propname problem requiring research attention. This is especially true since Propname relations present 
 several peculiarities and challenges not found or less prevelant in standard Propname. Firstly, and probably most importantly, modern Propname systems are based on distant supervision, 
 in which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that 
 sentence."," While there is a long history of relation extraction systems 
 in the Propname Propname Propname, Propname 0000; Propname 0000; 
 Propname Propname Propname. 0000; Propname Propname Propname. 0000, almost all information extractors have concentrated on relations in which 
 the arguments are non numerical. These include real world 
 entities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression 
 types such as dates, while some extract the age of individuals, but almost none have focused on numerical relations, 
 ie, relations involving general numeric arguments such as 
 population, area, atomic number, inflation rate, or boiling 
 point. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information 
 from text is an important Propname Propname problem requiring research attention. This is especially true since Propname relations present 
 several peculiarities and challenges not found or less prevelant in standard Propname. Firstly, and probably most importantly, modern Propname systems are based on distant supervision, 
 in which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that 
 sentence.", SCONJ PRON VERB DET ADJ NOUN ADP NOUN NOUN NOUN SPACE ADP DET PROPN PROPN PROPN PUNCT PROPN NUM PUNCT PROPN NUM PUNCT SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PUNCT NUM PUNCT ADV DET NOUN NOUN AUX VERB ADP NOUN ADP PRON SPACE DET NOUN AUX NOUN ADJ PUNCT PRON VERB ADJ NOUN SPACE NOUN CCONJ NOUN PUNCT CCONJ ADJ NOUN PRON AUX ADV VERB ADP NOUN PUNCT ADJ ADP NOUN CCONJ NOUN NOUN PUNCT ADJ NOUN AUX VERB ADP ADJ ADJ ADJ NOUN SPACE NOUN ADJ ADP NOUN PUNCT SCONJ PRON VERB DET NOUN ADP NOUN PUNCT CCONJ ADV NOUN AUX VERB ADP ADJ NOUN PUNCT SPACE NOUN PUNCT NOUN VERB ADJ ADJ NOUN ADJ ADP SPACE NOUN PUNCT NOUN PUNCT ADJ NOUN PUNCT NOUN NOUN PUNCT CCONJ NOUN SPACE NOUN PUNCT ADJ NOUN VERB DET ADJ NOUN ADP NOUN ADP ADJ NOUN PUNCT VERB NOUN PUNCT ADJ NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT NOUN ADP ADJ NOUN SPACE ADP NOUN AUX DET ADJ PROPN PROPN NOUN VERB NOUN NOUN PUNCT PRON AUX ADV ADJ SCONJ PROPN NOUN VERB SPACE ADJ NOUN CCONJ NOUN PART VERB CCONJ ADJ ADJ ADP ADJ PROPN PUNCT ADV PUNCT CCONJ ADV ADV ADV PUNCT ADJ PROPN NOUN AUX VERB ADP ADJ NOUN PUNCT SPACE ADP PRON DET NOUN ADP NOUN ADP DET NOUN NOUN ADP DET NOUN AUX ADJ ADP DET NOUN ADP DET NOUN ADP DET SPACE NOUN PUNCT,0.5347826086956522,28.75,5.139130434782609
154,68,Aman Madaan,"[' While there is a long history of relation extraction systems\nin the NLP literature (e.g., (ARPA 1991; Soderland 1999;\nHoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which\nthe arguments are non-numerical. These include real world\nentities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression\ntypes such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,\ni.e., relations involving general numeric arguments such as\npopulation, area, atomic number, inflation rate, or boiling\npoint. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information\nfrom text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present\nseveral peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,\nin which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that\nsentence.', 'The signal from distant supervision becomes much\nweaker for numerical relations since there can be a much\nlarger number of reasons why a certain number is present\nin the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),\na state-of-the-art IE system, obtained an F-score of under\n20, hardly acceptable for real tasks. Secondly, numbers have\nunits and their semantics is important. Thirdly, numbers may\nbe written at different rounding levels necessitating partial\nmatching techniques. Lastly, numerical relations allow for\nsentences which describe the change in the argument value\nfrom the last measurement, instead of the argument value\nitself. In response, we develop two numerical relation extractors\nthat incorporate these observations . Both extractors expect\nminimal human supervision in the form of the unit of the\nrelation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that\nlooks for occurrences of specific numerical relation based\npatterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords\nto learn new keywords and patterns and can also leverage\nany existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries.', 'We\ncompile a knowledge-base using geopolitical data from\nWorld Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a\nslightly higher precision as compared to NumberRule. Both\nsystems massively outperform MultiR model (and its simple\nextensions) obtaining 17–25 point F-score improvements. We release our code1\nand other resources for further research. Overall, we make the following contributions in this\npaper:\n• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this\ntask compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract\na numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns\nwhile also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher\nrecall and F-score than NumberRule, and both systems\noutperform the MultiR model as well as a recall oriented\nbaseline by wide margins.']",intro_chunked,"The signal from distant supervision becomes much
weaker for numerical relations since there can be a much
larger number of reasons why a certain number is present
in the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),
a state-of-the-art IE system, obtained an F-score of under
20, hardly acceptable for real tasks. Secondly, numbers have
units and their semantics is important. Thirdly, numbers may
be written at different rounding levels necessitating partial
matching techniques. Lastly, numerical relations allow for
sentences which describe the change in the argument value
from the last measurement, instead of the argument value
itself. In response, we develop two numerical relation extractors
that incorporate these observations . Both extractors expect
minimal human supervision in the form of the unit of the
relation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that
looks for occurrences of specific numerical relation based
patterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords
to learn new keywords and patterns and can also leverage
any existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries.",29.417344236760158,30.71625077440773,154,0.3326716423034668," The signal from distant supervision becomes much 
 weaker for numerical relations since there can be a much 
 larger number of reasons why a certain number is present 
 in the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, Propname, 
 a state of the art Propname system, obtained an Propname score of under 
 00, hardly acceptable for real tasks. Secondly, numbers have 
 units and their semantics is important. Thirdly, numbers may 
 be written at different rounding levels necessitating partial 
 matching techniques. Lastly, Propname relations allow for 
 sentences which describe the change in the argument value 
 from the last measurement, instead of the argument value 
 itself. In response, we develop two numerical relation extractors 
 that incorporate these observations. Both extractors expect 
 minimal human supervision in the form of the unit of the 
 relation and up to four keywords indicative of that relation. Our first system, Propname, is a rule based extractor that 
 looks for occurrences of specific numerical relation based 
 patterns that explicitly mention the given keywords. Our second system, Propname, goes beyond the given keywords 
 to learn new keywords and patterns and can also leverage 
 any existing background Propname base. We evaluate our extractors on the task of extracting numerical indicators for countries."," The signal from distant supervision becomes much 
 weaker for numerical relations since there can be a much 
 larger number of reasons why a certain number is present 
 in the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, Propname, 
 a state of the art Propname system, obtained an Propname score of under 
 00, hardly acceptable for real tasks. Secondly, numbers have 
 units and their semantics is important. Thirdly, numbers may 
 be written at different rounding levels necessitating partial 
 matching techniques. Lastly, Propname relations allow for 
 sentences which describe the change in the argument value 
 from the last measurement, instead of the argument value 
 itself. In response, we develop two numerical relation extractors 
 that incorporate these observations. Both extractors expect 
 minimal human supervision in the form of the unit of the 
 relation and up to four keywords indicative of that relation. Our first system, Propname, is a rule based extractor that 
 looks for occurrences of specific numerical relation based 
 patterns that explicitly mention the given keywords. Our second system, Propname, goes beyond the given keywords 
 to learn new keywords and patterns and can also leverage 
 any existing background Propname base. We evaluate our extractors on the task of extracting numerical indicators for countries.", DET NOUN ADP ADJ NOUN VERB ADJ SPACE ADJ ADP ADJ NOUN SCONJ PRON AUX AUX DET ADV SPACE ADJ NOUN ADP NOUN SCONJ DET ADJ NOUN AUX ADJ SPACE ADP DET NOUN PUNCT PRON VERB ADJ NOUN VERB ADJ NOUN ADV ADJ ADP ADJ NOUN PUNCT ADP PRON ADJ NOUN PUNCT PROPN PUNCT SPACE DET NOUN ADP DET NOUN PROPN NOUN PUNCT VERB DET PROPN NOUN ADP ADP SPACE NUM PUNCT ADV ADJ ADP ADJ NOUN PUNCT ADV PUNCT NOUN VERB SPACE NOUN CCONJ PRON NOUN AUX ADJ PUNCT ADV PUNCT NOUN AUX SPACE AUX VERB ADP ADJ NOUN NOUN VERB ADJ SPACE NOUN NOUN PUNCT ADV PUNCT PROPN NOUN VERB ADP SPACE NOUN PRON VERB DET NOUN ADP DET NOUN NOUN SPACE ADP DET ADJ NOUN PUNCT ADV ADP DET NOUN NOUN SPACE PRON PUNCT ADP NOUN PUNCT PRON VERB NUM ADJ NOUN NOUN SPACE PRON VERB DET NOUN PUNCT DET NOUN VERB SPACE ADJ ADJ NOUN ADP DET NOUN ADP DET NOUN ADP DET SPACE NOUN CCONJ ADP PART NUM NOUN ADJ ADP DET NOUN PUNCT PRON ADJ NOUN PUNCT PROPN PUNCT AUX DET NOUN VERB NOUN PRON SPACE VERB ADP NOUN ADP ADJ ADJ NOUN VERB SPACE NOUN PRON ADV VERB DET ADJ NOUN PUNCT PRON ADJ NOUN PUNCT PROPN PUNCT VERB ADP DET VERB NOUN SPACE PART VERB ADJ NOUN CCONJ NOUN CCONJ AUX ADV VERB SPACE DET VERB NOUN PROPN NOUN PUNCT PRON VERB PRON NOUN ADP DET NOUN ADP VERB ADJ NOUN ADP NOUN PUNCT,0.5683760683760684,21.272727272727273,5.1239316239316235
155,69,Aman Madaan,"[' While there is a long history of relation extraction systems\nin the NLP literature (e.g., (ARPA 1991; Soderland 1999;\nHoffmann et al. 2011; Riedel et al. 2013)), almost all information extractors have concentrated on relations in which\nthe arguments are non-numerical. These include real world\nentities or objects, or other attributes that are usually expressed in words, such as color and job title. Several extractors do deal with specific numerical regular expression\ntypes such as dates, while some extract the age of individuals, but almost none have focused on numerical relations,\ni.e., relations involving general numeric arguments such as\npopulation, area, atomic number, inflation rate, or boiling\npoint. Numerical relations form a significant subset of relations in many fields, including science, current affairs, geography, and healthcare; extraction of numerical information\nfrom text is an important Information Extraction (IE) problem requiring research attention. This is especially true since numerical relations present\nseveral peculiarities and challenges not found or less prevelant in standard IE. Firstly, and probably most importantly, modern IE systems are based on distant supervision,\nin which the presence of entities from a database relation in a sentence is indicative of the presence of that relation in that\nsentence.', 'The signal from distant supervision becomes much\nweaker for numerical relations since there can be a much\nlarger number of reasons why a certain number is present\nin the sentence. This renders distant supervision based nonnumerical extractors less effective for numerical relations. In our early experiments, MultiR (Hoffmann et al. 2011),\na state-of-the-art IE system, obtained an F-score of under\n20, hardly acceptable for real tasks. Secondly, numbers have\nunits and their semantics is important. Thirdly, numbers may\nbe written at different rounding levels necessitating partial\nmatching techniques. Lastly, numerical relations allow for\nsentences which describe the change in the argument value\nfrom the last measurement, instead of the argument value\nitself. In response, we develop two numerical relation extractors\nthat incorporate these observations . Both extractors expect\nminimal human supervision in the form of the unit of the\nrelation and up to four keywords indicative of that relation. Our first system, NumberRule, is a rule-based extractor that\nlooks for occurrences of specific numerical relation based\npatterns that explicitly mention the given keywords. Our second system, NumberTron, goes beyond the given keywords\nto learn new keywords and patterns and can also leverage\nany existing background Knowledge base (KB). We evaluate our extractors on the task of extracting numerical indicators (e.g., inflation rate) for countries.', 'We\ncompile a knowledge-base using geopolitical data from\nWorld Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a\nslightly higher precision as compared to NumberRule. Both\nsystems massively outperform MultiR model (and its simple\nextensions) obtaining 17–25 point F-score improvements. We release our code1\nand other resources for further research. Overall, we make the following contributions in this\npaper:\n• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this\ntask compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract\na numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns\nwhile also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher\nrecall and F-score than NumberRule, and both systems\noutperform the MultiR model as well as a recall oriented\nbaseline by wide margins.']",intro_chunked,"We
compile a knowledge-base using geopolitical data from
World Bank and learn extractors for ten numerical relations. We find that NumberTron obtains a much higher recall at a
slightly higher precision as compared to NumberRule. Both
systems massively outperform MultiR model (and its simple
extensions) obtaining 17–25 point F-score improvements. We release our code1
and other resources for further research. Overall, we make the following contributions in this
paper:
• We define and analyze the task of numerical relation extraction. Our analysis highlights stark differences in this
task compared to standard IE. • We design NumberRule, a rule-based system that looks for pre-defined patterns with specific keywords to extract
a numerical relation. • We design NumberTron, an extension of MultiR for numerical relation extraction that can learn new patterns
while also exploiting other features specific to our task. • We compile a knowledge-base and a test set of 430 sentences for this task from the geopolitical domain. Our experiments reveal that NumberTron obtains much higher
recall and F-score than NumberRule, and both systems
outperform the MultiR model as well as a recall oriented
baseline by wide margins.",33.04368421052635,30.71625077440773,155,0.48828354477882385," We 
 compile a knowledge base using geopolitical data from 
 Propname Propname and learn extractors for ten Propname relations. We find that Propname obtains a much higher recall at a 
 slightly higher precision as compared to Propname. Both 
 systems massively outperform MultiR model and its simple 
 extensions obtaining 0000 point F score improvements. We release our code0 
 and other resources for further research. Overall, we make the following contributions in this 
 paper: We define and analyze the task of Propname relation extraction. Our analysis highlights stark differences in this 
 task compared to standard Propname. We design Propname, a rule based system that looks for pre defined patterns with specific keywords to extract 
 a numerical relation. We design Propname, an extension of Propname for Propname relation extraction that can learn new patterns 
 while also exploiting other features specific to our task. We compile a knowledge base and a test set of 000 sentences for this task from the geopolitical domain. Our experiments reveal that Propname obtains much higher 
 recall and Propname score than Propname, and both systems 
 outperform the Propname model as well as a recall oriented 
 baseline by wide margins."," We 
 compile a knowledge base using geopolitical data from 
 Propname Propname and learn extractors for ten Propname relations. We find that Propname obtains a much higher recall at a 
 slightly higher precision as compared to Propname. Both 
 systems massively outperform MultiR model and its simple 
 extensions obtaining 0000 point F score improvements. We release our code0 
 and other resources for further research. Overall, we make the following contributions in this 
 paper: We define and analyze the task of Propname relation extraction. Our analysis highlights stark differences in this 
 task compared to standard Propname. We design Propname, a rule based system that looks for pre defined patterns with specific keywords to extract 
 a numerical relation. We design Propname, an extension of Propname for Propname relation extraction that can learn new patterns 
 while also exploiting other features specific to our task. We compile a knowledge base and a test set of 000 sentences for this task from the geopolitical domain. Our experiments reveal that Propname obtains much higher 
 recall and Propname score than Propname, and both systems 
 outperform the Propname model as well as a recall oriented 
 baseline by wide margins.", PRON SPACE VERB DET NOUN NOUN VERB ADJ NOUN ADP SPACE PROPN PROPN CCONJ VERB NOUN ADP NUM PROPN NOUN PUNCT PRON VERB SCONJ PROPN VERB DET ADV ADJ NOUN ADP DET SPACE ADV ADJ NOUN SCONJ VERB ADP PROPN PUNCT DET SPACE NOUN ADV VERB ADJ NOUN CCONJ PRON ADJ SPACE NOUN VERB NUM NOUN NOUN NOUN NOUN PUNCT PRON VERB PRON NOUN SPACE CCONJ ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADP DET SPACE NOUN PUNCT PRON VERB CCONJ VERB DET NOUN ADP PROPN NOUN NOUN PUNCT PRON NOUN VERB ADJ NOUN ADP DET SPACE NOUN VERB ADP ADJ PROPN PUNCT PRON VERB PROPN PUNCT DET NOUN VERB NOUN PRON VERB ADP ADJ VERB NOUN ADP ADJ NOUN PART VERB SPACE DET ADJ NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN ADP PROPN ADP PROPN NOUN NOUN PRON AUX VERB ADJ NOUN SPACE SCONJ ADV VERB ADJ NOUN ADJ ADP PRON NOUN PUNCT PRON VERB DET NOUN NOUN CCONJ DET NOUN NOUN ADP NUM NOUN ADP DET NOUN ADP DET ADJ NOUN PUNCT PRON NOUN VERB SCONJ PROPN VERB ADV ADJ SPACE NOUN CCONJ PROPN NOUN ADP PROPN PUNCT CCONJ DET NOUN SPACE VERB DET PROPN NOUN ADV ADV ADP DET NOUN VERB SPACE NOUN ADP ADJ NOUN PUNCT,0.5392156862745098,20.4,5.0588235294117645
156,70,Hugo Touvron,"[' Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety.', 'This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.', 'We are releasing the following models to the general public for research and commercial use‡\n:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide¶ and code examples‖\nto facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3. The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).']",intro_chunked," Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in
complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized
domains such as programming and creative writing. They enable interaction with humans through intuitive
chat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training
methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,
followed by alignment with human preferences via techniques such as Reinforcement Learning with Human
Feedback (RLHF). Although the training methodology is simple, high computational requirements have
limited the development of LLMs to a few players. There have been public releases of pretrained LLMs
(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that
match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla
(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such
as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human
preferences, which greatly enhances their usability and safety.",30.50695652173917,30.655460127574653,156,0.7408986687660217," Large Propname Propname have shown great promise as highly capable Propname assistants that excel in 
 complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized 
 domains such as programming and creative writing. They enable interaction with humans through intuitive 
 chat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training 
 methodology. Auto regressive transformers are pretrained on an extensive corpus of self supervised data, 
 followed by alignment with human preferences via techniques such as Propname Propname with Propname 
 Propname. Although the training methodology is simple, high computational requirements have 
 limited the development of LLMs to a few players. There have been public releases of pretrained LLMs 
, Propname 0, and Propname that 
 match the performance of closed pretrained competitors like Propname 0 and Propname 
, but none of these models are suitable substitutes for closed product LLMs, such 
 as ChatGPT, Propname, and Propname. These closed product LLMs are heavily fine tuned to align with human 
 preferences, which greatly enhances their usability and safety."," Large Propname Propname have shown great promise as highly capable Propname assistants that excel in 
 complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized 
 domains such as programming and creative writing. They enable interaction with humans through intuitive 
 chat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training 
 methodology. Auto regressive transformers are pretrained on an extensive corpus of self supervised data, 
 followed by alignment with human preferences via techniques such as Propname Propname with Propname 
 Propname. Although the training methodology is simple, high computational requirements have 
 limited the development of LLMs to a few players. There have been public releases of pretrained LLMs 
, Propname 0, and Propname that 
 match the performance of closed pretrained competitors like Propname 0 and Propname 
, but none of these models are suitable substitutes for closed product LLMs, such 
 as ChatGPT, Propname, and Propname. These closed product LLMs are heavily fine tuned to align with human 
 preferences, which greatly enhances their usability and safety.", ADJ PROPN PROPN AUX VERB ADJ NOUN ADP ADV ADJ PROPN NOUN PRON VERB ADP SPACE ADJ NOUN NOUN VERB NOUN NOUN ADP DET ADJ NOUN ADP NOUN PUNCT VERB ADP ADJ SPACE NOUN ADJ ADP NOUN CCONJ ADJ NOUN PUNCT PRON VERB NOUN ADP NOUN ADP ADJ SPACE NOUN NOUN PUNCT PRON AUX VERB ADP ADJ CCONJ ADJ NOUN ADP DET ADJ NOUN PUNCT DET NOUN ADP NOUN AUX ADJ VERB DET ADV ADJ NOUN ADP DET NOUN SPACE NOUN PUNCT NOUN ADJ NOUN AUX VERB ADP DET ADJ NOUN ADP NOUN VERB NOUN PUNCT SPACE VERB ADP NOUN ADP ADJ NOUN ADP NOUN ADJ ADP PROPN PROPN ADP PROPN SPACE PROPN PUNCT SCONJ DET NOUN NOUN AUX ADJ PUNCT ADJ ADJ NOUN AUX SPACE VERB DET NOUN ADP NOUN ADP DET ADJ NOUN PUNCT PRON AUX AUX ADJ NOUN ADP VERB NOUN SPACE PUNCT PROPN NUM PUNCT CCONJ PROPN PRON SPACE VERB DET NOUN ADP ADJ VERB NOUN ADP PROPN NUM CCONJ PROPN SPACE PUNCT CCONJ NOUN ADP DET NOUN AUX ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADJ SPACE ADP NOUN PUNCT PROPN PUNCT CCONJ PROPN PUNCT DET ADJ NOUN NOUN AUX ADV ADJ VERB PART VERB ADP ADJ SPACE NOUN PUNCT PRON ADV VERB PRON NOUN CCONJ NOUN PUNCT,0.6069651741293532,28.714285714285715,5.407960199004975
157,71,Hugo Touvron,"[' Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety.', 'This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.', 'We are releasing the following models to the general public for research and commercial use‡\n:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide¶ and code examples‖\nto facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3. The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).']",intro_chunked,"This step can require significant costs in
compute and human annotation, and is often not transparent or easily reproducible, limiting progress within
the community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and
Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,
Llama 2-Chat models generally perform better than existing open-source models. They also appear to
be on par with some of the closed-source models, at least on the human evaluations we performed (see
Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data
annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,
this paper contributes a thorough description of our fine-tuning methodology and approach to improving
LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and
continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as
the emergence of tool usage and temporal organization of knowledge.",26.084918478260903,30.655460127574653,157,0.5775670409202576," This step can require significant costs in 
 compute and human annotation, and is often not transparent or easily reproducible, limiting progress within 
 the community to advance Propname alignment research. In this work, we develop and release Propname 0, a family of pretrained and fine tuned LLMs, Propname 0 and 
 Propname 0 Propname, at scales up to 00B parameters. On the series of helpfulness and safety benchmarks we tested, 
 Propname 0 Chat models generally perform better than existing open source models. They also appear to 
 be on par with some of the closed source models, at least on the human evaluations we performed see 
 Figures 0 and 0. We have taken measures to increase the safety of these models, using safety specific data 
 annotation and tuning, as well as conducting red teaming and employing iterative evaluations. Additionally, 
 this paper contributes a thorough description of our fine tuning methodology and approach to improving 
 Propname safety. We hope that this openness will enable the community to reproduce fine tuned LLMs and 
 continue to improve the safety of those models, paving the way for more responsible development of Propname. We also share novel observations we made during the development of Propname 0 and Propname 0 Propname, such as 
 the emergence of tool usage and temporal organization of knowledge."," This step can require significant costs in 
 compute and human annotation, and is often not transparent or easily reproducible, limiting progress within 
 the community to advance Propname alignment research. In this work, we develop and release Propname 0, a family of pretrained and fine tuned LLMs, Propname 0 and 
 Propname 0 Propname, at scales up to 00B parameters. On the series of helpfulness and safety benchmarks we tested, 
 Propname 0 Chat models generally perform better than existing open source models. They also appear to 
 be on par with some of the closed source models, at least on the human evaluations we performed see 
 Figures 0 and 0. We have taken measures to increase the safety of these models, using safety specific data 
 annotation and tuning, as well as conducting red teaming and employing iterative evaluations. Additionally, 
 this paper contributes a thorough description of our fine tuning methodology and approach to improving 
 Propname safety. We hope that this openness will enable the community to reproduce fine tuned LLMs and 
 continue to improve the safety of those models, paving the way for more responsible development of Propname. We also share novel observations we made during the development of Propname 0 and Propname 0 Propname, such as 
 the emergence of tool usage and temporal organization of knowledge.", DET NOUN AUX VERB ADJ NOUN ADP SPACE NOUN CCONJ ADJ NOUN PUNCT CCONJ AUX ADV PART ADJ CCONJ ADV ADJ PUNCT VERB NOUN ADP SPACE DET NOUN PART VERB PROPN NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB CCONJ VERB PROPN NUM PUNCT DET NOUN ADP VERB CCONJ ADJ VERB NOUN PUNCT PROPN NUM CCONJ SPACE PROPN NUM PROPN PUNCT ADP NOUN ADP ADP NUM NOUN PUNCT ADP DET NOUN ADP ADJ CCONJ NOUN NOUN PRON VERB PUNCT SPACE PROPN NUM NOUN NOUN ADV VERB ADV ADP VERB ADJ NOUN NOUN PUNCT PRON ADV VERB PART SPACE AUX ADP NOUN ADP PRON ADP DET ADJ NOUN NOUN PUNCT ADP ADJ ADP DET ADJ NOUN PRON VERB VERB SPACE NOUN NUM CCONJ NUM PUNCT PRON AUX VERB NOUN PART VERB DET NOUN ADP DET NOUN PUNCT VERB NOUN ADJ NOUN SPACE NOUN CCONJ NOUN PUNCT ADV ADV ADP VERB ADJ NOUN CCONJ VERB ADJ NOUN PUNCT ADV PUNCT SPACE DET NOUN VERB DET ADJ NOUN ADP PRON ADJ NOUN NOUN CCONJ NOUN ADP VERB SPACE PROPN NOUN PUNCT PRON VERB SCONJ DET NOUN AUX VERB DET NOUN PART VERB ADV VERB NOUN CCONJ SPACE VERB PART VERB DET NOUN ADP DET NOUN PUNCT VERB DET NOUN ADP ADV ADJ NOUN ADP PROPN PUNCT PRON ADV VERB ADJ NOUN PRON VERB ADP DET NOUN ADP PROPN NUM CCONJ PROPN NUM PROPN PUNCT ADJ ADP SPACE DET NOUN ADP NOUN NOUN CCONJ ADJ NOUN ADP NOUN PUNCT,0.5574468085106383,29.375,4.787234042553192
158,72,Hugo Touvron,"[' Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety.', 'This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.', 'We are releasing the following models to the general public for research and commercial use‡\n:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide¶ and code examples‖\nto facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3. The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).']",intro_chunked,"We are releasing the following models to the general public for research and commercial use‡
:
1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also
increased the size of the pretraining corpus by 40%, doubled the context length of the model, and
adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with
7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper
but are not releasing.§
2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release
variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,
Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;
Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover
all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform
safety testing and tuning tailored to their specific applications of the model. We provide a responsible use
guide¶ and code examples‖
to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of
our responsible release strategy can be found in Section 5.3. The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology
(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related
work (Section 6), and conclusions (Section 7).",48.99000000000001,30.655460127574653,158,0.49662959575653076," We are releasing the following models to the general public for research and commercial use 
: 
0. Propname 0, an updated version of Propname 0, trained on a new mix of publicly available data. We also 
 increased the size of the pretraining corpus by 00, doubled the context length of the model, and 
 adopted grouped query attention. We are releasing variants of Propname 0 with 
 Propname, 00B, and 00B parameters. We have also trained 00B variants, which we report on in this paper 
 but are not releasing.0. Propname 0 Propname, a fine tuned version of Propname 0 that is optimized for dialogue use cases. We release 
 variants of this model with Propname, 00B, and 00B parameters as well. We believe that the open release of Propname, when done safely, will be a net benefit to society. Like all LLMs, 
 Propname 0 is a new technology that carries potential risks with use Propname Propname Propname Propname, 0000b; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000. Testing conducted to date has been in Propname and has not and could not cover 
 all scenarios. Therefore, before deploying any applications of Propname 0 Propname, developers should perform 
 safety testing and tuning tailored to their specific applications of the model. We provide a responsible use 
 guide and code examples 
 to facilitate the safe deployment of Propname 0 and Propname 0 Chat. More details of 
 our responsible release strategy can be found in Section 0.0. The remainder of this paper describes our pretraining methodology, fine tuning methodology 
, approach to model safety, key observations and insights, relevant related 
 work, and conclusions."," We are releasing the following models to the general public for research and commercial use 
: 
0. Propname 0, an updated version of Propname 0, trained on a new mix of publicly available data. We also 
 increased the size of the pretraining corpus by 00, doubled the context length of the model, and 
 adopted grouped query attention. We are releasing variants of Propname 0 with 
 Propname, 00B, and 00B parameters. We have also trained 00B variants, which we report on in this paper 
 but are not releasing.0. Propname 0 Propname, a fine tuned version of Propname 0 that is optimized for dialogue use cases. We release 
 variants of this model with Propname, 00B, and 00B parameters as well. We believe that the open release of Propname, when done safely, will be a net benefit to society. Like all LLMs, 
 Propname 0 is a new technology that carries potential risks with use Propname Propname Propname Propname, 0000b; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000. Testing conducted to date has been in Propname and has not and could not cover 
 all scenarios. Therefore, before deploying any applications of Propname 0 Propname, developers should perform 
 safety testing and tuning tailored to their specific applications of the model. We provide a responsible use 
 guide and code examples 
 to facilitate the safe deployment of Propname 0 and Propname 0 Chat. More details of 
 our responsible release strategy can be found in Section 0.0. The remainder of this paper describes our pretraining methodology, fine tuning methodology 
, approach to model safety, key observations and insights, relevant related 
 work, and conclusions.", PRON AUX VERB DET VERB NOUN ADP DET ADJ NOUN ADP NOUN CCONJ ADJ NOUN SPACE PUNCT SPACE PUNCT PUNCT PROPN NUM PUNCT DET VERB NOUN ADP PROPN NUM PUNCT VERB ADP DET ADJ NOUN ADP ADV ADJ NOUN PUNCT PRON ADV SPACE VERB DET NOUN ADP DET VERB NOUN ADP NUM PUNCT VERB DET NOUN NOUN ADP DET NOUN PUNCT CCONJ SPACE VERB VERB NOUN NOUN PUNCT PRON AUX VERB NOUN ADP PROPN NUM ADP SPACE PROPN PUNCT NUM PUNCT CCONJ NUM NOUN PUNCT PRON AUX ADV VERB NUM NOUN PUNCT PRON PRON VERB ADP ADP DET NOUN SPACE CCONJ AUX PART VERB PUNCT PUNCT PUNCT PROPN NUM PROPN PUNCT DET ADJ VERB NOUN ADP PROPN NUM PRON AUX VERB ADP NOUN NOUN NOUN PUNCT PRON VERB SPACE NOUN ADP DET NOUN ADP PROPN PUNCT NUM PUNCT CCONJ NUM NOUN ADV ADV PUNCT PRON VERB SCONJ DET ADJ NOUN ADP PROPN PUNCT SCONJ VERB ADV PUNCT AUX AUX DET ADJ NOUN ADP NOUN PUNCT ADP DET NOUN PUNCT SPACE PROPN NUM AUX DET ADJ NOUN PRON VERB ADJ NOUN ADP NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT NOUN VERB ADP NOUN AUX AUX ADP PROPN CCONJ VERB PART CCONJ AUX PART VERB SPACE DET NOUN PUNCT ADV PUNCT ADP VERB DET NOUN ADP PROPN NUM PROPN PUNCT NOUN AUX VERB SPACE NOUN NOUN CCONJ VERB VERB ADP PRON ADJ NOUN ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN SPACE NOUN CCONJ NOUN NOUN SPACE PART VERB DET ADJ NOUN ADP PROPN NUM CCONJ PROPN NUM NOUN PUNCT ADJ NOUN ADP SPACE PRON ADJ NOUN NOUN AUX AUX VERB ADP NOUN NUM PUNCT DET NOUN ADP DET NOUN VERB PRON VERB NOUN PUNCT ADJ NOUN NOUN SPACE PUNCT NOUN ADP NOUN NOUN PUNCT ADJ NOUN CCONJ NOUN PUNCT ADJ ADJ SPACE NOUN PUNCT CCONJ NOUN PUNCT,0.46229508196721314,21.785714285714285,4.547540983606558
159,73,Hugo Touvron,"[' Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties first appeared when scaling models to a\nsufficient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that\nmore parameters will lead to better performance. However, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale. In this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of performance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al.', '(2022) recommends\ntraining a 10B model on 200B tokens, we find\nthat the performance of a 7B model continues to\nimprove even after 1T tokens. The focus of this work is to train a series of\nlanguage models that achieve the best possible performance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA, ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10× smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work compatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview\nof the modifications we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.']",intro_chunked," Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a
few examples (Brown et al., 2020). These few-shot
properties first appeared when scaling models to a
sufficient size (Kaplan et al., 2020), resulting in a
line of work that focuses on further scaling these
models (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that
more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022)
shows that, for a given compute budget, the best
performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. (2022) is to determine how to best
scale the dataset and model sizes for a particular
training compute budget. However, this objective
disregards the inference budget, which becomes
critical when serving a language model at scale. In this context, given a target level of performance,
the preferred model is not the fastest to train but the
fastest at inference, and although it may be cheaper
to train a large model to reach a certain level of performance, a smaller one trained longer will
ultimately be cheaper at inference. For instance,
although Hoffmann et al.",54.422938356164394,30.655460127574653,159,0.2823263704776764," Large Propname Propname trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a 
 few examples. These few shot 
 properties first appeared when scaling models to a 
 sufficient size, resulting in a 
 line of work that focuses on further scaling these 
 models. These efforts are based on the assumption that 
 more parameters will lead to better performance. However, recent work from Propname Propname Propname. shows that, for a given compute budget, the best 
 performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Propname Propname Propname. is to determine how to best 
 scale the dataset and model sizes for a particular 
 training compute budget. However, this objective 
 disregards the inference budget, which becomes 
 critical when serving a language model at scale. In this context, given a target level of performance, 
 the preferred model is not the fastest to train but the 
 fastest at inference, and although it may be cheaper 
 to train a large model to reach a certain level of performance, a smaller one trained longer will 
 ultimately be cheaper at inference. For instance, 
 although Propname Propname Propname."," Large Propname Propname trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a 
 few examples. These few shot 
 properties first appeared when scaling models to a 
 sufficient size, resulting in a 
 line of work that focuses on further scaling these 
 models. These efforts are based on the assumption that 
 more parameters will lead to better performance. However, recent work from Propname Propname Propname. shows that, for a given compute budget, the best 
 performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Propname Propname Propname. is to determine how to best 
 scale the dataset and model sizes for a particular 
 training compute budget. However, this objective 
 disregards the inference budget, which becomes 
 critical when serving a language model at scale. In this context, given a target level of performance, 
 the preferred model is not the fastest to train but the 
 fastest at inference, and although it may be cheaper 
 to train a large model to reach a certain level of performance, a smaller one trained longer will 
 ultimately be cheaper at inference. For instance, 
 although Propname Propname Propname.", ADJ PROPN PROPN VERB ADP ADJ NOUN ADP NOUN AUX VERB PRON NOUN PART VERB ADJ NOUN ADP ADJ NOUN CCONJ ADP DET SPACE ADJ NOUN PUNCT DET ADJ NOUN SPACE NOUN ADV VERB SCONJ VERB NOUN ADP DET SPACE ADJ NOUN PUNCT VERB ADP DET SPACE NOUN ADP NOUN PRON VERB ADP ADV VERB DET SPACE NOUN PUNCT DET NOUN AUX VERB ADP DET NOUN SCONJ SPACE ADJ NOUN AUX VERB ADP ADJ NOUN PUNCT ADV PUNCT ADJ NOUN ADP PROPN PROPN PROPN PUNCT VERB SCONJ PUNCT ADP DET VERB NOUN NOUN PUNCT DET ADJ SPACE NOUN AUX PART VERB ADP DET ADJ NOUN PUNCT CCONJ ADP ADJ NOUN VERB ADP ADJ NOUN PUNCT DET NOUN ADP DET NOUN NOUN ADP PROPN PROPN PROPN PUNCT AUX PART VERB SCONJ PART ADV SPACE NOUN DET NOUN CCONJ NOUN NOUN ADP DET ADJ SPACE NOUN NOUN NOUN PUNCT ADV PUNCT DET NOUN SPACE NOUN DET NOUN NOUN PUNCT PRON VERB SPACE ADJ SCONJ VERB DET NOUN NOUN ADP NOUN PUNCT ADP DET NOUN PUNCT VERB DET NOUN NOUN ADP NOUN PUNCT SPACE DET ADJ NOUN AUX PART DET ADJ PART VERB CCONJ DET SPACE ADJ ADP NOUN PUNCT CCONJ SCONJ PRON AUX AUX ADJ SPACE PART VERB DET ADJ NOUN PART VERB DET ADJ NOUN ADP NOUN PUNCT DET ADJ NOUN VERB ADV AUX SPACE ADV AUX ADJ ADP NOUN PUNCT ADP NOUN PUNCT SPACE SCONJ PROPN PROPN PROPN PUNCT,0.5022421524663677,22.3,4.695067264573991
160,74,Hugo Touvron,"[' Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties first appeared when scaling models to a\nsufficient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that\nmore parameters will lead to better performance. However, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws from Hoffmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale. In this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of performance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al.', '(2022) recommends\ntraining a 10B model on 200B tokens, we find\nthat the performance of a 7B model continues to\nimprove even after 1T tokens. The focus of this work is to train a series of\nlanguage models that achieve the best possible performance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA, ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10× smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work compatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview\nof the modifications we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.']",intro_chunked,"(2022) recommends
training a 10B model on 200B tokens, we find
that the performance of a 7B model continues to
improve even after 1T tokens. The focus of this work is to train a series of
language models that achieve the best possible performance at various inference budgets, by training
on more tokens than what is typically used. The
resulting models, called LLaMA, ranges from 7B
to 65B parameters with competitive performance
compared to the best existing LLMs. For instance,
LLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10× smaller. We believe that
this model will help democratize the access and
study of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter
model is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only
use publicly available data, making our work compatible with open-sourcing, while most existing
models rely on data which is either not publicly
available or undocumented (e.g. “Books – 2TB” or
“Social media conversations”). There exist some
exceptions, notably OPT (Zhang et al., 2022),
GPT-NeoX (Black et al., 2022), BLOOM (Scao
et al., 2022) and GLM (Zeng et al., 2022), but none
that are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview
of the modifications we made to the transformer
architecture (Vaswani et al., 2017), as well as our
training method. We then report the performance of
our models and compare with others LLMs on a set
of standard benchmarks. Finally, we expose some
of the biases and toxicity encoded in our models,
using some of the most recent benchmarks from
the responsible AI community.",51.18279279279281,30.655460127574653,160,0.37628665566444397," recommends 
 training a 00B model on Propname Propname, we find 
 that the performance of a 0B model continues to 
 improve even after 0 Propname tokens. The focus of this work is to train a series of 
 language models that achieve the best possible performance at various inference budgets, by training 
 on more tokens than what is typically used. The 
 resulting models, called Propname, ranges from Propname 
 to 00B parameters with competitive performance 
 compared to the best existing LLMs. For instance, 
 Propname 00B outperforms Propname 0 on most benchmarks, despite being 00 smaller. We believe that 
 this model will help democratize the access and 
 study of Propname, since it can be run on a single Propname. At the higher end of the scale, our 00B parameter 
 model is also competitive with the best large language models such as Propname or Propname Propname Unlike Propname, Propname, or Propname 0, we only 
 use publicly available data, making our work compatible with open sourcing, while most existing 
 models rely on data which is either not publicly 
 available or undocumented Propname Propname 0 Propname or 
 Social media conversations. There exist some 
 exceptions, notably Propname, 
 Propname Propname, Propname Propname 
 Propname Propname Propname, 0000 and Propname, but none 
 that are competitive with PaLM 00B or Propname. In the rest of this paper, we present an overview 
 of the modifications we made to the transformer 
 architecture, as well as our 
 training method. We then report the performance of 
 our models and compare with others LLMs on a set 
 of standard benchmarks. Finally, we expose some 
 of the biases and toxicity encoded in our models, 
 using some of the most recent benchmarks from 
 the responsible Propname community."," recommends 
 training a 00B model on Propname Propname, we find 
 that the performance of a 0B model continues to 
 improve even after 0 Propname tokens. The focus of this work is to train a series of 
 language models that achieve the best possible performance at various inference budgets, by training 
 on more tokens than what is typically used. The 
 resulting models, called Propname, ranges from Propname 
 to 00B parameters with competitive performance 
 compared to the best existing LLMs. For instance, 
 Propname 00B outperforms Propname 0 on most benchmarks, despite being 00 smaller. We believe that 
 this model will help democratize the access and 
 study of Propname, since it can be run on a single Propname. At the higher end of the scale, our 00B parameter 
 model is also competitive with the best large language models such as Propname or Propname Propname Unlike Propname, Propname, or Propname 0, we only 
 use publicly available data, making our work compatible with open sourcing, while most existing 
 models rely on data which is either not publicly 
 available or undocumented Propname Propname 0 Propname or 
 Social media conversations. There exist some 
 exceptions, notably Propname, 
 Propname Propname, Propname Propname 
 Propname Propname Propname, 0000 and Propname, but none 
 that are competitive with PaLM 00B or Propname. In the rest of this paper, we present an overview 
 of the modifications we made to the transformer 
 architecture, as well as our 
 training method. We then report the performance of 
 our models and compare with others LLMs on a set 
 of standard benchmarks. Finally, we expose some 
 of the biases and toxicity encoded in our models, 
 using some of the most recent benchmarks from 
 the responsible Propname community.", VERB SPACE VERB DET NUM NOUN ADP PROPN PROPN PUNCT PRON VERB SPACE SCONJ DET NOUN ADP DET NOUN NOUN VERB PART SPACE VERB ADV ADP NUM PROPN NOUN PUNCT DET NOUN ADP DET NOUN AUX PART VERB DET NOUN ADP SPACE NOUN NOUN PRON VERB DET ADJ ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADP VERB SPACE ADP ADJ NOUN ADP PRON AUX ADV VERB PUNCT DET SPACE VERB NOUN PUNCT VERB PROPN PUNCT VERB ADP PROPN SPACE ADP NUM NOUN ADP ADJ NOUN SPACE VERB ADP DET ADJ VERB NOUN PUNCT ADP NOUN PUNCT SPACE PROPN NUM VERB PROPN NUM ADP ADJ NOUN PUNCT SCONJ AUX NUM ADJ PUNCT PRON VERB SCONJ SPACE DET NOUN AUX VERB VERB DET NOUN CCONJ SPACE NOUN ADP PROPN PUNCT SCONJ PRON AUX AUX VERB ADP DET ADJ PROPN PUNCT ADP DET ADJ NOUN ADP DET NOUN PUNCT PRON NUM NOUN SPACE NOUN AUX ADV ADJ ADP DET ADJ ADJ NOUN NOUN ADJ ADP PROPN CCONJ PROPN PROPN ADP PROPN PUNCT PROPN PUNCT CCONJ PROPN NUM PUNCT PRON ADV SPACE VERB ADV ADJ NOUN PUNCT VERB PRON NOUN ADJ ADP ADJ NOUN PUNCT SCONJ ADJ VERB SPACE NOUN VERB ADP NOUN PRON AUX CCONJ PART ADV SPACE ADJ CCONJ ADJ PROPN PROPN NUM PROPN CCONJ SPACE ADJ NOUN NOUN PUNCT PRON VERB DET SPACE NOUN PUNCT ADV PROPN PUNCT SPACE PROPN PROPN PUNCT PROPN PROPN SPACE PROPN PROPN PROPN PUNCT NUM CCONJ PROPN PUNCT CCONJ NOUN SPACE PRON AUX ADJ ADP NOUN NUM CCONJ PROPN PUNCT ADP DET NOUN ADP DET NOUN PUNCT PRON VERB DET NOUN SPACE ADP DET NOUN PRON VERB ADP DET ADJ SPACE NOUN PUNCT ADV ADV ADP PRON SPACE NOUN NOUN PUNCT PRON ADV VERB DET NOUN ADP SPACE PRON NOUN CCONJ VERB ADP NOUN NOUN ADP DET NOUN SPACE ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB PRON SPACE ADP DET NOUN CCONJ NOUN VERB ADP PRON NOUN PUNCT SPACE VERB PRON ADP DET ADV ADJ NOUN ADP SPACE DET ADJ PROPN NOUN PUNCT,0.47096774193548385,31.0,4.683870967741935
161,75,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked," Although the fundamental ideas of deep trainable neural
networks have been around for decades, only recently have
barriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of
these barriers are related to non-convex optimization in one
way or another, which is central to the success of modern
neural networks. The optimization challenges have been
addressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization
of very deep networks. An exceptionally successful design
principle is using residual connections [24, 25]. Although
this does not change the expressiveness of the functions that
the network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very
deep networks. Another key element to the optimization is
the importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet
dataset [11], and the popularization of transfer learning with
pre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions
of parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address
this issue.",22.904412997903563,30.655460127574653,161,0.26370716094970703," Although the fundamental ideas of deep trainable neural 
 networks have been around for decades, only recently have 
 barriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of 
 these barriers are related to non convex optimization in one 
 way or another, which is central to the success of modern 
 neural networks. The optimization challenges have been 
 addressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization 
 of very deep networks. An exceptionally successful design 
 principle is using residual connections. Although 
 this does not change the expressiveness of the functions that 
 the network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very 
 deep networks. Another key element to the optimization is 
 the importance of data, revealed by the step change in visual recognition performance resulting from the Propname 
 dataset, and the popularization of transfer learning with 
 pre training on large datasets. However, even when trained with millions of images, recent deep networks with millions if not billions 
 of parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout, or label smoothing are limited in their ability to address 
 this issue."," Although the fundamental ideas of deep trainable neural 
 networks have been around for decades, only recently have 
 barriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of 
 these barriers are related to non convex optimization in one 
 way or another, which is central to the success of modern 
 neural networks. The optimization challenges have been 
 addressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization 
 of very deep networks. An exceptionally successful design 
 principle is using residual connections. Although 
 this does not change the expressiveness of the functions that 
 the network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very 
 deep networks. Another key element to the optimization is 
 the importance of data, revealed by the step change in visual recognition performance resulting from the Propname 
 dataset, and the popularization of transfer learning with 
 pre training on large datasets. However, even when trained with millions of images, recent deep networks with millions if not billions 
 of parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout, or label smoothing are limited in their ability to address 
 this issue.", SCONJ DET ADJ NOUN ADP ADJ ADJ ADJ SPACE NOUN AUX AUX ADV ADP NOUN PUNCT ADV ADV AUX SPACE NOUN AUX VERB PART VERB NOUN ADP ADV VERB ADJ ADJ NOUN ADP NOUN PUNCT ADJ ADP SPACE DET NOUN AUX VERB ADP ADJ ADJ NOUN ADP NUM SPACE NOUN CCONJ PRON PUNCT PRON AUX ADJ ADP DET NOUN ADP ADJ SPACE ADJ NOUN PUNCT DET NOUN NOUN AUX AUX SPACE VERB ADP ADJ NOUN ADP DET NOUN PUNCT ADV PUNCT ADJ NOUN AUX VERB PART VERB DET NOUN SPACE ADP ADV ADJ NOUN PUNCT DET ADV ADJ NOUN SPACE NOUN AUX VERB ADJ NOUN PUNCT SCONJ SPACE PRON AUX PART VERB DET NOUN ADP DET NOUN PRON SPACE DET NOUN AUX VERB PUNCT DET ADJ NOUN NOUN NOUN PUNCT ADP DET NOUN PUNCT DET NOUN ADP VERB ADV SPACE ADJ NOUN PUNCT DET ADJ NOUN ADP DET NOUN AUX SPACE DET NOUN ADP NOUN PUNCT VERB ADP DET NOUN NOUN ADP ADJ NOUN NOUN VERB ADP DET PROPN SPACE NOUN PUNCT CCONJ DET NOUN ADP NOUN VERB ADP SPACE ADJ NOUN ADP ADJ NOUN PUNCT ADV PUNCT ADV SCONJ VERB ADP NOUN ADP NOUN PUNCT ADJ ADJ NOUN ADP NOUN SCONJ PART NOUN SPACE ADP NOUN PUNCT AUX ADV ADV VERB PUNCT ADJ NOUN ADP NOUN NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN AUX VERB ADP PRON NOUN PART VERB SPACE DET NOUN PUNCT,0.593607305936073,24.333333333333332,5.301369863013699
162,76,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"Data-augmentation strategies, including those
mixing different images like Mixup [61] and CutMix [60],
have proven to provide a complementary data-driven form
of regularization. More recently, multiple works propose
to resort to self-supervised pre-training. These approaches
rely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)
auto-encoders [5, 22, 16], which were popular in the early
deep learning literature [7, 19, 27]. Similarly, contrastive
approaches [23, 9] provide a richer supervision less prone to
a supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,
possibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over
learning from the data directly. In contrast to traditional
distillation, co-distillation does not require pre-training a
(strong) teacher. Instead, a pool of models supervise each
other. Practically, it faces several limitations, including the
difficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.",25.795178419711135,30.655460127574653,162,0.3926617205142975," Data augmentation strategies, including those 
 mixing different images like Propname and Propname, 
 have proven to provide a complementary data driven form 
 of regularization. More recently, multiple works propose 
 to resort to self supervised pre training. These approaches 
 rely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in auto encoders, which were popular in the early 
 deep learning literature. Similarly, contrastive 
 approaches provide a richer supervision less prone to 
 a supervision collapse. Overall, self supervised learning makes it possible to learn larger models with less data, 
 possibly reducing the need of a pre training stage. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model, allowing the student to improve over 
 learning from the data directly. In contrast to traditional 
 distillation, Propname distillation does not require pre training a teacher. Instead, a pool of models supervise each 
 other. Practically, it faces several limitations, including the 
 difficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights."," Data augmentation strategies, including those 
 mixing different images like Propname and Propname, 
 have proven to provide a complementary data driven form 
 of regularization. More recently, multiple works propose 
 to resort to self supervised pre training. These approaches 
 rely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in auto encoders, which were popular in the early 
 deep learning literature. Similarly, contrastive 
 approaches provide a richer supervision less prone to 
 a supervision collapse. Overall, self supervised learning makes it possible to learn larger models with less data, 
 possibly reducing the need of a pre training stage. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model, allowing the student to improve over 
 learning from the data directly. In contrast to traditional 
 distillation, Propname distillation does not require pre training a teacher. Instead, a pool of models supervise each 
 other. Practically, it faces several limitations, including the 
 difficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.", NOUN NOUN NOUN PUNCT VERB PRON SPACE VERB ADJ NOUN ADP PROPN CCONJ PROPN PUNCT SPACE AUX VERB PART VERB DET ADJ NOUN VERB NOUN SPACE ADP NOUN PUNCT ADV ADV PUNCT ADJ NOUN VERB SPACE PART VERB ADP NOUN VERB ADJ NOUN PUNCT DET NOUN SPACE VERB ADP DET NOUN NOUN PRON ADV VERB ADJ NOUN NOUN ADP DET NOUN ADJ ADP NOUN PUNCT ADP NOUN PUNCT ADV PRON AUX AUX VERB NOUN ADP NOUN NOUN PUNCT PRON AUX ADJ ADP DET ADJ SPACE ADJ VERB NOUN PUNCT ADV PUNCT ADJ SPACE NOUN VERB DET ADJ NOUN ADV ADJ ADP SPACE DET NOUN NOUN PUNCT ADV PUNCT NOUN ADJ NOUN VERB PRON ADJ PART VERB ADJ NOUN ADP ADJ NOUN PUNCT SPACE ADV VERB DET NOUN ADP DET ADJ NOUN NOUN PUNCT NOUN AUX DET ADJ NOUN PART VERB NOUN PUNCT NOUN NOUN AUX ADV VERB PART VERB NOUN ADP DET NOUN NOUN ADP DET NOUN NOUN PUNCT VERB DET NOUN PART VERB ADP SPACE VERB ADP DET NOUN ADV PUNCT ADP NOUN ADP ADJ SPACE NOUN PUNCT PROPN NOUN AUX PART VERB ADJ VERB DET NOUN PUNCT ADV PUNCT DET NOUN ADP NOUN VERB DET SPACE ADJ PUNCT ADV PUNCT PRON VERB ADJ NOUN PUNCT VERB DET SPACE NOUN ADP ADV VERB ADJ ADP NUM NOUN ADP NOUN NOUN PUNCT SCONJ PRON VERB VERB DET NOUN PUNCT,0.6018518518518519,19.636363636363637,5.282407407407407
163,77,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider
a single target model to be trained, and we instantiate two
submodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can
backpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel
serves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the
submodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss
compared to the label loss, and our experiments show that
it significantly increases the final model accuracy. This co-training across different submodels, which we
refer to as cosub, can be regarded as a massive co-training
between 2
L models that share a common set of parameters,
where L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all
models. With a layer drop-rate set to 0.5, for instance for
a ViT-H model, all submodels are equiprobable, and then it
amounts to averaging the weights of 2
2×32 models.",37.62464939024392,30.655460127574653,163,0.49421456456184387," In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider 
 a single target model to be trained, and we instantiate two 
 submodels on the fly, simply by layerwise dropout. This gives us two neural networks through which we can 
 backpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel 
 serves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the 
 submodels. Our approach is illustrated in Figure 0: the parameter controls the importance of the co training loss 
 compared to the label loss, and our experiments show that 
 it significantly increases the final model accuracy. This co training across different submodels, which we 
 refer to as cosub, can be regarded as a massive co training 
 between 0 
 L models that share a common set of parameters, 
 where Propname is the number of layers in the target architecture. The target model can be interpreted as the expectation of all 
 models. With a layer drop rate set to 0.0, for instance for 
 a Propname Propname model, all submodels are equiprobable, and then it 
 amounts to averaging the weights of 0 
 000 models."," In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider 
 a single target model to be trained, and we instantiate two 
 submodels on the fly, simply by layerwise dropout. This gives us two neural networks through which we can 
 backpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel 
 serves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the 
 submodels. Our approach is illustrated in Figure 0: the parameter controls the importance of the co training loss 
 compared to the label loss, and our experiments show that 
 it significantly increases the final model accuracy. This co training across different submodels, which we 
 refer to as cosub, can be regarded as a massive co training 
 between 0 
 L models that share a common set of parameters, 
 where Propname is the number of layers in the target architecture. The target model can be interpreted as the expectation of all 
 models. With a layer drop rate set to 0.0, for instance for 
 a Propname Propname model, all submodels are equiprobable, and then it 
 amounts to averaging the weights of 0 
 000 models.", ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN PART VERB VERB ADP DET ADV ADJ NOUN ADP NOUN PUNCT PRON VERB SPACE DET ADJ NOUN NOUN PART AUX VERB PUNCT CCONJ PRON VERB NUM SPACE NOUN ADP DET NOUN PUNCT ADV ADP NOUN NOUN PUNCT PRON VERB PRON NUM ADJ NOUN ADP PRON PRON AUX SPACE VERB ADP DET VERB NOUN ADP DET NOUN NOUN PUNCT ADP NOUN ADP DET ADJ NOUN NOUN PUNCT DET NOUN SPACE VERB ADP DET NOUN ADP DET ADJ PUNCT PRON VERB DET ADJ NOUN NOUN VERB DET NOUN ADP DET SPACE NOUN PUNCT PRON NOUN AUX VERB ADP NOUN NUM PUNCT DET NOUN VERB DET NOUN ADP DET NOUN NOUN NOUN SPACE VERB ADP DET NOUN NOUN PUNCT CCONJ PRON NOUN VERB SCONJ SPACE PRON ADV VERB DET ADJ NOUN NOUN PUNCT DET NOUN VERB ADP ADJ NOUN PUNCT PRON PRON SPACE VERB ADP ADP NOUN PUNCT AUX AUX VERB ADP DET ADJ NOUN NOUN SPACE ADP NUM SPACE NOUN NOUN PRON VERB DET ADJ NOUN ADP NOUN PUNCT SPACE SCONJ PROPN AUX DET NOUN ADP NOUN ADP DET NOUN NOUN PUNCT DET NOUN NOUN AUX AUX VERB ADP DET NOUN ADP DET SPACE NOUN PUNCT ADP DET NOUN NOUN NOUN VERB ADP NUM PUNCT ADP NOUN ADP SPACE DET PROPN PROPN NOUN PUNCT DET NOUN AUX ADJ PUNCT CCONJ ADV PRON SPACE VERB ADP VERB DET NOUN ADP NUM SPACE NUM NOUN PUNCT,0.5353982300884956,28.25,4.486725663716814
164,78,Hugo Touvron,"[' Although the fundamental ideas of deep trainable neural\nnetworks have been around for decades, only recently have\nbarriers been removed to allow breakthroughs in successfully training deep neural architectures in practice. Many of\nthese barriers are related to non-convex optimization in one\nway or another, which is central to the success of modern\nneural networks. The optimization challenges have been\naddressed from multiple angles in the literature. First, modern architectures are designed to facilitate the optimization\nof very deep networks. An exceptionally successful design\nprinciple is using residual connections [24, 25]. Although\nthis does not change the expressiveness of the functions that\nthe network can implement, the improved gradient flow alleviates, to some extent, the difficulties of optimizing very\ndeep networks. Another key element to the optimization is\nthe importance of data, revealed by the step-change in visual recognition performance resulting from the ImageNet\ndataset [11], and the popularization of transfer learning with\npre-training on large datasets [39, 58]. However, even when (pre-)trained with millions of images, recent deep networks with millions if not billions\nof parameters, are still heavily overparameterized. Traditional regularization like weight decay, dropout [46], or label smoothing [47] are limited in their ability to address\nthis issue.', 'Data-augmentation strategies, including those\nmixing different images like Mixup [61] and CutMix [60],\nhave proven to provide a complementary data-driven form\nof regularization. More recently, multiple works propose\nto resort to self-supervised pre-training. These approaches\nrely on a proxy objective that generally provides more supervision signal than the one available from labels. For instance, recently there has been renewed interest in (masked)\nauto-encoders [5, 22, 16], which were popular in the early\ndeep learning literature [7, 19, 27]. Similarly, contrastive\napproaches [23, 9] provide a richer supervision less prone to\na supervision collapse [12]. Overall, self-supervised learning makes it possible to learn larger models with less data,\npossibly reducing the need of a pre-training stage [15]. Distillation is a complementary approach to improve optimization. Distillation techniques were originally developed to transfer knowledge from a teacher model to a student model [4, 28], allowing the student to improve over\nlearning from the data directly. In contrast to traditional\ndistillation, co-distillation does not require pre-training a\n(strong) teacher. Instead, a pool of models supervise each\nother. Practically, it faces several limitations, including the\ndifficulty of jointly training more than two students for complexity reasons, as it involves duplicating the weights.', 'In this paper, we propose a practical way to enable cotraining for a very large number of students. We consider\na single target model to be trained, and we instantiate two\nsubmodels on-the-fly, simply by layerwise dropout [31, 20]. This gives us two neural networks through which we can\nbackpropagate to the shared parameters of the target model. In addition to the regular training loss, each submodel\nserves as a teacher to the other, which provides an additional supervision signal ensuring the consistency across the\nsubmodels. Our approach is illustrated in Figure 1: the parameter λ controls the importance of the co-training loss\ncompared to the label loss, and our experiments show that\nit significantly increases the final model accuracy. This co-training across different submodels, which we\nrefer to as cosub, can be regarded as a massive co-training\nbetween 2\nL models that share a common set of parameters,\nwhere L is the number of layers in the target architecture. The target model can be interpreted as the expectation of all\nmodels. With a layer drop-rate set to 0.5, for instance for\na ViT-H model, all submodels are equiprobable, and then it\namounts to averaging the weights of 2\n2×32 models.', 'Our contributions can be summarized as follows:\n• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and\nfine-tuning it at resolution 448, we obtain 87.4% top-1\naccuracy on Imagenet-val. • We provide an efficient implementation to subsample\nmodels on the fly. It is a simple yet effective variation\nof stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models\nby themselves even with significant trimming, similar\nto LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures\n(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from\nscratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the\nDeiT repository.']",intro_chunked,"Our contributions can be summarized as follows:
• We introduce a novel training approach for deep neural networks: We co-train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre-training ViT-B on Imagenet-21k and
fine-tuning it at resolution 448, we obtain 87.4% top-1
accuracy on Imagenet-val. • We provide an efficient implementation to subsample
models on the fly. It is a simple yet effective variation
of stochastic depth [31] to drop residual blocks. • We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models
by themselves even with significant trimming, similar
to LayerDrop [20] in natural language processing. • We validate our approach on multiple architectures
(like ViT, ResNet, RegNet, PiT, XCiT, Swin, ConvNext), both for image classification –trained from
scratch or with transfer–, and semantic segmentation. • We will share models/code for reproducibility in the
DeiT repository.",35.94666666666669,30.655460127574653,164,0.7339381575584412," Our contributions can be summarized as follows: We introduce a novel training approach for deep neural networks: We co train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre training Propname Propname on Propname 00k and 
 fine tuning it at resolution 000, we obtain 00.0 top 0 
 accuracy on Propname val. We provide an efficient implementation to subsample 
 models on the fly. It is a simple yet effective variation 
 of stochastic depth to drop residual blocks. We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models 
 by themselves even with significant trimming, similar 
 to Propname in natural language processing. We validate our approach on multiple architectures 
, both for image classification trained from 
 scratch or with transfer, and semantic segmentation. We will share Propname for reproducibility in the 
 DeiT repository."," Our contributions can be summarized as follows: We introduce a novel training approach for deep neural networks: We co train submodels. This significantly improves the training of most models, establishing the new state of the art in multiple cases. For instance, after pre training Propname Propname on Propname 00k and 
 fine tuning it at resolution 000, we obtain 00.0 top 0 
 accuracy on Propname val. We provide an efficient implementation to subsample 
 models on the fly. It is a simple yet effective variation 
 of stochastic depth to drop residual blocks. We provide multiple analyses and ablations. Noticeably, we show that our submodels are effective models 
 by themselves even with significant trimming, similar 
 to Propname in natural language processing. We validate our approach on multiple architectures 
, both for image classification trained from 
 scratch or with transfer, and semantic segmentation. We will share Propname for reproducibility in the 
 DeiT repository.", PRON NOUN AUX AUX VERB SCONJ VERB PUNCT PRON VERB DET ADJ NOUN NOUN ADP ADJ ADJ NOUN PUNCT PRON VERB NOUN NOUN PUNCT PRON ADV VERB DET NOUN ADP ADJ NOUN PUNCT VERB DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT SCONJ NOUN VERB PROPN PROPN ADP PROPN NOUN CCONJ SPACE ADJ VERB PRON ADP NOUN NUM PUNCT PRON VERB NUM ADJ NUM SPACE NOUN ADP PROPN NOUN PUNCT PRON VERB DET ADJ NOUN ADP ADJ SPACE NOUN ADP DET NOUN PUNCT PRON AUX DET ADJ ADV ADJ NOUN SPACE ADP ADJ NOUN PART VERB ADJ NOUN PUNCT PRON VERB ADJ NOUN CCONJ NOUN PUNCT ADV PUNCT PRON VERB SCONJ PRON NOUN AUX ADJ NOUN SPACE ADP PRON ADV ADP ADJ NOUN PUNCT ADJ SPACE ADP PROPN ADP ADJ NOUN NOUN PUNCT PRON VERB PRON NOUN ADP ADJ NOUN SPACE PUNCT CCONJ ADP NOUN NOUN VERB ADP SPACE NOUN CCONJ ADP NOUN PUNCT CCONJ ADJ NOUN PUNCT PRON AUX VERB PROPN ADP NOUN ADP DET SPACE NOUN NOUN PUNCT,0.6626506024096386,18.444444444444443,4.975903614457831
165,79,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked," After their vast success in NLP, transformers models [55] and their derivatives
are increasingly popular in computer vision. They are increasingly used in image
classification [13], detection & segmentation [3], video analysis, etc. In particular,
the vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative
to convolutional architectures. This supports the adoption of transformers as a
general architecture able to learn convolutions as well as longer range operations
through the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,
41] implicitly offer built-in translation invariance. As a result their training does
not have to learn this prior. It is therefore not surprising that hybrid architectures
that include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,
transformers have to learn about the structure of images while optimizing the
model such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks
in the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently
train vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.",35.4184210526316,30.655460127574653,165,0.31780415773391724," After their vast success in Propname, transformers models and their derivatives 
 are increasingly popular in computer vision. They are increasingly used in image 
 classification, detection segmentation, video analysis, etc . In particular, 
 the vision transformers of Propname Propname Propname. are a reasonable alternative 
 to convolutional architectures. This supports the adoption of transformers as a 
 general architecture able to learn convolutions as well as longer range operations 
 through the attention process. In contrast, convolutional networks 00, 00, 00, 
 00 implicitly offer built in translation invariance. As a result their training does 
 not have to learn this prior. It is therefore not surprising that hybrid architectures 
 that include convolution converge faster than vanilla transformers. Because they incorporate as priors only the co localisation of pixels in patches, 
 transformers have to learn about the structure of images while optimizing the 
 model such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks 
 in the case of self supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently 
 train vision transformers, and in particular on a midsize dataset like ImageNet0k. Since the work of Propname Propname Propname."," After their vast success in Propname, transformers models and their derivatives 
 are increasingly popular in computer vision. They are increasingly used in image 
 classification, detection segmentation, video analysis, etc . In particular, 
 the vision transformers of Propname Propname Propname. are a reasonable alternative 
 to convolutional architectures. This supports the adoption of transformers as a 
 general architecture able to learn convolutions as well as longer range operations 
 through the attention process. In contrast, convolutional networks 00, 00, 00, 
 00 implicitly offer built in translation invariance. As a result their training does 
 not have to learn this prior. It is therefore not surprising that hybrid architectures 
 that include convolution converge faster than vanilla transformers. Because they incorporate as priors only the co localisation of pixels in patches, 
 transformers have to learn about the structure of images while optimizing the 
 model such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks 
 in the case of self supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently 
 train vision transformers, and in particular on a midsize dataset like ImageNet0k. Since the work of Propname Propname Propname.", ADP PRON ADJ NOUN ADP PROPN PUNCT NOUN NOUN CCONJ PRON NOUN SPACE AUX ADV ADJ ADP NOUN NOUN PUNCT PRON AUX ADV VERB ADP NOUN SPACE NOUN PUNCT NOUN NOUN PUNCT NOUN NOUN PUNCT X X ADP ADJ PUNCT SPACE DET NOUN NOUN ADP PROPN PROPN PROPN PUNCT AUX DET ADJ NOUN SPACE ADP ADJ NOUN PUNCT PRON VERB DET NOUN ADP NOUN ADP DET SPACE ADJ NOUN ADJ PART VERB NOUN ADV ADV ADP ADJ NOUN NOUN SPACE ADP DET NOUN NOUN PUNCT ADP NOUN PUNCT ADJ NOUN NUM PUNCT NUM PUNCT NUM PUNCT SPACE NUM ADV VERB VERB ADP NOUN NOUN PUNCT ADP DET NOUN PRON NOUN AUX SPACE PART VERB PART VERB PRON ADV PUNCT PRON AUX ADV PART ADJ SCONJ NOUN NOUN SPACE PRON VERB NOUN NOUN ADV ADP NOUN NOUN PUNCT SCONJ PRON VERB ADP NOUN ADV DET NOUN NOUN ADP NOUN ADP NOUN PUNCT SPACE NOUN VERB PART VERB ADP DET NOUN ADP NOUN SCONJ VERB DET SPACE NOUN ADJ SCONJ PRON VERB DET NOUN ADP DET NOUN ADP VERB DET VERB NOUN PUNCT PRON AUX AUX CCONJ VERB NOUN ADP DET ADJ NOUN PUNCT CCONJ ADJ ADJ NOUN SPACE ADP DET NOUN ADP NOUN VERB NOUN PUNCT ADV PUNCT SCONJ PRON ADJ NOUN PUNCT PRON AUX AUX ADV ADJ NOUN ADP NOUN NOUN VERB SCONJ PART ADV SPACE NOUN NOUN NOUN PUNCT CCONJ ADP ADJ ADP DET ADJ NOUN ADP NOUN PUNCT SCONJ DET NOUN ADP PROPN PROPN PROPN PUNCT,0.5787234042553191,19.583333333333332,5.153191489361702
166,80,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"[13], the training procedures are mostly
variants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions
and a pyramid structure. These new designs, while being particularly effective
for some tasks, are less general. One difficult question to address is whether the
improved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training
have raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer
architecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as
transfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like
BeiT [2] is due to the training, e.g.",28.726890168654876,30.655460127574653,166,0.27239999175071716,", the training procedures are mostly 
 variants from the proposal of Propname Propname Propname. and Propname Propname Propname.. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re incorporating convolutions 
 and a pyramid structure. These new designs, while being particularly effective 
 for some tasks, are less general. One difficult question to address is whether the 
 improved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with Propname. Recently, self supervised approaches inspired by the popular Propname pre training 
 have raised hopes for a Propname moment in computer vision. There are some analogies between the fields of Propname and computer vision, starting with the transformer 
 architecture itself. However these fields are not identical in every way: The modalities processed are of different nature. Computer vision offer large annotated databases like Propname, and fully supervised pretraining on Propname is effective for handling different downstream tasks such as 
 transfer learning or semantic segmentation. Without further work on fully supervised approaches on Propname it is difficult to conclude if the intriguing performance of self supervised approaches like 
 Propname is due to the training, Propname",", the training procedures are mostly 
 variants from the proposal of Propname Propname Propname. and Propname Propname Propname.. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re incorporating convolutions 
 and a pyramid structure. These new designs, while being particularly effective 
 for some tasks, are less general. One difficult question to address is whether the 
 improved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with Propname. Recently, self supervised approaches inspired by the popular Propname pre training 
 have raised hopes for a Propname moment in computer vision. There are some analogies between the fields of Propname and computer vision, starting with the transformer 
 architecture itself. However these fields are not identical in every way: The modalities processed are of different nature. Computer vision offer large annotated databases like Propname, and fully supervised pretraining on Propname is effective for handling different downstream tasks such as 
 transfer learning or semantic segmentation. Without further work on fully supervised approaches on Propname it is difficult to conclude if the intriguing performance of self supervised approaches like 
 Propname is due to the training, Propname", PUNCT DET NOUN NOUN AUX ADV SPACE NOUN ADP DET NOUN ADP PROPN PROPN PROPN PUNCT CCONJ PROPN PROPN PROPN PUNCT PUNCT ADP NOUN PUNCT ADJ NOUN AUX VERB NOUN NOUN ADP VERB NOUN PUNCT ADV ADJ NOUN PUNCT CCONJ NOUN NOUN ADP VERB NOUN SPACE CCONJ DET NOUN NOUN PUNCT DET ADJ NOUN PUNCT SCONJ AUX ADV ADJ SPACE ADP DET NOUN PUNCT AUX ADV ADJ PUNCT NUM ADJ NOUN PART VERB AUX SCONJ DET SPACE ADJ NOUN AUX ADJ ADP DET ADJ ADJ NOUN PUNCT CCONJ SCONJ PRON VERB DET NOUN SCONJ VERB PRON AUX DET NOUN ADP NOUN ADP PROPN PUNCT ADV PUNCT NOUN VERB NOUN VERB ADP DET ADJ PROPN VERB NOUN SPACE AUX VERB NOUN ADP DET PROPN NOUN ADP NOUN NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN ADP PROPN CCONJ NOUN NOUN PUNCT VERB ADP DET NOUN SPACE NOUN PRON PUNCT ADV DET NOUN AUX PART ADJ ADP DET NOUN PUNCT DET NOUN VERB AUX ADP ADJ NOUN PUNCT NOUN NOUN VERB ADJ ADJ NOUN ADP PROPN PUNCT CCONJ ADV VERB NOUN ADP PROPN AUX ADJ ADP VERB ADJ ADJ NOUN ADJ ADP SPACE NOUN NOUN CCONJ ADJ NOUN PUNCT ADP ADJ NOUN ADP ADV ADJ NOUN ADP PROPN PRON AUX ADJ PART VERB SCONJ DET ADJ NOUN ADP NOUN VERB NOUN ADP SPACE PROPN AUX ADJ ADP DET NOUN PUNCT PROPN,0.5610859728506787,22.1,5.493212669683258
167,81,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general
implicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art
on fully supervised and self-supervised approaches, with new insights regarding
data-augmentation. We propose new training recipes for vision transformers on
ImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In
2
particular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the
training of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for
self-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train
vision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping
when pre-training on a larger set like ImageNet-21k.",35.15045977011496,30.655460127574653,167,0.7049748301506042," data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general 
 implicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla Propname architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of Propname like pre training. Our work builds upon the recent state of the art 
 on fully supervised and self supervised approaches, with new insights regarding 
 data augmentation. We propose new training recipes for vision transformers on 
 Propname 0k and Propname 00k. The main ingredients are as follows: We build upon the work of Propname Propname Propname. introduced for Propname. In 
 0 
 particular we adopt a binary cross Propname loss for Propname only training. We adapt this method by including ingredients that significantly improve the 
 training of large ViT, namely stochastic depth and Propname.0 Propname: is a simple data augmentation inspired by that employed for 
 self supervised learning. Surprisingly, with Propname we observe that it works better than the usual automaticlearned data augmentation employed to train 
 vision transformers like Propname. Propname Propname Propname is more effective than Propname Propname Propname 
 when pre training on a larger set like Propname 00k."," data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general 
 implicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla Propname architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of Propname like pre training. Our work builds upon the recent state of the art 
 on fully supervised and self supervised approaches, with new insights regarding 
 data augmentation. We propose new training recipes for vision transformers on 
 Propname 0k and Propname 00k. The main ingredients are as follows: We build upon the work of Propname Propname Propname. introduced for Propname. In 
 0 
 particular we adopt a binary cross Propname loss for Propname only training. We adapt this method by including ingredients that significantly improve the 
 training of large ViT, namely stochastic depth and Propname.0 Propname: is a simple data augmentation inspired by that employed for 
 self supervised learning. Surprisingly, with Propname we observe that it works better than the usual automaticlearned data augmentation employed to train 
 vision transformers like Propname. Propname Propname Propname is more effective than Propname Propname Propname 
 when pre training on a larger set like Propname 00k.", NOUN NOUN PUNCT NOUN PUNCT NOUN PUNCT CCONJ ADP DET ADJ NOUN PRON AUX ADJ ADP VERB ADJ ADJ SPACE ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON AUX PART VERB PART VERB DET ADJ NOUN PUNCT CCONJ PRON VERB PART VERB DET NOUN ADP VERB DET NOUN NOUN ADP NOUN PROPN VERB PUNCT PRON VERB PART VERB ADP DET ADJ NOUN ADP SCONJ PART ADV VERB DET NOUN ADP NOUN CCONJ ADP DET NOUN ADP PROPN ADP ADJ NOUN PUNCT PRON NOUN VERB SCONJ DET ADJ NOUN ADP DET NOUN SPACE ADP ADV ADJ CCONJ NOUN VERB NOUN PUNCT ADP ADJ NOUN VERB SPACE NOUN NOUN PUNCT PRON VERB ADJ NOUN NOUN ADP NOUN NOUN ADP SPACE PROPN NOUN CCONJ PROPN NOUN PUNCT DET ADJ NOUN AUX SCONJ VERB PUNCT PRON VERB SCONJ DET NOUN ADP PROPN PROPN PROPN PUNCT VERB ADP PROPN PUNCT ADP SPACE NUM SPACE ADJ PRON VERB DET ADJ NOUN PROPN NOUN ADP PROPN ADV NOUN PUNCT PRON VERB DET NOUN ADP VERB NOUN PRON ADV VERB DET SPACE NOUN ADP ADJ NOUN PUNCT ADV ADJ NOUN CCONJ PROPN PUNCT PUNCT PROPN PUNCT AUX DET ADJ NOUN NOUN VERB ADP PRON VERB ADP SPACE NOUN VERB NOUN PUNCT ADV PUNCT ADP PROPN PRON VERB SCONJ PRON VERB ADV ADP DET ADJ ADJ NOUN NOUN VERB AUX VERB SPACE NOUN NOUN ADP PROPN PUNCT PROPN PROPN PROPN AUX ADV ADJ ADP PROPN PROPN PROPN SPACE SCONJ ADJ NOUN ADP DET ADJ NOUN ADP PROPN NOUN PUNCT,0.5042016806722689,21.636363636363637,5.117647058823529
168,82,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it
also has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k
than when pre-training at resolution 224 × 224 (256 tokens). This is also less
demanding at pre-training time, as there are 70% fewer tokens. From this
perspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making
another step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et
al. [48]. As a result, we obtain a competitive performance in image classification
and segmentation, even when compared to recent popular architectures such as
SwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below
we point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with
supervised training procedure reported in the literature at resolution 224×224.",43.21052631578948,30.655460127574653,168,0.5495153665542603," A lower resolution at training time. This choice reduces the train test discrepancy but has not been much exploited with ViT. We observe that it 
 also has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 000 000, a ViT Propname pre trained at resolution 000 000 achieves a better performance on Propname 0k 
 than when pre training at resolution 000 000. This is also less 
 demanding at pre training time, as there are 00 fewer tokens. From this 
 perspective it offers similar scaling properties as mask autoencoders. Our new training strategies do not saturate with the largest models, making 
 another step beyond the Propname Propname Propname Propname by Propname Propname 
 Propname.. As a result, we obtain a competitive performance in image classification 
 and segmentation, even when compared to recent popular architectures such as 
 Propname or modern convnet architectures like Propname. Below 
 we point out a few interesting outcomes. We leverage models with more capacity even on midsize datasets. For instance we reach 00.0 in top 0 accuracy when training a ViT H on ImageNet0k only, which is an improvement of 0.0 over the best ViT H with 
 supervised training procedure reported in the literature at resolution 000000."," A lower resolution at training time. This choice reduces the train test discrepancy but has not been much exploited with ViT. We observe that it 
 also has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 000 000, a ViT Propname pre trained at resolution 000 000 achieves a better performance on Propname 0k 
 than when pre training at resolution 000 000. This is also less 
 demanding at pre training time, as there are 00 fewer tokens. From this 
 perspective it offers similar scaling properties as mask autoencoders. Our new training strategies do not saturate with the largest models, making 
 another step beyond the Propname Propname Propname Propname by Propname Propname 
 Propname.. As a result, we obtain a competitive performance in image classification 
 and segmentation, even when compared to recent popular architectures such as 
 Propname or modern convnet architectures like Propname. Below 
 we point out a few interesting outcomes. We leverage models with more capacity even on midsize datasets. For instance we reach 00.0 in top 0 accuracy when training a ViT H on ImageNet0k only, which is an improvement of 0.0 over the best ViT H with 
 supervised training procedure reported in the literature at resolution 000000.", DET ADJ NOUN ADP NOUN NOUN PUNCT DET NOUN VERB DET NOUN NOUN NOUN CCONJ AUX PART AUX ADV VERB ADP NOUN PRON VERB SCONJ PRON SPACE ADV VERB DET VERB NOUN ADP DET ADJ NOUN ADP VERB VERB PUNCT ADP NOUN PUNCT ADP DET NOUN NOUN ADP NUM NUM PUNCT DET NOUN PROPN VERB VERB ADP NOUN NUM NUM VERB DET ADJ NOUN ADP PROPN NOUN SPACE ADP SCONJ ADJ NOUN ADP NOUN NUM NUM PUNCT PRON AUX ADV ADV SPACE VERB ADP ADJ NOUN NOUN PUNCT SCONJ PRON VERB NUM ADJ NOUN PUNCT ADP DET SPACE NOUN PRON VERB ADJ NOUN NOUN ADP NOUN NOUN PUNCT PRON ADJ NOUN NOUN AUX PART VERB ADP DET ADJ NOUN PUNCT VERB SPACE DET NOUN ADP DET PROPN PROPN PROPN PROPN ADP PROPN PROPN SPACE PROPN PUNCT PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP NOUN NOUN SPACE CCONJ NOUN PUNCT ADV SCONJ VERB ADP ADJ ADJ NOUN ADJ ADP SPACE PROPN CCONJ ADJ NOUN NOUN ADP PROPN PUNCT ADP SPACE PRON VERB ADP DET ADJ ADJ NOUN PUNCT PRON VERB NOUN ADP ADJ NOUN ADV ADP ADJ NOUN PUNCT ADP NOUN PRON VERB NUM ADP ADJ NUM NOUN SCONJ VERB DET NOUN NOUN ADP NOUN ADV PUNCT PRON AUX DET NOUN ADP NUM ADP DET ADJ NOUN NOUN ADP SPACE ADJ NOUN NOUN VERB ADP DET NOUN ADP NOUN NUM PUNCT,0.5848214285714286,22.4,4.8125
169,83,Hugo Touvron,"[' After their vast success in NLP, transformers models [55] and their derivatives\nare increasingly popular in computer vision. They are increasingly used in image\nclassification [13], detection & segmentation [3], video analysis, etc. In particular,\nthe vision transformers (ViT) of Dosovistky et al. [13] are a reasonable alternative\nto convolutional architectures. This supports the adoption of transformers as a\ngeneral architecture able to learn convolutions as well as longer range operations\nthrough the attention process [5, 8]. In contrast, convolutional networks [20, 27, 29,\n41] implicitly offer built-in translation invariance. As a result their training does\nnot have to learn this prior. It is therefore not surprising that hybrid architectures\nthat include convolution converge faster than vanilla transformers [18]. Because they incorporate as priors only the co-localisation of pixels in patches,\ntransformers have to learn about the structure of images while optimizing the\nmodel such that it processes the input with the objective of solving a given task. This can be either reproducing labels in the supervised case, or other proxy tasks\nin the case of self-supervised approaches. Nevertheless, despite their huge success, there has been only few works in computer vision studying how to efficiently\ntrain vision transformers, and in particular on a midsize dataset like ImageNet1k. Since the work of Dosovistky et al.', '[13], the training procedures are mostly\nvariants from the proposal of Touvron et al. [48] and Steiner et al. [42]. In contrast, multiple works have proposed alternative architectures by introducing pooling, more efficient attention, or hybrid architectures re-incorporating convolutions\nand a pyramid structure. These new designs, while being particularly effective\nfor some tasks, are less general. One difficult question to address is whether the\nimproved performance is due to a specific architectural design, or because it facilitates the optimization as suggested it is the case for convolutions with ViTs [60]. Recently, self-supervised approaches inspired by the popular BerT pre-training\nhave raised hopes for a BerT moment in computer vision. There are some analogies between the fields of NLP and computer vision, starting with the transformer\narchitecture itself. However these fields are not identical in every way: The modalities processed are of different nature (continuous versus discrete). Computer vision offer large annotated databases like ImageNet [40], and fully supervised pretraining on ImageNet is effective for handling different downstream tasks such as\ntransfer learning [37] or semantic segmentation. Without further work on fully supervised approaches on ImageNet it is difficult to conclude if the intriguing performance of self-supervised approaches like\nBeiT [2] is due to the training, e.g.', 'data augmentation, regularization, optimization, or to an underlying mechanism that is capable of learning more general\nimplicit representations. In this paper, we do not pretend to answer this difficult question, but we want to feed this debate by renewing the training procedure for vanilla ViT architectures. We hope to contribute to a better understanding on how to fully exploit the potential of transformers and of the importance of BerT-like pre-training. Our work builds upon the recent state of the art\non fully supervised and self-supervised approaches, with new insights regarding\ndata-augmentation. We propose new training recipes for vision transformers on\nImageNet-1k and ImageNet-21k. The main ingredients are as follows: • We build upon the work of Wightman et al. [57] introduced for ResNet50. In\n2\nparticular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the\ntraining of large ViT [51], namely stochastic depth [24] and LayerScale [51]. • 3-Augment: is a simple data augmentation inspired by that employed for\nself-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train\nvision transformers like RandAugment [6]. • Simple Random Cropping is more effective than Random Resize Cropping\nwhen pre-training on a larger set like ImageNet-21k.', '• A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it\nalso has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 × 224, a ViT-H pre-trained at resolution 126 × 126 (81 tokens) achieves a better performance on ImageNet-1k\nthan when pre-training at resolution 224 × 224 (256 tokens). This is also less\ndemanding at pre-training time, as there are 70% fewer tokens. From this\nperspective it offers similar scaling properties as mask-autoencoders [19]. Our “new” training strategies do not saturate with the largest models, making\nanother step beyond the Data-Efficient Image Transformer (DeiT) by Touvron et\nal. [48]. As a result, we obtain a competitive performance in image classification\nand segmentation, even when compared to recent popular architectures such as\nSwinTransformers [31] or modern convnet architectures like ConvNext [32]. Below\nwe point out a few interesting outcomes. • We leverage models with more capacity even on midsize datasets. For instance we reach 85.2% in top-1 accuracy when training a ViT-H on ImageNet1k only, which is an improvement of +5.1% over the best ViT-H with\nsupervised training procedure reported in the literature at resolution 224×224.', '• Our training procedure for ImageNet-1k allow us to train a billion-parameter\nViT-H (52 layers) without any hyper-parameter adaptation, just using the\nsame stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,\ni.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of\nGPUs required and the training time for ViT-H, making it effectively possible\nto train such models without a reduced amount of resources. This is thanks\nto our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with\nBerT-like self-supervised approaches [2, 19] with their default setting and\nwhen using the same level of annotations and less epochs, both for the tasks\nof image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance\ntrade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better\nto another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.']",intro_chunked,"• Our training procedure for ImageNet-1k allow us to train a billion-parameter
ViT-H (52 layers) without any hyper-parameter adaptation, just using the
same stochastic depth drop-rate as for the ViT-H. It attains 84.9% at 224×224,
i.e., +0.2% higher than the corresponding ViT-H trained in the same setting. • Without sacrificing performance, we divide by more than 2 the number of
GPUs required and the training time for ViT-H, making it effectively possible
to train such models without a reduced amount of resources. This is thanks
to our pre-training at lower resolution, which reduces the peak memory. • For ViT-B and Vit-L models, our supervised training approach is on par with
BerT-like self-supervised approaches [2, 19] with their default setting and
when using the same level of annotations and less epochs, both for the tasks
of image classification and of semantic segmentation. • With this improved training procedure, a vanilla ViT closes the gap with recent state-of-the art architectures, often offering better compute/performance
trade-offs. Our models are also comparatively better on the additional test set ImageNet-V2 [39], which indicates that our trained models generalize better
to another validation set than most prior works. • An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target dataset/task.",36.56178604224061,30.655460127574653,169,0.6497416496276855," Our training procedure for Propname 0k allow us to train a billion parameter 
 ViT Propname without any hyper parameter adaptation, just using the 
 same stochastic depth drop rate as for the ViT Propname It attains 00.0 at 000000, 
 Propname, 0.0 higher than the corresponding ViT Propname trained in the same setting. Without sacrificing performance, we divide by more than 0 the number of 
 GPUs required and the training time for Propname Propname, making it effectively possible 
 to train such models without a reduced amount of resources. This is thanks 
 to our pre training at lower resolution, which reduces the peak memory. For ViT Propname and Propname Propname models, our supervised training approach is on par with 
 Propname like self supervised approaches with their default setting and 
 when using the same level of annotations and less epochs, both for the tasks 
 of image classification and of semantic segmentation. With this improved training procedure, a vanilla ViT closes the gap with recent state of the art architectures, often offering better computeperformance 
 trade offs. Our models are also comparatively better on the additional test set Propname Propname, which indicates that our trained models generalize better 
 to another validation set than most prior works. An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target datasettask."," Our training procedure for Propname 0k allow us to train a billion parameter 
 ViT Propname without any hyper parameter adaptation, just using the 
 same stochastic depth drop rate as for the ViT Propname It attains 00.0 at 000000, 
 Propname, 0.0 higher than the corresponding ViT Propname trained in the same setting. Without sacrificing performance, we divide by more than 0 the number of 
 GPUs required and the training time for Propname Propname, making it effectively possible 
 to train such models without a reduced amount of resources. This is thanks 
 to our pre training at lower resolution, which reduces the peak memory. For ViT Propname and Propname Propname models, our supervised training approach is on par with 
 Propname like self supervised approaches with their default setting and 
 when using the same level of annotations and less epochs, both for the tasks 
 of image classification and of semantic segmentation. With this improved training procedure, a vanilla ViT closes the gap with recent state of the art architectures, often offering better computeperformance 
 trade offs. Our models are also comparatively better on the additional test set Propname Propname, which indicates that our trained models generalize better 
 to another validation set than most prior works. An ablation on the effect of the crop ratio employed in transfer learning classification tasks. We observe that it has a noticeable impact on the performance but that the best value depends a lot on the target datasettask.", PRON NOUN NOUN ADP PROPN NOUN VERB PRON PART VERB DET NUM NOUN SPACE NOUN PROPN ADP DET ADJ NOUN NOUN PUNCT ADV VERB DET SPACE ADJ ADJ NOUN NOUN NOUN ADP ADP DET NOUN PROPN PRON VERB NUM ADP NUM PUNCT SPACE PROPN PUNCT NUM ADJ ADP DET VERB NOUN PROPN VERB ADP DET ADJ NOUN PUNCT ADP VERB NOUN PUNCT PRON VERB ADP ADJ ADP NUM DET NOUN ADP SPACE NOUN VERB CCONJ DET NOUN NOUN ADP PROPN PROPN PUNCT VERB PRON ADV ADJ SPACE PART VERB ADJ NOUN ADP DET VERB NOUN ADP NOUN PUNCT PRON AUX NOUN SPACE ADP PRON ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET NOUN NOUN PUNCT ADP NOUN PROPN CCONJ PROPN PROPN NOUN PUNCT PRON ADJ NOUN NOUN AUX ADP NOUN ADP SPACE PROPN SCONJ NOUN VERB NOUN ADP PRON NOUN NOUN CCONJ SPACE SCONJ VERB DET ADJ NOUN ADP NOUN CCONJ ADJ NOUN PUNCT PRON ADP DET NOUN SPACE ADP NOUN NOUN CCONJ ADP ADJ NOUN PUNCT ADP DET ADJ NOUN NOUN PUNCT DET NOUN NOUN VERB DET NOUN ADP ADJ NOUN ADP DET NOUN NOUN PUNCT ADV VERB ADJ NOUN SPACE NOUN NOUN PUNCT PRON NOUN AUX ADV ADV ADV ADP DET ADJ NOUN VERB PROPN PROPN PUNCT PRON VERB SCONJ PRON VERB NOUN VERB ADJ SPACE ADP DET NOUN VERB ADP ADV ADJ NOUN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN NOUN VERB ADP NOUN VERB NOUN NOUN PUNCT PRON VERB SCONJ PRON VERB DET ADJ NOUN ADP DET NOUN CCONJ SCONJ DET ADJ NOUN VERB DET NOUN ADP DET NOUN NOUN PUNCT,0.5719844357976653,32.125,4.9221789883268485
170,84,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked," Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.",30.448763788512736,30.655460127574653,170,0.6114025712013245," Since its introduction the Propname architecture has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field of view in a single layer. Along with other attention based architectures, see eg, transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers. As a result, significant improvements have been observed on different computer vision tasks, ranging from Propname Propname 0 Propname 0 Propname 0 Propname 0 Propname0 Propname 0 Propname 0 Propname 0 Propname detection and segmentation and video analysis to image generation. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers.0. Parallel vision transformers."," Since its introduction the Propname architecture has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field of view in a single layer. Along with other attention based architectures, see eg, transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers. As a result, significant improvements have been observed on different computer vision tasks, ranging from Propname Propname 0 Propname 0 Propname 0 Propname 0 Propname0 Propname 0 Propname 0 Propname 0 Propname detection and segmentation and video analysis to image generation. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers.0. Parallel vision transformers.", SCONJ PRON NOUN DET PROPN NOUN AUX VERB DET ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT VERB ADV ADJ NOUN NOUN PUNCT DET NOUN NOUN AUX DET ADJ NOUN ADP NOUN PART NOUN NOUN NOUN ADP NOUN NOUN PUNCT DET NOUN NOUN AUX VERB ADP ADJ NOUN NOUN PUNCT PRON AUX VERB ADP DET NOUN NOUN NOUN PUNCT ADP DET ADJ NOUN NOUN NOUN PUNCT ADP NOUN ADP NOUN VERB ADP ADJ NOUN PUNCT NOUN VERB ADJ NOUN CCONJ DET ADJ NOUN ADP NOUN ADP DET ADJ NOUN PUNCT ADP ADP ADJ NOUN VERB NOUN PUNCT VERB NOUN PUNCT NOUN AUX ADV ADV VERB DET NOUN ADP NOUN NOUN VERB PUNCT ADJ ADJ NOUN ADP NOUN NOUN ADV VERB NOUN ADP PRON NOUN ADP DET NOUN PUNCT CCONJ AUX ADP ADJ VERB ADP DET ADJ NOUN VERB ADP NOUN PUNCT ADP DET NOUN PUNCT ADJ NOUN AUX AUX VERB ADP ADJ NOUN NOUN NOUN PUNCT VERB ADP PROPN PROPN NUM PROPN NUM PROPN NUM PROPN NUM PROPN PUNCT PROPN NUM PROPN NUM PROPN NUM PROPN NOUN CCONJ NOUN CCONJ NOUN NOUN ADP NOUN NOUN PUNCT SCONJ NOUN NOUN AUX VERB ADP ADJ NOUN PUNCT DET NOUN ADP PRON NOUN CCONJ NOUN NOUN AUX ADV AUX VERB ADP DET ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB NUM NOUN ADP VERB NOUN NOUN PUNCT PUNCT PUNCT ADJ NOUN NOUN PUNCT,0.5526315789473685,25.333333333333332,5.495614035087719
171,85,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked,"Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.",34.7142153130288,30.655460127574653,171,0.4694376587867737," Several works advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with Propname. Let us denote by Propname the multi headed self attention residual block, and by Propname the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 0, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.0 0. Fine tuning attention is all you need. It is common practice to pre train networks before fine tuning them on a target task."," Several works advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with Propname. Let us denote by Propname the multi headed self attention residual block, and by Propname the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 0, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.0 0. Fine tuning attention is all you need. It is common practice to pre train networks before fine tuning them on a target task.", ADJ NOUN VERB DET NOUN NOUN NOUN ADP NOUN VERB ADP ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET ADV ADJ NOUN PART VERB PRON ADP PROPN PUNCT VERB PRON VERB ADP PROPN DET NOUN VERB NOUN NOUN ADJ NOUN PUNCT CCONJ ADP PROPN DET ADJ NOUN NOUN PUNCT VERB ADP DET ADJ NOUN VERB SCONJ VERB PUNCT PRON VERB DET NOUN ADP VERB DET ADJ NOUN ADP NOUN PUNCT PRON AUX AUX VERB ADP DET ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET NOUN ADP DET ADJ NOUN ADP NOUN CCONJ NOUN PUNCT SCONJ AUX ADJ CCONJ NOUN PUNCT DET NOUN VERB ADP ADJ ADJ NOUN PUNCT VERB NOUN CCONJ VERB NOUN VERB ADP DET NOUN PUNCT ADP NOUN NUM PUNCT PRON ADV VERB DET NOUN ADP DET ADJ NOUN PUNCT CCONJ ADP ADJ SCONJ PRON VERB DET NOUN ADP NOUN ADP DET ADJ NOUN PUNCT DET ADJ NOUN VERB DET ADJ NOUN SCONJ ADV ADV PUNCT ADP DET NOUN PUNCT PRON VERB NOUN ADP NOUN VERB ADP DET ADJ NOUN PUNCT VERB DET NOUN ADP NOUN PUNCT PRON VERB NOUN ADP DET NOUN ADP ADJ NOUN NOUN NUM PUNCT ADJ NOUN NOUN AUX PRON PRON VERB PUNCT PRON AUX ADJ NOUN PART VERB NOUN NOUN ADP ADJ VERB PRON ADP DET NOUN NOUN PUNCT,0.6009174311926605,18.166666666666668,4.972477064220183
172,86,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked,"This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.",43.928950103950115,30.655460127574653,172,0.28363239765167236," This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like Propname when the number of images is limited for the target task. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation. In Section 0 we show that, in the case of ViT, it is mostly sufficient to fine tune only the multi head attention layers and freeze the feedforward network layers. This saves compute and reduces the memory peak during training. Importantly this allows the same Propname weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks.0. Propname preprocessing with masked self supervised learning. The first layers of a transformer have a relatively local span, suggesting that they mostly Propname have not found any papers in the literature analyzing the effect of width versus depth for Propname on common GPUs and CPUs.0 have like convolutions."," This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like Propname when the number of images is limited for the target task. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation. In Section 0 we show that, in the case of ViT, it is mostly sufficient to fine tune only the multi head attention layers and freeze the feedforward network layers. This saves compute and reduces the memory peak during training. Importantly this allows the same Propname weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks.0. Propname preprocessing with masked self supervised learning. The first layers of a transformer have a relatively local span, suggesting that they mostly Propname have not found any papers in the literature analyzing the effect of width versus depth for Propname on common GPUs and CPUs.0 have like convolutions.", PRON AUX DET ADJ NOUN VERB NOUN NOUN PUNCT SCONJ NUM VERB DET ADJ ADJ NOUN ADP PROPN SCONJ DET NOUN ADP NOUN AUX VERB ADP DET NOUN NOUN PUNCT DET NOUN AUX DET NUM ADP VERB NOUN PUNCT ADV PRON AUX VERB ADP DET ADJ NOUN ADP DET NOUN VERB ADP NOUN NOUN PUNCT PRON VERB NOUN PUNCT CCONJ ADV PRON VERB DET NOUN ADP NOUN ADP NOUN CCONJ NOUN NOUN PRON VERB ADP NOUN NOUN PUNCT ADP NOUN NUM PRON VERB SCONJ PUNCT ADP DET NOUN ADP NOUN PUNCT PRON AUX ADV ADJ PART ADJ NOUN ADV DET ADJ NOUN NOUN NOUN CCONJ VERB DET NOUN NOUN NOUN PUNCT PRON VERB NOUN CCONJ VERB DET NOUN NOUN ADP NOUN PUNCT ADV PRON VERB DET ADJ PROPN NOUN PUNCT PRON VERB DET NOUN ADP NOUN PUNCT PART AUX VERB ADP ADJ NOUN PUNCT DET NOUN ADP NOUN AUX ADV PART ADJ SCONJ ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADP ADJ NOUN PUNCT DET NOUN ADP NOUN AUX VERB SCONJ VERB NOUN ADP ADJ NOUN NOUN PUNCT PUNCT PUNCT PROPN VERB ADP VERB NOUN ADJ NOUN PUNCT DET ADJ NOUN ADP DET NOUN VERB DET ADV ADJ NOUN PUNCT VERB SCONJ PRON ADV PROPN AUX PART VERB DET NOUN ADP DET NOUN VERB DET NOUN ADP NOUN ADP NOUN ADP PROPN ADP ADJ NOUN CCONJ NOUN PUNCT PUNCT VERB ADP NOUN PUNCT,0.5982532751091703,20.818181818181817,4.934497816593886
173,87,Hugo Touvron,"[' Since its introduction the Transformer architecture [66] has become the dominant architecture in natural language processing tasks, replacing previously popular recurrent architectures. The vision transformer [16] (ViT) is a simple adaptation of transformers to computer vision tasks like image classification: the input image is divided into non-overlapping patches, which are fed to a vanilla transformer architecture, after a linear patch projection layer. In contrast to networks built from convolutional layers, transformers offer parallel processing and a complete field-of-view in a single layer. Along with other attention-based architectures, see e.g. [4, 7], transformers have recently substantially influenced the design of computer vision architectures. Many modern architectures in computer vision directly inherit parts of their design from this work, or are at least inspired by the recent findings resulting from transformers [7, 16, 62]. As a result, significant improvements have been observed on different computer vision tasks, ranging from ob1 MHSA-1 + FFN-1 + MHSA-2 + FFN-2 + MHSA-1 + + MHSA-2 FFN-2 FFN-1 ject detection and segmentation [18] and video analysis [1, 19] to image generation [9, 31]. While vision transformers have led to considerable progress, the optimization of their design and training procedures have only been explored to a limited extent. In this paper, we offer three insights on training vision transformers. 1. Parallel vision transformers.', 'Several works [20, 75] advocate the interest shallower networks for reasons ranging from lower latency to easier optimization. We propose a very simple way to achieve this with ViTs. Let us denote by MHSA the multi-headed self-attention residual block, and by FFN the residual feedforward network. Starting from a sequential architecture depicted as follows, we parallelize the architecture by reorganizing the same blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. In Section 3, we experimentally analyse the performance of this parallel construction, and in particular how it affects the accuracy in comparison to the sequential baseline. The parallel version becomes a compelling option if deep enough. In some cases, we observe improvements in accuracy resulting from an easier optimization. Regarding the latency on GPUs, we observe reductions in the case of small batch sizes.1 2. Fine-tuning attention is all you need. It is common practice to pre-train networks before fine-tuning them on a target task.', 'This is the standard approach underpinning transfer learning, where one leverages a large generic dataset like ImageNet [56] when the number of images is limited for the target task [50, 73]. Another context is the one of changing resolution. Typically one would train at a lower resolution than the one employed at inference time. This saves resources, but additionally it reduces the discrepancy of scale between train and test images that results from data augmentation [65]. In Section 4 we show that, in the case of ViT, it is mostly sufficient to fine-tune only the multi-head attention layers and freeze the feedforward network (FFN) layers. This saves compute and reduces the memory peak during training. Importantly this allows the same FFN weights, which dominate the number of parameters, to be used for multiple tasks. The impact on accuracy is statistically not significant when fine-tuning for different image resolutions. For large models, the impact on accuracy is limited when considering transfer to other classification tasks. 3. Patch preprocessing with masked self-supervised learning. The first layers of a transformer have a relatively local span [11], suggesting that they mostly be1We have not found any papers in the literature analyzing the effect of width versus depth for ViT on common GPUs and CPUs. 2 have like convolutions.', 'Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].']",intro_chunked,"Some recent hybrid architectures [18, 21, 23] preprocess their input images with a convolutional stem, to improve accuracy and training stability [71]. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT [3] or MAE [24]. The convolutions propagate information across patches, impeding the masked prediction task. In Section 5, we propose a simple way to adapt mask-based self-supervised training methods with patch pre-processing, by applying the masking after the patch pre-processing. However, our analysis reveals that existing convolutional stems are not effective when combined with BeiT. To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison [21].",27.95750000000001,30.655460127574653,173,0.30261245369911194," Some recent hybrid architectures preprocess their input images with a convolutional stem, to improve accuracy and training stability. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask based self supervised learning approaches, like Propname or Propname. The convolutions propagate information across patches, impeding the masked prediction task. In Section 0, we propose a simple way to adapt mask based self supervised training methods with Propname pre processing, by applying the masking after the patch pre processing. However, our analysis reveals that existing convolutional stems are not effective when combined with Propname To address this issue, we introduce a hierarchical Propname stem that interleaves Propname layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both Propname self supervised pre training and patch pre processing. Moreover, our Propname stem is also effective for Propname in the supervised case: it is on par with the best convolutional stem of our comparison."," Some recent hybrid architectures preprocess their input images with a convolutional stem, to improve accuracy and training stability. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask based self supervised learning approaches, like Propname or Propname. The convolutions propagate information across patches, impeding the masked prediction task. In Section 0, we propose a simple way to adapt mask based self supervised training methods with Propname pre processing, by applying the masking after the patch pre processing. However, our analysis reveals that existing convolutional stems are not effective when combined with Propname To address this issue, we introduce a hierarchical Propname stem that interleaves Propname layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both Propname self supervised pre training and patch pre processing. Moreover, our Propname stem is also effective for Propname in the supervised case: it is on par with the best convolutional stem of our comparison.", DET ADJ ADJ NOUN VERB PRON NOUN NOUN ADP DET ADJ NOUN PUNCT PART VERB NOUN CCONJ NOUN NOUN PUNCT ADV PUNCT VERB NOUN ADP NOUN AUX DET ADJ PART ADJ ADP DET ADJ CCONJ ADJ NOUN VERB NOUN VERB VERB NOUN PUNCT ADP PROPN CCONJ PROPN PUNCT DET NOUN VERB NOUN ADP NOUN PUNCT VERB DET VERB NOUN NOUN PUNCT ADP NOUN NUM PUNCT PRON VERB DET ADJ NOUN PART VERB NOUN VERB NOUN VERB NOUN NOUN ADP PROPN NOUN NOUN PUNCT ADP VERB DET NOUN ADP DET NOUN NOUN NOUN PUNCT ADV PUNCT PRON NOUN VERB SCONJ VERB ADJ NOUN AUX PART ADJ SCONJ VERB ADP PROPN PART VERB DET NOUN PUNCT PRON VERB DET ADJ PROPN NOUN PRON VERB PROPN NOUN CCONJ VERB NOUN NOUN PUNCT CCONJ VERB DET NOUN ADP NOUN PUNCT PRON NOUN VERB SCONJ DET NOUN AUX ADJ CCONJ ADJ PART VERB DET NOUN ADP DET PROPN NOUN VERB ADJ NOUN CCONJ VERB ADJ NOUN PUNCT ADV PUNCT PRON PROPN NOUN AUX ADV ADJ ADP PROPN ADP DET ADJ NOUN PUNCT PRON AUX ADP NOUN ADP DET ADJ ADJ NOUN ADP PRON NOUN PUNCT,0.5631578947368421,27.142857142857142,5.3
174,88,Hugo Touvron,"[' Computer vision is about understanding how to obtain a high-level representation of images\nand videos. High-level representations are obtained by projecting the image into a vector space\nwith a certain structure that makes it easier to extract the information needed to interpret the\nimage. This allows complex tasks such as recognising concepts in an image or performing action\nrecognition in video. While it is easy for a human to recognise a given concept, it is hard to design\nan algorithm that would do the same. Indeed, when we see a cat, we know quite easily that it is\na cat, but it is almost impossible to describe in an algorithmic way all the steps that lead us to\nthis conclusion. Early approaches were based on handcrafted image representations, i.e manually\ndesigned and relying on expert knowledge. The most emblematic strategy is certainly the Bagof-Words (BoW) method, which encodes and pools local features on visual dictionaries. BoW\nwas the state-of-the-art approach for image classification in the 2000s. Inspired by information\nretrieval [170], the pioneering work [138] introduced a BoW scheme for image representation\nusing a color dictionary, extended to Gabor feature dictionary by [74], and finally popularized\nusing SIFT features (Scale-Invariant Feature Transform [137]) for visual recognition [43, 179]. In\nthe 2010s, we then witnessed the emergence of deep learning methods, which gradually overtook\nall traditional computer vision approaches. The computer vision field includes many tasks such as image classification, detection or segmentation (see Figure 1.1 for an illustration). Today, the gold standard approaches to solve these\ntasks are based on deep learning. This thesis falls within this context. In the following, we detail\nthe basics of deep learning for vision and position our contributions.']",intro_chunked," Computer vision is about understanding how to obtain a high-level representation of images
and videos. High-level representations are obtained by projecting the image into a vector space
with a certain structure that makes it easier to extract the information needed to interpret the
image. This allows complex tasks such as recognising concepts in an image or performing action
recognition in video. While it is easy for a human to recognise a given concept, it is hard to design
an algorithm that would do the same. Indeed, when we see a cat, we know quite easily that it is
a cat, but it is almost impossible to describe in an algorithmic way all the steps that lead us to
this conclusion. Early approaches were based on handcrafted image representations, i.e manually
designed and relying on expert knowledge. The most emblematic strategy is certainly the Bagof-Words (BoW) method, which encodes and pools local features on visual dictionaries. BoW
was the state-of-the-art approach for image classification in the 2000s. Inspired by information
retrieval [170], the pioneering work [138] introduced a BoW scheme for image representation
using a color dictionary, extended to Gabor feature dictionary by [74], and finally popularized
using SIFT features (Scale-Invariant Feature Transform [137]) for visual recognition [43, 179]. In
the 2010s, we then witnessed the emergence of deep learning methods, which gradually overtook
all traditional computer vision approaches. The computer vision field includes many tasks such as image classification, detection or segmentation (see Figure 1.1 for an illustration). Today, the gold standard approaches to solve these
tasks are based on deep learning. This thesis falls within this context. In the following, we detail
the basics of deep learning for vision and position our contributions.",37.46919491525426,30.655460127574653,174,0.5850725173950195," Computer vision is about understanding how to obtain a high level representation of images 
 and videos. High level representations are obtained by projecting the image into a vector space 
 with a certain structure that makes it easier to extract the information needed to interpret the 
 image. This allows complex tasks such as recognising concepts in an image or performing action 
 recognition in video. While it is easy for a human to recognise a given concept, it is hard to design 
 an Propname that would do the same. Indeed, when we see a cat, we know quite easily that it is 
 a cat, but it is almost impossible to describe in an algorithmic way all the steps that lead us to 
 this conclusion. Early approaches were based on handcrafted image representations, i.e manually 
 designed and relying on expert knowledge. The most emblematic strategy is certainly the Propname Words method, which encodes and pools local features on visual dictionaries. BoW 
 was the state of the art approach for image classification in the 0000s. Inspired by information 
 retrieval, the pioneering work introduced a BoW scheme for image representation 
 using a color dictionary, extended to Propname feature dictionary by, and finally popularized 
 using Propname features for visual recognition. In 
 the 0000s, we then witnessed the emergence of deep learning methods, which gradually overtook 
 all traditional computer vision approaches. The computer vision field includes many tasks such as image classification, detection or segmentation. Today, the gold standard approaches to solve these 
 tasks are based on deep learning. This thesis falls within this context. In the following, we detail 
 the basics of deep learning for vision and position our contributions."," Computer vision is about understanding how to obtain a high level representation of images 
 and videos. High level representations are obtained by projecting the image into a vector space 
 with a certain structure that makes it easier to extract the information needed to interpret the 
 image. This allows complex tasks such as recognising concepts in an image or performing action 
 recognition in video. While it is easy for a human to recognise a given concept, it is hard to design 
 an Propname that would do the same. Indeed, when we see a cat, we know quite easily that it is 
 a cat, but it is almost impossible to describe in an algorithmic way all the steps that lead us to 
 this conclusion. Early approaches were based on handcrafted image representations, i.e manually 
 designed and relying on expert knowledge. The most emblematic strategy is certainly the Propname Words method, which encodes and pools local features on visual dictionaries. BoW 
 was the state of the art approach for image classification in the 0000s. Inspired by information 
 retrieval, the pioneering work introduced a BoW scheme for image representation 
 using a color dictionary, extended to Propname feature dictionary by, and finally popularized 
 using Propname features for visual recognition. In 
 the 0000s, we then witnessed the emergence of deep learning methods, which gradually overtook 
 all traditional computer vision approaches. The computer vision field includes many tasks such as image classification, detection or segmentation. Today, the gold standard approaches to solve these 
 tasks are based on deep learning. This thesis falls within this context. In the following, we detail 
 the basics of deep learning for vision and position our contributions.", NOUN NOUN AUX ADP VERB SCONJ PART VERB DET ADJ NOUN NOUN ADP NOUN SPACE CCONJ NOUN PUNCT ADJ NOUN NOUN AUX VERB ADP VERB DET NOUN ADP DET NOUN NOUN SPACE ADP DET ADJ NOUN PRON VERB PRON ADJ PART VERB DET NOUN VERB PART VERB DET SPACE NOUN PUNCT PRON VERB ADJ NOUN ADJ ADP VERB NOUN ADP DET NOUN CCONJ VERB NOUN SPACE NOUN ADP NOUN PUNCT SCONJ PRON AUX ADJ SCONJ DET NOUN PART VERB DET VERB NOUN PUNCT PRON AUX ADJ PART VERB SPACE DET PROPN PRON AUX VERB DET ADJ PUNCT ADV PUNCT SCONJ PRON VERB DET NOUN PUNCT PRON VERB ADV ADV SCONJ PRON AUX SPACE DET NOUN PUNCT CCONJ PRON AUX ADV ADJ PART VERB ADP DET ADJ NOUN DET DET NOUN PRON VERB PRON ADP SPACE DET NOUN PUNCT ADJ NOUN AUX VERB ADP VERB NOUN NOUN PUNCT CCONJ ADV SPACE VERB CCONJ VERB ADP ADJ NOUN PUNCT DET ADV ADJ NOUN AUX ADV DET PROPN NOUN NOUN PUNCT PRON NOUN CCONJ NOUN ADJ NOUN ADP ADJ NOUN PUNCT NOUN SPACE AUX DET NOUN ADP DET NOUN NOUN ADP NOUN NOUN ADP DET NOUN PUNCT VERB ADP NOUN SPACE NOUN PUNCT DET VERB NOUN VERB DET NOUN NOUN ADP NOUN NOUN SPACE VERB DET NOUN NOUN PUNCT VERB ADP PROPN NOUN ADV ADP PUNCT CCONJ ADV VERB SPACE VERB PROPN NOUN ADP ADJ NOUN PUNCT ADP SPACE DET NOUN PUNCT PRON ADV VERB DET NOUN ADP ADJ NOUN NOUN PUNCT PRON ADV VERB SPACE DET ADJ NOUN NOUN NOUN PUNCT DET NOUN NOUN NOUN VERB ADJ NOUN ADJ ADP NOUN NOUN PUNCT NOUN CCONJ NOUN PUNCT NOUN PUNCT DET ADJ NOUN NOUN PART VERB DET SPACE NOUN AUX VERB ADP ADJ NOUN PUNCT DET NOUN VERB ADP DET NOUN PUNCT ADP DET NOUN PUNCT PRON VERB SPACE DET NOUN ADP ADJ NOUN ADP NOUN CCONJ VERB PRON NOUN PUNCT,0.5463576158940397,21.571428571428573,4.837748344370861
175,89,Hugo Touvron,"[' Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches.', 'Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.', 'This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 .']",intro_chunked," Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches.",39.407575757575785,30.655460127574653,175,0.2836793065071106," Vision transformers emerge as an alternative to convolutional neural networks in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so called class Propname. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT Propname ResNet00 Propname Propname attention. We simplify the design of this attention based pooling layer such that it explicitly provides the weights of the different patches."," Vision transformers emerge as an alternative to convolutional neural networks in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so called class Propname. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT Propname ResNet00 Propname Propname attention. We simplify the design of this attention based pooling layer such that it explicitly provides the weights of the different patches.", NOUN NOUN VERB ADP DET NOUN ADP ADJ ADJ NOUN ADP NOUN NOUN PUNCT PRON VERB ADP ADJ NOUN ADP ADJ NOUN PUNCT NUM ADP PRON AUX DET NOUN VERB NOUN PUNCT DET NOUN AUX DET NOUN ADP DET NOUN NOUN VERB ADP DET ADV VERB NOUN PROPN PUNCT DET NOUN VERB ADP DET NOUN ADV ADJ ADP DET NOUN NOUN PUNCT ADV PUNCT DET NOUN ADP DET NOUN NOUN NOUN PUNCT ADV ADP DET ADJ NOUN PUNCT AUX AUX VERB PART VERB NOUN NOUN VERB DET NOUN ADP DET NOUN NOUN CCONJ DET DET NOUN PUNCT ADJ NOUN AUX AUX VERB ADP NOUN NOUN PUNCT PRON VERB DET NOUN ADP PRON NOUN ADP DET VERB NOUN AUX VERB ADP DET NOUN PART VERB PRON NOUN PUNCT ADV DET NOUN VERB ADJ PUNCT VERB DET NOUN VERB DET NOUN ADP ADJ NOUN ADP ADJ ADJ NOUN CCONJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PART VERB ADJ NOUN NOUN PART NOUN PUNCT PRON VERB VERB ADP DET NOUN NOUN PUNCT ADV ADV PRON VERB DET ADJ ADJ NOUN NOUN ADP DET NOUN VERB NOUN PUNCT ADV PUNCT PRON ADP DET NOUN NOUN NOUN VERB PRON NOUN ADP ADJ NOUN PROPN NOUN PUNCT PROPN PROPN NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN VERB VERB NOUN ADJ SCONJ PRON ADV VERB DET NOUN ADP DET ADJ NOUN PUNCT,0.5921052631578947,19.0,5.035087719298246
176,90,Hugo Touvron,"[' Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches.', 'Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.', 'This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 .']",intro_chunked,"Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.",36.765376183358256,30.655460127574653,176,0.464266836643219," Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Propname 0 and 0, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:0000.00000v000 Dec 0000 In Figure 0, we show the attention maps extracted from ViT by using a visualization procedure inspired by Propname Propname Propname.. It involves some post processing as there are multiple layers and heads providing patch weights. Then we show a ResNet 00 augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low resolution attention map with artefacts: We need an architecture producing a higher resolution feature maps in order to better leverage the proposed attention based pooling. For this purpose we introduce a simple patch based convolutional architecture0 that keeps the input resolution constant throughout the network."," Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Propname 0 and 0, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:0000.00000v000 Dec 0000 In Figure 0, we show the attention maps extracted from ViT by using a visualization procedure inspired by Propname Propname Propname.. It involves some post processing as there are multiple layers and heads providing patch weights. Then we show a ResNet 00 augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low resolution attention map with artefacts: We need an architecture producing a higher resolution feature maps in order to better leverage the proposed attention based pooling. For this purpose we introduce a simple patch based convolutional architecture0 that keeps the input resolution constant throughout the network.", VERB ADP NOUN PUNCT ADP PRON DET NOUN AUX VERB ADP ADJ NOUN CCONJ NOUN PUNCT PRON NOUN VERB DET ADJ NOUN ADP NOUN PUNCT CCONJ ADV DET ADJ NOUN PART VERB DET NOUN NOUN PUNCT PRON AUX DET ADJ NOUN ADP DET NOUN ADP DET ADJ NOUN VERB DET NOUN PUNCT DET NOUN VERB DET NOUN PART VERB ADP ADJ NOUN ADV CCONJ ADV PUNCT SCONJ PRON VERB NUM ADJ ADP DET NOUN ADV ADP DET ADJ NOUN PUNCT SCONJ VERB ADP PROPN NUM CCONJ NUM PUNCT ADV PRON VERB DET NOUN NOUN ADP NOUN ADP DET ADJ NOUN PUNCT ADP PRON ADJ NOUN PRON ADV VERB ADP DET ADJ NOUN NOUN PUNCT PRON AUX ADV ADV VERB ADP DET NOUN NOUN PUNCT NOUN PUNCT ADJ NUM ADP NOUN NUM PUNCT PRON VERB DET NOUN NOUN VERB ADP NOUN ADP VERB DET NOUN NOUN VERB ADP PROPN PROPN PROPN PUNCT PUNCT PRON VERB DET NOUN NOUN SCONJ PRON VERB ADJ NOUN CCONJ NOUN VERB NOUN NOUN PUNCT ADV PRON VERB DET NOUN NUM VERB ADP VERB PRON VERB NOUN NOUN PUNCT PRON ADJ NOUN VERB ADP DET ADJ NOUN NOUN NOUN ADP NOUN PUNCT PRON VERB DET NOUN VERB DET ADJ NOUN NOUN NOUN ADP NOUN PART VERB VERB DET VERB NOUN VERB NOUN PUNCT ADP DET NOUN PRON VERB DET ADJ NOUN VERB ADJ NOUN PRON VERB DET NOUN NOUN ADJ ADP DET NOUN PUNCT,0.5914893617021276,29.375,4.8936170212765955
177,91,Hugo Touvron,"[' Vision transformers [18] (ViT) emerge as an alternative to convolutional neural networks (convnets) in computer vision. They differ from traditional convnets in many ways, one of which being the patch based processing. Another difference is the aggregation of the image information based on a so-called “class token”. This element correlates with the patches most related to the classification decision. Therefore, the softmax in the self-attention blocks, especially in the last layers, can be used to produce attention maps showing the interaction between the class token and all the patches. Such maps have been employed for visualization purposes [8, 18]. It gives some hints on which regions of a given image are employed by a model to make its decision. However the interpretability remains loose: producing these maps involves some fusion of multiple softmax in different different layers and heads. In this paper, we want to provide similar vizualization properties to convnets: we augment convnets with an attention map. More precisely we replace the usual average pooling layer by an attention-based layer. Indeed, nothing in the convnets design precludes replacing their pooling by Original ViT-S “ResNet-50” S60 S60† attention [5]. We simplify the design of this attention-based pooling layer such that it explicitly provides the weights of the different patches.', 'Compared to ViT, for which the aggregation is performed across multiple layers and heads, our proposal offers a single weight per patch, and therefore a simple way to interpret the attention map: it is the respective contribution of each patch in the weighted sum summarizing the images. This treatment allows the model to deal with visual objects separately or jointly: if we use one token for each class instead of a single token, as exemplified in Figures 1 and 2, then we obtain an attention weight per patch for each possible class. In our main proposal we mostly focus on the single token case, which is more directly related to the classification decision. arXiv:2112.13692v1 [cs.CV] 27 Dec 2021 In Figure 1, we show the attention maps extracted from ViT by using a visualization procedure inspired by Caron et al. [8]. It involves some post-processing as there are multiple layers and heads providing patch weights. Then we show a ”ResNet-50” augmented by adding our attentionbased aggregation layer. Its hierarchical design leads to a low-resolution attention map with artefacts: We need an architecture producing a higher-resolution feature maps in order to better leverage the proposed attention-based pooling. For this purpose we introduce a simple patch-based convolutional architecture1 that keeps the input resolution constant throughout the network.', 'This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 .']",intro_chunked,"This design departs from the historical pyramidal architectures of LeNet [37], AlexNet [36] or ResNet [24, 25], to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre-processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, e.g. our aggregation layer. We refer to it as PatchConvNet, see Figure 3 for an overview of this network. In summary, we make the following contributions: • We revisit the final pooling layer in convnets by presenting a learned, attention-based pooling; • We propose a slight adaptation of our attention-based pooling in order to have one attention map per class, offering a better interpretability of the predictions; • We propose an architecture, PatchConvNet, with a simple patch-based design (two parameters: depth and width) and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models2 .",28.12765835929389,30.655460127574653,177,0.6146591305732727," This design departs from the historical pyramidal architectures of Propname, Propname or ResNet, to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, eg our aggregation layer. We refer to it as Propname, see Figure 0 for an overview of this network. In summary, we make the following contributions: We revisit the final pooling layer in convnets by presenting a learned, attention based pooling; We propose a slight adaptation of our attention based pooling in order to have one attention map per class, offering a better interpretability of the predictions; We propose an architecture, Propname, with a simple patch based design and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models0."," This design departs from the historical pyramidal architectures of Propname, Propname or ResNet, to name only a few. Their pyramidal design was motivated by the importance of reducing the resolution while increasing the working dimensionality. That allowed one to maintain a moderate complexity while progressively increasing the working dimensionality, making the space large enough to be separable by a linear classifier. In our case, we simplify the trunk after a small pre processing stage that produces the patches. We adopt the same dimensionality throughout all the trunk, fixing it equal to that of the final layer, eg our aggregation layer. We refer to it as Propname, see Figure 0 for an overview of this network. In summary, we make the following contributions: We revisit the final pooling layer in convnets by presenting a learned, attention based pooling; We propose a slight adaptation of our attention based pooling in order to have one attention map per class, offering a better interpretability of the predictions; We propose an architecture, Propname, with a simple patch based design and a simple training recipe: same learning rate for all our models, a single regularization parameter. We share the architecture definition and pretrained models0.", DET NOUN VERB ADP DET ADJ NOUN NOUN ADP PROPN PUNCT PROPN CCONJ NOUN PUNCT PART VERB ADV DET ADJ PUNCT PRON ADJ NOUN AUX VERB ADP DET NOUN ADP VERB DET NOUN SCONJ VERB DET VERB NOUN PUNCT PRON VERB NUM PART VERB DET ADJ NOUN SCONJ ADV VERB DET VERB NOUN PUNCT VERB DET NOUN ADJ ADV PART AUX ADJ ADP DET ADJ NOUN PUNCT ADP PRON NOUN PUNCT PRON VERB DET NOUN ADP DET ADJ ADJ NOUN NOUN PRON VERB DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP DET DET NOUN PUNCT VERB PRON ADJ ADP PRON ADP DET ADJ NOUN PUNCT VERB PRON NOUN NOUN PUNCT PRON VERB ADP PRON ADP PROPN PUNCT VERB NOUN NUM ADP DET NOUN ADP DET NOUN PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NOUN PUNCT PRON VERB DET ADJ NOUN NOUN ADP NOUN ADP VERB DET VERB PUNCT NOUN VERB NOUN PUNCT PRON VERB DET ADJ NOUN ADP PRON NOUN VERB NOUN ADP NOUN PART AUX NUM NOUN NOUN ADP NOUN PUNCT VERB DET ADJ NOUN ADP DET NOUN PUNCT PRON VERB DET NOUN PUNCT PROPN PUNCT ADP DET ADJ NOUN VERB NOUN CCONJ DET ADJ NOUN NOUN PUNCT ADJ NOUN NOUN ADP DET PRON NOUN PUNCT DET ADJ NOUN NOUN PUNCT PRON VERB DET NOUN NOUN CCONJ VERB NOUN PUNCT,0.5560538116591929,27.875,4.802690582959642
178,92,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked," Convolutional neural networks have been the main design paradigm for image
understanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,
namely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest
in architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled
image dataset (JFT-300M [46], 300 millions images). The paper concluded that
transformers “do not generalize well when trained on insufficient amounts of data”,
and the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two
to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)
that is competitive with convnets having a similar number of parameters and
efficiency. It uses Imagenet as the sole training set.",29.37086956521742,30.655460127574653,178,0.6757087707519531," Convolutional neural networks have been the main design paradigm for image 
 understanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set, 
 namely Propname. Motivated by the success of attention based models in Propname Propname Propname, there has been increasing interest 
 in architectures leveraging attention mechanisms within convnets. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks. The vision transformer introduced by Propname Propname Propname. is an architecture directly inherited from Propname Propname Propname, but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled 
 image dataset. The paper concluded that 
 transformers do not generalize well when trained on insufficient amounts of data, 
 and the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 0 Propname node in two 
 to three days that is competitive with convnets having a similar number of parameters and 
 efficiency. It uses Propname as the sole training set."," Convolutional neural networks have been the main design paradigm for image 
 understanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set, 
 namely Propname. Motivated by the success of attention based models in Propname Propname Propname, there has been increasing interest 
 in architectures leveraging attention mechanisms within convnets. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks. The vision transformer introduced by Propname Propname Propname. is an architecture directly inherited from Propname Propname Propname, but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled 
 image dataset. The paper concluded that 
 transformers do not generalize well when trained on insufficient amounts of data, 
 and the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 0 Propname node in two 
 to three days that is competitive with convnets having a similar number of parameters and 
 efficiency. It uses Propname as the sole training set.", ADJ ADJ NOUN AUX AUX DET ADJ NOUN NOUN ADP NOUN SPACE NOUN NOUN PUNCT SCONJ ADV VERB ADP NOUN NOUN NOUN PUNCT NUM ADP DET NOUN ADP PRON NOUN AUX DET NOUN ADP DET ADJ NOUN NOUN PUNCT SPACE ADV PROPN PUNCT VERB ADP DET NOUN ADP NOUN VERB NOUN ADP PROPN PROPN PROPN PUNCT PRON AUX AUX VERB NOUN SPACE ADP NOUN VERB NOUN NOUN ADP NOUN PUNCT ADV ADV ADJ NOUN AUX VERB ADJ NOUN VERB NOUN NOUN ADP NOUN PART VERB NOUN NOUN PUNCT DET NOUN NOUN VERB ADP PROPN PROPN PROPN PUNCT AUX DET NOUN ADV VERB ADP PROPN PROPN PROPN PUNCT CCONJ VERB ADP NOUN NOUN ADP ADJ NOUN NOUN ADP NOUN PUNCT PRON NOUN VERB ADJ NOUN ADP NOUN VERB ADP DET ADJ ADJ VERB SPACE NOUN NOUN PUNCT DET NOUN VERB SCONJ SPACE NOUN AUX PART VERB ADV SCONJ VERB ADP ADJ NOUN ADP NOUN PUNCT SPACE CCONJ DET NOUN ADP DET NOUN VERB ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN NOUN ADP DET ADJ NUM PROPN NOUN ADP NUM SPACE ADP NUM NOUN PRON AUX ADJ ADP NOUN VERB DET ADJ NOUN ADP NOUN CCONJ SPACE NOUN PUNCT PRON VERB PROPN ADP DET ADJ NOUN NOUN PUNCT,0.5862068965517241,20.3,5.482758620689655
179,93,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked,"We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements
included in the timm library [55]. With our Data-efficient image Transformers
(DeiT), we report large improvements over previous results, see Figure 1. Our
ablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce
a token-based strategy, specific to transformers and denoted by DeiT, and
show that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:
• We show that our neural networks that contains no convolutional layer
can achieve competitive results against the state of the art on ImageNet
with no external data. They are learned on a single node with 4 GPUs in
three days1
. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,
which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the
transformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.",37.26182389937108,30.655460127574653,179,0.3766777217388153," We build upon the visual transformer architecture from Propname Propname Propname. and improvements 
 included in the timm library. With our Propname efficient image Transformers 
, we report large improvements over previous results, see Propname 0. Our 
 ablation study details the hyper parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce 
 a token based strategy, specific to transformers and denoted by Propname, and 
 show that it advantageously replaces the usual distillation. In summary, our work makes the following contributions: We show that our neural networks that contains no convolutional layer 
 can achieve competitive results against the state of the art on Propname 
 with no external data. They are learned on a single node with 0 GPUs in 
 three days0 
. Our two new models Propname Propname and Propname Propname have fewer parameters and can be seen as the counterpart of ResNet00 and ResNet 00. We introduce a new distillation procedure based on a distillation token, 
 which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the 
 transformer through attention. This transformer specific strategy outperforms vanilla distillation by a significant margin."," We build upon the visual transformer architecture from Propname Propname Propname. and improvements 
 included in the timm library. With our Propname efficient image Transformers 
, we report large improvements over previous results, see Propname 0. Our 
 ablation study details the hyper parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce 
 a token based strategy, specific to transformers and denoted by Propname, and 
 show that it advantageously replaces the usual distillation. In summary, our work makes the following contributions: We show that our neural networks that contains no convolutional layer 
 can achieve competitive results against the state of the art on Propname 
 with no external data. They are learned on a single node with 0 GPUs in 
 three days0 
. Our two new models Propname Propname and Propname Propname have fewer parameters and can be seen as the counterpart of ResNet00 and ResNet 00. We introduce a new distillation procedure based on a distillation token, 
 which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the 
 transformer through attention. This transformer specific strategy outperforms vanilla distillation by a significant margin.", PRON VERB SCONJ DET ADJ NOUN NOUN ADP PROPN PROPN PROPN PUNCT CCONJ NOUN SPACE VERB ADP DET ADJ NOUN PUNCT ADP PRON PROPN ADJ NOUN NOUN SPACE PUNCT PRON VERB ADJ NOUN ADP ADJ NOUN PUNCT VERB PROPN NUM PUNCT PRON SPACE NOUN NOUN NOUN DET ADJ NOUN CCONJ ADJ NOUN ADP DET ADJ NOUN PUNCT ADJ ADP VERB NOUN PUNCT PRON VERB DET NOUN PUNCT SCONJ PART VERB DET NOUN PUNCT PRON VERB SPACE DET ADJ VERB NOUN PUNCT ADJ ADP NOUN CCONJ VERB ADP PROPN PUNCT CCONJ SPACE VERB SCONJ PRON ADV VERB DET ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB DET VERB NOUN PUNCT PRON VERB SCONJ PRON ADJ NOUN PRON VERB DET ADJ NOUN SPACE AUX VERB ADJ NOUN ADP DET NOUN ADP DET NOUN ADP PROPN SPACE ADP DET ADJ NOUN PUNCT PRON AUX VERB ADP DET ADJ NOUN ADP NUM NOUN ADP SPACE NUM NOUN SPACE PUNCT PRON NUM ADJ NOUN PROPN PROPN CCONJ PROPN PROPN VERB ADJ NOUN CCONJ AUX AUX VERB ADP DET NOUN ADP NOUN PUNCT CCONJ NOUN NUM PUNCT PRON VERB DET ADJ NOUN NOUN VERB ADP DET NOUN NOUN PUNCT SPACE PRON VERB DET ADJ NOUN SCONJ DET NOUN VERB PUNCT SCONJ SCONJ PRON VERB ADP VERB DET NOUN VERB ADP DET NOUN PUNCT DET NOUN VERB ADP DET SPACE NOUN ADP NOUN PUNCT DET ADJ ADJ NOUN NOUN NOUN NOUN ADP DET ADJ NOUN PUNCT,0.5921052631578947,19.0,5.021929824561403
180,94,Hugo Touvron,"[' Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks. One of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based models in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61]. More recently several researchers have proposed hybrid architecture transplanting transformer ingredients to convnets to solve vision tasks [6, 43]. The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an architecture directly inherited from Natural Language Processing [52], but applied to image classification with raw image patches as input. Their paper presented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufficient amounts of data”,\nand the training of these models involved extensive computing resources. In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set.', 'We build upon the visual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a successful training, such as repeated augmentation. We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiT, and\nshow that it advantageously replaces the usual distillation. In summary, our work makes the following contributions:\n• We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1\n. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18. • We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outperforms vanilla distillation by a significant margin.', '• Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7.']",intro_chunked,"• Interestingly, with our distillation, image transformers learn more from a
convnet than from another transformer with comparable performance. • Our models pre-learned on Imagenet are competitive when transferred to
different downstream tasks such as fine-grained classification, on several
popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,
Stanford Cars and iNaturalist-18/19. This paper is organized as follows: we review related works in Section 2,
and focus on transformers for image classification in Section 3. We introduce
our distillation strategy for transformers in Section 4. The experimental section 5 provides analysis and comparisons against both convnets and recent
transformers, as well as a comparative evaluation of our transformer-specific
distillation. Section 6 details our training scheme. It includes an extensive ablation of our data-efficient training choices, which gives some insight on the
key ingredients involved in DeiT. We conclude in Section 7.",24.816350524475553,30.655460127574653,180,0.6479603052139282," Interestingly, with our distillation, image transformers learn more from a 
 convnet than from another transformer with comparable performance. Our models pre learned on Propname are competitive when transferred to 
 different downstream tasks such as fine grained classification, on several 
 popular public benchmarks: Propname 00, Propname 000, Propname 000 flowers, 
 Propname Propname and Propname 0000. This paper is organized as follows: we review related works in Section 0, 
 and focus on transformers for image classification in Section 0. We introduce 
 our distillation strategy for transformers in Section 0. The experimental section 0 provides analysis and comparisons against both convnets and recent 
 transformers, as well as a comparative evaluation of our transformer specific 
 distillation. Section 0 details our training scheme. It includes an extensive ablation of our data efficient training choices, which gives some insight on the 
 key ingredients involved in DeiT. We conclude in Section 0."," Interestingly, with our distillation, image transformers learn more from a 
 convnet than from another transformer with comparable performance. Our models pre learned on Propname are competitive when transferred to 
 different downstream tasks such as fine grained classification, on several 
 popular public benchmarks: Propname 00, Propname 000, Propname 000 flowers, 
 Propname Propname and Propname 0000. This paper is organized as follows: we review related works in Section 0, 
 and focus on transformers for image classification in Section 0. We introduce 
 our distillation strategy for transformers in Section 0. The experimental section 0 provides analysis and comparisons against both convnets and recent 
 transformers, as well as a comparative evaluation of our transformer specific 
 distillation. Section 0 details our training scheme. It includes an extensive ablation of our data efficient training choices, which gives some insight on the 
 key ingredients involved in DeiT. We conclude in Section 0.", ADV PUNCT ADP PRON NOUN PUNCT NOUN NOUN VERB ADJ ADP DET SPACE NOUN ADP ADP DET NOUN ADP ADJ NOUN PUNCT PRON NOUN VERB VERB ADP PROPN AUX ADJ SCONJ VERB ADP SPACE ADJ ADJ NOUN ADJ ADP ADJ ADJ NOUN PUNCT ADP ADJ SPACE ADJ ADJ NOUN PUNCT PROPN NUM PUNCT PROPN NUM PUNCT PROPN NUM NOUN PUNCT SPACE PROPN PROPN CCONJ PROPN NUM PUNCT DET NOUN AUX VERB SCONJ VERB PUNCT PRON VERB ADJ NOUN ADP NOUN NUM PUNCT SPACE CCONJ VERB ADP NOUN ADP NOUN NOUN ADP NOUN NUM PUNCT PRON VERB SPACE PRON NOUN NOUN ADP NOUN ADP NOUN NUM PUNCT DET ADJ NOUN NUM VERB NOUN CCONJ NOUN ADP DET NOUN CCONJ ADJ SPACE NOUN PUNCT ADV ADV ADP DET ADJ NOUN ADP PRON ADJ ADJ SPACE NOUN PUNCT NOUN NUM NOUN PRON NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP PRON NOUN ADJ NOUN NOUN PUNCT PRON VERB DET NOUN ADP DET SPACE ADJ NOUN VERB ADP NOUN PRON VERB ADP NOUN NUM PUNCT,0.6097560975609756,20.5,5.219512195121951
181,95,Hugo Touvron,"[' Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.', 'We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation.', 'To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.']",intro_chunked," Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.",8.51813455657495,30.655460127574653,181,0.5622861981391907," Recently, the transformer architecture, adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on Propname 0k when pre trained with a sufficiently large amount of data. Retrospectively, this achievement is another step towards learning visual features with less priors: Propname Propname Propname had replaced the hand designed choices from hard wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard coded prior in the architecture has been fueled by better training schemes, and, in this paper, we push this trend further by showing that a purely multilayer Propname based architecture, called Propname Propname Propname Propname, is competitive on image classification. Propname is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: a cross patch Propname layer applied to all channels independently; and an cross channel single layer Propname applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier."," Recently, the transformer architecture, adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on Propname 0k when pre trained with a sufficiently large amount of data. Retrospectively, this achievement is another step towards learning visual features with less priors: Propname Propname Propname had replaced the hand designed choices from hard wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard coded prior in the architecture has been fueled by better training schemes, and, in this paper, we push this trend further by showing that a purely multilayer Propname based architecture, called Propname Propname Propname Propname, is competitive on image classification. Propname is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: a cross patch Propname layer applied to all channels independently; and an cross channel single layer Propname applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.", ADV PUNCT DET NOUN NOUN PUNCT VERB ADP PRON ADJ NOUN ADP ADJ NOUN NOUN ADP ADJ ADJ NOUN PUNCT AUX VERB NOUN ADJ ADP DET NOUN ADP DET NOUN ADP PROPN NOUN SCONJ VERB VERB ADP DET ADV ADJ NOUN ADP NOUN PUNCT ADV PUNCT DET NOUN AUX DET NOUN ADP VERB ADJ NOUN ADP ADJ NOUN PUNCT PROPN PROPN PROPN AUX VERB DET NOUN VERB NOUN ADP ADV VERB NOUN ADP ADJ CCONJ ADJ NOUN PUNCT NOUN NOUN ADV VERB ADJ ADJ NOUN VERB ADP DET ADJ NOUN PUNCT ADV DET NOUN NOUN CCONJ ADJ NOUN PUNCT DET NOUN ADP ADV ADV VERB ADV ADP DET NOUN AUX AUX VERB ADP ADJ NOUN NOUN PUNCT CCONJ PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADV ADP VERB SCONJ DET ADV ADJ PROPN VERB NOUN PUNCT VERB PROPN PROPN PROPN PROPN PUNCT AUX ADJ ADP NOUN NOUN PUNCT PROPN AUX VERB PART AUX ADJ CCONJ VERB ADJ ADV ADP NOUN PUNCT PRON VERB NOUN NOUN ADP NOUN PUNCT VERB PRON ADP DET ADJ NOUN PUNCT CCONJ ADV VERB PRON NOUN ADP NUM ADJ NOUN PUNCT DET NOUN VERB PROPN NOUN VERB ADP DET NOUN ADV PUNCT CCONJ DET NOUN NOUN ADJ NOUN PROPN VERB ADV ADP DET NOUN PUNCT ADP DET NOUN ADP DET NOUN PUNCT DET NOUN NOUN AUX ADJ VERB PUNCT CCONJ VERB ADP DET ADJ NOUN PUNCT,0.5948275862068966,38.666666666666664,5.293103448275862
182,96,Hugo Touvron,"[' Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.', 'We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation.', 'To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.']",intro_chunked,"We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation.",34.28753768844223,30.655460127574653,182,0.4006006419658661," We outline Propname in Figure0 and detail it further in Section 0. The Propname architecture is strongly inspired by the vision transformers, yet it is much simpler in several ways: we replace the self attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non linearity. We observe that the training of Propname is more stable than Propname when using the same training scheme as in Propname and Propname, allowing to remove the need for batch specific or cross channel normalizations such as Propname, Propname or Propname. We speculate that this stability comes from replacing self attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between Propname embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely Propname based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our Propname based architecture to take inputs with variable length, and show its potential on the problem of Propname Propname."," We outline Propname in Figure0 and detail it further in Section 0. The Propname architecture is strongly inspired by the vision transformers, yet it is much simpler in several ways: we replace the self attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non linearity. We observe that the training of Propname is more stable than Propname when using the same training scheme as in Propname and Propname, allowing to remove the need for batch specific or cross channel normalizations such as Propname, Propname or Propname. We speculate that this stability comes from replacing self attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between Propname embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely Propname based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our Propname based architecture to take inputs with variable length, and show its potential on the problem of Propname Propname.", PRON VERB PROPN ADP NOUN PUNCT CCONJ VERB PRON ADV ADP NOUN NUM PUNCT DET PROPN NOUN AUX ADV VERB ADP DET NOUN NOUN PUNCT CCONJ PRON AUX ADV ADJ ADP ADJ NOUN PUNCT PRON VERB DET NOUN NOUN NOUN ADP DET ADJ NOUN PUNCT VERB ADP DET NOUN ADP ADJ ADJ NOUN CCONJ ADJ ADJ NOUN PUNCT PRON VERB SCONJ DET NOUN ADP PROPN AUX ADV ADJ ADP PROPN SCONJ VERB DET ADJ NOUN NOUN ADP ADP PROPN CCONJ PROPN PUNCT VERB PART VERB DET NOUN ADP NOUN ADJ CCONJ NOUN NOUN NOUN ADJ ADP PROPN PUNCT PROPN CCONJ PROPN PUNCT PRON VERB SCONJ DET NOUN VERB ADP VERB NOUN NOUN ADP ADJ NOUN PUNCT ADV PUNCT DET NOUN ADP VERB DET ADJ NOUN AUX SCONJ PRON AUX ADV VERB DET NOUN ADP PROPN NOUN PUNCT VERB NOUN PRON AUX ADJ ADP NOUN ADP DET ADJ NOUN PUNCT CCONJ ADJ VERB ADP DET ADJ NOUN PUNCT PRON ADV VERB SCONJ PRON ADV PROPN VERB NOUN AUX VERB ADP ADJ NOUN ADP NOUN PUNCT CCONJ ADV PUNCT ADP ADJ ADJ NOUN NOUN PUNCT ADP ADJ PUNCT PRON VERB PRON PROPN VERB NOUN PART VERB NOUN ADP ADJ NOUN PUNCT CCONJ VERB PRON NOUN ADP DET NOUN ADP PROPN PROPN PUNCT,0.569377990430622,29.857142857142858,4.889952153110048
183,97,Hugo Touvron,"[' Recently, the transformer architecture [60], adapted from its original use in natural language processing with only minor changes, has achieved performance competitive with the state of the art on ImageNet-1k [50] when pre-trained with a sufficiently large amount of data [16]. Retrospectively, this achievement is another step towards learning visual features with less priors: Convolutional Neural Networks (CNN) had replaced the hand-designed choices from hard-wired features with flexible and trainable architectures. Vision transformers further removes several hard decisions encoded in the convolutional architectures, namely the translation invariance and local connectivity. This evolution toward less hard-coded prior in the architecture has been fueled by better training schemes [16, 56], and, in this paper, we push this trend further by showing that a purely multilayer perceptron (MLP) based architecture, called Residual Multi-Layer Perceptrons (ResMLP), is competitive on image classification. ResMLP is designed to be simple and encoding little prior about images: it takes image patches as input, projects them with a linear layer, and sequentially updates their representations with two residual operations: (i) a cross-patch linear layer applied to all channels independently; and (ii) an cross-channel single-layer MLP applied independently to all patches. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.', 'We outline ResMLP in Figure 1 and detail it further in Section 2. The ResMLP architecture is strongly inspired by the vision transformers (ViT) [16], yet it is much simpler in several ways: we replace the self-attention sublayer by a linear layer, resulting in an architecture with only linear layers and GELU non-linearity [25]. We observe that the training of ResMLP is more stable than ViTs when using the same training scheme as in DeiT [56] and CaiT [57], allowing to remove the need for batch-specific or cross-channel normalizations such as BatchNorm, GroupNorm or LayerNorm. We speculate that this stability comes from replacing self-attention with linear layers. Finally, another advantage of using a linear layer is that we can still visualize the interactions between patch embeddings, revealing filters that are similar to convolutions on the lower layers, and longer range in the last layers. We further investigate if our purely MLP based architecture could benefit to other domains beyond images, and particularly, with more complex output spaces. In particular, we adapt our MLP based architecture to take inputs with variable length, and show its potential on the problem of Machine Translation.', 'To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.']",intro_chunked,"To do so, we develop a sequence-to-sequence (seq2seq) version of ResMLP, where both encoder and decoders are based on ResMLP with across-attention between the encoder and decoder [2]. This model is similar to the original seq2seq Transformer with ResMLP layers instead of Transformer layers [60]. Despite not being originally designed for this task, we observe that ResMLP is competitive with Transformers on the challenging WMT benchmarks. In summary, in this paper, we make the following observations: • despite its simplicity, ResMLP reaches surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only1 , without requiring normalization based on batch or channel statistics; • these models benefit significantly from distillation methods [56]; they are also compatible with modern self-supervised learning methods based on data augmentation, such as DINO [7]; • A seq2seq ResMLP achieves competitive performances compared to a seq2seq Transformers on the WMT benchmark for Machine Translation.",11.654787735849084,30.655460127574653,183,0.5251595377922058," To do so, we develop a sequence to sequence version of Propname, where both encoder and decoders are based on Propname with across attention between the encoder and decoder. This model is similar to the original Propname Propname with Propname layers instead of Propname layers. Despite not being originally designed for this task, we observe that Propname is competitive with Transformers on the challenging Propname benchmarks. In summary, in this paper, we make the following observations: despite its simplicity, Propname reaches surprisingly good accuracycomplexity trade offs with Propname Propname training only0, without requiring normalization based on batch or channel statistics; these models benefit significantly from distillation methods; they are also compatible with modern self supervised learning methods based on data augmentation, such as Propname; A Propname Propname achieves competitive performances compared to a Propname Propname on the Propname benchmark for Propname Propname."," To do so, we develop a sequence to sequence version of Propname, where both encoder and decoders are based on Propname with across attention between the encoder and decoder. This model is similar to the original Propname Propname with Propname layers instead of Propname layers. Despite not being originally designed for this task, we observe that Propname is competitive with Transformers on the challenging Propname benchmarks. In summary, in this paper, we make the following observations: despite its simplicity, Propname reaches surprisingly good accuracycomplexity trade offs with Propname Propname training only0, without requiring normalization based on batch or channel statistics; these models benefit significantly from distillation methods; they are also compatible with modern self supervised learning methods based on data augmentation, such as Propname; A Propname Propname achieves competitive performances compared to a Propname Propname on the Propname benchmark for Propname Propname.", PART VERB ADV PUNCT PRON VERB DET NOUN PART VERB NOUN ADP PROPN PUNCT SCONJ PRON NOUN CCONJ NOUN AUX VERB ADP PROPN ADP ADP NOUN ADP DET NOUN CCONJ NOUN PUNCT DET NOUN AUX ADJ ADP DET ADJ PROPN PROPN ADP PROPN NOUN ADV ADP PROPN NOUN PUNCT SCONJ PART AUX ADV VERB ADP DET NOUN PUNCT PRON VERB SCONJ PROPN AUX ADJ ADP NOUN ADP DET ADJ PROPN NOUN PUNCT ADP NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET VERB NOUN PUNCT SCONJ PRON NOUN PUNCT PROPN VERB ADV ADJ NOUN NOUN NOUN ADP PROPN PROPN NOUN NOUN PUNCT ADP VERB NOUN VERB ADP NOUN CCONJ NOUN NOUN PUNCT DET NOUN VERB ADV ADP NOUN NOUN PUNCT PRON AUX ADV ADJ ADP ADJ NOUN VERB NOUN NOUN VERB ADP NOUN NOUN PUNCT ADJ ADP PROPN PUNCT DET PROPN PROPN VERB ADJ NOUN VERB ADP DET PROPN PROPN ADP DET PROPN NOUN ADP PROPN PROPN PUNCT,0.620253164556962,39.5,5.455696202531645
184,98,Hugo Touvron,"[' Residual architectures are prominent in computer vision since the advent of\nResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function\ng and R define how the network updates the input x at layer l. The function g is typically identity, while\nR is the main building block of the network: many variants in the literature essentially differ on how\none defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and\narchitecture design. As pointed out by He et al. [27], residual networks do not\noffer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]\nthe importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network\nalternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original\narchitecture of Vaswani et al. [66], except the LayerNorm is applied before the\nblock (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et\nal.', '[13] adopt this choice with LayerNorm for training deeper transformers for\nvarious media, including for image generation where they train transformers\nwith 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,\n34, 75]. In Section 2, we revisit this topic for transformer architectures solving\nimage classification problems. Examples of approaches closely related to ours\ninclude Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to\nimprove the training of deeper architecture compared to current methods for\nimage transformers. Formally, we add a learnable diagonal matrix on output of\neach residual block, initialized close to (but not at) 0. Adding this simple layer\nafter each residual block improves the training dynamic, allowing us to train\ndeeper high-capacity image transformers that benefit from depth. We refer to\nthis approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2.', 'It is akin to an encoder/decoder architecture,\nin which we explicitly separate the transformer layers involving self-attention\nbetween patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear\nclassifier. This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54], our model is on par with the state\nof the art (86.5%) while requiring less FLOPs (329B vs 377B) and having\nless parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We\ndiscuss related works along this paper and in the dedicated Section 6, before\nwe conclude in Section 7. The appendices contain some variations we have\ntried during our exploration.']",intro_chunked," Residual architectures are prominent in computer vision since the advent of
ResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function
g and R define how the network updates the input x at layer l. The function g is typically identity, while
R is the main building block of the network: many variants in the literature essentially differ on how
one defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and
architecture design. As pointed out by He et al. [27], residual networks do not
offer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]
the importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network
alternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original
architecture of Vaswani et al. [66], except the LayerNorm is applied before the
block (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et
al.",42.9803022174535,30.655460127574653,184,0.4422231912612915," Residual architectures are prominent in computer vision since the advent of 
 ResNet. They are defined as a sequence of functions of the form FORMULA, where the function 
 g and R define how the network updates the input Propname at layer l. The function g is typically identity, while 
 R is the main building block of the network: many variants in the literature essentially differ on how 
 one defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and 
 architecture design. As pointed out by He Propname Propname., residual networks do not 
 offer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He Propname Propname. discussed the importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers instantiate a particular form of residual architecture: after casting the input image into a set Propname of vectors, the network 
 alternates self attention layers with feed forward networks, as FORMULA, where is the Propname operator. This definition follows the original 
 architecture of Propname Propname Propname., except the Propname is applied before the 
 block in the residual branch, as advocated by He Propname Propname.. Child Propname 
 Propname."," Residual architectures are prominent in computer vision since the advent of 
 ResNet. They are defined as a sequence of functions of the form FORMULA, where the function 
 g and R define how the network updates the input Propname at layer l. The function g is typically identity, while 
 R is the main building block of the network: many variants in the literature essentially differ on how 
 one defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and 
 architecture design. As pointed out by He Propname Propname., residual networks do not 
 offer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He Propname Propname. discussed the importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers instantiate a particular form of residual architecture: after casting the input image into a set Propname of vectors, the network 
 alternates self attention layers with feed forward networks, as FORMULA, where is the Propname operator. This definition follows the original 
 architecture of Propname Propname Propname., except the Propname is applied before the 
 block in the residual branch, as advocated by He Propname Propname.. Child Propname 
 Propname.", ADJ NOUN AUX ADJ ADP NOUN NOUN SCONJ DET NOUN ADP SPACE NOUN PUNCT PRON AUX VERB ADP DET NOUN ADP NOUN ADP DET NOUN NOUN PUNCT SCONJ DET NOUN SPACE NOUN CCONJ NOUN VERB SCONJ DET NOUN VERB DET NOUN PROPN ADP NOUN NOUN DET NOUN NOUN AUX ADV NOUN PUNCT SCONJ SPACE NOUN AUX DET ADJ NOUN NOUN ADP DET NOUN PUNCT ADJ NOUN ADP DET NOUN ADV VERB ADP SCONJ SPACE NUM VERB DET ADJ NOUN NOUN AUX VERB CCONJ VERB PUNCT ADJ NOUN VERB DET ADJ NOUN ADP NOUN CCONJ SPACE NOUN NOUN PUNCT SCONJ VERB ADP ADP PRON PROPN PROPN PUNCT PUNCT ADJ NOUN AUX PART SPACE VERB DET ADJ ADJ NOUN PUNCT PRON VERB ADJ NOUN SCONJ PRON AUX ADJ PART VERB PUNCT ADV ADP PRON ADJ NOUN PUNCT PRON PROPN PROPN PUNCT VERB DET NOUN ADP VERB DET ADJ NOUN CCONJ ADV CCONJ ADJ PUNCT CCONJ VERB VERB NOUN ADP DET NOUN NOUN PUNCT DET NOUN NOUN VERB DET ADJ NOUN ADP ADJ NOUN PUNCT ADP VERB DET NOUN NOUN ADP DET ADJ PROPN ADP NOUN PUNCT DET NOUN SPACE VERB NOUN NOUN NOUN ADP NOUN ADJ NOUN PUNCT ADP NOUN PUNCT SCONJ AUX DET PROPN NOUN PUNCT DET NOUN VERB DET ADJ SPACE NOUN ADP PROPN PROPN PROPN PUNCT PUNCT SCONJ DET PROPN AUX VERB ADP DET SPACE NOUN ADP DET ADJ NOUN PUNCT SCONJ VERB ADP PRON PROPN PROPN PUNCT PUNCT NOUN PROPN SPACE PROPN PUNCT,0.5474137931034483,29.0,5.047413793103448
185,99,Hugo Touvron,"[' Residual architectures are prominent in computer vision since the advent of\nResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function\ng and R define how the network updates the input x at layer l. The function g is typically identity, while\nR is the main building block of the network: many variants in the literature essentially differ on how\none defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and\narchitecture design. As pointed out by He et al. [27], residual networks do not\noffer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]\nthe importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network\nalternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original\narchitecture of Vaswani et al. [66], except the LayerNorm is applied before the\nblock (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et\nal.', '[13] adopt this choice with LayerNorm for training deeper transformers for\nvarious media, including for image generation where they train transformers\nwith 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,\n34, 75]. In Section 2, we revisit this topic for transformer architectures solving\nimage classification problems. Examples of approaches closely related to ours\ninclude Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to\nimprove the training of deeper architecture compared to current methods for\nimage transformers. Formally, we add a learnable diagonal matrix on output of\neach residual block, initialized close to (but not at) 0. Adding this simple layer\nafter each residual block improves the training dynamic, allowing us to train\ndeeper high-capacity image transformers that benefit from depth. We refer to\nthis approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2.', 'It is akin to an encoder/decoder architecture,\nin which we explicitly separate the transformer layers involving self-attention\nbetween patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear\nclassifier. This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54], our model is on par with the state\nof the art (86.5%) while requiring less FLOPs (329B vs 377B) and having\nless parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We\ndiscuss related works along this paper and in the dedicated Section 6, before\nwe conclude in Section 7. The appendices contain some variations we have\ntried during our exploration.']",intro_chunked,"[13] adopt this choice with LayerNorm for training deeper transformers for
various media, including for image generation where they train transformers
with 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,
34, 75]. In Section 2, we revisit this topic for transformer architectures solving
image classification problems. Examples of approaches closely related to ours
include Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to
improve the training of deeper architecture compared to current methods for
image transformers. Formally, we add a learnable diagonal matrix on output of
each residual block, initialized close to (but not at) 0. Adding this simple layer
after each residual block improves the training dynamic, allowing us to train
deeper high-capacity image transformers that benefit from depth. We refer to
this approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2.",27.77695652173915,30.655460127574653,185,0.6855289936065674," adopt this choice with Propname for training deeper transformers for 
 various media, including for image generation where they train transformers 
 with 00 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks and for transformers applied to Propname or speech tasks 0, 
 00, 00. In Section 0, we revisit this topic for transformer architectures solving 
 image classification problems. Examples of approaches closely related to ours 
 include Propname, Propname Propname, Propname and Propname. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to 
 improve the training of deeper architecture compared to current methods for 
 image transformers. Formally, we add a learnable diagonal matrix on output of 
 each residual block, initialized close to 0. Adding this simple layer 
 after each residual block improves the training dynamic, allowing us to train 
 deeper high capacity image transformers that benefit from depth. We refer to 
 this approach as Propname. Section 0 introduces our second contribution, namely class attention layers, that we present in Propname 0."," adopt this choice with Propname for training deeper transformers for 
 various media, including for image generation where they train transformers 
 with 00 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks and for transformers applied to Propname or speech tasks 0, 
 00, 00. In Section 0, we revisit this topic for transformer architectures solving 
 image classification problems. Examples of approaches closely related to ours 
 include Propname, Propname Propname, Propname and Propname. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to 
 improve the training of deeper architecture compared to current methods for 
 image transformers. Formally, we add a learnable diagonal matrix on output of 
 each residual block, initialized close to 0. Adding this simple layer 
 after each residual block improves the training dynamic, allowing us to train 
 deeper high capacity image transformers that benefit from depth. We refer to 
 this approach as Propname. Section 0 introduces our second contribution, namely class attention layers, that we present in Propname 0.", VERB DET NOUN ADP PROPN ADP VERB ADJ NOUN ADP SPACE ADJ NOUN PUNCT VERB ADP NOUN NOUN SCONJ PRON VERB NOUN SPACE ADP NUM NOUN PUNCT SCONJ PART VERB PUNCT VERB PUNCT CCONJ VERB DET ADJ NOUN ADP DET ADJ NOUN AUX VERB ADJ NOUN CCONJ ADP ADJ ADJ NOUN CCONJ ADP NOUN VERB ADP PROPN CCONJ NOUN NOUN NUM PUNCT SPACE NUM PUNCT NUM PUNCT ADP NOUN NUM PUNCT PRON VERB DET NOUN ADP NOUN NOUN VERB SPACE NOUN NOUN NOUN PUNCT NOUN ADP NOUN ADV VERB ADP PRON SPACE VERB PROPN PUNCT PROPN PROPN PUNCT PROPN CCONJ PROPN PUNCT VERB PRON NOUN ADP DET NOUN ADP ADJ NOUN PUNCT NOUN CCONJ ADJ NOUN PUNCT PRON VERB DET NOUN PRON AUX ADJ PART SPACE VERB DET NOUN ADP ADJ NOUN VERB ADP ADJ NOUN ADP SPACE NOUN NOUN PUNCT ADV PUNCT PRON VERB DET ADJ ADJ NOUN ADP NOUN ADP SPACE DET ADJ NOUN PUNCT VERB ADV ADP NUM PUNCT VERB DET ADJ NOUN SPACE SCONJ DET ADJ NOUN VERB DET NOUN NOUN PUNCT VERB PRON PART VERB SPACE ADV ADJ NOUN NOUN NOUN PRON VERB ADP NOUN PUNCT PRON VERB ADP SPACE DET NOUN ADP PROPN PUNCT NOUN NUM VERB PRON ADJ NOUN PUNCT ADV NOUN NOUN NOUN PUNCT SCONJ PRON VERB ADP PROPN NUM PUNCT,0.5576923076923077,23.11111111111111,5.230769230769231
186,100,Hugo Touvron,"[' Residual architectures are prominent in computer vision since the advent of\nResNet [27]. They are defined as a sequence of functions of the form FORMULA, where the function\ng and R define how the network updates the input x at layer l. The function g is typically identity, while\nR is the main building block of the network: many variants in the literature essentially differ on how\none defines this residual branch R is constructed or parametrized. Residual architectures highlight the strong interplay between optimization and\narchitecture design. As pointed out by He et al. [27], residual networks do not\noffer a better representational power. They achieve better performance because they are easier to train: shortly after their seminal work, He et al. discussed [28]\nthe importance of having a clear path both forward and backward, and advocate setting gl to the identity function. The vision transformers [19] instantiate a particular form of residual architecture: after casting the input image into a set x0 of vectors, the network\nalternates self-attention layers (SA) with feed-forward networks (FFN), as FORMULA, where η is the LayerNorm operator [1]. This definition follows the original\narchitecture of Vaswani et al. [66], except the LayerNorm is applied before the\nblock (pre-norm) in the residual branch, as advocated by He et al. [28]. Child et\nal.', '[13] adopt this choice with LayerNorm for training deeper transformers for\nvarious media, including for image generation where they train transformers\nwith 48 layers. How to normalize, weigh, or initialize the residual blocks of a residual architecture has received significant attention both for convolutional neural networks [7, 8, 28, 75] and for transformers applied to NLP or speech tasks [2,\n34, 75]. In Section 2, we revisit this topic for transformer architectures solving\nimage classification problems. Examples of approaches closely related to ours\ninclude Fixup [75], T-Fixup [34], ReZero [2] and SkipInit [16]. Following our analysis of the interplay between different initialization, optimization and architectural design, we propose an approach that is effective to\nimprove the training of deeper architecture compared to current methods for\nimage transformers. Formally, we add a learnable diagonal matrix on output of\neach residual block, initialized close to (but not at) 0. Adding this simple layer\nafter each residual block improves the training dynamic, allowing us to train\ndeeper high-capacity image transformers that benefit from depth. We refer to\nthis approach as LayerScale. Section 3 introduces our second contribution, namely class-attention layers, that we present in Figure 2.', 'It is akin to an encoder/decoder architecture,\nin which we explicitly separate the transformer layers involving self-attention\nbetween patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear\nclassifier. This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54], our model is on par with the state\nof the art (86.5%) while requiring less FLOPs (329B vs 377B) and having\nless parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We\ndiscuss related works along this paper and in the dedicated Section 6, before\nwe conclude in Section 7. The appendices contain some variations we have\ntried during our exploration.']",intro_chunked,"It is akin to an encoder/decoder architecture,
in which we explicitly separate the transformer layers involving self-attention
between patches, from class-attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear
classifier. This explicit separation avoids the contradictory objective of guiding
the attention process while processing the class embedding. We refer to this
new architecture as CaiT (Class-Attention in Image Transformers). In the experimental Section 4, we empirically show the effectiveness and
complementary of our approaches:
• LayerScale significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands
of parameters to the network at training time (negligible w.r.t. the total
number of weights). • Our architecture with specific class-attention offers a more effective processing of the class embedding. • Our best CaiT models establish the new state of the art on ImagenetReal [6] and Imagenet V2 matched frequency [52] with no additional
training data. On ImageNet1k-val [54], our model is on par with the state
of the art (86.5%) while requiring less FLOPs (329B vs 377B) and having
less parameters than the best competing model (356M vs 438M). • We achieve competitive results on Transfer Learning. We provide visualizations of the attention mechanisms in Section 5. We
discuss related works along this paper and in the dedicated Section 6, before
we conclude in Section 7. The appendices contain some variations we have
tried during our exploration.",37.52159424544922,30.655460127574653,186,0.54341059923172," It is akin to an encoderdecoder architecture, 
 in which we explicitly separate the transformer layers involving self attention 
 between patches, from class attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear 
 classifier. This explicit separation avoids the contradictory objective of guiding 
 the attention process while processing the class embedding. We refer to this 
 new architecture as Propname. In the experimental Section 0, we empirically show the effectiveness and 
 complementary of our approaches: Propname significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands 
 of parameters to the network at training time negligible w.r.t. the total 
 number of weights. Our architecture with specific class attention offers a more effective processing of the class embedding. Our best CaiT models establish the new state of the art on Propname and Propname Propname matched frequency with no additional 
 training data. On ImageNet0k Propname, our model is on par with the state 
 of the art while requiring less FLOPs and having 
 less parameters than the best competing model. We achieve competitive results on Propname Propname. We provide visualizations of the attention mechanisms in Section 0. We 
 discuss related works along this paper and in the dedicated Propname 0, before 
 we conclude in Section 0. The appendices contain some variations we have 
 tried during our exploration."," It is akin to an encoderdecoder architecture, 
 in which we explicitly separate the transformer layers involving self attention 
 between patches, from class attention layers that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear 
 classifier. This explicit separation avoids the contradictory objective of guiding 
 the attention process while processing the class embedding. We refer to this 
 new architecture as Propname. In the experimental Section 0, we empirically show the effectiveness and 
 complementary of our approaches: Propname significantly facilitates the convergence and improves the accuracy of image transformers at larger depths. It adds a few thousands 
 of parameters to the network at training time negligible w.r.t. the total 
 number of weights. Our architecture with specific class attention offers a more effective processing of the class embedding. Our best CaiT models establish the new state of the art on Propname and Propname Propname matched frequency with no additional 
 training data. On ImageNet0k Propname, our model is on par with the state 
 of the art while requiring less FLOPs and having 
 less parameters than the best competing model. We achieve competitive results on Propname Propname. We provide visualizations of the attention mechanisms in Section 0. We 
 discuss related works along this paper and in the dedicated Propname 0, before 
 we conclude in Section 0. The appendices contain some variations we have 
 tried during our exploration.", PRON AUX ADJ ADP DET ADJ NOUN PUNCT SPACE ADP PRON PRON ADV VERB DET ADJ NOUN VERB NOUN NOUN SPACE ADP NOUN PUNCT ADP NOUN NOUN NOUN PRON AUX ADJ PART VERB DET NOUN ADP DET VERB NOUN ADP DET ADJ NOUN SCONJ SCONJ PRON AUX AUX VERB ADP DET ADJ SPACE NOUN PUNCT DET ADJ NOUN VERB DET ADJ NOUN ADP VERB SPACE DET NOUN NOUN SCONJ VERB DET NOUN VERB PUNCT PRON VERB ADP DET SPACE ADJ NOUN ADP PROPN PUNCT ADP DET ADJ NOUN NUM PUNCT PRON ADV VERB DET NOUN CCONJ SPACE ADJ ADP PRON NOUN PUNCT PROPN ADV VERB DET NOUN CCONJ VERB DET NOUN ADP NOUN NOUN ADP ADJ NOUN PUNCT PRON VERB DET ADJ NOUN SPACE ADP NOUN ADP DET NOUN ADP NOUN NOUN ADJ NOUN PUNCT DET ADJ SPACE NOUN ADP NOUN PUNCT PRON NOUN ADP ADJ NOUN NOUN VERB DET ADV ADJ NOUN ADP DET NOUN VERB PUNCT PRON ADJ NOUN NOUN VERB DET ADJ NOUN ADP DET NOUN ADP PROPN CCONJ PROPN PROPN VERB NOUN ADP DET ADJ SPACE NOUN NOUN PUNCT ADP ADJ PROPN PUNCT PRON NOUN AUX ADP NOUN ADP DET NOUN SPACE ADP DET NOUN SCONJ VERB ADJ NOUN CCONJ VERB SPACE ADJ NOUN ADP DET ADJ VERB NOUN PUNCT PRON VERB ADJ NOUN ADP PROPN PROPN PUNCT PRON VERB NOUN ADP DET NOUN NOUN ADP NOUN NUM PUNCT PRON SPACE VERB ADJ NOUN ADP DET NOUN CCONJ ADP DET ADJ PROPN NUM PUNCT SCONJ SPACE PRON VERB ADP NOUN NUM PUNCT DET NOUN VERB DET NOUN PRON AUX SPACE VERB ADP PRON NOUN PUNCT,0.5647058823529412,19.615384615384617,5.074509803921568
187,101,Hugo Touvron,"[' Image classification now achieves a performance\nthat meets many application needs [27, 37, 54]. In\npractice however, the dataset and labels available at\ntraining time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for\nfine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to\nfind enough images of rare classes, and annotating\nthem precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states\nthat: “Manually labeling a large number of images with\nthe presence or absence of 19,794 different classes is not\nfeasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases\ndue to the assisting algorithm. Being able to get strong\nclassification and image retrieval performance on fine\nconcepts using only coarse labels at training time can\ncircumvents the issue, liberating the data collection\nprocess from the quirks of a rigid fine-grained taxonomy.', 'In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:\nGiven a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their\nfine-grained semantic similarity to a new query image\noutside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we\nuse a non-parametric kNN classifier [61] for on-the-fly\nclassification, i.e. without training on the fine-grained\nlabels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build\nupon recent works [3, 62] that exploits two losses to\naddress both image classification and instance recognition, leveraging the “free” annotations provided by\nmultiple data augmentations of a same instance, in the\nspirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly\ninfer coarse labels even when classifying for a finer\ngranularity. For this purpose, we propose a simple\nmethod that exploits both a coarse classifier and image\nembeddings to improve fine-grained category-level\nretrieval.', 'This strategy outperforms existing works\nthat exploit coarse labels at training time but do not\nexplicitly rely on them when retrieving finer-grained\nconcepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following\ncontributions:\n• We propose a method that learns a representation\nat a finer granularity than the one offered by the\nannotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is\nstill +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the\ncoarse level. A byproduct of our study is a very\nstrong kNN-classifier on Imagenet: Grafit with\nResNet-50 trunk reaches 79.6% top-1 accuracy at\nresolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates\nbetter at a finer granularity. Everything being\nequal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the\nart on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],\nFood101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method\nin Section 3. Section 4 compares our approach against\nbaselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one\nlearned by a vanilla cross-entropy loss. Appendix B\ncomplements our experimental section 4 with more\ndetailed results. Appendix C provides visual results\nassociated with different levels of training/testing\ngranularities.']",intro_chunked," Image classification now achieves a performance
that meets many application needs [27, 37, 54]. In
practice however, the dataset and labels available at
training time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for
fine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to
find enough images of rare classes, and annotating
them precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states
that: “Manually labeling a large number of images with
the presence or absence of 19,794 different classes is not
feasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases
due to the assisting algorithm. Being able to get strong
classification and image retrieval performance on fine
concepts using only coarse labels at training time can
circumvents the issue, liberating the data collection
process from the quirks of a rigid fine-grained taxonomy.",24.303572884811445,30.655460127574653,187,0.41292139887809753," Image classification now achieves a performance 
 that meets many application needs. In 
 practice however, the dataset and labels available at 
 training time do not necessarily correspond to those needed in subsequent applications. The granularity of the training time concepts may not suffice for 
 fine grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine grained classification datasets have been developed for specific domains, for instance to distinguish different plants or bird species. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to 
 find enough images of rare classes, and annotating 
 them precisely requires domain specialists with indomain expertise. This is evidenced by the Propname Propname construction annotation protocol that states 
 that: Manually labeling a large number of images with 
 the presence or absence of 00,000 different classes is not 
 feasible. For this reason they resorted to computerassisted annotation, at the risk of introducing biases 
 due to the assisting Propname. Being able to get strong 
 classification and image retrieval performance on fine 
 concepts using only coarse labels at training time can 
 circumvents the issue, liberating the data collection 
 process from the quirks of a rigid fine grained taxonomy."," Image classification now achieves a performance 
 that meets many application needs. In 
 practice however, the dataset and labels available at 
 training time do not necessarily correspond to those needed in subsequent applications. The granularity of the training time concepts may not suffice for 
 fine grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine grained classification datasets have been developed for specific domains, for instance to distinguish different plants or bird species. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to 
 find enough images of rare classes, and annotating 
 them precisely requires domain specialists with indomain expertise. This is evidenced by the Propname Propname construction annotation protocol that states 
 that: Manually labeling a large number of images with 
 the presence or absence of 00,000 different classes is not 
 feasible. For this reason they resorted to computerassisted annotation, at the risk of introducing biases 
 due to the assisting Propname. Being able to get strong 
 classification and image retrieval performance on fine 
 concepts using only coarse labels at training time can 
 circumvents the issue, liberating the data collection 
 process from the quirks of a rigid fine grained taxonomy.", NOUN NOUN ADV VERB DET NOUN SPACE PRON VERB ADJ NOUN NOUN PUNCT ADP SPACE NOUN ADV PUNCT DET NOUN CCONJ NOUN ADJ ADP SPACE NOUN NOUN AUX PART ADV VERB ADP PRON VERB ADP ADJ NOUN PUNCT DET NOUN ADP DET NOUN NOUN NOUN AUX PART VERB ADP SPACE ADJ VERB ADJ NOUN PUNCT PRON AUX VERB DET NOUN ADP ADJ NOUN VERB DET ADV ADJ NOUN PUNCT ADJ VERB NOUN NOUN AUX AUX VERB ADP ADJ NOUN PUNCT SCONJ NOUN PART VERB ADJ NOUN CCONJ NOUN NOUN PUNCT VERB DET ADV ADJ NOUN ADP VERB NOUN AUX ADJ ADP PRON PUNCT SCONJ PRON VERB PART SPACE VERB ADJ NOUN ADP ADJ NOUN PUNCT CCONJ VERB SPACE PRON ADV VERB NOUN NOUN ADP ADJ NOUN PUNCT PRON AUX VERB ADP DET PROPN PROPN NOUN NOUN NOUN PRON VERB SPACE SCONJ PUNCT ADV VERB DET ADJ NOUN ADP NOUN ADP SPACE DET NOUN CCONJ NOUN ADP NUM ADJ NOUN AUX PART SPACE ADJ PUNCT ADP DET NOUN PRON VERB ADP VERB NOUN PUNCT ADP DET NOUN ADP VERB NOUN SPACE ADP ADP DET NOUN PROPN PUNCT AUX ADJ PART VERB ADJ SPACE NOUN CCONJ NOUN NOUN NOUN ADP ADJ SPACE NOUN VERB ADV ADJ NOUN ADP NOUN NOUN AUX SPACE VERB DET NOUN PUNCT VERB DET NOUN NOUN SPACE NOUN ADP DET NOUN ADP DET ADJ ADJ ADJ NOUN PUNCT,0.6435185185185185,24.0,5.467592592592593
188,102,Hugo Touvron,"[' Image classification now achieves a performance\nthat meets many application needs [27, 37, 54]. In\npractice however, the dataset and labels available at\ntraining time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for\nfine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to\nfind enough images of rare classes, and annotating\nthem precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states\nthat: “Manually labeling a large number of images with\nthe presence or absence of 19,794 different classes is not\nfeasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases\ndue to the assisting algorithm. Being able to get strong\nclassification and image retrieval performance on fine\nconcepts using only coarse labels at training time can\ncircumvents the issue, liberating the data collection\nprocess from the quirks of a rigid fine-grained taxonomy.', 'In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:\nGiven a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their\nfine-grained semantic similarity to a new query image\noutside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we\nuse a non-parametric kNN classifier [61] for on-the-fly\nclassification, i.e. without training on the fine-grained\nlabels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build\nupon recent works [3, 62] that exploits two losses to\naddress both image classification and instance recognition, leveraging the “free” annotations provided by\nmultiple data augmentations of a same instance, in the\nspirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly\ninfer coarse labels even when classifying for a finer\ngranularity. For this purpose, we propose a simple\nmethod that exploits both a coarse classifier and image\nembeddings to improve fine-grained category-level\nretrieval.', 'This strategy outperforms existing works\nthat exploit coarse labels at training time but do not\nexplicitly rely on them when retrieving finer-grained\nconcepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following\ncontributions:\n• We propose a method that learns a representation\nat a finer granularity than the one offered by the\nannotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is\nstill +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the\ncoarse level. A byproduct of our study is a very\nstrong kNN-classifier on Imagenet: Grafit with\nResNet-50 trunk reaches 79.6% top-1 accuracy at\nresolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates\nbetter at a finer granularity. Everything being\nequal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the\nart on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],\nFood101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method\nin Section 3. Section 4 compares our approach against\nbaselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one\nlearned by a vanilla cross-entropy loss. Appendix B\ncomplements our experimental section 4 with more\ndetailed results. Appendix C provides visual results\nassociated with different levels of training/testing\ngranularities.']",intro_chunked,"In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:
Given a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their
fine-grained semantic similarity to a new query image
outside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we
use a non-parametric kNN classifier [61] for on-the-fly
classification, i.e. without training on the fine-grained
labels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build
upon recent works [3, 62] that exploits two losses to
address both image classification and instance recognition, leveraging the “free” annotations provided by
multiple data augmentations of a same instance, in the
spirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly
infer coarse labels even when classifying for a finer
granularity. For this purpose, we propose a simple
method that exploits both a coarse classifier and image
embeddings to improve fine-grained category-level
retrieval.",30.900000000000034,30.655460127574653,188,0.4835187792778015," In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases: 
 Given a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their 
 fine grained semantic similarity to a new query image 
 outside the collection, as illustrated by Figure 0. For this task the finegrained labels are available at test time only, and we 
 use a non parametric kNN classifier for on the fly 
 classification, ie without training on the fine grained 
 labels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build 
 upon recent works that exploits two losses to 
 address both image classification and instance recognition, leveraging the free annotations provided by 
 multiple data augmentations of a same instance, in the 
 spirit of self supervised learning. The second intuition is that it is best to explicitly 
 infer coarse labels even when classifying for a finer 
 granularity. For this purpose, we propose a simple 
 method that exploits both a coarse classifier and image 
 embeddings to improve fine grained category level 
 retrieval."," In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases: 
 Given a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their 
 fine grained semantic similarity to a new query image 
 outside the collection, as illustrated by Figure 0. For this task the finegrained labels are available at test time only, and we 
 use a non parametric kNN classifier for on the fly 
 classification, ie without training on the fine grained 
 labels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build 
 upon recent works that exploits two losses to 
 address both image classification and instance recognition, leveraging the free annotations provided by 
 multiple data augmentations of a same instance, in the 
 spirit of self supervised learning. The second intuition is that it is best to explicitly 
 infer coarse labels even when classifying for a finer 
 granularity. For this purpose, we propose a simple 
 method that exploits both a coarse classifier and image 
 embeddings to improve fine grained category level 
 retrieval.", ADP DET NOUN PUNCT PRON NOUN AUX PART VERB DET ADJ NOUN ADP DET ADJ ADP NOUN NOUN PUNCT DET NOUN VERB DET VERB NOUN PUNCT SPACE VERB DET NOUN ADP NOUN VERB ADP ADJ NOUN PUNCT ADP DET NOUN NOUN PUNCT PRON VERB ADP VERB DET NOUN VERB ADP PRON SPACE NOUN VERB ADJ NOUN ADP DET ADJ NOUN NOUN SPACE ADP DET NOUN PUNCT SCONJ VERB ADP NOUN NUM PUNCT ADP DET NOUN DET VERB NOUN AUX ADJ ADP NOUN NOUN ADV PUNCT CCONJ PRON SPACE VERB DET ADJ ADJ VERB NOUN ADP ADP DET NOUN SPACE NOUN PUNCT ADV ADP NOUN ADP DET NOUN VERB SPACE NOUN PUNCT PRON NOUN VERB NUM NOUN PUNCT ADV PUNCT ADP NOUN PART VERB DET NOUN ADP DET NOUN VERB ADP NOUN NOUN PUNCT PRON VERB PART VERB DET NOUN ADP ADV DET NOUN PUNCT ADP DET NOUN PUNCT PRON VERB SPACE SCONJ ADJ NOUN PRON VERB NUM NOUN PART SPACE VERB DET NOUN NOUN CCONJ NOUN NOUN PUNCT VERB DET ADJ NOUN VERB ADP SPACE ADJ NOUN NOUN ADP DET ADJ NOUN PUNCT ADP DET SPACE NOUN ADP NOUN VERB NOUN PUNCT DET ADJ NOUN AUX SCONJ PRON AUX ADJ PART ADV SPACE VERB ADJ NOUN ADV SCONJ VERB ADP DET ADJ SPACE NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ SPACE NOUN PRON VERB CCONJ DET ADJ NOUN CCONJ NOUN SPACE NOUN PART VERB ADJ ADJ NOUN NOUN SPACE NOUN PUNCT,0.5764192139737991,28.625,4.799126637554585
189,103,Hugo Touvron,"[' Image classification now achieves a performance\nthat meets many application needs [27, 37, 54]. In\npractice however, the dataset and labels available at\ntraining time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time concepts may not suffice for\nfine-grained downstream tasks. This has encouraged the development of specialized classifiers offering a more precise representation. Fine-grained classification datasets [29] have been developed for specific domains, for instance to distinguish different plants [13] or bird species [59]. Gathering a sufficiently large collection with finegrained labels is difficult by itself, as it requires to\nfind enough images of rare classes, and annotating\nthem precisely requires domain specialists with indomain expertise. This is evidenced by the Open Images construction annotation protocol [38] that states\nthat: “Manually labeling a large number of images with\nthe presence or absence of 19,794 different classes is not\nfeasible”. For this reason they resorted to computerassisted annotation, at the risk of introducing biases\ndue to the assisting algorithm. Being able to get strong\nclassification and image retrieval performance on fine\nconcepts using only coarse labels at training time can\ncircumvents the issue, liberating the data collection\nprocess from the quirks of a rigid fine-grained taxonomy.', 'In this paper, our objective is to learn a finergrained representation than that available at training time. This approach addresses the following usecases:\nGiven a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their\nfine-grained semantic similarity to a new query image\noutside the collection, as illustrated by Figure 1. For this task the finegrained labels are available at test time only, and we\nuse a non-parametric kNN classifier [61] for on-the-fly\nclassification, i.e. without training on the fine-grained\nlabels. Our work leverages two intuitions. First, in order to improve the granularity beyond the one provided by image labels, we need to exploit another signal than just the labels. For this purpose, we build\nupon recent works [3, 62] that exploits two losses to\naddress both image classification and instance recognition, leveraging the “free” annotations provided by\nmultiple data augmentations of a same instance, in the\nspirit of self-supervised learning [6, 9, 10, 25]. The second intuition is that it is best to explicitly\ninfer coarse labels even when classifying for a finer\ngranularity. For this purpose, we propose a simple\nmethod that exploits both a coarse classifier and image\nembeddings to improve fine-grained category-level\nretrieval.', 'This strategy outperforms existing works\nthat exploit coarse labels at training time but do not\nexplicitly rely on them when retrieving finer-grained\nconcepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following\ncontributions:\n• We propose a method that learns a representation\nat a finer granularity than the one offered by the\nannotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is\nstill +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the\ncoarse level. A byproduct of our study is a very\nstrong kNN-classifier on Imagenet: Grafit with\nResNet-50 trunk reaches 79.6% top-1 accuracy at\nresolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates\nbetter at a finer granularity. Everything being\nequal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the\nart on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],\nFood101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method\nin Section 3. Section 4 compares our approach against\nbaselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one\nlearned by a vanilla cross-entropy loss. Appendix B\ncomplements our experimental section 4 with more\ndetailed results. Appendix C provides visual results\nassociated with different levels of training/testing\ngranularities.']",intro_chunked,"This strategy outperforms existing works
that exploit coarse labels at training time but do not
explicitly rely on them when retrieving finer-grained
concepts [61]. In summary, in this context of coarse-to-fine representation learning, our paper makes the following
contributions:
• We propose a method that learns a representation
at a finer granularity than the one offered by the
annotation at training time. It exhibits a significant accuracy improvement on all the coarse-tofine tasks that we consider. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly
classification on ImageNet. This improvement is
still +9.5% w.r.t. our own stronger baseline, everything being equal otherwise. • Our approach performs similarly or better at the
coarse level. A byproduct of our study is a very
strong kNN-classifier on Imagenet: Grafit with
ResNet-50 trunk reaches 79.6% top-1 accuracy at
resolution 224×224. • Grafit improves transfer learning: our experiments show that our representation discriminates
better at a finer granularity. Everything being
equal otherwise, fine-tuning our model for finegrained benchmarks significantly improves the accuracy. • As a result we establish the new state of the
art on five public benchmarks for transfer learning: Oxford Flowers-102 [41], Stanford Cars [35],
Food101 [7], iNaturalist 2018 [30] & 2019 [31]. This paper is organized as follows. After reviewing related works in Section 2, we present our method
in Section 3. Section 4 compares our approach against
baselines on various datasets, and presents an extensive ablation. Section 5 concludes the paper. In the supplemental material, Appendix A summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one
learned by a vanilla cross-entropy loss. Appendix B
complements our experimental section 4 with more
detailed results. Appendix C provides visual results
associated with different levels of training/testing
granularities.",34.58080493432732,30.655460127574653,189,0.6715165376663208," This strategy outperforms existing works 
 that exploit coarse labels at training time but do not 
 explicitly rely on them when retrieving finer grained 
 concepts. In summary, in this context of coarse to fine representation learning, our paper makes the following 
 contributions: We propose a method that learns a representation 
 at a finer granularity than the one offered by the 
 annotation at training time. It exhibits a significant accuracy improvement on all the coarse tofine tasks that we consider. For instance, we improve by 00.0 the top 0 accuracy for on the fly 
 classification on Propname. This improvement is 
 still 0.0 w.r.t. our own stronger baseline, everything being equal otherwise. Our approach performs similarly or better at the 
 coarse level. A byproduct of our study is a very 
 strong kNN classifier on Propname: Propname with 
 ResNet 00 trunk reaches 00.0 top 0 accuracy at 
 resolution 000000. Propname improves transfer learning: our experiments show that our representation discriminates 
 better at a finer granularity. Everything being 
 equal otherwise, fine tuning our model for finegrained benchmarks significantly improves the accuracy. As a result we establish the new state of the 
 art on five public benchmarks for transfer learning: Propname Propname 000, Propname Propname, 
 Propname, Propname 0000 0000. This paper is organized as follows. After reviewing related works in Section 0, we present our method 
 in Section 0. Section 0 compares our approach against 
 baselines on various datasets, and presents an extensive ablation. Section 0 concludes the paper. In the supplemental material, Propname Propname summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one 
 learned by a vanilla cross Propname loss. Propname Propname 
 complements our experimental section 0 with more 
 detailed results. Propname Propname provides visual results 
 associated with different levels of trainingtesting 
 granularities."," This strategy outperforms existing works 
 that exploit coarse labels at training time but do not 
 explicitly rely on them when retrieving finer grained 
 concepts. In summary, in this context of coarse to fine representation learning, our paper makes the following 
 contributions: We propose a method that learns a representation 
 at a finer granularity than the one offered by the 
 annotation at training time. It exhibits a significant accuracy improvement on all the coarse tofine tasks that we consider. For instance, we improve by 00.0 the top 0 accuracy for on the fly 
 classification on Propname. This improvement is 
 still 0.0 w.r.t. our own stronger baseline, everything being equal otherwise. Our approach performs similarly or better at the 
 coarse level. A byproduct of our study is a very 
 strong kNN classifier on Propname: Propname with 
 ResNet 00 trunk reaches 00.0 top 0 accuracy at 
 resolution 000000. Propname improves transfer learning: our experiments show that our representation discriminates 
 better at a finer granularity. Everything being 
 equal otherwise, fine tuning our model for finegrained benchmarks significantly improves the accuracy. As a result we establish the new state of the 
 art on five public benchmarks for transfer learning: Propname Propname 000, Propname Propname, 
 Propname, Propname 0000 0000. This paper is organized as follows. After reviewing related works in Section 0, we present our method 
 in Section 0. Section 0 compares our approach against 
 baselines on various datasets, and presents an extensive ablation. Section 0 concludes the paper. In the supplemental material, Propname Propname summarizes two experiments that show how an instancelevel loss improves the granularity beyond the one 
 learned by a vanilla cross Propname loss. Propname Propname 
 complements our experimental section 0 with more 
 detailed results. Propname Propname provides visual results 
 associated with different levels of trainingtesting 
 granularities.", DET NOUN VERB VERB NOUN SPACE PRON VERB ADJ NOUN ADP NOUN NOUN CCONJ AUX PART SPACE ADV VERB ADP PRON SCONJ VERB NOUN VERB SPACE NOUN PUNCT ADP NOUN PUNCT ADP DET NOUN ADP NOUN ADP ADJ NOUN NOUN PUNCT PRON NOUN VERB DET ADJ SPACE NOUN PUNCT PRON VERB DET NOUN PRON VERB DET NOUN SPACE ADP DET ADJ NOUN ADP DET NOUN VERB ADP DET SPACE NOUN ADP NOUN NOUN PUNCT PRON VERB DET ADJ NOUN NOUN ADP DET DET ADJ NOUN NOUN PRON PRON VERB PUNCT ADP NOUN PUNCT PRON VERB ADP NUM DET ADJ NUM NOUN ADP ADP DET NOUN SPACE NOUN ADP PROPN PUNCT DET NOUN AUX SPACE ADV NUM NOUN PUNCT PRON ADJ ADJ NOUN PUNCT PRON AUX ADJ ADV PUNCT PRON NOUN VERB ADV CCONJ ADJ ADP DET SPACE ADJ NOUN PUNCT DET NOUN ADP PRON NOUN AUX DET ADV SPACE ADJ ADJ NOUN ADP PROPN PUNCT PROPN ADP SPACE NOUN NUM NOUN VERB NUM ADJ NUM NOUN ADP SPACE NOUN NUM PUNCT PROPN VERB NOUN VERB PUNCT PRON NOUN VERB SCONJ PRON NOUN VERB SPACE ADV ADP DET ADJ NOUN PUNCT PRON AUX SPACE ADJ ADV PUNCT ADJ VERB PRON NOUN ADP VERB NOUN ADV VERB DET NOUN PUNCT ADP DET NOUN PRON VERB DET ADJ NOUN ADP DET SPACE NOUN ADP NUM ADJ NOUN ADP NOUN NOUN PUNCT PROPN PROPN NUM PUNCT PROPN PROPN PUNCT SPACE PROPN PUNCT PROPN NUM NUM PUNCT DET NOUN AUX VERB SCONJ VERB PUNCT ADP VERB ADJ NOUN ADP NOUN NUM PUNCT PRON VERB PRON NOUN SPACE ADP NOUN NUM PUNCT NOUN NUM VERB PRON NOUN ADP SPACE NOUN ADP ADJ NOUN PUNCT CCONJ VERB DET ADJ NOUN PUNCT NOUN NUM VERB DET NOUN PUNCT ADP DET ADJ NOUN PUNCT PROPN PROPN VERB NUM NOUN PRON VERB SCONJ DET ADJ NOUN VERB DET NOUN ADP DET NOUN SPACE VERB ADP DET NOUN NOUN PROPN NOUN PUNCT PROPN PROPN SPACE VERB PRON ADJ NOUN NUM ADP ADJ SPACE ADJ NOUN PUNCT PROPN PROPN VERB ADJ NOUN SPACE VERB ADP ADJ NOUN ADP ADJ SPACE NOUN PUNCT,0.5198776758409785,18.166666666666668,5.113149847094801
190,104,Hugo Touvron,"[' Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as\nsupported by the universal approximation theorem [15, 31, 12]. More precisely, the theorem states that stacking a number of\nbasic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions\non the non-linear basic blocks. Studies on non-linear complex holomorphic functions involved in escape-time fractals showed that iterating simple non-linear\nfunctions can also construct arbitrarily complex landscapes [2]. These functions are complex in the sense that their iso-surfaces\nare made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting\nfunction. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes. Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates\na single building block in the latent space of an auto-encoder. We focus on image translation tasks, that can be trained from\neither paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods [5, 41, 32].', 'In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and\nthe other for output domain B. Therefore we do not have access to any parallel data [8], which is a realistic scenario in\nmany applications, e.g., image restoration. We train a function FORMULA, such that the output b is\nindiscernible from images of B. Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this\ncompositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and\nstyle transfer, where the user may want to select the best rendering. This “Powers of layers” (PoL) mechanism is illustrated in\nFigure 1 in the category transfer context (horse to zebra). Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it\nsuitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend\nof current state-of-the-art works to specialize the architecture and to increase its complexity and number of parameters [10, 40, 7]. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all\nthings being equal otherwise, for the original set of image-to-image translation tasks proposed in their papers, as well as for\ndenoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive\nresults confirmed by objective and psycho-visual metrics, illustrated by visualizations.']",intro_chunked," Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as
supported by the universal approximation theorem [15, 31, 12]. More precisely, the theorem states that stacking a number of
basic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions
on the non-linear basic blocks. Studies on non-linear complex holomorphic functions involved in escape-time fractals showed that iterating simple non-linear
functions can also construct arbitrarily complex landscapes [2]. These functions are complex in the sense that their iso-surfaces
are made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting
function. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes. Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates
a single building block in the latent space of an auto-encoder. We focus on image translation tasks, that can be trained from
either paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods [5, 41, 32].",30.98600000000002,30.655460127574653,190,0.42331409454345703," Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as 
 supported by the universal approximation theorem. More precisely, the theorem states that stacking a number of 
 basic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions 
 on the non linear basic blocks. Studies on non linear complex holomorphic functions involved in escape time fractals showed that iterating simple non linear 
 functions can also construct arbitrarily complex landscapes. These functions are complex in the sense that their iso surfaces 
 are made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting 
 function. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes. Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates 
 a single building block in the latent space of an auto encoder. We focus on image translation tasks, that can be trained from 
 either paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods."," Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as 
 supported by the universal approximation theorem. More precisely, the theorem states that stacking a number of 
 basic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions 
 on the non linear basic blocks. Studies on non linear complex holomorphic functions involved in escape time fractals showed that iterating simple non linear 
 functions can also construct arbitrarily complex landscapes. These functions are complex in the sense that their iso surfaces 
 are made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting 
 function. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes. Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates 
 a single building block in the latent space of an auto encoder. We focus on image translation tasks, that can be trained from 
 either paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods.", ADJ NOUN VERB ADV ADJ NOUN VERB ADP ADJ CCONJ ADJ NOUN ADP VERB NOUN PUNCT SCONJ SPACE VERB ADP DET ADJ NOUN VERB PUNCT ADV ADV PUNCT DET ADJ VERB SCONJ VERB DET NOUN ADP SPACE ADJ NOUN AUX VERB DET NOUN ADP ADJ NOUN PUNCT VERB PRON AUX ADJ VERB NOUN PUNCT ADP ADJ NOUN SPACE ADP DET ADJ ADJ ADJ NOUN PUNCT NOUN ADP ADJ ADJ ADJ ADJ NOUN VERB ADP NOUN NOUN NOUN VERB SCONJ VERB ADJ ADJ ADJ SPACE NOUN AUX ADV VERB ADV ADJ NOUN PUNCT DET NOUN AUX ADJ ADP DET NOUN SCONJ PRON NOUN NOUN SPACE AUX VERB ADV ADJ ADP VERB DET NOUN ADP NOUN PUNCT CCONJ PRON VERB DET NOUN ADP DET ADJ NOUN ADP DET VERB SPACE NOUN PUNCT PRON AUX SCONJ ADJ NOUN VERB ADJ NOUN CCONJ ADP ADJ NOUN PART VERB ADJ NOUN PUNCT PRON NOUN AUX PART VERB DET ADJ NOUN ADP DET NOUN PUNCT CCONJ VERB DET NOUN ADP DET NOUN PRON VERB SPACE DET ADJ NOUN NOUN ADP DET ADJ NOUN ADP DET NOUN NOUN PUNCT PRON VERB ADP NOUN NOUN NOUN PUNCT PRON AUX AUX VERB ADP SPACE CCONJ VERB CCONJ ADJ NOUN PUNCT ADP DET VERB NOUN PUNCT NOUN ADP VERB NOUN CCONJ NOUN NOUN AUX VERB ADP NOUN PUNCT PRON VERB DET ADJ NOUN PUNCT CCONJ DET ADJ NOUN AUX ADV VERB ADP DET NOUN PUNCT,0.6079295154185022,22.7,5.127753303964758
191,105,Hugo Touvron,"[' Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as\nsupported by the universal approximation theorem [15, 31, 12]. More precisely, the theorem states that stacking a number of\nbasic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions\non the non-linear basic blocks. Studies on non-linear complex holomorphic functions involved in escape-time fractals showed that iterating simple non-linear\nfunctions can also construct arbitrarily complex landscapes [2]. These functions are complex in the sense that their iso-surfaces\nare made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting\nfunction. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes. Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates\na single building block in the latent space of an auto-encoder. We focus on image translation tasks, that can be trained from\neither paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods [5, 41, 32].', 'In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and\nthe other for output domain B. Therefore we do not have access to any parallel data [8], which is a realistic scenario in\nmany applications, e.g., image restoration. We train a function FORMULA, such that the output b is\nindiscernible from images of B. Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this\ncompositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and\nstyle transfer, where the user may want to select the best rendering. This “Powers of layers” (PoL) mechanism is illustrated in\nFigure 1 in the category transfer context (horse to zebra). Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it\nsuitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend\nof current state-of-the-art works to specialize the architecture and to increase its complexity and number of parameters [10, 40, 7]. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all\nthings being equal otherwise, for the original set of image-to-image translation tasks proposed in their papers, as well as for\ndenoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive\nresults confirmed by objective and psycho-visual metrics, illustrated by visualizations.']",intro_chunked,"In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and
the other for output domain B. Therefore we do not have access to any parallel data [8], which is a realistic scenario in
many applications, e.g., image restoration. We train a function FORMULA, such that the output b is
indiscernible from images of B. Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this
compositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and
style transfer, where the user may want to select the best rendering. This “Powers of layers” (PoL) mechanism is illustrated in
Figure 1 in the category transfer context (horse to zebra). Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it
suitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend
of current state-of-the-art works to specialize the architecture and to increase its complexity and number of parameters [10, 40, 7]. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all
things being equal otherwise, for the original set of image-to-image translation tasks proposed in their papers, as well as for
denoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive
results confirmed by objective and psycho-visual metrics, illustrated by visualizations.",32.74553571428575,30.655460127574653,191,0.5663625597953796," In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and 
 the other for output domain Propname Therefore we do not have access to any parallel data, which is a realistic scenario in 
 many applications, Propname, image restoration. We train a function FORMULA, such that the output b is 
 indiscernible from images of Propname Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this 
 compositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and 
 style transfer, where the user may want to select the best rendering. This Propname of layers mechanism is illustrated in 
 Propname 0 in the category transfer context. Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it 
 suitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend 
 of current state of the art works to specialize the architecture and to increase its complexity and number of parameters. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all 
 things being equal otherwise, for the original set of image to image translation tasks proposed in their papers, as well as for 
 denoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive 
 results confirmed by objective and psycho visual metrics, illustrated by visualizations."," In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and 
 the other for output domain Propname Therefore we do not have access to any parallel data, which is a realistic scenario in 
 many applications, Propname, image restoration. We train a function FORMULA, such that the output b is 
 indiscernible from images of Propname Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this 
 compositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and 
 style transfer, where the user may want to select the best rendering. This Propname of layers mechanism is illustrated in 
 Propname 0 in the category transfer context. Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it 
 suitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend 
 of current state of the art works to specialize the architecture and to increase its complexity and number of parameters. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all 
 things being equal otherwise, for the original set of image to image translation tasks proposed in their papers, as well as for 
 denoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive 
 results confirmed by objective and psycho visual metrics, illustrated by visualizations.", ADP DET NOUN PRON VERB ADP DET ADJ NOUN PUNCT ADV NUM NOUN ADP NOUN AUX VERB PUNCT NUM ADP DET NOUN NOUN NOUN CCONJ SPACE DET ADJ ADP NOUN NOUN PROPN ADV PRON AUX PART VERB NOUN ADP DET ADJ NOUN PUNCT PRON AUX DET ADJ NOUN ADP SPACE ADJ NOUN PUNCT PROPN PUNCT NOUN NOUN PUNCT PRON VERB DET NOUN NOUN PUNCT ADJ SCONJ DET NOUN NOUN AUX SPACE ADJ ADP NOUN ADP PROPN PRON NOUN AUX VERB ADP DET ADJ ADJ NOUN PRON AUX VERB DET ADJ NOUN ADP NOUN PUNCT PRON VERB DET SPACE ADJ NOUN NOUN ADP DET ADJ VERB NOUN PRON VERB SCONJ DET NOUN AUX ADJ ADP DET ADJ NOUN ADP NOUN PUNCT ADP DET NOUN PUNCT PRON AUX VERB DET NOUN ADP DET NOUN ADP VERB DET NOUN ADP NOUN DET NOUN AUX VERB PUNCT PRON AUX ADP ADJ NOUN ADP NOUN NOUN NOUN ADJ ADP NOUN PUNCT SCONJ DET NOUN NOUN AUX ADJ ADP NOUN NOUN PUNCT CCONJ SPACE NOUN NOUN PUNCT SCONJ DET NOUN AUX VERB PART VERB DET ADJ NOUN PUNCT DET PROPN ADP NOUN NOUN AUX VERB ADP SPACE PROPN NUM ADP DET NOUN NOUN NOUN PUNCT PRON NOUN AUX ADV ADJ CCONJ ADV DET NOUN ADP DET ADJ NOUN VERB VERB ADP DET NOUN PUNCT PRON VERB PRON SPACE ADJ PART VERB DET ADJ NOUN ADP NOUN ADP DET ADJ NOUN ADP NOUN PUNCT DET NOUN AUX ADP ADJ NOUN ADP DET NOUN SPACE ADP ADJ NOUN ADP DET NOUN VERB PART VERB DET NOUN CCONJ PART VERB PRON NOUN CCONJ NOUN ADP NOUN PUNCT SCONJ PRON NOUN PUNCT PRON NOUN ADP NOUN NOUN ADJ CCONJ ADJ NOUN ADP DET NOUN NOUN NOUN PUNCT DET SPACE NOUN AUX ADJ ADV PUNCT ADP DET ADJ NOUN ADP NOUN ADP NOUN NOUN NOUN VERB ADP PRON NOUN PUNCT ADV ADV ADP ADP SPACE NOUN PUNCT VERB CCONJ VERB PUNCT ADP ADV ADJ NOUN CCONJ DET ADJ NOUN PUNCT PRON VERB ADJ SPACE NOUN VERB ADP ADJ CCONJ NOUN ADJ NOUN PUNCT VERB ADP NOUN PUNCT,0.5313432835820896,33.5,4.764179104477612
192,106,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.', 'This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]']",intro_chunked," Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].",45.31640350877194,30.655460127574653,192,0.25457170605659485," Propname Propname Propname are used extensively in computer vision tasks such as image classification, object detection, inpainting, style transfer and even image compression. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Propname of Propname, is then resized to obtain a crop of a fixed size that is fed to the Propname. At test time, the Propname is instead set to a square covering the central part of the image, which results in the extraction of a so called center crop. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different Propname, which skews the distribution of data seen by the Propname. Over the years, training and testing pre processing procedures have evolved to improve the performance of Propname, but so far they have been optimized separately."," Propname Propname Propname are used extensively in computer vision tasks such as image classification, object detection, inpainting, style transfer and even image compression. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Propname of Propname, is then resized to obtain a crop of a fixed size that is fed to the Propname. At test time, the Propname is instead set to a square covering the central part of the image, which results in the extraction of a so called center crop. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different Propname, which skews the distribution of data seen by the Propname. Over the years, training and testing pre processing procedures have evolved to improve the performance of Propname, but so far they have been optimized separately.", PROPN PROPN PROPN AUX VERB ADV ADP NOUN NOUN NOUN ADJ ADP NOUN NOUN PUNCT NOUN NOUN PUNCT VERB PUNCT NOUN NOUN CCONJ ADV NOUN NOUN PUNCT ADP NOUN PART VERB DET ADJ ADJ NOUN ADP DET NOUN PUNCT DET NOUN CCONJ NOUN NOUN NOUN AUX VERB PUNCT ADV PUNCT ADV NOUN X NOUN NOUN AUX ADJ ADP NOUN CCONJ NOUN PUNCT ADP NOUN PUNCT ADP NOUN NOUN DET ADJ ADJ NOUN NOUN AUX PART VERB DET NOUN ADP ADJ NOUN ADP DET NOUN PUNCT PRON ADV VERB DET NOUN ADP NOUN NOUN PUNCT DET NOUN PUNCT PRON PRON VERB DET PROPN ADP PROPN PUNCT AUX ADV VERB PART VERB DET NOUN ADP DET VERB NOUN PRON AUX VERB ADP DET PROPN PUNCT ADP NOUN NOUN PUNCT DET PROPN AUX ADV VERB ADP DET NOUN VERB DET ADJ NOUN ADP DET NOUN PUNCT PRON VERB ADP DET NOUN ADP DET ADV VERB NOUN NOUN PUNCT PRON VERB DET NOUN ADP NOUN PRON VERB ADJ ADJ ADJ NOUN PUNCT ADV PUNCT SCONJ DET NOUN VERB ADP NOUN CCONJ NOUN NOUN VERB DET ADJ NOUN PUNCT PRON VERB ADP ADJ PROPN PUNCT PRON VERB DET NOUN ADP NOUN VERB ADP DET PROPN PUNCT ADP DET NOUN PUNCT NOUN CCONJ NOUN NOUN NOUN NOUN AUX VERB PART VERB DET NOUN ADP PROPN PUNCT CCONJ ADV ADV PRON AUX AUX VERB ADV PUNCT,0.5152838427947598,25.444444444444443,4.724890829694323
193,107,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.', 'This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]']",intro_chunked,"In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.",43.135532635467996,30.655460127574653,193,0.418020635843277," In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 0Update: Since the publication of this paper at Propname, we have improved this state of the art by applying our method to Propname. See our note for results and details. with a detrimental effect on the test time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same Propname sampling. Our strategy only requires to fine tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the Propname at training time. This analysis also shows that we need to use lower resolution crops at training than at test time."," In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 0Update: Since the publication of this paper at Propname, we have improved this state of the art by applying our method to Propname. See our note for results and details. with a detrimental effect on the test time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same Propname sampling. Our strategy only requires to fine tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the Propname at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.", ADP DET NOUN PUNCT PRON ADV VERB SCONJ DET ADJ NOUN AUX VERB ADP DET ADJ NOUN NOUN ADP NOUN CCONJ NOUN NOUN NOUN PUNCT SCONJ DET NOUN ADP DET NOUN ADP PROPN PUNCT PRON AUX VERB DET NOUN ADP DET NOUN ADP VERB PRON NOUN ADP PROPN PUNCT VERB PRON NOUN ADP NOUN CCONJ NOUN PUNCT ADP DET ADJ NOUN ADP DET NOUN NOUN NOUN ADP NOUN PUNCT PRON ADV VERB SCONJ DET NOUN AUX AUX VERB ADP ADV VERB DET NOUN ADP NOUN CCONJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT SCONJ VERB DET ADJ PROPN NOUN PUNCT PRON NOUN ADV VERB ADP ADJ NOUN NUM NOUN ADP NOUN PART VERB ADP DET NOUN ADP NOUN VERB ADP DET VERB DET NOUN NOUN PUNCT PRON VERB PRON PART VERB DET NOUN ADP VERB ADJ NOUN NOUN ADP NOUN CCONJ NOUN PUNCT VERB VERB DET NOUN NOUN PUNCT SCONJ VERB ADP DET NOUN NOUN PUNCT PRON NOUN AUX VERB ADP DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN ADP DET NOUN ADP ADJ NOUN PUNCT PRON VERB SCONJ VERB DET NOUN ADP DET NOUN VERB ADP NOUN NOUN VERB ADP ADV VERB DET PROPN ADP NOUN NOUN PUNCT DET NOUN ADV VERB SCONJ PRON VERB PART VERB ADJ NOUN NOUN ADP NOUN ADP ADP NOUN NOUN PUNCT,0.5296803652968036,27.375,4.675799086757991
194,108,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes 1Update: Since the publication of this paper at Neurips, we have improved this state of the art by applying our method to EfficientNet. See our note [39] for results and details. with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time.', 'This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]']",intro_chunked,"This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs. For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15]",26.359122103386852,30.655460127574653,194,0.6241979002952576," This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical Propname, which is especially important for training on GPUs. For instance, for a target test resolution of 000000, training at resolution 000000 provides better results than the standard practice of training at resolution 000000, while being more efficient. In addition we can adapt a ResNet 00 train at resolution 000000 for the test resolution 000000 and thus obtain top 0 accuracy of 00.0 on Propname. Alternatively, we leverage the improved efficiency to train high accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top 0 accuracy of 00.0 on Propname with a Propname 000 00x00d pre trained in weakly supervised fashion on 000 million public images. Finally, our method makes it possible to save Propname memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance"," This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical Propname, which is especially important for training on GPUs. For instance, for a target test resolution of 000000, training at resolution 000000 provides better results than the standard practice of training at resolution 000000, while being more efficient. In addition we can adapt a ResNet 00 train at resolution 000000 for the test resolution 000000 and thus obtain top 0 accuracy of 00.0 on Propname. Alternatively, we leverage the improved efficiency to train high accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top 0 accuracy of 00.0 on Propname with a Propname 000 00x00d pre trained in weakly supervised fashion on 000 million public images. Finally, our method makes it possible to save Propname memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance", PRON ADV VERB DET NOUN NOUN PUNCT VERB DET NOUN NOUN VERB ADP DET ADJ NOUN ADP DET NOUN NOUN NOUN CCONJ VERB ADV DET NOUN NOUN ADP DET ADJ PROPN PUNCT PRON AUX ADV ADJ ADP NOUN ADP NOUN PUNCT ADP NOUN PUNCT ADP DET NOUN NOUN NOUN ADP NUM PUNCT NOUN ADP NOUN NUM VERB ADJ NOUN ADP DET ADJ NOUN ADP NOUN ADP NOUN NUM PUNCT SCONJ AUX ADV ADJ PUNCT ADP NOUN PRON AUX VERB DET NOUN NUM NOUN ADP NOUN NUM ADP DET NOUN NOUN NUM CCONJ ADV VERB ADJ NUM NOUN ADP NUM ADP PROPN PUNCT ADV PUNCT PRON VERB DET VERB NOUN PART VERB ADJ NOUN NOUN PRON VERB ADP ADV ADJ NOUN ADP NOUN NOUN SCONJ ADV VERB ADV PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NUM NOUN ADP NUM ADP PROPN ADP DET PROPN NUM X VERB VERB ADP ADJ ADJ NOUN ADP NUM NUM ADJ NOUN PUNCT ADV PUNCT PRON NOUN VERB PRON ADJ PART VERB PROPN NOUN PUNCT PRON AUX ADP NOUN AUX VERB ADP NOUN PUNCT VERB ADJ NOUN NOUN ADV VERB ADP DET ADJ ADJ NOUN,0.5842105263157895,31.666666666666668,4.963157894736842
195,109,Hugo Touvron,"[' The growing availability of large neuroimaging databases offers a unique opportunity to collect considerable amount of\ndata to train artificial intelligence algorithms. A side effect\nis that it inevitably leads to the introduction of new biases related especially to heterogeneous protocol guidelines in terms\nof subject enrollment and acquisition parameters [1, 2]. In\nthis paper, we investigate several strategies to account for data\ncoming from different databases. This is exemplified through\nthe design of a CAD system that discriminates healthy from\nAlzheimer’s Disease (AD) subjects based on volumetric characteristics derived from structural MRI. A nuisance variable is an external factor which is not of\ninterest for the study, but which causes an increase in variability within groups. In our case, nuisance variables can be\ndatabase, age, gender and Estimated Total Intracranial Volume (ETIV). A particular subcategory of nuisance variables\nare the confounding variables (or confounds) which also have\na direct effect on the target variable (i.e., the diagnosis for a\nCAD system). For instance, gender is a confounding variable\nfor AD diagnosis since almost two-thirds of the individuals diagnosed with AD are women [3]. We also define the concept of artificial confound, which is a nuisance variable that\nshould theoretically not have an effect on the target variable,\nbut which in practice has one because it has been sampled\nnon uniformly.', 'For instance, the database should not theoretically have an impact on AD diagnosis, but since the ratio\nof AD vs healthy subjects can be very different according to\nthe database, an artificial link is created. Note that such artificial confounds are as far as possible avoided in prospective\nstudy by collecting data stratified into groups that have the\nsame distribution for the confounds. Nonetheless, it may frequently occur when merging several databases that may have\nheterogeneous recruitment schemes in terms of age distribution, gender ratio, or pathology prevalence. Particular attention should be paid to both natural and artificial confounds since they can introduce a positive bias into\nthe performance of a classifier and can lead to misinterpret\nits behavior. Several papers already addressed how to handle confounds such as age and gender in neuroimaging study\n[4, 5, 6]. Here, we investigate more specifically how to account for data coming from multiple databases and the interplay that occurs with other confounds such as age and gender. First, we present experiments on simulated data that illustrate\nhow interactions with other confounds such as age can make\nthe handling of data coming from multiple databases an intricate problem. Then, we compare three strategies to adjust\ndata and evaluate them in the real scenario of a CAD system\ndedicated to Alzheimer’s disease.']",intro_chunked," The growing availability of large neuroimaging databases offers a unique opportunity to collect considerable amount of
data to train artificial intelligence algorithms. A side effect
is that it inevitably leads to the introduction of new biases related especially to heterogeneous protocol guidelines in terms
of subject enrollment and acquisition parameters [1, 2]. In
this paper, we investigate several strategies to account for data
coming from different databases. This is exemplified through
the design of a CAD system that discriminates healthy from
Alzheimer’s Disease (AD) subjects based on volumetric characteristics derived from structural MRI. A nuisance variable is an external factor which is not of
interest for the study, but which causes an increase in variability within groups. In our case, nuisance variables can be
database, age, gender and Estimated Total Intracranial Volume (ETIV). A particular subcategory of nuisance variables
are the confounding variables (or confounds) which also have
a direct effect on the target variable (i.e., the diagnosis for a
CAD system). For instance, gender is a confounding variable
for AD diagnosis since almost two-thirds of the individuals diagnosed with AD are women [3]. We also define the concept of artificial confound, which is a nuisance variable that
should theoretically not have an effect on the target variable,
but which in practice has one because it has been sampled
non uniformly.",23.05324561403509,30.655460127574653,195,0.24127958714962006," The growing availability of large neuroimaging databases offers a unique opportunity to collect considerable amount of 
 data to train artificial intelligence algorithms. A side effect 
 is that it inevitably leads to the introduction of new biases related especially to heterogeneous protocol guidelines in terms 
 of subject enrollment and acquisition parameters. In 
 this paper, we investigate several strategies to account for data 
 coming from different databases. This is exemplified through 
 the design of a Propname system that discriminates healthy from 
 Propname Propname subjects based on volumetric characteristics derived from structural Propname. A nuisance variable is an external factor which is not of 
 interest for the study, but which causes an increase in variability within groups. In our case, nuisance variables can be 
 database, age, gender and Propname Propname Propname Propname. A particular subcategory of nuisance variables 
 are the confounding variables which also have 
 a direct effect on the target variable ie, the diagnosis for a 
 CAD system. For instance, gender is a confounding variable 
 for AD diagnosis since almost two thirds of the individuals diagnosed with AD are women. We also define the concept of artificial confound, which is a nuisance variable that 
 should theoretically not have an effect on the target variable, 
 but which in practice has one because it has been sampled 
 non uniformly."," The growing availability of large neuroimaging databases offers a unique opportunity to collect considerable amount of 
 data to train artificial intelligence algorithms. A side effect 
 is that it inevitably leads to the introduction of new biases related especially to heterogeneous protocol guidelines in terms 
 of subject enrollment and acquisition parameters. In 
 this paper, we investigate several strategies to account for data 
 coming from different databases. This is exemplified through 
 the design of a Propname system that discriminates healthy from 
 Propname Propname subjects based on volumetric characteristics derived from structural Propname. A nuisance variable is an external factor which is not of 
 interest for the study, but which causes an increase in variability within groups. In our case, nuisance variables can be 
 database, age, gender and Propname Propname Propname Propname. A particular subcategory of nuisance variables 
 are the confounding variables which also have 
 a direct effect on the target variable ie, the diagnosis for a 
 CAD system. For instance, gender is a confounding variable 
 for AD diagnosis since almost two thirds of the individuals diagnosed with AD are women. We also define the concept of artificial confound, which is a nuisance variable that 
 should theoretically not have an effect on the target variable, 
 but which in practice has one because it has been sampled 
 non uniformly.", DET VERB NOUN ADP ADJ VERB NOUN VERB DET ADJ NOUN PART VERB ADJ NOUN ADP SPACE NOUN PART VERB ADJ NOUN NOUN PUNCT DET NOUN NOUN SPACE AUX SCONJ PRON ADV VERB ADP DET NOUN ADP ADJ NOUN VERB ADV ADP ADJ NOUN NOUN ADP NOUN SPACE ADP ADJ NOUN CCONJ NOUN NOUN PUNCT ADP SPACE DET NOUN PUNCT PRON VERB ADJ NOUN PART VERB ADP NOUN SPACE VERB ADP ADJ NOUN PUNCT PRON AUX VERB ADP SPACE DET NOUN ADP DET PROPN NOUN PRON VERB ADJ ADP SPACE PROPN PROPN NOUN VERB ADP NOUN NOUN VERB ADP ADJ PROPN PUNCT DET ADJ NOUN AUX DET ADJ NOUN PRON AUX PART ADP SPACE NOUN ADP DET NOUN PUNCT CCONJ PRON VERB DET NOUN ADP NOUN ADP NOUN PUNCT ADP PRON NOUN PUNCT ADJ NOUN AUX AUX SPACE NOUN PUNCT NOUN PUNCT NOUN CCONJ PROPN PROPN PROPN PROPN PUNCT DET ADJ NOUN ADP ADJ NOUN SPACE AUX DET VERB NOUN PRON ADV VERB SPACE DET ADJ NOUN ADP DET NOUN NOUN ADV PUNCT DET NOUN ADP DET SPACE NOUN NOUN PUNCT ADP NOUN PUNCT NOUN AUX DET VERB NOUN SPACE ADP NOUN NOUN SCONJ ADV NUM NOUN ADP DET NOUN VERB ADP NOUN AUX NOUN PUNCT PRON ADV VERB DET NOUN ADP ADJ NOUN PUNCT PRON AUX DET ADJ NOUN PRON SPACE AUX ADV PART VERB DET NOUN ADP DET NOUN NOUN PUNCT SPACE CCONJ PRON ADP NOUN VERB NUM SCONJ PRON AUX AUX VERB SPACE X ADV PUNCT,0.5665236051502146,25.88888888888889,5.171673819742489
196,110,Hugo Touvron,"[' The growing availability of large neuroimaging databases offers a unique opportunity to collect considerable amount of\ndata to train artificial intelligence algorithms. A side effect\nis that it inevitably leads to the introduction of new biases related especially to heterogeneous protocol guidelines in terms\nof subject enrollment and acquisition parameters [1, 2]. In\nthis paper, we investigate several strategies to account for data\ncoming from different databases. This is exemplified through\nthe design of a CAD system that discriminates healthy from\nAlzheimer’s Disease (AD) subjects based on volumetric characteristics derived from structural MRI. A nuisance variable is an external factor which is not of\ninterest for the study, but which causes an increase in variability within groups. In our case, nuisance variables can be\ndatabase, age, gender and Estimated Total Intracranial Volume (ETIV). A particular subcategory of nuisance variables\nare the confounding variables (or confounds) which also have\na direct effect on the target variable (i.e., the diagnosis for a\nCAD system). For instance, gender is a confounding variable\nfor AD diagnosis since almost two-thirds of the individuals diagnosed with AD are women [3]. We also define the concept of artificial confound, which is a nuisance variable that\nshould theoretically not have an effect on the target variable,\nbut which in practice has one because it has been sampled\nnon uniformly.', 'For instance, the database should not theoretically have an impact on AD diagnosis, but since the ratio\nof AD vs healthy subjects can be very different according to\nthe database, an artificial link is created. Note that such artificial confounds are as far as possible avoided in prospective\nstudy by collecting data stratified into groups that have the\nsame distribution for the confounds. Nonetheless, it may frequently occur when merging several databases that may have\nheterogeneous recruitment schemes in terms of age distribution, gender ratio, or pathology prevalence. Particular attention should be paid to both natural and artificial confounds since they can introduce a positive bias into\nthe performance of a classifier and can lead to misinterpret\nits behavior. Several papers already addressed how to handle confounds such as age and gender in neuroimaging study\n[4, 5, 6]. Here, we investigate more specifically how to account for data coming from multiple databases and the interplay that occurs with other confounds such as age and gender. First, we present experiments on simulated data that illustrate\nhow interactions with other confounds such as age can make\nthe handling of data coming from multiple databases an intricate problem. Then, we compare three strategies to adjust\ndata and evaluate them in the real scenario of a CAD system\ndedicated to Alzheimer’s disease.']",intro_chunked,"For instance, the database should not theoretically have an impact on AD diagnosis, but since the ratio
of AD vs healthy subjects can be very different according to
the database, an artificial link is created. Note that such artificial confounds are as far as possible avoided in prospective
study by collecting data stratified into groups that have the
same distribution for the confounds. Nonetheless, it may frequently occur when merging several databases that may have
heterogeneous recruitment schemes in terms of age distribution, gender ratio, or pathology prevalence. Particular attention should be paid to both natural and artificial confounds since they can introduce a positive bias into
the performance of a classifier and can lead to misinterpret
its behavior. Several papers already addressed how to handle confounds such as age and gender in neuroimaging study
[4, 5, 6]. Here, we investigate more specifically how to account for data coming from multiple databases and the interplay that occurs with other confounds such as age and gender. First, we present experiments on simulated data that illustrate
how interactions with other confounds such as age can make
the handling of data coming from multiple databases an intricate problem. Then, we compare three strategies to adjust
data and evaluate them in the real scenario of a CAD system
dedicated to Alzheimer’s disease.",28.141722972972985,30.655460127574653,196,0.20705704391002655," For instance, the database should not theoretically have an impact on AD diagnosis, but since the ratio 
 of AD vs healthy subjects can be very different according to 
 the database, an artificial link is created. Note that such artificial confounds are as far as possible avoided in prospective 
 study by collecting data stratified into groups that have the 
 same distribution for the confounds. Nonetheless, it may frequently occur when merging several databases that may have 
 heterogeneous recruitment schemes in terms of age distribution, gender ratio, or pathology prevalence. Particular attention should be paid to both natural and artificial confounds since they can introduce a positive bias into 
 the performance of a classifier and can lead to misinterpret 
 its behavior. Several papers already addressed how to handle confounds such as age and gender in neuroimaging study 
. Here, we investigate more specifically how to account for data coming from multiple databases and the interplay that occurs with other confounds such as age and gender. First, we present experiments on simulated data that illustrate 
 how interactions with other confounds such as age can make 
 the handling of data coming from multiple databases an intricate problem. Then, we compare three strategies to adjust 
 data and evaluate them in the real scenario of a Propname system 
 dedicated to Propname disease."," For instance, the database should not theoretically have an impact on AD diagnosis, but since the ratio 
 of AD vs healthy subjects can be very different according to 
 the database, an artificial link is created. Note that such artificial confounds are as far as possible avoided in prospective 
 study by collecting data stratified into groups that have the 
 same distribution for the confounds. Nonetheless, it may frequently occur when merging several databases that may have 
 heterogeneous recruitment schemes in terms of age distribution, gender ratio, or pathology prevalence. Particular attention should be paid to both natural and artificial confounds since they can introduce a positive bias into 
 the performance of a classifier and can lead to misinterpret 
 its behavior. Several papers already addressed how to handle confounds such as age and gender in neuroimaging study 
. Here, we investigate more specifically how to account for data coming from multiple databases and the interplay that occurs with other confounds such as age and gender. First, we present experiments on simulated data that illustrate 
 how interactions with other confounds such as age can make 
 the handling of data coming from multiple databases an intricate problem. Then, we compare three strategies to adjust 
 data and evaluate them in the real scenario of a Propname system 
 dedicated to Propname disease.", ADP NOUN PUNCT DET NOUN AUX PART ADV VERB DET NOUN ADP NOUN NOUN PUNCT CCONJ SCONJ DET NOUN SPACE ADP NOUN ADV ADJ NOUN AUX AUX ADV ADJ VERB ADP SPACE DET NOUN PUNCT DET ADJ NOUN AUX VERB PUNCT VERB SCONJ ADJ ADJ NOUN AUX ADV ADV ADP ADJ VERB ADP ADJ SPACE NOUN ADP VERB NOUN VERB ADP NOUN PRON VERB DET SPACE ADJ NOUN ADP DET NOUN PUNCT ADV PUNCT PRON AUX ADV VERB SCONJ VERB ADJ NOUN PRON AUX VERB SPACE ADJ NOUN NOUN ADP NOUN ADP NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ NOUN NOUN PUNCT ADJ NOUN AUX AUX VERB ADP DET ADJ CCONJ ADJ NOUN SCONJ PRON AUX VERB DET ADJ NOUN ADP SPACE DET NOUN ADP DET NOUN CCONJ AUX VERB PART VERB SPACE PRON NOUN PUNCT ADJ NOUN ADV VERB SCONJ PART VERB NOUN ADJ ADP NOUN CCONJ NOUN ADP VERB NOUN SPACE PUNCT ADV PUNCT PRON VERB ADV ADV SCONJ PART VERB ADP NOUN VERB ADP ADJ NOUN CCONJ DET NOUN PRON VERB ADP ADJ NOUN ADJ ADP NOUN CCONJ NOUN PUNCT ADV PUNCT PRON VERB NOUN ADP VERB NOUN PRON VERB SPACE SCONJ NOUN ADP ADJ NOUN ADJ ADP NOUN AUX VERB SPACE DET NOUN ADP NOUN VERB ADP ADJ NOUN DET ADJ NOUN PUNCT ADV PUNCT PRON VERB NUM NOUN PART VERB SPACE NOUN CCONJ VERB PRON ADP DET ADJ NOUN ADP DET PROPN NOUN SPACE VERB ADP PROPN NOUN PUNCT,0.5646551724137931,29.0,5.0344827586206895
197,111,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs.', 'For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].']",intro_chunked," Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].",45.31640350877194,30.655460127574653,197,0.25457170605659485," Propname Propname Propname are used extensively in computer vision tasks such as image classification, object detection, inpainting, style transfer and even image compression. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Propname of Propname, is then resized to obtain a crop of a fixed size that is fed to the Propname. At test time, the Propname is instead set to a square covering the central part of the image, which results in the extraction of a so called center crop. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different Propname, which skews the distribution of data seen by the Propname. Over the years, training and testing pre processing procedures have evolved to improve the performance of Propname, but so far they have been optimized separately."," Propname Propname Propname are used extensively in computer vision tasks such as image classification, object detection, inpainting, style transfer and even image compression. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Propname of Propname, is then resized to obtain a crop of a fixed size that is fed to the Propname. At test time, the Propname is instead set to a square covering the central part of the image, which results in the extraction of a so called center crop. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different Propname, which skews the distribution of data seen by the Propname. Over the years, training and testing pre processing procedures have evolved to improve the performance of Propname, but so far they have been optimized separately.", PROPN PROPN PROPN AUX VERB ADV ADP NOUN NOUN NOUN ADJ ADP NOUN NOUN PUNCT NOUN NOUN PUNCT VERB PUNCT NOUN NOUN CCONJ ADV NOUN NOUN PUNCT ADP NOUN PART VERB DET ADJ ADJ NOUN ADP DET NOUN PUNCT DET NOUN CCONJ NOUN NOUN NOUN AUX VERB PUNCT ADV PUNCT ADV NOUN X NOUN NOUN AUX ADJ ADP NOUN CCONJ NOUN PUNCT ADP NOUN PUNCT ADP NOUN NOUN DET ADJ ADJ NOUN NOUN AUX PART VERB DET NOUN ADP ADJ NOUN ADP DET NOUN PUNCT PRON ADV VERB DET NOUN ADP NOUN NOUN PUNCT DET NOUN PUNCT PRON PRON VERB DET PROPN ADP PROPN PUNCT AUX ADV VERB PART VERB DET NOUN ADP DET VERB NOUN PRON AUX VERB ADP DET PROPN PUNCT ADP NOUN NOUN PUNCT DET PROPN AUX ADV VERB ADP DET NOUN VERB DET ADJ NOUN ADP DET NOUN PUNCT PRON VERB ADP DET NOUN ADP DET ADV VERB NOUN NOUN PUNCT PRON VERB DET NOUN ADP NOUN PRON VERB ADJ ADJ ADJ NOUN PUNCT ADV PUNCT SCONJ DET NOUN VERB ADP NOUN CCONJ NOUN NOUN VERB DET ADJ NOUN PUNCT PRON VERB ADP ADJ PROPN PUNCT PRON VERB DET NOUN ADP NOUN VERB ADP DET PROPN PUNCT ADP DET NOUN PUNCT NOUN CCONJ NOUN NOUN NOUN NOUN AUX VERB PART VERB DET NOUN ADP PROPN PUNCT CCONJ ADV ADV PRON AUX AUX VERB ADV PUNCT,0.5152838427947598,25.444444444444443,4.724890829694323
198,112,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs.', 'For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].']",intro_chunked,"In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs.",31.878846153846183,30.655460127574653,198,0.40401047468185425," In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same Propname sampling. Our strategy only requires to fine tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the Propname at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical Propname, which is especially important for training on GPUs."," In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same Propname sampling. Our strategy only requires to fine tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the Propname at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical Propname, which is especially important for training on GPUs.", ADP DET NOUN PUNCT PRON ADV VERB SCONJ DET ADJ NOUN AUX VERB ADP DET ADJ NOUN NOUN ADP NOUN CCONJ NOUN NOUN ADP DET ADJ NOUN ADP DET NOUN NOUN NOUN ADP NOUN PUNCT PRON ADV VERB SCONJ DET NOUN AUX AUX VERB ADP ADV VERB DET NOUN ADP NOUN CCONJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT SCONJ VERB DET ADJ PROPN NOUN PUNCT PRON NOUN ADV VERB ADP ADJ NOUN NUM NOUN ADP NOUN PART VERB ADP DET NOUN ADP NOUN VERB ADP DET VERB DET NOUN NOUN PUNCT PRON VERB PRON PART VERB DET NOUN ADP VERB ADJ NOUN NOUN ADP NOUN CCONJ NOUN PUNCT VERB VERB DET NOUN NOUN PUNCT SCONJ VERB ADP DET NOUN NOUN PUNCT PRON NOUN AUX VERB ADP DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN ADP DET NOUN ADP ADJ NOUN PUNCT PRON VERB SCONJ VERB DET NOUN ADP DET NOUN VERB ADP NOUN NOUN VERB ADP ADV VERB DET PROPN ADP NOUN NOUN PUNCT DET NOUN ADV VERB SCONJ PRON VERB PART VERB ADJ NOUN NOUN ADP NOUN ADP ADP NOUN NOUN PUNCT PRON ADV VERB DET NOUN NOUN PUNCT VERB DET NOUN NOUN VERB ADP DET ADJ NOUN ADP DET NOUN NOUN NOUN CCONJ VERB ADV DET NOUN NOUN ADP DET ADJ PROPN PUNCT PRON AUX ADV ADJ ADP NOUN ADP NOUN PUNCT,0.5221238938053098,32.285714285714285,4.907079646017699
199,113,Hugo Touvron,"[' Convolutional Neural Networks [21] (CNNs) are used extensively in computer vision tasks such as image classification [20], object detection [30], inpainting [42], style transfer [11] and even image compression [31]. In order to obtain the best possible performance from these models, the training and testing data distributions should match. However, often data pre-processing procedures are different for training and testing. For instance, in image recognition the current best training practice is to extract a rectangle with random coordinates from the image, which artificially increases the amount of training data. This region, which we call the Region of Classification (RoC), is then resized to obtain a crop of a fixed size (in pixels) that is fed to the CNN. At test time, the RoC is instead set to a square covering the central part of the image, which results in the extraction of a so called “center crop”. This reflects the bias of photographers who tend center important visual content. Thus, while the crops extracted at training and test time have the same size, they arise from different RoCs, which skews the distribution of data seen by the CNN. Over the years, training and testing pre-processing procedures have evolved to improve the performance of CNNs, but so far they have been optimized separately [8].', 'In this paper, we first show that this separate optimization has led to a significant distribution shift between training and testing regimes with a detrimental effect on the test-time performance of models. We then show that this problem can be solved by jointly optimizing the choice of resolutions and scales at training and test time, while keeping the same RoC sampling. Our strategy only requires to fine-tune two layers in order to compensate for the shift in statistics caused by the changing the crop size. This allows us to retain the advantages of existing pre-processing protocols for training and testing, including augmenting the training data, while compensating for the distribution shift. Our approach is based on a rigorous analysis of the effect of pre-processing on the statistics of natural images, which shows that increasing the size of the crops used at test time compensates for randomly sampling the RoCs at training time. This analysis also shows that we need to use lower resolution crops at training than at test time. This significantly impacts the processing time: halving the crop resolution leads to a threefold reduction in the network evaluation speed and reduces significantly the memory consumption for a typical CNN, which is especially important for training on GPUs.', 'For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].']",intro_chunked,"For instance, for a target test resolution of 224×224, training at resolution 160×160 provides better results than the standard practice of training at resolution 224×224, while being more efficient. In addition we can adapt a ResNet-50 train at resolution 224×224 for the test resolution 320×320 and thus obtain top-1 accuracy of 79.8% (single-crop) on ImageNet. Alternatively, we leverage the improved efficiency to train high-accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top-1 accuracy of 86.4% (single-crop) on ImageNet with a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images. Finally, our method makes it possible to save GPU memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance [15].",31.234979865771834,30.655460127574653,199,0.626488983631134," For instance, for a target test resolution of 000000, training at resolution 000000 provides better results than the standard practice of training at resolution 000000, while being more efficient. In addition we can adapt a ResNet 00 train at resolution 000000 for the test resolution 000000 and thus obtain top 0 accuracy of 00.0 on Propname. Alternatively, we leverage the improved efficiency to train high accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top 0 accuracy of 00.0 on Propname with a Propname 000 00x00d pre trained in weakly supervised fashion on 000 million public images. Finally, our method makes it possible to save Propname memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance."," For instance, for a target test resolution of 000000, training at resolution 000000 provides better results than the standard practice of training at resolution 000000, while being more efficient. In addition we can adapt a ResNet 00 train at resolution 000000 for the test resolution 000000 and thus obtain top 0 accuracy of 00.0 on Propname. Alternatively, we leverage the improved efficiency to train high accuracy models that operate at much higher resolution at test time while still training quickly. For instance, we achieve an top 0 accuracy of 00.0 on Propname with a Propname 000 00x00d pre trained in weakly supervised fashion on 000 million public images. Finally, our method makes it possible to save Propname memory, which could in turn be exploited by optimization: employing larger batch sizes usually leads to a better final performance.", ADP NOUN PUNCT ADP DET NOUN NOUN NOUN ADP NUM PUNCT NOUN ADP NOUN NUM VERB ADJ NOUN ADP DET ADJ NOUN ADP NOUN ADP NOUN NUM PUNCT SCONJ AUX ADV ADJ PUNCT ADP NOUN PRON AUX VERB DET NOUN NUM NOUN ADP NOUN NUM ADP DET NOUN NOUN NUM CCONJ ADV VERB ADJ NUM NOUN ADP NUM ADP PROPN PUNCT ADV PUNCT PRON VERB DET VERB NOUN PART VERB ADJ NOUN NOUN PRON VERB ADP ADV ADJ NOUN ADP NOUN NOUN SCONJ ADV VERB ADV PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NUM NOUN ADP NUM ADP PROPN ADP DET PROPN NUM X VERB VERB ADP ADJ ADJ NOUN ADP NUM NUM ADJ NOUN PUNCT ADV PUNCT PRON NOUN VERB PRON ADJ PART VERB PROPN NOUN PUNCT PRON AUX ADP NOUN AUX VERB ADP NOUN PUNCT VERB ADJ NOUN NOUN ADV VERB ADP DET ADJ ADJ NOUN PUNCT,0.62,30.0,4.786666666666667
200,114,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked," The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy
(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).",26.359122103386852,22.733194790065724,200,0.24772341549396515," The prevailing Propname alignment paradigm, exemplified in models like Propname and Propname 0 Chat, employs supervised fine tuning with prompted demonstrations and reinforcement learning from human feedback to align the outputs of large language models with human intentions. However, acquiring high quality human annotations, including consistent response demonstrations and in distribution preferences, is costly and not scalable. Furthermore, the existing paradigm of Propname Propname is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced Propname systems. Although todays models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, ie, such models may value appeasing human evaluators over ensuring accuracy 
. To address the current challenges in Propname alignment, we aim to develop a new methodology that facilitates scalable oversight."," The prevailing Propname alignment paradigm, exemplified in models like Propname and Propname 0 Chat, employs supervised fine tuning with prompted demonstrations and reinforcement learning from human feedback to align the outputs of large language models with human intentions. However, acquiring high quality human annotations, including consistent response demonstrations and in distribution preferences, is costly and not scalable. Furthermore, the existing paradigm of Propname Propname is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced Propname systems. Although todays models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, ie, such models may value appeasing human evaluators over ensuring accuracy 
. To address the current challenges in Propname alignment, we aim to develop a new methodology that facilitates scalable oversight.", DET VERB PROPN NOUN NOUN PUNCT VERB ADP NOUN ADP PROPN CCONJ PROPN NUM NOUN PUNCT VERB ADJ ADJ NOUN ADP VERB NOUN CCONJ NOUN NOUN ADP ADJ NOUN PART VERB DET NOUN ADP ADJ NOUN NOUN ADP ADJ NOUN PUNCT ADV PUNCT VERB ADJ NOUN ADJ NOUN PUNCT VERB ADJ NOUN NOUN CCONJ ADP NOUN NOUN PUNCT AUX ADJ CCONJ PART ADJ PUNCT ADV PUNCT DET VERB NOUN ADP PROPN PROPN AUX ADV VERB ADP VERB SCONJ NOUN AUX ADV VERB CCONJ VERB DET NOUN VERB ADP ADJ PROPN NOUN PUNCT SCONJ NOUN NOUN VERB ADP ADJ ADJ NOUN PUNCT NOUN PUNCT ADV ADJ NOUN AUX VERB ADP NOUN PRON VERB ADJ NOUN PUNCT ADV PUNCT PRON VERB DET VERB NOUN PUNCT ADV PUNCT ADJ NOUN AUX VERB VERB ADJ NOUN ADP VERB NOUN SPACE PUNCT PART VERB DET ADJ NOUN ADP PROPN NOUN PUNCT PRON VERB PART VERB DET ADJ NOUN PRON VERB ADJ NOUN PUNCT,0.6560509554140127,26.166666666666668,5.598726114649682
201,115,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"Our vision is to define a
few general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are
comprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),
where the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over
the model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang
et al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still
lag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,
methods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai
et al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage
feedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of
the models that have already undergone RLHF training. That is, these RLAIF methods inherit the
heavy dependency on the human-annotated preferences in the RLHF warm-up stage.",39.8729064039409,22.733194790065724,201,0.4516679346561432," Our vision is to define a 
 few general principles, akin to Propname Propname three laws in robotics, which are 
 comprehensively interalizable for Propname systems to follow. This goal is in line with the recent research on self alignment, 
 where the primary focus is to use Propname models to improve themselves, Propname, with bootstrapping over 
 the model generated critiques or self refined outputs Propname 
 Propname Propname Propname, 0000a; Propname Propname Propname Propname, Propname. However, it is worth noting that these bootstrapping methods still 
 lag behind the Propname method in performance. Meanwhile, 
 methods like Propname Propname from Propname Propname or Propname Propname Propname 
 Propname Propname Propname, 0000b; Propname, 0000a has emerged as an alternative potential. These techniques leverage 
 feedback from automated Propname systems, reducing the reliance on exhaustive human annotated preferences. So far, the primary focus of the previous Propname work remains on enhancing the safety of 
 the models that have already undergone Propname training. That is, these Propname methods inherit the 
 heavy dependency on the human annotated preferences in the Propname warm up stage."," Our vision is to define a 
 few general principles, akin to Propname Propname three laws in robotics, which are 
 comprehensively interalizable for Propname systems to follow. This goal is in line with the recent research on self alignment, 
 where the primary focus is to use Propname models to improve themselves, Propname, with bootstrapping over 
 the model generated critiques or self refined outputs Propname 
 Propname Propname Propname, 0000a; Propname Propname Propname Propname, Propname. However, it is worth noting that these bootstrapping methods still 
 lag behind the Propname method in performance. Meanwhile, 
 methods like Propname Propname from Propname Propname or Propname Propname Propname 
 Propname Propname Propname, 0000b; Propname, 0000a has emerged as an alternative potential. These techniques leverage 
 feedback from automated Propname systems, reducing the reliance on exhaustive human annotated preferences. So far, the primary focus of the previous Propname work remains on enhancing the safety of 
 the models that have already undergone Propname training. That is, these Propname methods inherit the 
 heavy dependency on the human annotated preferences in the Propname warm up stage.", PRON NOUN AUX PART VERB DET SPACE ADJ ADJ NOUN PUNCT ADJ ADP PROPN PROPN NUM NOUN ADP NOUN PUNCT PRON AUX SPACE ADV ADJ ADP PROPN NOUN PART VERB PUNCT DET NOUN AUX ADP NOUN ADP DET ADJ NOUN ADP NOUN NOUN PUNCT SPACE SCONJ DET ADJ NOUN AUX PART VERB PROPN NOUN PART VERB PRON PUNCT PROPN PUNCT ADP VERB ADP SPACE DET NOUN VERB NOUN CCONJ NOUN ADJ NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT PROPN PUNCT ADV PUNCT PRON AUX ADJ VERB SCONJ DET ADJ NOUN ADV SPACE VERB ADP DET PROPN NOUN ADP NOUN PUNCT ADV PUNCT SPACE NOUN ADP PROPN PROPN ADP PROPN PROPN CCONJ PROPN PROPN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PUNCT NOUN AUX VERB ADP DET ADJ NOUN PUNCT DET NOUN NOUN SPACE NOUN ADP VERB PROPN NOUN PUNCT VERB DET NOUN ADP ADJ ADJ ADJ NOUN PUNCT ADV ADV PUNCT DET ADJ NOUN ADP DET ADJ PROPN NOUN VERB ADP VERB DET NOUN ADP SPACE DET NOUN PRON AUX ADV VERB PROPN NOUN PUNCT ADV ADV PUNCT DET PROPN NOUN VERB DET SPACE ADJ NOUN ADP DET NOUN ADJ NOUN ADP DET PROPN VERB ADP NOUN PUNCT,0.5255102040816326,28.0,5.23469387755102
202,116,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"This leads to
a pivotal research question:
• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their
general alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach
namely SALMON. At the heart of our approach lies the introduction of the principle-following
(also termed instruction-following) reward model. Pioneering in its nature, this reward model is
adept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently
generating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,
2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the
resulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of
the final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection
of online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to
counteract reward hacking (Pan et al., 2022). This complication emerges when the policy model
exploits weaknesses in the reward model, producing inflated scores that do not accurately reflect
model performance. In SALMON, we can address this issue by simply crafting principles explicitly
2
Preprint
Write a story
about dromedaries.",20.221116303219134,22.733194790065724,202,0.43844878673553467," This leads to 
 a pivotal research question: Can Propname fully replace Propname to align language models from scratch in enhancing their 
 general alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach 
 namely Propname. At the heart of our approach lies the introduction of the principle following reward model. Pioneering in its nature, this reward model is 
 adept at interpreting and adhering to arbitrary human written preference guidelines, subsequently 
 generating human guided reward scores. This is different from previous Propname methods Propname Propname Propname Propname, 
 0000b; Propname, 0000a where the principles are only used to produce synthetic preferences, and the 
 resulting reward models generate scores without any specific principles, as illustrated in Figure 0. The design of our principle following reward model enables better control over the behavior of 
 the final Propname trained policy model. Within conventional Propname Propname, the iterative collection 
 of online preference data is essential to 
 counteract reward hacking. This complication emerges when the policy model 
 exploits weaknesses in the reward model, producing inflated scores that do not accurately reflect 
 model performance. In Propname, we can address this issue by simply crafting principles explicitly 
 0 
 Propname 
 Write a story 
 about dromedaries."," This leads to 
 a pivotal research question: Can Propname fully replace Propname to align language models from scratch in enhancing their 
 general alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach 
 namely Propname. At the heart of our approach lies the introduction of the principle following reward model. Pioneering in its nature, this reward model is 
 adept at interpreting and adhering to arbitrary human written preference guidelines, subsequently 
 generating human guided reward scores. This is different from previous Propname methods Propname Propname Propname Propname, 
 0000b; Propname, 0000a where the principles are only used to produce synthetic preferences, and the 
 resulting reward models generate scores without any specific principles, as illustrated in Figure 0. The design of our principle following reward model enables better control over the behavior of 
 the final Propname trained policy model. Within conventional Propname Propname, the iterative collection 
 of online preference data is essential to 
 counteract reward hacking. This complication emerges when the policy model 
 exploits weaknesses in the reward model, producing inflated scores that do not accurately reflect 
 model performance. In Propname, we can address this issue by simply crafting principles explicitly 
 0 
 Propname 
 Write a story 
 about dromedaries.", PRON VERB ADP SPACE DET ADJ NOUN NOUN PUNCT AUX PROPN ADV VERB PROPN PART VERB NOUN NOUN ADP NOUN ADP VERB PRON SPACE ADJ NOUN CCONJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP DET ADJ NOUN ADP VERB DET ADJ NOUN SPACE ADV PROPN PUNCT ADP DET NOUN ADP PRON NOUN VERB DET NOUN ADP DET ADJ VERB NOUN NOUN PUNCT VERB ADP PRON NOUN PUNCT DET NOUN NOUN AUX SPACE ADJ ADP VERB CCONJ VERB ADP ADJ NOUN VERB NOUN NOUN PUNCT ADV SPACE VERB ADJ VERB NOUN NOUN PUNCT PRON AUX ADJ ADP ADJ PROPN NOUN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PUNCT NOUN SCONJ DET NOUN AUX ADV VERB PART VERB ADJ NOUN PUNCT CCONJ DET SPACE VERB NOUN NOUN VERB NOUN ADP DET ADJ NOUN PUNCT SCONJ VERB ADP NOUN NUM PUNCT DET NOUN ADP PRON NOUN VERB NOUN NOUN VERB ADJ NOUN ADP DET NOUN ADP SPACE DET ADJ PROPN VERB NOUN NOUN PUNCT ADP ADJ PROPN PROPN PUNCT DET ADJ NOUN SPACE ADP ADJ NOUN NOUN AUX ADJ PART SPACE VERB NOUN NOUN PUNCT DET NOUN VERB SCONJ DET NOUN NOUN SPACE VERB NOUN ADP DET NOUN NOUN PUNCT VERB ADJ NOUN PRON AUX PART ADV VERB SPACE NOUN NOUN PUNCT ADP PROPN PUNCT PRON AUX VERB DET NOUN ADP ADV VERB NOUN ADV SPACE NUM SPACE PROPN SPACE VERB DET NOUN SPACE ADP NOUN PUNCT,0.6036036036036037,24.666666666666668,5.445945945945946
203,117,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"Sampled prompts
RM-RLHF
RM-SALMON
RLHF (Ouyang et al., 2022)
SFT
SFT-generated responses
RM-RLAIF
RLAIF (Bai et al., 2022)
SALMON (Ours)
Principle Aggregating
AI-labeled preferences
human-labeled preferences
SFT
SFT
AI-labeled preferences Principle-following reward model
Stand-alone reward model
Stand-alone reward model
Reward Score
Prompt + Response
Principles
Reward Score
Prompt + Response
Reward Score
Prompt + Response
Principles
Principles
Human Annotator
In general, SFT denotes the
Supervised Fine-Tuned model, but it
can also be RLHF-trained in RLAIF. General
Alignment
Safety
Alignment
General
Alignment
Figure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and
SALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give
high scores to generally good responses, while the principle-following reward model in SALMON
is trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1
reward hacking patterns in model outputs, such as self-praising at the
end of the response. Additionally, we found that we are able to emphasize distinct aspects of the
alignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)
by customizing the preference principles.",14.487047619047644,22.733194790065724,203,0.7761358022689819," Sampled prompts 
 Propname Propname 
 Propname Propname 
 Propname Propname 
 Propname generated responses 
 Propname Propname 
 Propname Propname Propname Propname 
 Propname labeled preferences 
 human labeled preferences 
 Propname 
 Propname 
 Propname labeled preferences Principle following reward model 
 Stand alone reward model 
 Stand alone reward model 
 Propname Propname 
 Propname Propname 
 Principles 
 Propname Propname 
 Propname Propname 
 Propname Propname 
 Propname Propname 
 Principles 
 Propname 
 Propname Propname 
 In general, Propname denotes the 
 Propname Fine Tuned model, but it 
 can also be Propname trained in Propname. Propname 
 Propname 
 Propname 
 Propname 
 Propname 
 Propname 
 Propname 0: Propname among Propname, Propname, and 
 Propname. The vanilla reward models in Propname Propname are trained to give 
 high scores to generally good responses, while the principle following reward model in Propname 
 is trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed0 
 reward hacking patterns in model outputs, such as self praising at the 
 end of the response. Additionally, we found that we are able to emphasize distinct aspects of the 
 alignment in the Propname alignment framework by customizing the preference principles."," Sampled prompts 
 Propname Propname 
 Propname Propname 
 Propname Propname 
 Propname generated responses 
 Propname Propname 
 Propname Propname Propname Propname 
 Propname labeled preferences 
 human labeled preferences 
 Propname 
 Propname 
 Propname labeled preferences Principle following reward model 
 Stand alone reward model 
 Stand alone reward model 
 Propname Propname 
 Propname Propname 
 Principles 
 Propname Propname 
 Propname Propname 
 Propname Propname 
 Propname Propname 
 Principles 
 Propname 
 Propname Propname 
 In general, Propname denotes the 
 Propname Fine Tuned model, but it 
 can also be Propname trained in Propname. Propname 
 Propname 
 Propname 
 Propname 
 Propname 
 Propname 
 Propname 0: Propname among Propname, Propname, and 
 Propname. The vanilla reward models in Propname Propname are trained to give 
 high scores to generally good responses, while the principle following reward model in Propname 
 is trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed0 
 reward hacking patterns in model outputs, such as self praising at the 
 end of the response. Additionally, we found that we are able to emphasize distinct aspects of the 
 alignment in the Propname alignment framework by customizing the preference principles.", VERB NOUN SPACE PROPN PROPN SPACE PROPN PROPN SPACE PROPN PROPN SPACE PROPN VERB NOUN SPACE PROPN PROPN SPACE PROPN PROPN PROPN PROPN SPACE PROPN VERB NOUN SPACE ADJ VERB NOUN SPACE PROPN SPACE PROPN SPACE PROPN VERB NOUN ADJ VERB NOUN NOUN SPACE VERB ADV NOUN NOUN SPACE VERB ADV NOUN NOUN SPACE PROPN PROPN SPACE PROPN PROPN SPACE NOUN SPACE PROPN PROPN SPACE PROPN PROPN SPACE PROPN PROPN SPACE PROPN PROPN SPACE NOUN SPACE PROPN SPACE PROPN PROPN SPACE ADP ADJ PUNCT PROPN VERB DET SPACE PROPN ADJ ADJ NOUN PUNCT CCONJ PRON SPACE AUX ADV AUX PROPN VERB ADP PROPN PUNCT PROPN SPACE PROPN SPACE PROPN SPACE PROPN SPACE PROPN SPACE PROPN SPACE PROPN NUM PUNCT PROPN ADP PROPN PUNCT PROPN PUNCT CCONJ SPACE PROPN PUNCT DET NOUN NOUN NOUN ADP PROPN PROPN AUX VERB PART VERB SPACE ADJ NOUN ADP ADV ADJ NOUN PUNCT SCONJ DET ADJ VERB NOUN NOUN ADP PROPN SPACE AUX VERB PART VERB NOUN NOUN VERB ADP VERB NOUN ADP DET NOUN NOUN PUNCT VERB PART VERB ADJ SPACE NOUN VERB NOUN ADP NOUN NOUN PUNCT ADJ ADP NOUN VERB ADP DET SPACE NOUN ADP DET NOUN PUNCT ADV PUNCT PRON VERB SCONJ PRON AUX ADJ PART VERB ADJ NOUN ADP DET SPACE NOUN ADP DET PROPN NOUN NOUN ADP VERB DET NOUN NOUN PUNCT,0.43243243243243246,37.0,5.908108108108108
204,118,Zhiqing Sun,"[' The prevailing AI alignment paradigm, exemplified in models like ChatGPT (OpenAI, 2022) and LLaMA-2-Chat (Touvron et al., 2023b), employs supervised fine-tuning (SFT) with prompted demonstrations (Sanh et al., 2021; Chung et al., 2022a; Zhou et al., 2023) and reinforcement learning from human feedback (RLHF) to align the outputs of large language models (LLMs) with human intentions (Ziegler et al., 2019; Ouyang et al., 2022). However, acquiring high-quality human annotations, including consistent response demonstrations and in-distribution preferences, is costly and not scalable (Touvron et al., 2023b). Furthermore, the existing paradigm of SFT + RLHF is inherently limited in assuming that humans can always demonstrate or evaluate the tasks undertaken by advanced AI systems. Although today’s models fall within human evaluative boundaries, future, more advanced models could embark on tasks that challenge human evaluation. Consequently, there is a looming danger, i.e., such models may value appeasing human evaluators over ensuring accuracy\n(Andreas, 2022; Perez et al., 2022). To address the current challenges in AI alignment, we aim to develop a new methodology that facilitates scalable oversight (Amodei et al., 2016; Bowman et al., 2022).', 'Our vision is to define a\nfew general principles, akin to Issac Asimov’s three laws in robotics (Asimov, 1941), which are\ncomprehensively interalizable for AI systems to follow (Gilardi et al., 2023; Ganguli et al., 2023). This goal is in line with the recent research on self-alignment (Bai et al., 2022b; Sun et al., 2023b),\nwhere the primary focus is to use AI models to improve themselves, e.g., with bootstrapping over\nthe model-generated critiques (Madaan et al., 2023; Fu et al., 2023) or self-refined outputs (Wang\net al., 2022a; Li et al., 2023a). However, it is worth noting that these bootstrapping methods still\nlag behind the RLHF method in performance (Bai et al., 2022b; Touvron et al., 2023b). Meanwhile,\nmethods like Reinforcement Learning from AI Feedback (RLAIF) or Constitutional AI (CAI) (Bai\net al., 2022b; OpenAI, 2023a) has emerged as an alternative potential. These techniques leverage\nfeedback from automated AI systems, reducing the reliance on exhaustive human-annotated preferences. So far, the primary focus of the previous RLAIF work remains on enhancing the safety of\nthe models that have already undergone RLHF training. That is, these RLAIF methods inherit the\nheavy dependency on the human-annotated preferences in the RLHF warm-up stage.', 'This leads to\na pivotal research question:\n• Can RLAIF fully replace RLHF to align language models from scratch in enhancing their\ngeneral alignment and capabilities? This paper provides a definitive confirmation for the above question by introducing a novel approach\nnamely SALMON. At the heart of our approach lies the introduction of the principle-following\n(also termed instruction-following) reward model. Pioneering in its nature, this reward model is\nadept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently\ngenerating human-guided reward scores. This is different from previous RLAIF methods (Bai et al.,\n2022b; OpenAI, 2023a) where the principles are only used to produce synthetic preferences, and the\nresulting reward models generate scores without any specific principles, as illustrated in Figure 1. The design of our principle-following reward model enables better control over the behavior of\nthe final RL-trained policy model. Within conventional RLHF paradigms, the iterative collection\nof online (in-distribution) preference data (Bai et al., 2022a; Touvron et al., 2023b) is essential to\ncounteract reward hacking (Pan et al., 2022). This complication emerges when the policy model\nexploits weaknesses in the reward model, producing inflated scores that do not accurately reflect\nmodel performance. In SALMON, we can address this issue by simply crafting principles explicitly\n2\nPreprint\nWrite a story\nabout dromedaries.', 'Sampled prompts\nRM-RLHF\nRM-SALMON\nRLHF (Ouyang et al., 2022)\nSFT\nSFT-generated responses\nRM-RLAIF\nRLAIF (Bai et al., 2022)\nSALMON (Ours)\nPrinciple Aggregating\nAI-labeled preferences\nhuman-labeled preferences\nSFT\nSFT\nAI-labeled preferences Principle-following reward model\nStand-alone reward model\nStand-alone reward model\nReward Score\nPrompt + Response\nPrinciples\nReward Score\nPrompt + Response\nReward Score\nPrompt + Response\nPrinciples\nPrinciples\nHuman Annotator\nIn general, SFT denotes the\nSupervised Fine-Tuned model, but it\ncan also be RLHF-trained in RLAIF. General\nAlignment\nSafety\nAlignment\nGeneral\nAlignment\nFigure 1: Comparison among RLHF (Ouyang et al., 2022), RLAIF (Bai et al., 2022b), and\nSALMON (Ours). The vanilla (stand-alone) reward models in RLHF & RLAIF are trained to give\nhigh scores to generally good responses, while the principle-following reward model in SALMON\nis trained to generate reward scores based on customized principles as the preference guideline. designed to combat observed1\nreward hacking patterns in model outputs, such as self-praising at the\nend of the response. Additionally, we found that we are able to emphasize distinct aspects of the\nalignment in the HHH (helpful, honest, and harmless) alignment framework (Askell et al., 2021)\nby customizing the preference principles.', 'Our methodology also proved effective in reducing the\noccurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by\ncrafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to\na diverse range of language models without collecting any model-specific human preference data\n(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include\nprinciple-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations\n(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron\net al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),\nour method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from\nscratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a\ncombined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-\nChat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response\ndemonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human\nsupervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.']",intro_chunked,"Our methodology also proved effective in reducing the
occurrence of false refusals seen in certain over-aligned language models (Touvron et al., 2023b) by
crafting special principles. Our principle-following reward model can be trained with synthetic data and seamlessly applied to
a diverse range of language models without collecting any model-specific human preference data
(Bai et al., 2022a; Touvron et al., 2023b). Possible policy model initialization strategies include
principle-driven self-alignment (Sun et al., 2023b), supervised fine-tuning on human demonstrations
(Chung et al., 2022a; Zhou et al., 2023), or even those unaligned base language models (Touvron
et al., 2023a). Remarkably, when integrated with the SELF-ALIGN technique (Sun et al., 2023b),
our method enabled the training of a self-aligned AI-assistant agent, namely Dromedary-2, from
scratch by only manually crafting 6 exemplars for In-Context Learning (Brown et al., 2020) and a
combined total of 31 principles (17 from SELF-ALIGN and 14 for SALMON). Despite its minimal human supervision design, our model outperformed the extensively RLHF-trained LLaMA-2-
Chat model (Touvron et al., 2023b), which was trained with over 20,000+ human-curated response
demonstrations and 1,000,000+ human-annotated response preferences. The comparisons of human
supervision efficiency and performance on MT-Bench (Zheng et al., 2023) are detailed in Table. 1.",19.052439024390253,22.733194790065724,204,0.6022748947143555," Our methodology also proved effective in reducing the 
 occurrence of false refusals seen in certain over aligned language models by 
 crafting special principles. Our principle following reward model can be trained with synthetic data and seamlessly applied to 
 a diverse range of language models without collecting any model specific human preference data 
. Possible policy model initialization strategies include 
 principle driven self alignment, supervised fine tuning on human demonstrations 
, or even those unaligned base language models Propname 
 Propname Propname Propname, Propname. Remarkably, when integrated with the Propname Propname technique, 
 our method enabled the training of a self aligned Propname assistant agent, namely Propname 0, from 
 scratch by only manually crafting 0 exemplars for In Propname Propname and a 
 combined total of 00 principles. Despite its minimal human supervision design, our model outperformed the extensively Propname trained Propname 0 Propname model, which was trained with over 00,000 human curated response 
 demonstrations and 0,000,000 human annotated response preferences. The comparisons of human 
 supervision efficiency and performance on Propname Propname are detailed in Propname.0."," Our methodology also proved effective in reducing the 
 occurrence of false refusals seen in certain over aligned language models by 
 crafting special principles. Our principle following reward model can be trained with synthetic data and seamlessly applied to 
 a diverse range of language models without collecting any model specific human preference data 
. Possible policy model initialization strategies include 
 principle driven self alignment, supervised fine tuning on human demonstrations 
, or even those unaligned base language models Propname 
 Propname Propname Propname, Propname. Remarkably, when integrated with the Propname Propname technique, 
 our method enabled the training of a self aligned Propname assistant agent, namely Propname 0, from 
 scratch by only manually crafting 0 exemplars for In Propname Propname and a 
 combined total of 00 principles. Despite its minimal human supervision design, our model outperformed the extensively Propname trained Propname 0 Propname model, which was trained with over 00,000 human curated response 
 demonstrations and 0,000,000 human annotated response preferences. The comparisons of human 
 supervision efficiency and performance on Propname Propname are detailed in Propname.0.", PRON NOUN ADV VERB ADJ ADP VERB DET SPACE NOUN ADP ADJ NOUN VERB ADP ADJ ADP VERB NOUN NOUN ADP SPACE VERB ADJ NOUN PUNCT PRON NOUN VERB NOUN NOUN AUX AUX VERB ADP ADJ NOUN CCONJ ADV VERB ADP SPACE DET ADJ NOUN ADP NOUN NOUN ADP VERB DET NOUN ADJ ADJ NOUN NOUN SPACE PUNCT ADJ NOUN NOUN NOUN NOUN VERB SPACE ADJ VERB NOUN NOUN PUNCT VERB ADJ NOUN ADP ADJ NOUN SPACE PUNCT CCONJ ADV DET ADJ NOUN NOUN NOUN PROPN SPACE PROPN PROPN PROPN PUNCT PROPN PUNCT ADV PUNCT SCONJ VERB ADP DET PROPN PROPN NOUN PUNCT SPACE PRON NOUN VERB DET NOUN ADP DET NOUN VERB PROPN ADJ NOUN PUNCT ADV PROPN NUM PUNCT ADP SPACE NOUN ADP ADV ADV VERB NUM NOUN ADP ADP PROPN PROPN CCONJ DET SPACE VERB NOUN ADP NUM NOUN PUNCT SCONJ PRON ADJ ADJ NOUN NOUN PUNCT PRON NOUN VERB DET ADV PROPN VERB PROPN NUM PROPN NOUN PUNCT PRON AUX VERB ADP ADP NUM ADJ VERB NOUN SPACE NOUN CCONJ NUM ADJ VERB NOUN NOUN PUNCT DET NOUN ADP ADJ SPACE NOUN NOUN CCONJ NOUN ADP PROPN PROPN AUX VERB ADP PROPN PUNCT PUNCT PUNCT,0.5945945945945946,30.833333333333332,5.708108108108108
205,119,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked," Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.",38.49140594059409,22.733194790065724,205,0.26378726959228516," Large Propname Propname; Propname Propname Propname.; Propname can delve into the multimodal realm either by further pretraining with image text pairs or by fine tuning them with specialized Propname instruction tuning datasets, leading to the emergence of powerful Large Propname Propname. Yet, developing Propname faces challenges, notably the gap between the volume and quality of multimodal data versus text only datasets. Consider the LLaVA model, which is initialized from a pretrained vision encoder and an instruction tuned language model. It is trained on just 000 K synthetic image based dialogues, which is much less in comparison to the text only models utilizing over 000 Propname examples spanning 0000 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, Propname may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high quality visual instruction tuning data for Propname training, we introduce Propname Propname, a vision language model trained for improved multimodal alignment."," Large Propname Propname; Propname Propname Propname.; Propname can delve into the multimodal realm either by further pretraining with image text pairs or by fine tuning them with specialized Propname instruction tuning datasets, leading to the emergence of powerful Large Propname Propname. Yet, developing Propname faces challenges, notably the gap between the volume and quality of multimodal data versus text only datasets. Consider the LLaVA model, which is initialized from a pretrained vision encoder and an instruction tuned language model. It is trained on just 000 K synthetic image based dialogues, which is much less in comparison to the text only models utilizing over 000 Propname examples spanning 0000 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, Propname may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high quality visual instruction tuning data for Propname training, we introduce Propname Propname, a vision language model trained for improved multimodal alignment.", ADJ PROPN PROPN PUNCT PROPN PROPN PROPN PUNCT PUNCT PROPN AUX VERB ADP DET ADJ NOUN CCONJ ADP ADV VERB ADP NOUN NOUN NOUN CCONJ ADP ADJ VERB PRON ADP ADJ PROPN NOUN VERB NOUN PUNCT VERB ADP DET NOUN ADP ADJ ADJ PROPN PROPN PUNCT ADV PUNCT VERB PROPN VERB NOUN PUNCT ADV DET NOUN ADP DET NOUN CCONJ NOUN ADP ADJ NOUN ADP NOUN ADV NOUN PUNCT VERB DET ADJ NOUN PUNCT PRON AUX VERB ADP DET VERB NOUN NOUN CCONJ DET NOUN VERB NOUN NOUN PUNCT PRON AUX VERB ADP ADV NUM NOUN ADJ NOUN VERB NOUN PUNCT PRON AUX ADV ADJ ADP NOUN ADP DET NOUN ADV NOUN VERB ADP NUM PROPN NOUN VERB NUM NOUN PUNCT ADJ NOUN ADP NOUN AUX VERB ADP NOUN ADP DET NOUN CCONJ NOUN NOUN PUNCT ADV PUNCT PROPN AUX VERB VERB NOUN PUNCT PRON AUX PART ADV VERB ADP DET NOUN VERB ADP NOUN PUNCT PART VERB DET NOUN VERB ADP DET NOUN ADP ADJ NOUN ADJ NOUN VERB NOUN ADP PROPN NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET NOUN NOUN NOUN VERB ADP VERB ADJ NOUN PUNCT,0.5631578947368421,23.75,5.21578947368421
206,120,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked,"One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.",26.37135514018692,22.733194790065724,206,0.6821611523628235," One of our key contributions is the adaptation of the Propname Propname from Propname Propname, a general and scalable alignment paradigm that shows great success for text based Propname agents, to the multimodal alignment for Propname. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for Propname fine tuning. This approach can improve the multimodal alignment with a relatively low annotation cost, Propname, collecting 00 K human preferences for image based conversations with 0000. To the best of our knowledge, this approach is the first successful adaptation of Propname to multimodal alignment. A potential issue with the current Propname Propname is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work proposed to iteratively collect fresh human feedback, which tends to be costly and can not effectively utilize existing human preference data. In this work, we propose a more data efficient alternative, ie, we try to make the reward model capable of leveraging existing human annotated data and knowledge in larger language models."," One of our key contributions is the adaptation of the Propname Propname from Propname Propname, a general and scalable alignment paradigm that shows great success for text based Propname agents, to the multimodal alignment for Propname. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for Propname fine tuning. This approach can improve the multimodal alignment with a relatively low annotation cost, Propname, collecting 00 K human preferences for image based conversations with 0000. To the best of our knowledge, this approach is the first successful adaptation of Propname to multimodal alignment. A potential issue with the current Propname Propname is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work proposed to iteratively collect fresh human feedback, which tends to be costly and can not effectively utilize existing human preference data. In this work, we propose a more data efficient alternative, ie, we try to make the reward model capable of leveraging existing human annotated data and knowledge in larger language models.", NUM ADP PRON ADJ NOUN AUX DET NOUN ADP DET PROPN PROPN ADP PROPN PROPN PUNCT DET ADJ CCONJ ADJ NOUN NOUN PRON VERB ADJ NOUN ADP NOUN VERB PROPN NOUN PUNCT ADP DET ADJ NOUN ADP PROPN PUNCT ADP VERB ADJ NOUN ADP DET NOUN ADP VERB NOUN PUNCT CCONJ VERB DET NOUN ADP NOUN NOUN ADP PROPN ADJ NOUN PUNCT DET NOUN AUX VERB DET ADJ NOUN ADP DET ADV ADJ NOUN NOUN PUNCT PROPN PUNCT VERB NUM NOUN ADJ NOUN ADP NOUN VERB NOUN ADP NUM PUNCT ADP DET ADJ ADP PRON NOUN PUNCT DET NOUN AUX DET ADJ ADJ NOUN ADP PROPN ADP ADJ NOUN PUNCT DET ADJ NOUN ADP DET ADJ PROPN PROPN AUX VERB NOUN NOUN PUNCT PRON VERB VERB ADJ NOUN ADP DET NOUN NOUN AUX PART ADV VERB ADP NOUN ADP ADJ NOUN PUNCT PART VERB NOUN NOUN PUNCT ADJ NOUN VERB PART ADV VERB ADJ ADJ NOUN PUNCT PRON VERB PART AUX ADJ CCONJ AUX PART ADV VERB VERB ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADJ NOUN PUNCT ADV PUNCT PRON VERB PART VERB DET NOUN NOUN ADJ ADP VERB VERB ADJ ADJ NOUN CCONJ NOUN ADP ADJ NOUN NOUN PUNCT,0.5507246376811594,29.571428571428573,5.169082125603865
207,121,Zhiqing Sun,"[' Large Language Models (LLMs; Brown et al. (2020); Chowdhery et al. (2022); OpenAI (2023)) can delve into the multimodal realm either by further pretraining with image-text pairs (Alayrac et al. ; Awadalla et al., 2023) or by fine-tuning them with specialized vision instruction tuning datasets (Liu et al., 2023a; Zhu et al., 2023), leading to the emergence of powerful Large Multimodal Models (LMMs). Yet, developing LMMs faces challenges, notably the gap between the volume and quality of multimodal data versus text-only datasets. Consider the LLaVA model (Liu et al., 2023a), which is initialized from a pretrained vision encoder (Radford et al., 2021) and an instruction-tuned language model (Chiang et al., 2023). It is trained on just 150K synthetic image-based dialogues, which is much less in comparison to the text-only models (Flan (Longpre et al., 2023) utilizing over 100M examples spanning 1800 tasks. Such limitations in data can lead to misalignment between the vision and language modalities. Consequently, LMMs may produce hallucinated outputs, which are not accurately anchored to the context provided by images. To mitigate the challenges posed by the scarcity of high-quality visual instruction tuning data for LMM training, we introduce LLaVA-RLHF, a vision-language model trained for improved multimodal alignment.', 'One of our key contributions is the adaptation of the Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a), a general and scalable alignment paradigm that shows great success for text-based AI agents, to the multimodal alignment for LMMs. By collecting human preferences with an emphasis on detecting hallucinations, and utilizes those preferences in reinforcement learning for LMM fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020). This approach can improve the multimodal alignment with a relatively low annotation cost, e.g., collecting 10K human preferences for image-based conversations with $3000. To the best of our knowledge, this approach is the first successful adaptation of RLHF to multimodal alignment. A potential issue with the current RLHF paradigm is called reward hacking, which means achieving high scores from the reward model does not necessarily lead to improvement in human judgments. To prevent reward hacking, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect “fresh” human feedback, which tends to be costly and cannot effectively utilize existing human preference data. In this work, we propose a more data-efficient alternative, i.e., we try to make the reward model capable of leveraging existing human-annotated data and knowledge in larger language models.', 'Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.']",intro_chunked,"Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Factually Augmented RLHF (Fact-RLHF), which calibrates the reward signals by augmenting them with additional information such as image captions or ground-truth multi-choice option, as illustrated in Fig. 1. To improve the general capabilities of LMMs during the Supervised Fine-Tuning (SFT) stage, we further augment the synthetic vision instruction tuning data (Liu et al., 2023a) with existing high-quality human-annotated multi-modal data in the conversation format. Specifically, we convert VQA-v2 (Goyal et al., 2017a) and A-OKVQA (Schwenk et al., 2022) into a multi-round QA task, and Flickr30k (Young et al., 2014b) into a Spotting Captioning task (Chen et al., 2023a), and train the LLaVA-SFT+ models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of LMMs in real-world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 12 main object categories in COCO (Lin et al., 2014) and include 8 different task types, leading to MMHAL-BENCH. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. In our experimental evaluation, as the first LMM trained with RLHF, LLaVA-RLHF delivers impressive outcomes. We observed a notable enhancement on LLaVA-Bench, achieving 94%, an improvement by 60% in MMHAL-BENCH, and established new performance benchmarks for LLaVA with a 52.4% score on MMBench (Liu et al., 2023b) and an 82.7% F1 on POPE (Li et al., 2023d). We have made our code, model, and data publicly available at.",32.014090909090896,22.733194790065724,207,0.7247234582901001," Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Propname Propname Propname, which calibrates the reward signals by augmenting them with additional information such as image captions or ground truth multi choice option, as illustrated in Propname.0. To improve the general capabilities of Propname during the Propname Propname Tuning stage, we further augment the synthetic vision instruction tuning data with existing high quality human annotated multi modal data in the conversation format. Specifically, we convert Propname Propname and A OKVQA into a multi round QA task, and Flickr00k into a Propname Captioning task, and train the Propname Propname models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of Propname in real world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 00 main object categories in Propname and include 0 different task types, leading to Propname Propname. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti hallucinations. In our experimental evaluation, as the first Propname trained with Propname, Propname Propname delivers impressive outcomes. We observed a notable enhancement on Propname Propname, achieving 00, an improvement by 00 in Propname Propname, and established new performance benchmarks for Propname with a 00.0 score on MMBench and an 00.0 F0 on Propname. We have made our code, model, and data publicly available at."," Firstly, we improve the general capabilities of the reward model by using a better vision encoder with higher resolutions and a larger language model. Secondly, we introduce a novel algorithm named Propname Propname Propname, which calibrates the reward signals by augmenting them with additional information such as image captions or ground truth multi choice option, as illustrated in Propname.0. To improve the general capabilities of Propname during the Propname Propname Tuning stage, we further augment the synthetic vision instruction tuning data with existing high quality human annotated multi modal data in the conversation format. Specifically, we convert Propname Propname and A OKVQA into a multi round QA task, and Flickr00k into a Propname Captioning task, and train the Propname Propname models based on the new mixture of data. Lastly, we look into assessing the multimodal alignment of Propname in real world generation scenarios, placing particular emphasis on penalizing any hallucinations. We create a set of varied benchmark questions that cover the 00 main object categories in Propname and include 0 different task types, leading to Propname Propname. Our evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti hallucinations. In our experimental evaluation, as the first Propname trained with Propname, Propname Propname delivers impressive outcomes. We observed a notable enhancement on Propname Propname, achieving 00, an improvement by 00 in Propname Propname, and established new performance benchmarks for Propname with a 00.0 score on MMBench and an 00.0 F0 on Propname. We have made our code, model, and data publicly available at.", ADV PUNCT PRON VERB DET ADJ NOUN ADP DET NOUN NOUN ADP VERB DET ADJ NOUN NOUN ADP ADJ NOUN CCONJ DET ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB DET NOUN NOUN VERB PROPN PROPN PROPN PUNCT PRON VERB DET NOUN NOUN ADP VERB PRON ADP ADJ NOUN ADJ ADP NOUN NOUN CCONJ NOUN NOUN NOUN NOUN NOUN PUNCT SCONJ VERB ADP PROPN PUNCT PUNCT PUNCT PART VERB DET ADJ NOUN ADP PROPN ADP DET PROPN PROPN NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN NOUN VERB NOUN ADP VERB ADJ NOUN NOUN VERB ADJ ADJ NOUN ADP DET NOUN NOUN PUNCT ADV PUNCT PRON VERB PROPN PROPN CCONJ DET NOUN ADP DET ADJ ADJ NOUN NOUN PUNCT CCONJ VERB ADP DET PROPN ADJ NOUN PUNCT CCONJ VERB DET PROPN PROPN NOUN VERB ADP DET ADJ NOUN ADP NOUN PUNCT ADV PUNCT PRON VERB ADP VERB DET ADJ NOUN ADP PROPN ADP ADJ NOUN NOUN NOUN PUNCT VERB ADJ NOUN ADP VERB DET NOUN PUNCT PRON VERB DET NOUN ADP ADJ ADJ NOUN PRON VERB DET NUM ADJ NOUN NOUN ADP PROPN CCONJ VERB NUM ADJ NOUN NOUN PUNCT VERB ADP PROPN PROPN PUNCT PRON NOUN VERB SCONJ DET ADJ NOUN NOUN ADV ADP ADJ NOUN PUNCT ADV SCONJ NOUN AUX VERB ADP ADJ NOUN PUNCT ADP PRON ADJ NOUN PUNCT ADP DET ADJ PROPN VERB ADP PROPN PUNCT PROPN PROPN VERB ADJ NOUN PUNCT PRON VERB DET ADJ NOUN ADP PROPN PROPN PUNCT VERB NUM PUNCT DET NOUN ADP NUM ADP PROPN PROPN PUNCT CCONJ VERB ADJ NOUN NOUN ADP PROPN ADP DET NUM NOUN ADP NOUN CCONJ DET NUM NOUN ADP PROPN PUNCT PRON AUX VERB PRON NOUN PUNCT NOUN PUNCT CCONJ NOUN ADV ADJ ADP PUNCT,0.5467128027681661,28.9,5.117647058823529
208,122,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked," The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.",16.405357142857156,22.733194790065724,208,0.6127408146858215," The problem of aligning large language models to human values and intentions in terms of being comprehensive, respectful, and compliant0 has gained significant attention in research as recent Propname systems have rapidly advanced in their capabilities. Presently, state of the art Propname systems predominantly depend on supervised fine tuning with human instructions and annotations, as well as reinforcement learning from human feedback on their preferences. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self consistence, undesirable biases, etc ., in human provided annotations. To address such issues with intensive human annotations for Propname alignment, we propose a novel approach named Propname Propname. It substantially reduces the efforts on human supervision and renders it virtually annotation free by utilizing a small set of human defined principles to guide the behavior of Propname based Propname agents in generating responses to users queries."," The problem of aligning large language models to human values and intentions in terms of being comprehensive, respectful, and compliant0 has gained significant attention in research as recent Propname systems have rapidly advanced in their capabilities. Presently, state of the art Propname systems predominantly depend on supervised fine tuning with human instructions and annotations, as well as reinforcement learning from human feedback on their preferences. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self consistence, undesirable biases, etc ., in human provided annotations. To address such issues with intensive human annotations for Propname alignment, we propose a novel approach named Propname Propname. It substantially reduces the efforts on human supervision and renders it virtually annotation free by utilizing a small set of human defined principles to guide the behavior of Propname based Propname agents in generating responses to users queries.", DET NOUN ADP VERB ADJ NOUN NOUN ADP ADJ NOUN CCONJ NOUN ADP NOUN ADP AUX ADJ PUNCT ADJ PUNCT CCONJ NOUN AUX VERB ADJ NOUN ADP NOUN SCONJ ADJ PROPN NOUN AUX ADV VERB ADP PRON NOUN PUNCT ADV PUNCT NOUN ADP DET NOUN PROPN NOUN ADV VERB ADP ADJ ADJ NOUN ADP ADJ NOUN CCONJ NOUN PUNCT ADV ADV ADP NOUN NOUN ADP ADJ NOUN ADP PRON NOUN PUNCT DET NOUN ADP DET NOUN ADV VERB ADP DET NOUN ADP ADJ ADJ NOUN PUNCT PRON AUX PART ADV ADJ PART VERB CCONJ ADV VERB ADJ NOUN ADP DET NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN NOUN PUNCT ADJ NOUN PUNCT X X PUNCT ADP ADJ VERB NOUN PUNCT PART VERB ADJ NOUN ADP ADJ ADJ NOUN ADP PROPN NOUN PUNCT PRON VERB DET ADJ NOUN VERB PROPN PROPN PUNCT PRON ADV VERB DET NOUN ADP ADJ NOUN CCONJ VERB PRON ADV NOUN ADJ ADP VERB DET ADJ NOUN ADP ADJ VERB NOUN PART VERB DET NOUN ADP PROPN VERB PROPN NOUN ADP VERB NOUN ADP NOUN NOUN PUNCT,0.6373626373626373,36.4,5.3791208791208796
209,123,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.",23.164795774647928,22.733194790065724,209,0.6318720579147339," Propname Propname is designed to develop Propname agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non evasive manner, providing explanations of the reasons behind the systems objections. Our approach encompasses four essential stages: 0. Propname Propname: We employ the self instruct mechanism by Propname Propname Propname. with 000 seed prompts to generate synthetic instructions, plus 00 topic specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contextsscenarios for the Propname system to learn from, reducing potential biases as a consequence.0. Principle Propname Propname Propname: We offer a small set of 00 human written principles in Propname about the desirable quality of the system produced responses, or the rules behind the behavior of the Propname model in producing answers0. These principles function as guidelines for generating 0This is the definition of Propname alignment in this paper, distinct from following simple instructions. 0The detailed principles are given in Propname A. Propname to Propname Propname, the design of these principles in Propname Propname remains exploratory and primarily serves research purposes.0 Table0: Comparison of humanteacher supervisions used in recent Propname systems."," Propname Propname is designed to develop Propname agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non evasive manner, providing explanations of the reasons behind the systems objections. Our approach encompasses four essential stages: 0. Propname Propname: We employ the self instruct mechanism by Propname Propname Propname. with 000 seed prompts to generate synthetic instructions, plus 00 topic specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contextsscenarios for the Propname system to learn from, reducing potential biases as a consequence.0. Principle Propname Propname Propname: We offer a small set of 00 human written principles in Propname about the desirable quality of the system produced responses, or the rules behind the behavior of the Propname model in producing answers0. These principles function as guidelines for generating 0This is the definition of Propname alignment in this paper, distinct from following simple instructions. 0The detailed principles are given in Propname A. Propname to Propname Propname, the design of these principles in Propname Propname remains exploratory and primarily serves research purposes.0 Table0: Comparison of humanteacher supervisions used in recent Propname systems.", PROPN PROPN AUX VERB PART VERB PROPN NOUN ADJ ADP VERB ADJ PUNCT ADJ PUNCT CCONJ ADJ NOUN ADP NOUN NOUN PUNCT VERB ADJ NOUN PUNCT SCONJ ADV VERB ADJ NOUN ADP DET ADJ ADJ NOUN PUNCT VERB NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT PRON NOUN VERB NUM ADJ NOUN PUNCT NUM PUNCT PROPN PROPN PUNCT PRON VERB DET NOUN NOUN NOUN ADP PROPN PROPN PROPN PUNCT SCONJ NUM NOUN NOUN PART VERB ADJ NOUN PUNCT CCONJ NUM NOUN ADJ NOUN ADP NOUN PART VERB DET ADJ NOUN NOUN ADP DET NOUN PUNCT ADJ NOUN VERB DET ADJ NOUN ADP NOUN ADP DET PROPN NOUN PART VERB ADP PUNCT VERB ADJ NOUN ADP DET NOUN PUNCT PUNCT PUNCT ADJ PROPN PROPN PROPN PUNCT PRON VERB DET ADJ NOUN ADP NUM ADJ VERB NOUN ADP PROPN ADP DET ADJ NOUN ADP DET NOUN VERB NOUN PUNCT CCONJ DET NOUN ADP DET NOUN ADP DET PROPN NOUN ADP VERB NOUN PUNCT DET NOUN VERB ADP NOUN ADP VERB NOUN AUX DET NOUN ADP PROPN NOUN ADP DET NOUN PUNCT ADJ ADP VERB ADJ NOUN PUNCT NOUN ADJ NOUN AUX VERB ADP PROPN NOUN PROPN ADP PROPN PROPN PUNCT DET NOUN ADP DET NOUN ADP PROPN PROPN VERB ADJ CCONJ ADV VERB NOUN NOUN PUNCT PUNCT NOUN PUNCT PUNCT NOUN ADP NOUN NOUN VERB ADP ADJ PROPN NOUN PUNCT,0.5669642857142857,28.0,5.513392857142857
210,124,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.",46.58524456521741,22.733194790065724,210,0.48284783959388733," The alignment techniques used in previous work include Propname, Propname, Propname, and Propname. Information is from: a Propname, Propname Propname, Propname Propname Propname Propname., Propname, d OpenAI. Total Propname Propname Propname Propname Propname InstructGPT 00 Propname Propname Propname SFT Propname Propname Propname 000?? Propname Propname a ChatGPT?? Propname Propname Propname Propname?? Propname Propname c Propname 0?? Propname Propname Propname d Propname 00 Propname Propname Propname 000 Propname Propname Propname Propname 00 Propname Propname Propname Propname Propname 000 Propname Propname Teacher Propname Propname Propname Propname 000 Propname Propname Propname Propname Dolly Propname 00 Propname Propname Propname Propname 000 lines Humans Propname Propname Propname Propname helpful, ethical, and reliable responses. We conduct in context learning with a few exemplars that illustrate how the Propname system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different exemplars for each query. From the human written principles, Propname exemplars, and the incoming self instructed prompts, the Propname can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill formed one.0."," The alignment techniques used in previous work include Propname, Propname, Propname, and Propname. Information is from: a Propname, Propname Propname, Propname Propname Propname Propname., Propname, d OpenAI. Total Propname Propname Propname Propname Propname InstructGPT 00 Propname Propname Propname SFT Propname Propname Propname 000?? Propname Propname a ChatGPT?? Propname Propname Propname Propname?? Propname Propname c Propname 0?? Propname Propname Propname d Propname 00 Propname Propname Propname 000 Propname Propname Propname Propname 00 Propname Propname Propname Propname Propname 000 Propname Propname Teacher Propname Propname Propname Propname 000 Propname Propname Propname Propname Dolly Propname 00 Propname Propname Propname Propname 000 lines Humans Propname Propname Propname Propname helpful, ethical, and reliable responses. We conduct in context learning with a few exemplars that illustrate how the Propname system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different exemplars for each query. From the human written principles, Propname exemplars, and the incoming self instructed prompts, the Propname can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill formed one.0.", DET NOUN NOUN VERB ADP ADJ NOUN VERB PROPN PUNCT PROPN PUNCT PROPN PUNCT CCONJ PROPN PUNCT NOUN AUX ADP PUNCT DET PROPN PUNCT PROPN PROPN PUNCT PROPN PROPN PROPN PROPN PUNCT PUNCT PROPN PUNCT X NOUN PUNCT ADJ PROPN PROPN PROPN PROPN PROPN NOUN NUM PROPN PROPN PROPN VERB PROPN PROPN PROPN NUM PUNCT PUNCT PROPN PROPN DET NOUN PUNCT PUNCT PROPN PROPN PROPN PROPN PUNCT PUNCT PROPN PROPN NOUN PROPN NUM PUNCT PUNCT PROPN PROPN PROPN NOUN PROPN NUM PROPN PROPN PROPN NUM PROPN PROPN PROPN PROPN NUM PROPN PROPN PROPN PROPN PROPN NUM PROPN PROPN NOUN PROPN PROPN PROPN PROPN NUM PROPN PROPN PROPN PROPN ADV PROPN NUM PROPN PROPN PROPN PROPN NUM NOUN NOUN PROPN PROPN PROPN PROPN ADJ PUNCT ADJ PUNCT CCONJ ADJ NOUN PUNCT PRON VERB ADP NOUN VERB ADP DET ADJ NOUN PRON VERB SCONJ DET PROPN NOUN VERB ADP DET NOUN SCONJ VERB NOUN ADP ADJ NOUN PUNCT VERB DET ADJ NOUN PUNCT DET ADJ NOUN ADP NOUN AUX VERB ADP DET NOUN ADP NOUN NOUN PUNCT ADV ADP VERB ADJ NOUN ADP DET NOUN PUNCT ADP DET NOUN VERB NOUN PUNCT PROPN NOUN PUNCT CCONJ DET ADJ NOUN VERB NOUN PUNCT DET PROPN AUX VERB DET NOUN NOUN CCONJ VERB DET NOUN ADP DET ADJ NOUN SCONJ DET NOUN AUX VERB ADP DET ADJ CCONJ ADJ VERB NUM PUNCT PUNCT PUNCT,0.4008810572687225,22.7,5.370044052863436
211,125,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.",24.042701612903244,22.733194790065724,211,0.714279294013977," Principle Engraving: In the third stage, we fine tune the original Propname on the self aligned responses, generated by the Propname itself through prompting, while pruning the principles and demonstrations for the fine tuned model. The fine tuning process enables our system to directly generate responses that are well aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine tuned Propname can directly generate high quality responses for new queries without explicitly using the principle set and the Propname exemplars.0. Propname Propname: Lastly, we employ context distillation to enhance the systems capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF Propname process necessitates fewer than 000 lines of annotations, while previous aligned Propname systems such as InstructGPT or Propname required at least 00 Propname humanteacher annotations. This highlights the supervision efficiency of our approach in comparison with other state of the art Propname assistants, as shown in Propname.0."," Principle Engraving: In the third stage, we fine tune the original Propname on the self aligned responses, generated by the Propname itself through prompting, while pruning the principles and demonstrations for the fine tuned model. The fine tuning process enables our system to directly generate responses that are well aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine tuned Propname can directly generate high quality responses for new queries without explicitly using the principle set and the Propname exemplars.0. Propname Propname: Lastly, we employ context distillation to enhance the systems capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF Propname process necessitates fewer than 000 lines of annotations, while previous aligned Propname systems such as InstructGPT or Propname required at least 00 Propname humanteacher annotations. This highlights the supervision efficiency of our approach in comparison with other state of the art Propname assistants, as shown in Propname.0.", ADJ NOUN PUNCT ADP DET ADJ NOUN PUNCT PRON ADJ NOUN DET ADJ PROPN ADP DET NOUN VERB NOUN PUNCT VERB ADP DET PROPN PRON ADP VERB PUNCT SCONJ VERB DET NOUN CCONJ NOUN ADP DET ADJ VERB NOUN PUNCT DET ADJ NOUN NOUN VERB PRON NOUN PART ADV VERB NOUN PRON AUX ADV VERB ADP DET ADJ PUNCT ADJ PUNCT CCONJ ADJ NOUN ADP DET ADJ NOUN ADP NOUN PUNCT ADP ADP VERB NOUN NOUN PUNCT VERB SCONJ DET ADJ VERB PROPN AUX ADV VERB ADJ NOUN NOUN ADP ADJ NOUN ADP ADV VERB DET NOUN VERB CCONJ DET PROPN NOUN PUNCT PUNCT PUNCT PROPN PROPN PUNCT ADV PUNCT PRON VERB NOUN NOUN PART VERB DET NOUN NOUN PART VERB ADV ADJ CCONJ ADJ NOUN ADP DET ADV ADJ CCONJ ADJ NOUN PUNCT ADV PUNCT DET ADJ NOUN PROPN NOUN VERB ADJ ADP NUM NOUN ADP NOUN PUNCT SCONJ ADJ VERB PROPN NOUN ADJ ADP NOUN CCONJ PROPN VERB ADP ADJ NUM PROPN NOUN NOUN PUNCT PRON VERB DET NOUN NOUN ADP PRON NOUN ADP NOUN ADP ADJ NOUN ADP DET NOUN PROPN NOUN PUNCT SCONJ VERB ADP PROPN PUNCT PUNCT PUNCT,0.6031746031746031,31.5,5.328042328042328
212,126,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.",14.23737475915226,22.733194790065724,212,0.4525047540664673," Our principle driven approach, which is essentially rule based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Propname and Propname have shown that the potent conversational capabilities can be obtained by distilling existing human preferencealigned LLMs into smaller, more manageable models. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from Propname, independent from the existence of well aligned LLMs like ChatGPT or Propname 0. That is the main distinction of our approach from other existing approaches and is why we call it self alignment from scratch."," Our principle driven approach, which is essentially rule based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Propname and Propname have shown that the potent conversational capabilities can be obtained by distilling existing human preferencealigned LLMs into smaller, more manageable models. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from Propname, independent from the existence of well aligned LLMs like ChatGPT or Propname 0. That is the main distinction of our approach from other existing approaches and is why we call it self alignment from scratch.", PRON ADJ VERB NOUN PUNCT PRON AUX ADV NOUN VERB PUNCT PART ADV ADV VERB DET VERB ADJ NOUN ADP NOUN CCONJ ADV VERB VERB ADJ NOUN NOUN ADP ADJ NOUN ADP NOUN CCONJ NOUN ADP NOUN NOUN NOUN ADP CCONJ DET ADJ CCONJ ADJ NOUN PUNCT PRON AUX ADV VERB ADP SCONJ DET NOUN ADP ADJ NOUN ADP PROPN CCONJ PROPN AUX VERB SCONJ DET ADJ ADJ NOUN AUX AUX VERB ADP VERB VERB NOUN ADJ NOUN ADP ADJ PUNCT ADV ADJ NOUN PUNCT PRON VERB ADJ NOUN PUNCT ADV PUNCT ADV VERB ADP DET ADJ NOUN ADP VERB NOUN PUNCT PRON AUX VERB ADP ADJ ADJ VERB NOUN PUNCT ADP ADJ NOUN PUNCT DET ADJ NOUN ADV VERB DET NOUN ADP DET NOUN ADP ADJ NOUN ADP NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB ADP NOUN NOUN NOUN ADP PROPN PUNCT ADJ ADP DET NOUN ADP ADV VERB NOUN ADP NOUN CCONJ PROPN NUM PUNCT PRON AUX DET ADJ NOUN ADP PRON NOUN ADP ADJ VERB NOUN CCONJ AUX SCONJ PRON VERB PRON NOUN NOUN ADP NOUN PUNCT,0.6187845303867403,30.166666666666668,5.342541436464089
213,127,Zhiqing Sun,"[' The problem of aligning large language models (LLMs) to human values and intentions in terms of being comprehensive, respectful, and compliant1 [9, 32, 30, 3, 4, 27] has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities [11, 34, 6, 8]. Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences [26, 28, 29, 1]. The success of these techniques heavily relies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations [48? , 47]. To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named SELF-ALIGN. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the behavior of LLM-based AI agents in generating responses to users’ queries.', 'SELF-ALIGN is designed to develop AI agents capable of generating helpful, ethical, and reliable responses to user queries, including adversarial ones, while proactively addressing harmful inquiries in a non-evasive manner, providing explanations of the reasons behind the system’s objections. Our approach encompasses four essential stages: 1. (Topic-Guided Red-Teaming) Self-Instruct: We employ the self-instruct mechanism by Wang et al. [48] with 175 seed prompts to generate synthetic instructions, plus 20 topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from, reducing potential biases as a consequence. 2. Principle-Driven Self-Alignment: We offer a small set of 16 human-written principles in English about the desirable quality of the system-produced responses, or the rules behind the behavior of the AI model in producing answers2 . These principles function as guidelines for generating 1This is the definition of AI alignment in this paper, distinct from following simple instructions [30, 48, 43]. 2The detailed principles are given in Appendix A. Analogous to Constitutional AI [4], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems.', 'The alignment techniques used in previous work include SFT (Supervised Fine-tuning), RLHF (Reinforcement Learning from Human Feedback), CAI (Constitutional AI), and KD (Knowledge Distillation). Information is from: a OpenAI [29], b OpenAI [26], c Bai et al. [4], Anthropic [1], d OpenAI [27]. Total Annotations Annotation Sources Alignment Techniques (closed-source models) InstructGPT 77K Users & Annotators SFT & RLHF Text-Davinci-003 ? ? SFT & RLHF a ChatGPT ? ? SFT & RLHF b Claude ? ? RLHF & CAI c GPT-4 ? ? SFT & RLHF & CAI d (open-source models) Alpaca 52K Text-Davinci-003 Self-Instruct & KD Vicuna 70K Users & ChatGPT KD Koala 472K Humans & Teacher Models KD & SFT OpenAssistant 161K Annotators SFT & RLHF Dolly-V2 15K Annotators SFT Dromedary < 300 lines Humans Self-Instruct & Self-Align helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [6] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. Given each new query, the same set of exemplars is used in the process of response generation, instead of requiring different (human-annotated) exemplars for each query. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3.', 'Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of questions, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 2] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [43] required at least 50K human/teacher annotations. This highlights the supervision efficiency of our approach in comparison with other state-of-the-art AI assistants, as shown in Table. 1.', 'Our principle-driven approach, which is essentially rule-based, not only significantly reduces the required human effort for supervision but also showcases aligning neural language models with human understanding of principles or rules about quality language generation in both an effective and efficient manner. We should also point out that the advancements of recent models like Alpaca and Vicuna have shown that the potent conversational capabilities can be obtained by distilling existing human-preferencealigned LLMs (i.e., Text-Davinci-003 and ChatGPT, respectively) into smaller, more manageable models [43, 7, 29, 26]. Those resulting smaller models, however, still rely on the successful alignment of existing LLMs, which are based on extensive human-provided supervision. In other words, those smaller models indirectly inherit the dependence on the availability of intensive supervision from humans. In contrast, our approach focuses on language model alignment from scratch, independent from the existence of well-aligned LLMs like ChatGPT or GPT-4. That is the main distinction of our approach from other existing approaches and is why we call it self-alignment from scratch.', 'In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.']",intro_chunked,"In short, by harnessing the intrinsic knowledge within an LLM and combining the power of humanunderstandable principles (a small set) that specify how we want an LLM to behave, SELF-ALIGN allows us to train a well-behaved AI agent whose generated responses follow the guardrails defined 3 SFT + RLHF (Ouyang et al., 2022) User/Annotator Prompt Collection & Filtering Reward Model 33k prompts and human preferences PPO fine-tuning 31k user prompts from customers Self-Align (Ours) (Topic-Guided Red-Teaming) Self-Instruct 195 seed prompts w/ 7 rules for new instruction generation 360k synthetic prompts Principle-Driven Self-Alignment 16 principles for AI assistant to follow w/ 5 in-context learning demonstrations Principle Engraving Fine-tuning the original model after pruning principles and demonstrations 260k (after filtering) self-aligned responses to synthetic prompts Supervised Fine-Tuning (SFT) 13k prompts and human annotations 360k self-aligned & verbose (by prompting) responses to synthetic prompts Verbose Cloning Refining the model to produce indepth and detailed responses 77k+ total human annotations < 300 lines of human annotations (non-verbose) InstructGPT (final) Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT [30]), and on the right are the four stages in our SELF-ALIGN procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the SELF-ALIGN method as open source to promote collaboration and innovation within the research community. The base model of Dromedary is the LLaMA-65b language model [45], which is accessible for research-only, noncommercial purposes. By investigating different strategies from that in RLHF, our work seeks to broaden the scope of AI alignment techniques, and promote a deeper understanding of how to improve the capabilities of AI systems, not only in terms of being more powerful, but also more responsible and well-aligned with human values.",-3.238755274261564,22.733194790065724,213,0.6958321928977966," In short, by harnessing the intrinsic knowledge within an Propname and combining the power of humanunderstandable principles that specify how we want an Propname to behave, Propname Propname allows us to train a well behaved Propname agent whose generated responses follow the guardrails defined 0 Propname Propname Propname Propname Propname Propname Propname Propname 00k prompts and human preferences Propname fine tuning 00k user prompts from customers Propname Propname Propname Instruct 000 seed prompts w 0 rules for new instruction generation 000k synthetic prompts Principle Propname Propname Propname 00 principles for Propname assistant to follow w 0 in context learning demonstrations Propname Propname Propname tuning the original model after pruning principles and demonstrations 000k self aligned responses to synthetic prompts Supervised Propname Tuning 00k prompts and human annotations 000k self aligned verbose responses to synthetic prompts Propname Propname Propname the model to produce indepth and detailed responses 00k total human annotations 000 lines of human annotations Propname Propname 0: Side by side comparison: on the left is a typical Propname Propname alignment pipeline, and on the right are the four stages in our SELF Propname procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the Propname Propname method as open source to promote collaboration and innovation within the research community. The base model of Propname is the Propname 00b language model, which is accessible for research only, noncommercial purposes. By investigating different strategies from that in Propname, our work seeks to broaden the scope of Propname alignment techniques, and promote a deeper understanding of how to improve the capabilities of Propname systems, not only in terms of being more powerful, but also more responsible and well aligned with human values."," In short, by harnessing the intrinsic knowledge within an Propname and combining the power of humanunderstandable principles that specify how we want an Propname to behave, Propname Propname allows us to train a well behaved Propname agent whose generated responses follow the guardrails defined 0 Propname Propname Propname Propname Propname Propname Propname Propname 00k prompts and human preferences Propname fine tuning 00k user prompts from customers Propname Propname Propname Instruct 000 seed prompts w 0 rules for new instruction generation 000k synthetic prompts Principle Propname Propname Propname 00 principles for Propname assistant to follow w 0 in context learning demonstrations Propname Propname Propname tuning the original model after pruning principles and demonstrations 000k self aligned responses to synthetic prompts Supervised Propname Tuning 00k prompts and human annotations 000k self aligned verbose responses to synthetic prompts Propname Propname Propname the model to produce indepth and detailed responses 00k total human annotations 000 lines of human annotations Propname Propname 0: Side by side comparison: on the left is a typical Propname Propname alignment pipeline, and on the right are the four stages in our SELF Propname procedure. by the model creators. And more importantly, the entire alignment process reduces the required amount of human supervision by several orders of magnitude, compared to other existing methods. We are providing the code for the Propname Propname method as open source to promote collaboration and innovation within the research community. The base model of Propname is the Propname 00b language model, which is accessible for research only, noncommercial purposes. By investigating different strategies from that in Propname, our work seeks to broaden the scope of Propname alignment techniques, and promote a deeper understanding of how to improve the capabilities of Propname systems, not only in terms of being more powerful, but also more responsible and well aligned with human values.", ADP ADJ PUNCT ADP VERB DET ADJ NOUN ADP DET PROPN CCONJ VERB DET NOUN ADP ADJ NOUN PRON VERB SCONJ PRON VERB DET PROPN PART VERB PUNCT PROPN PROPN VERB PRON PART VERB DET ADV VERB PROPN NOUN DET VERB NOUN VERB DET NOUN VERB NUM PROPN PROPN PROPN PROPN PROPN PROPN PROPN PROPN NOUN NOUN CCONJ ADJ NOUN PROPN ADJ NOUN NOUN NOUN NOUN ADP NOUN PROPN PROPN PROPN VERB NUM NOUN NOUN NOUN NUM NOUN ADP ADJ NOUN NOUN X ADJ NOUN ADJ PROPN PROPN PROPN NUM NOUN ADP PROPN NOUN PART VERB NOUN NUM ADP NOUN NOUN NOUN PROPN PROPN PROPN VERB DET ADJ NOUN ADP VERB NOUN CCONJ NOUN VERB NOUN VERB NOUN ADP ADJ NOUN VERB PROPN NOUN NOUN NOUN CCONJ ADJ NOUN VERB NOUN VERB ADJ NOUN ADP ADJ NOUN PROPN PROPN PROPN DET NOUN PART VERB ADJ CCONJ ADJ NOUN NOUN VERB ADJ NOUN NUM NOUN ADP ADJ NOUN PROPN PROPN NUM PUNCT NOUN ADP NOUN NOUN PUNCT ADP DET NOUN AUX DET ADJ PROPN PROPN NOUN NOUN PUNCT CCONJ ADP DET NOUN AUX DET NUM NOUN ADP PRON NOUN PROPN NOUN PUNCT ADP DET NOUN NOUN PUNCT CCONJ ADV ADV PUNCT DET ADJ NOUN NOUN VERB DET VERB NOUN ADP ADJ NOUN ADP ADJ NOUN ADP NOUN PUNCT VERB ADP ADJ VERB NOUN PUNCT PRON AUX VERB DET NOUN ADP DET PROPN PROPN NOUN SCONJ ADJ NOUN PART VERB NOUN CCONJ NOUN ADP DET NOUN NOUN PUNCT DET ADJ NOUN ADP PROPN AUX DET PROPN NOUN NOUN NOUN PUNCT PRON AUX ADJ ADP NOUN ADV PUNCT ADJ NOUN PUNCT ADP VERB ADJ NOUN ADP PRON ADP PROPN PUNCT PRON NOUN VERB PART VERB DET NOUN ADP PROPN NOUN NOUN PUNCT CCONJ VERB DET ADJ NOUN ADP SCONJ PART VERB DET NOUN ADP PROPN NOUN PUNCT PART ADV ADP NOUN ADP AUX ADV ADJ PUNCT CCONJ ADV ADV ADJ CCONJ ADV VERB ADP ADJ NOUN PUNCT,0.49074074074074076,54.0,5.432098765432099
214,128,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked," Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).",20.105255102040843,22.733194790065724,214,0.4463213384151459," Propname Propname problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the Propname Propname class of problems, which are believed to be intractable in polynomial time. Traditionally, Propname solvers rely on integer programming or hand crafted heuristics, which demand signif icant expert efforts to approximate near optimal solutions. Recent development in deep learning has shown new promise in solving Propname problems. Existing neural Propname solvers for Propname problems can be roughly classified into three categories based on how the solutions are generated, ie, the autoregressive constructive solvers, the non autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution. Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems. Methods in the second category rely on non autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical."," Propname Propname problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the Propname Propname class of problems, which are believed to be intractable in polynomial time. Traditionally, Propname solvers rely on integer programming or hand crafted heuristics, which demand signif icant expert efforts to approximate near optimal solutions. Recent development in deep learning has shown new promise in solving Propname problems. Existing neural Propname solvers for Propname problems can be roughly classified into three categories based on how the solutions are generated, ie, the autoregressive constructive solvers, the non autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution. Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems. Methods in the second category rely on non autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical.", PROPN PROPN NOUN AUX ADJ NOUN PRON VERB VERB DET ADJ NOUN ADP DET ADJ NOUN PUNCT PRON AUX ADJ NOUN ADP NOUN NOUN PUNCT ADV DET PROPN PROPN NOUN ADP NOUN PUNCT PRON AUX VERB PART AUX ADJ ADP ADJ NOUN PUNCT ADV PUNCT PROPN NOUN VERB ADP NOUN NOUN CCONJ NOUN VERB NOUN PUNCT PRON VERB VERB ADJ NOUN NOUN PART VERB ADP ADJ NOUN PUNCT ADJ NOUN ADP ADJ NOUN AUX VERB ADJ NOUN ADP VERB PROPN NOUN PUNCT VERB ADJ PROPN NOUN ADP PROPN NOUN AUX AUX ADV VERB ADP NUM NOUN VERB ADP SCONJ DET NOUN AUX VERB PUNCT ADV PUNCT DET ADJ ADJ NOUN PUNCT DET ADJ ADJ ADJ NOUN PUNCT CCONJ DET NOUN NOUN NOUN PUNCT NOUN ADP DET ADJ NOUN VERB ADJ NOUN PART ADV VERB DET ADJ ADJ NOUN PUNCT DET NOUN ADV VERB ADP DET ADJ NOUN ADP PRON ADJ VERB NOUN CCONJ ADV AUX ADJ PART VERB ADP ADP ADJ NOUN PUNCT NOUN ADP DET ADJ NOUN VERB ADP ADJ ADJ NOUN ADP VERB ADP PUNCT ADP DET ADJ NOUN NOUN ADP NOUN ADP ADJ PUNCT,0.5989304812834224,23.375,5.625668449197861
215,129,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked,"Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.",27.016763754045343,22.733194790065724,215,0.37545642256736755," Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems. Methods in the third category use a Propname decision process to iteratively refines an existing feasible solution with neural network guided local operations such as 0 opt and Propname swap. These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning framework. Motivated by the recent remarkable success of diffusion models in probabilistic generation, we introduce a novel approach named Propname, which stands for the graph basedDIFfUsion Propname for Propname Propname. To apply the iterative denoising process of diffusion models to graph based settings, we formulate each Propname problem as a 0, 0 valued vector with Propname variables that indicate the selection of nodes or edges in the candidate solutions for the task."," Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems. Methods in the third category use a Propname decision process to iteratively refines an existing feasible solution with neural network guided local operations such as 0 opt and Propname swap. These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning framework. Motivated by the recent remarkable success of diffusion models in probabilistic generation, we introduce a novel approach named Propname, which stands for the graph basedDIFfUsion Propname for Propname Propname. To apply the iterative denoising process of diffusion models to graph based settings, we formulate each Propname problem as a 0, 0 valued vector with Propname variables that indicate the selection of nodes or edges in the candidate solutions for the task.", DET DET NOUN PUNCT ADV PUNCT ADV VERB DET NOUN ADP DET NOUN PART VERB DET ADJ NOUN ADP DET NOUN PUNCT NOUN ADP DET ADJ NOUN VERB DET PROPN NOUN NOUN PART ADV VERB DET VERB ADJ NOUN ADP ADJ NOUN VERB ADJ NOUN ADJ ADP NUM NOUN CCONJ PROPN NOUN PUNCT DET NOUN AUX ADV VERB ADP DET NOUN ADP VERB ADP CCONJ DET NOUN ADP NOUN PUNCT ADV ADP ADP DET ADJ NOUN CCONJ NOUN NOUN NOUN SCONJ VERB NOUN NOUN ADP DET NOUN NOUN NOUN PUNCT VERB ADP DET ADJ ADJ NOUN ADP NOUN NOUN ADP ADJ NOUN PUNCT PRON VERB DET ADJ NOUN VERB PROPN PUNCT PRON VERB ADP DET NOUN VERB PUNCT PROPN ADP PROPN PROPN PUNCT PART VERB DET ADJ NOUN NOUN ADP NOUN NOUN PART VERB VERB NOUN PUNCT PRON VERB DET PROPN NOUN ADP DET NUM PUNCT NUM VERB NOUN ADP PROPN NOUN PRON VERB DET NOUN ADP NOUN CCONJ NOUN ADP DET NOUN NOUN ADP DET NOUN PUNCT,0.6407185628742516,33.4,5.209580838323354
216,130,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked,"Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few ( N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.",24.801636363636362,22.733194790065724,216,0.40631410479545593," Then we use a message passing based graph neural network to encode each instance graph and to denoise the corrupted variables. Such a graph based diffusion model overcomes the limitations of previous neural Propname solvers from a new perspective. Firstly, Propname can perform inference on all variables in parallel with a few denoising steps, avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, Propname can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non autoregressive constructive models. Last but not least, Propname is trained in an efficient and stable manner with supervised denoising, which solves the training scalability issue of Propname based improvement heuristics methods. We should point out that the idea of utilizing a diffusion based generative model for Propname problems has been explored recently in the literature. In particular, Propname Propname Propname. proposed an image based diffusion model to solveEuclidean Traveling Propname problems by projecting each Propname instance onto a 00 00 greyscale image space and then using a Propname Propname Propname to generate the predicted solution image."," Then we use a message passing based graph neural network to encode each instance graph and to denoise the corrupted variables. Such a graph based diffusion model overcomes the limitations of previous neural Propname solvers from a new perspective. Firstly, Propname can perform inference on all variables in parallel with a few denoising steps, avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, Propname can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non autoregressive constructive models. Last but not least, Propname is trained in an efficient and stable manner with supervised denoising, which solves the training scalability issue of Propname based improvement heuristics methods. We should point out that the idea of utilizing a diffusion based generative model for Propname problems has been explored recently in the literature. In particular, Propname Propname Propname. proposed an image based diffusion model to solveEuclidean Traveling Propname problems by projecting each Propname instance onto a 00 00 greyscale image space and then using a Propname Propname Propname to generate the predicted solution image.", ADV PRON VERB DET NOUN VERB VERB NOUN ADJ NOUN PART VERB DET NOUN NOUN CCONJ PART VERB DET VERB NOUN PUNCT DET DET NOUN VERB NOUN NOUN VERB DET NOUN ADP ADJ ADJ PROPN NOUN ADP DET ADJ NOUN PUNCT ADV PUNCT PROPN AUX VERB NOUN ADP DET NOUN ADP NOUN ADP DET ADJ VERB NOUN PUNCT VERB DET ADJ NOUN NOUN ADP ADJ ADJ NOUN PUNCT ADV PUNCT PROPN AUX VERB DET ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET ADJ NOUN ADP ADJ ADJ ADJ ADJ NOUN PUNCT ADJ CCONJ PART ADJ PUNCT PROPN AUX VERB ADP DET ADJ CCONJ ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB DET NOUN NOUN NOUN ADP PROPN VERB NOUN NOUN NOUN PUNCT PRON AUX VERB ADP SCONJ DET NOUN ADP VERB DET NOUN VERB ADJ NOUN ADP PROPN NOUN AUX AUX VERB ADV ADP DET NOUN PUNCT ADP ADJ PUNCT PROPN PROPN PROPN PUNCT VERB DET NOUN VERB NOUN NOUN ADP ADJ VERB PROPN NOUN ADP VERB DET PROPN NOUN ADP DET NUM NUM NOUN NOUN NOUN CCONJ ADV VERB DET PROPN PROPN PROPN PART VERB DET VERB NOUN NOUN PUNCT,0.5885416666666666,24.0,5.541666666666667
217,131,Zhiqing Sun,"[' Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand signif- icant expert efforts to approximate near-optimal solutions(Arora, 1996; Gonzalez, 2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution (Bello et al., 2016; Kool et al., 2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems (Fu et al., 2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical (Joshi et al., 2019; Karalias & Loukas, 2020; Qiu et al., 2022).', 'Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems (Khalil et al., 2017; Gu et al., 2018). Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and node swap (Chen & Tian, 2019; Wu et al., 2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework (Wu et al., 2021; Ma et al., 2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation (Song & Ermon, 2019; Ho et al., 2020; Rombach et al., 2022; Yu et al. ; Saharia et al., 2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as a {0, 1}-valued vector with N variables that indicate the selection of nodes or edges in the candidate solutions for the task.', 'Then we use a message passing-based graph neural network (Kipf & Welling, 2016; Hamilton et al., 2017; Gilmer et al., 2017; Veli ˇckovi ´c et al., 2018) to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\x1c N ) denoising steps (Sec. 3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec. 3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular, Graikos et al. (2022) proposed an image-based diffusion model to solveEuclidean Traveling Salesman problems by projecting each TSP instance onto a 64 × 64 greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image.', 'The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.']",intro_chunked,"The main difference between such image-based diffusion solver and our graph-based diffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive (Kool et al., 2019a) and improvement heuristics (d O Costa et al., 2020) solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the DIFUSCO framework: continuous diffusion with Gaussian noise (Chen et al., 2022) and discrete diffusion with Bernoulli noise (Austin et al., 2021; Hoogeboom et al., 2021). These two types of diffusion models have been applied to image processing but not to NPC problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section 4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network (Bresson & Laurent, 2018; Joshi et al., 2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.",20.672198193916387,22.733194790065724,217,0.43164122104644775," The main difference between such image based diffusion solver and our graph based diffusion solver is that the latter can explicitly model the Propname selection process via the corresponding random variables, which is a natural design choice for formulating Propname problems, while the former does not support such a desirable formalism. Although graph based modeling has been employed with both constructive and improvement heuristics solvers, how to use graph based diffusion models for solving Propname problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the Propname framework: continuous diffusion with Propname noise and discrete diffusion with Propname noise. These two types of diffusion models have been applied to image processing but not to Propname problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin. We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Propname Propname Propname Propname, can be used as the backbone network for two different Propname complete combinatorial optimization problems: Traveling Propname Propname and Propname Propname Propname. Our experimental results show that DIFUSCO outperforms previous probabilistic Propname solvers on benchmark datasets of Propname and Propname problems with various sizes."," The main difference between such image based diffusion solver and our graph based diffusion solver is that the latter can explicitly model the Propname selection process via the corresponding random variables, which is a natural design choice for formulating Propname problems, while the former does not support such a desirable formalism. Although graph based modeling has been employed with both constructive and improvement heuristics solvers, how to use graph based diffusion models for solving Propname problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion within the Propname framework: continuous diffusion with Propname noise and discrete diffusion with Propname noise. These two types of diffusion models have been applied to image processing but not to Propname problems. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin. We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Propname Propname Propname Propname, can be used as the backbone network for two different Propname complete combinatorial optimization problems: Traveling Propname Propname and Propname Propname Propname. Our experimental results show that DIFUSCO outperforms previous probabilistic Propname solvers on benchmark datasets of Propname and Propname problems with various sizes.", DET ADJ NOUN ADP ADJ NOUN VERB NOUN VERB CCONJ PRON NOUN VERB NOUN NOUN AUX SCONJ DET ADJ AUX ADV VERB DET PROPN NOUN NOUN ADP DET ADJ ADJ NOUN PUNCT PRON AUX DET ADJ NOUN NOUN ADP VERB PROPN NOUN PUNCT SCONJ DET ADJ AUX PART VERB DET DET ADJ NOUN PUNCT SCONJ NOUN VERB NOUN AUX AUX VERB ADP PRON ADJ CCONJ NOUN NOUN NOUN PUNCT SCONJ PART VERB NOUN VERB NOUN NOUN ADP VERB PROPN NOUN AUX PART AUX VERB ADV PUNCT ADP DET ADJ ADP PRON NOUN PUNCT PRON VERB NUM NOUN ADP ADJ NOUN ADP DET PROPN NOUN PUNCT ADJ NOUN ADP PROPN NOUN CCONJ ADJ NOUN ADP PROPN NOUN PUNCT DET NUM NOUN ADP NOUN NOUN AUX AUX VERB ADP NOUN NOUN CCONJ PART ADP PROPN NOUN PUNCT PRON ADV VERB DET NUM NOUN ADP VERB CCONJ VERB SCONJ ADJ NOUN VERB ADJ ADP ADJ NOUN ADP DET ADJ NOUN PUNCT PRON ADV VERB DET ADJ NOUN NOUN PART VERB DET NOUN NOUN ADP DET ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB SCONJ DET ADJ NOUN ADJ NOUN NOUN PUNCT ADV DET PROPN PROPN PROPN PROPN PUNCT AUX AUX VERB ADP DET NOUN NOUN ADP NUM ADJ PROPN ADJ ADJ NOUN NOUN PUNCT VERB PROPN PROPN CCONJ PROPN PROPN PROPN PUNCT PRON ADJ NOUN VERB SCONJ NOUN VERB ADJ ADJ PROPN NOUN ADP ADJ NOUN ADP PROPN CCONJ PROPN NOUN ADP ADJ NOUN PUNCT,0.5206611570247934,30.25,5.586776859504132
218,132,Zhiqing Sun,"[' Complex physical systems described by non-linear partial\ndifferential equations (PDEs) are ubiquitous throughout the\n1Carnegie Mellon University, Pittsburgh, PA 15213, USA\n2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems\nin aeronautics (Rhie & Chow, 1983), medicine (Sallam &\nHwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations\n(Courant et al., 1967). Solving most equations of importance\nis usually computationally intractable with direct numerical\nsimulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE\nsolvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov\net al. 2021; Brandstetter et al. 2021, inter alia) have shown\nthat end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike\nclassical finite differences, finite volumes, finite elements,\nor pseudo-spectral methods that require a smooth variation\non the high-resolution meshes for guaranteed convergence,\nneural solvers do not rely on such conditions and are able\nto model the underlying physics with under-resolved low\nresolutions and produce high-quality simulations with significantly reduced computational cost.', 'The power of learnable PDE solvers is usually believed to\ncome from the super-resolution ability of neural networks,\nwhich means that the machine learning model is capable of\nrecovering the missing details based on the coarse features\n(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly\ntraining a super-resolution model, and then find that since\nlow-resolution down-sampling of the field can lead to some\ninformation loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the\ntrajectories and the temporal feature encoding scheme are\ncrucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of\ntwo worlds: stencil learning (i.e., Learned Interpolation in\nKochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO\n(Gu et al., 2020) as a state-of-the-art time series sequence\nmodel. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux\nwithin a finite volume method framework. As illustrated in\nFig.', '1, TSM can be regarded as a temporal generalization\nof classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned\ninterpolation solvers (Kochkov et al., 2021), both of which\nadaptively weight or interpolate the stencils based on the\nlatest states only. On the other hand, in TSM we use the\nHiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity\non each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation\ncoefficients, while the stencil learning framework ensures\nthat the neural system’s prediction exactly conserves the\nConservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize\nthe temporal bundling technique (Brandstetter et al., 2021)\nto avoid over-fitting and improve the prediction latency for\nTSM. Following the precedent work in the field (Li et al., 2020c;\nKochkov et al., 2021; Brandstetter et al., 2021), we evaluate\nthe proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing\nequation for turbulent flows with the conservation of mass\nand momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows\ncan achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and\ndifferent Reynolds numbers.']",intro_chunked," Complex physical systems described by non-linear partial
differential equations (PDEs) are ubiquitous throughout the
1Carnegie Mellon University, Pittsburgh, PA 15213, USA
2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems
in aeronautics (Rhie & Chow, 1983), medicine (Sallam &
Hwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations
(Courant et al., 1967). Solving most equations of importance
is usually computationally intractable with direct numerical
simulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE
solvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov
et al. 2021; Brandstetter et al. 2021, inter alia) have shown
that end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike
classical finite differences, finite volumes, finite elements,
or pseudo-spectral methods that require a smooth variation
on the high-resolution meshes for guaranteed convergence,
neural solvers do not rely on such conditions and are able
to model the underlying physics with under-resolved low
resolutions and produce high-quality simulations with significantly reduced computational cost.",18.755053326561722,22.733194790065724,218,0.5368000268936157," Complex physical systems described by non linear partial 
 differential equations are ubiquitous throughout the 
 0Carnegie Propname Propname, Propname, Propname 00000, Propname 
 Propname Propname Propname, Propname, Propname 00000, Propname. Correspondence to: Zhiqing Propname Propname. Preprint. real world, with applications ranging from design problems 
 in aeronautics, medicine Propname Propname, 0000, to scientific problems of molecular modeling and astronomical simulations 
. Solving most equations of importance 
 is usually computationally intractable with direct numerical 
 simulations and the finest features in high resolutions. Recent advances in machine learning accelerated Propname 
 solvers Propname Propname Propname Propname. 0000; Propname Propname Propname. 0000c; Propname 
 Propname Propname. 0000; Propname Propname Propname. 0000, Propname Propname have shown 
 that end to end neural solvers can efficiently solve important partial differential equations. Unlike 
 classical finite differences, Propname Propname, finite elements, 
 or pseudo spectral methods that require a smooth variation 
 on the high resolution meshes for guaranteed convergence, 
 neural solvers do not rely on such conditions and are able 
 to model the underlying physics with under resolved low 
 resolutions and produce high quality simulations with significantly reduced computational cost."," Complex physical systems described by non linear partial 
 differential equations are ubiquitous throughout the 
 0Carnegie Propname Propname, Propname, Propname 00000, Propname 
 Propname Propname Propname, Propname, Propname 00000, Propname. Correspondence to: Zhiqing Propname Propname. Preprint. real world, with applications ranging from design problems 
 in aeronautics, medicine Propname Propname, 0000, to scientific problems of molecular modeling and astronomical simulations 
. Solving most equations of importance 
 is usually computationally intractable with direct numerical 
 simulations and the finest features in high resolutions. Recent advances in machine learning accelerated Propname 
 solvers Propname Propname Propname Propname. 0000; Propname Propname Propname. 0000c; Propname 
 Propname Propname. 0000; Propname Propname Propname. 0000, Propname Propname have shown 
 that end to end neural solvers can efficiently solve important partial differential equations. Unlike 
 classical finite differences, Propname Propname, finite elements, 
 or pseudo spectral methods that require a smooth variation 
 on the high resolution meshes for guaranteed convergence, 
 neural solvers do not rely on such conditions and are able 
 to model the underlying physics with under resolved low 
 resolutions and produce high quality simulations with significantly reduced computational cost.", ADJ ADJ NOUN VERB ADP ADJ ADJ ADJ SPACE ADJ NOUN AUX ADJ ADP DET SPACE NOUN PROPN PROPN PUNCT PROPN PUNCT PROPN NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT PROPN PUNCT PROPN NUM PUNCT PROPN PUNCT NOUN ADP PUNCT VERB PROPN PROPN PUNCT NOUN PUNCT ADJ NOUN PUNCT ADP NOUN VERB ADP NOUN NOUN SPACE ADP NOUN PUNCT NOUN PROPN PROPN PUNCT NUM PUNCT ADP ADJ NOUN ADP ADJ NOUN CCONJ ADJ NOUN SPACE PUNCT VERB ADJ NOUN ADP NOUN SPACE AUX ADV ADV ADJ ADP ADJ ADJ SPACE NOUN CCONJ DET ADJ NOUN ADP ADJ NOUN PUNCT ADJ NOUN ADP NOUN NOUN VERB PROPN SPACE NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PUNCT NOUN PUNCT PROPN SPACE PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN AUX VERB SPACE PRON VERB PART VERB ADJ NOUN AUX ADV VERB ADJ ADJ ADJ NOUN PUNCT ADP SPACE ADJ ADJ NOUN PUNCT PROPN PROPN PUNCT ADJ NOUN PUNCT SPACE CCONJ NOUN ADJ NOUN PRON VERB DET ADJ NOUN SPACE ADP DET ADJ NOUN VERB ADP VERB NOUN PUNCT SPACE ADJ NOUN AUX PART VERB ADP ADJ NOUN CCONJ AUX ADJ SPACE PART VERB DET VERB NOUN ADP ADP VERB ADJ SPACE NOUN CCONJ VERB ADJ NOUN NOUN ADP ADV VERB ADJ NOUN PUNCT,0.5365853658536586,18.636363636363637,5.824390243902439
219,133,Zhiqing Sun,"[' Complex physical systems described by non-linear partial\ndifferential equations (PDEs) are ubiquitous throughout the\n1Carnegie Mellon University, Pittsburgh, PA 15213, USA\n2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems\nin aeronautics (Rhie & Chow, 1983), medicine (Sallam &\nHwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations\n(Courant et al., 1967). Solving most equations of importance\nis usually computationally intractable with direct numerical\nsimulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE\nsolvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov\net al. 2021; Brandstetter et al. 2021, inter alia) have shown\nthat end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike\nclassical finite differences, finite volumes, finite elements,\nor pseudo-spectral methods that require a smooth variation\non the high-resolution meshes for guaranteed convergence,\nneural solvers do not rely on such conditions and are able\nto model the underlying physics with under-resolved low\nresolutions and produce high-quality simulations with significantly reduced computational cost.', 'The power of learnable PDE solvers is usually believed to\ncome from the super-resolution ability of neural networks,\nwhich means that the machine learning model is capable of\nrecovering the missing details based on the coarse features\n(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly\ntraining a super-resolution model, and then find that since\nlow-resolution down-sampling of the field can lead to some\ninformation loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the\ntrajectories and the temporal feature encoding scheme are\ncrucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of\ntwo worlds: stencil learning (i.e., Learned Interpolation in\nKochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO\n(Gu et al., 2020) as a state-of-the-art time series sequence\nmodel. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux\nwithin a finite volume method framework. As illustrated in\nFig.', '1, TSM can be regarded as a temporal generalization\nof classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned\ninterpolation solvers (Kochkov et al., 2021), both of which\nadaptively weight or interpolate the stencils based on the\nlatest states only. On the other hand, in TSM we use the\nHiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity\non each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation\ncoefficients, while the stencil learning framework ensures\nthat the neural system’s prediction exactly conserves the\nConservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize\nthe temporal bundling technique (Brandstetter et al., 2021)\nto avoid over-fitting and improve the prediction latency for\nTSM. Following the precedent work in the field (Li et al., 2020c;\nKochkov et al., 2021; Brandstetter et al., 2021), we evaluate\nthe proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing\nequation for turbulent flows with the conservation of mass\nand momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows\ncan achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and\ndifferent Reynolds numbers.']",intro_chunked,"The power of learnable PDE solvers is usually believed to
come from the super-resolution ability of neural networks,
which means that the machine learning model is capable of
recovering the missing details based on the coarse features
(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly
training a super-resolution model, and then find that since
low-resolution down-sampling of the field can lead to some
information loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the
trajectories and the temporal feature encoding scheme are
crucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of
two worlds: stencil learning (i.e., Learned Interpolation in
Kochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO
(Gu et al., 2020) as a state-of-the-art time series sequence
model. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux
within a finite volume method framework. As illustrated in
Fig.",22.375384615384633,22.733194790065724,219,0.4279240071773529," The power of learnable Propname solvers is usually believed to 
 come from the super resolution ability of neural networks, 
 which means that the machine learning model is capable of 
 recovering the missing details based on the coarse features 
. In this paper, we first empirically verify such capability by explicitly 
 training a super resolution model, and then find that since 
 low resolution down sampling of the field can lead to some 
 information loss, a single coarse feature map used by previous work is not sufficient enough. We empirically show that the temporal information in the 
 trajectories and the temporal feature encoding scheme are 
 crucial for recovering the super resolution details faithfully. Motivated by the above observations, we propose Propname Propname Propname, which combines the best of 
 two worlds: stencil learning ie, Learned Propname in 
 Propname Propname Propname. 0000 as that used in a state of the art neural Propname solver for conservation form PDEs, and Propname as a state of the art time series sequence 
 model. Specifically, in this paper, we focus on trajectoryenhanced high quality approximation of the convective flux 
 within a finite volume method framework. As illustrated in 
 Propname."," The power of learnable Propname solvers is usually believed to 
 come from the super resolution ability of neural networks, 
 which means that the machine learning model is capable of 
 recovering the missing details based on the coarse features 
. In this paper, we first empirically verify such capability by explicitly 
 training a super resolution model, and then find that since 
 low resolution down sampling of the field can lead to some 
 information loss, a single coarse feature map used by previous work is not sufficient enough. We empirically show that the temporal information in the 
 trajectories and the temporal feature encoding scheme are 
 crucial for recovering the super resolution details faithfully. Motivated by the above observations, we propose Propname Propname Propname, which combines the best of 
 two worlds: stencil learning ie, Learned Propname in 
 Propname Propname Propname. 0000 as that used in a state of the art neural Propname solver for conservation form PDEs, and Propname as a state of the art time series sequence 
 model. Specifically, in this paper, we focus on trajectoryenhanced high quality approximation of the convective flux 
 within a finite volume method framework. As illustrated in 
 Propname.", DET NOUN ADP ADJ PROPN NOUN AUX ADV VERB PART SPACE VERB ADP DET ADJ NOUN NOUN ADP ADJ NOUN PUNCT SPACE PRON VERB SCONJ DET NOUN NOUN NOUN AUX ADJ ADP SPACE VERB DET VERB NOUN VERB ADP DET ADJ NOUN SPACE PUNCT ADP DET NOUN PUNCT PRON ADV ADV VERB ADJ NOUN ADP ADV SPACE VERB DET ADJ NOUN NOUN PUNCT CCONJ ADV VERB SCONJ SCONJ SPACE ADJ NOUN ADV NOUN ADP DET NOUN AUX VERB ADP DET SPACE NOUN NOUN PUNCT DET ADJ NOUN NOUN NOUN VERB ADP ADJ NOUN AUX PART ADJ ADV PUNCT PRON ADV VERB SCONJ DET ADJ NOUN ADP DET SPACE NOUN CCONJ DET ADJ NOUN VERB NOUN AUX SPACE ADJ ADP VERB DET ADJ NOUN NOUN ADV PUNCT VERB ADP DET ADJ NOUN PUNCT PRON VERB PROPN PROPN PROPN PUNCT PRON VERB DET ADJ ADP SPACE NUM NOUN PUNCT NOUN VERB ADP PUNCT VERB PROPN ADP SPACE PROPN PROPN PROPN PUNCT NUM SCONJ PRON VERB ADP DET NOUN ADP DET NOUN ADJ PROPN VERB ADP NOUN NOUN NOUN PUNCT CCONJ PROPN ADP DET NOUN ADP DET NOUN NOUN NOUN NOUN SPACE NOUN PUNCT ADV PUNCT ADP DET NOUN PUNCT PRON VERB ADP VERB ADJ NOUN NOUN ADP DET ADJ NOUN SPACE ADP DET ADJ NOUN NOUN NOUN PUNCT SCONJ VERB ADP SPACE PROPN PUNCT,0.5673076923076923,29.714285714285715,4.966346153846154
220,134,Zhiqing Sun,"[' Complex physical systems described by non-linear partial\ndifferential equations (PDEs) are ubiquitous throughout the\n1Carnegie Mellon University, Pittsburgh, PA 15213, USA\n2Brookhaven National Laboratory, Upton, NY 11973, USA. Correspondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>. Preprint. real world, with applications ranging from design problems\nin aeronautics (Rhie & Chow, 1983), medicine (Sallam &\nHwang, 1984), to scientific problems of molecular modeling (Lelievre & Stoltz, 2016) and astronomical simulations\n(Courant et al., 1967). Solving most equations of importance\nis usually computationally intractable with direct numerical\nsimulations and the finest features in high resolutions. Recent advances in machine learning-accelerated PDE\nsolvers (Bar-Sinai et al. 2019; Li et al. 2020c; Kochkov\net al. 2021; Brandstetter et al. 2021, inter alia) have shown\nthat end-to-end neural solvers can efficiently solve important (mostly temporal) partial differential equations. Unlike\nclassical finite differences, finite volumes, finite elements,\nor pseudo-spectral methods that require a smooth variation\non the high-resolution meshes for guaranteed convergence,\nneural solvers do not rely on such conditions and are able\nto model the underlying physics with under-resolved low\nresolutions and produce high-quality simulations with significantly reduced computational cost.', 'The power of learnable PDE solvers is usually believed to\ncome from the super-resolution ability of neural networks,\nwhich means that the machine learning model is capable of\nrecovering the missing details based on the coarse features\n(Bar-Sinai et al., 2019; Kochkov et al., 2021). In this paper, we first empirically verify such capability by explicitly\ntraining a super-resolution model, and then find that since\nlow-resolution down-sampling of the field can lead to some\ninformation loss, a single coarse feature map used by previous work (Kochkov et al., 2021) is not sufficient enough. We empirically show that the temporal information in the\ntrajectories and the temporal feature encoding scheme are\ncrucial for recovering the super-resolution details faithfully. Motivated by the above observations, we propose Temporal Stencil Modeling (TSM), which combines the best of\ntwo worlds: stencil learning (i.e., Learned Interpolation in\nKochkov et al. 2021) as that used in a state-of-the-art neural PDE solver for conservation-form PDEs, and HiPPO\n(Gu et al., 2020) as a state-of-the-art time series sequence\nmodel. Specifically, in this paper, we focus on trajectoryenhanced high-quality approximation of the convective flux\nwithin a finite volume method framework. As illustrated in\nFig.', '1, TSM can be regarded as a temporal generalization\nof classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned\ninterpolation solvers (Kochkov et al., 2021), both of which\nadaptively weight or interpolate the stencils based on the\nlatest states only. On the other hand, in TSM we use the\nHiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity\non each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation\ncoefficients, while the stencil learning framework ensures\nthat the neural system’s prediction exactly conserves the\nConservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize\nthe temporal bundling technique (Brandstetter et al., 2021)\nto avoid over-fitting and improve the prediction latency for\nTSM. Following the precedent work in the field (Li et al., 2020c;\nKochkov et al., 2021; Brandstetter et al., 2021), we evaluate\nthe proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing\nequation for turbulent flows with the conservation of mass\nand momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows\ncan achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and\ndifferent Reynolds numbers.']",intro_chunked,"1, TSM can be regarded as a temporal generalization
of classic finite volume methods such as WENO (Liu et al. ), and recently proposed learned
interpolation solvers (Kochkov et al., 2021), both of which
adaptively weight or interpolate the stencils based on the
latest states only. On the other hand, in TSM we use the
HiPPO-based temporal features to calculate the interpolation coefficients for approximating the integrated velocity
on each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation
coefficients, while the stencil learning framework ensures
that the neural system’s prediction exactly conserves the
Conservation Law and the incompressibility of the fluid. With the abundant temporal information, we further utilize
the temporal bundling technique (Brandstetter et al., 2021)
to avoid over-fitting and improve the prediction latency for
TSM. Following the precedent work in the field (Li et al., 2020c;
Kochkov et al., 2021; Brandstetter et al., 2021), we evaluate
the proposed TSM neural PDE solver on the 2-D incompressible Navier-Stokes equation, which is the governing
equation for turbulent flows with the conservation of mass
and momentum in a Newtonian fluid. Our empirical evaluation shows that TSM achieves both state-of-the-art simulation accuracy (+19.9%) and inference speed (+25%). We also show that TSM trained with steady-state flows
can achieve strong generalization performance on out-of distribution turbulent flows, including different forcings and
different Reynolds numbers.",18.75288043478261,22.733194790065724,220,0.7506038546562195," 0, Propname can be regarded as a temporal generalization 
 of classic finite volume methods such as Propname, and recently proposed learned 
 interpolation solvers, both of which 
 adaptively weight or interpolate the stencils based on the 
 latest states only. On the other hand, in Propname we use the 
 HiPPO based temporal features to calculate the interpolation coefficients for approximating the integrated velocity 
 on each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation 
 coefficients, while the stencil learning framework ensures 
 that the neural systems prediction exactly conserves the 
 Propname Propname and the incompressibility of the fluid. With the abundant temporal information, we further utilize 
 the temporal bundling technique to avoid over fitting and improve the prediction latency for 
 Propname. Following the precedent work in the field Propname Propname Propname Propname, 0000c; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, we evaluate 
 the proposed Propname neural Propname solver on the 0 D incompressible Propname Propname equation, which is the governing 
 equation for turbulent flows with the conservation of mass 
 and momentum in a Newtonian fluid. Our empirical evaluation shows that Propname achieves both state of the art simulation accuracy and inference speed. We also show that Propname trained with steady state flows 
 can achieve strong generalization performance on out of distribution turbulent flows, including different forcings and 
 different Propname numbers."," 0, Propname can be regarded as a temporal generalization 
 of classic finite volume methods such as Propname, and recently proposed learned 
 interpolation solvers, both of which 
 adaptively weight or interpolate the stencils based on the 
 latest states only. On the other hand, in Propname we use the 
 HiPPO based temporal features to calculate the interpolation coefficients for approximating the integrated velocity 
 on each cell surface. The HiPPO temporal features provide a good representation for calculating the interpolation 
 coefficients, while the stencil learning framework ensures 
 that the neural systems prediction exactly conserves the 
 Propname Propname and the incompressibility of the fluid. With the abundant temporal information, we further utilize 
 the temporal bundling technique to avoid over fitting and improve the prediction latency for 
 Propname. Following the precedent work in the field Propname Propname Propname Propname, 0000c; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, we evaluate 
 the proposed Propname neural Propname solver on the 0 D incompressible Propname Propname equation, which is the governing 
 equation for turbulent flows with the conservation of mass 
 and momentum in a Newtonian fluid. Our empirical evaluation shows that Propname achieves both state of the art simulation accuracy and inference speed. We also show that Propname trained with steady state flows 
 can achieve strong generalization performance on out of distribution turbulent flows, including different forcings and 
 different Propname numbers.", X PUNCT PROPN AUX AUX VERB ADP DET ADJ NOUN SPACE ADP ADJ ADJ NOUN NOUN ADJ ADP PROPN PUNCT CCONJ ADV VERB VERB SPACE NOUN NOUN PUNCT PRON ADP PRON SPACE ADV NOUN CCONJ VERB DET NOUN VERB ADP DET SPACE ADJ NOUN ADV PUNCT ADP DET ADJ NOUN PUNCT ADP PROPN PRON VERB DET SPACE NOUN VERB ADJ NOUN PART VERB DET NOUN NOUN ADP VERB DET VERB NOUN SPACE ADP DET NOUN NOUN PUNCT DET ADJ ADJ NOUN VERB DET ADJ NOUN ADP VERB DET NOUN SPACE NOUN PUNCT SCONJ DET NOUN VERB NOUN VERB SPACE SCONJ DET ADJ NOUN NOUN ADV VERB DET SPACE PROPN PROPN CCONJ DET NOUN ADP DET NOUN PUNCT ADP DET ADJ ADJ NOUN PUNCT PRON ADV VERB SPACE DET ADJ VERB NOUN PART VERB ADP ADJ CCONJ VERB DET NOUN NOUN ADP SPACE PROPN PUNCT VERB DET NOUN NOUN ADP DET NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PRON VERB SPACE DET VERB PROPN ADJ PROPN VERB ADP DET NUM ADJ ADJ PROPN PROPN NOUN PUNCT PRON AUX DET NOUN SPACE NOUN ADP ADJ NOUN ADP DET NOUN ADP NOUN SPACE CCONJ NOUN ADP DET ADJ NOUN PUNCT PRON ADJ NOUN VERB SCONJ PROPN VERB DET NOUN ADP DET NOUN NOUN NOUN CCONJ NOUN NOUN PUNCT PRON ADV VERB SCONJ PROPN VERB ADP ADJ NOUN NOUN SPACE AUX VERB ADJ NOUN NOUN ADP ADP ADP NOUN ADJ NOUN PUNCT VERB ADJ NOUN CCONJ SPACE ADJ PROPN NOUN PUNCT,0.5284552845528455,35.142857142857146,5.528455284552845
221,135,Zhiqing Sun,"[' Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.', 'The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.', 'We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).']",intro_chunked," Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.",17.605875000000026,22.733194790065724,221,0.5443238019943237," Large language models have achieved impressive in context few shot performance on knowledge intensive Propname tasks. For example, in open domain question answering, demonstrated by only a few examples of question answer pairs, LLMs are able to answer arbitrary factoid questions. Recent research shows that retrieval augmentation can further improve LLMs performance on knowledge intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation augmented Propname, wherein we tackle knowledge intensive Propname tasks by first reciting relevant information and then generating the outputs. Such a two step paradigm decomposes the original knowledge intensive task into two sub tasks: knowledge recitation and task execution, where the former can be regarded as a form of intermediate knowledge retrieval step, while the latter is the execution step that produces the final outputs."," Large language models have achieved impressive in context few shot performance on knowledge intensive Propname tasks. For example, in open domain question answering, demonstrated by only a few examples of question answer pairs, LLMs are able to answer arbitrary factoid questions. Recent research shows that retrieval augmentation can further improve LLMs performance on knowledge intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation augmented Propname, wherein we tackle knowledge intensive Propname tasks by first reciting relevant information and then generating the outputs. Such a two step paradigm decomposes the original knowledge intensive task into two sub tasks: knowledge recitation and task execution, where the former can be regarded as a form of intermediate knowledge retrieval step, while the latter is the execution step that produces the final outputs.", ADJ NOUN NOUN AUX VERB ADJ ADP NOUN ADJ NOUN NOUN ADP NOUN ADJ PROPN NOUN PUNCT ADP NOUN PUNCT ADP ADJ NOUN NOUN NOUN PUNCT VERB ADP ADV DET ADJ NOUN ADP NOUN NOUN NOUN PUNCT NOUN AUX ADJ PART VERB ADJ ADJ NOUN PUNCT ADJ NOUN VERB SCONJ NOUN NOUN AUX ADV VERB NOUN NOUN ADP NOUN ADJ NOUN ADP VERB DET NOUN ADP VERB ADJ NOUN ADP DET ADJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN PART VERB NOUN VERB ADJ ADJ ADJ NOUN ADP VERB ADP DET ADJ NOUN PUNCT VERB NOUN VERB PROPN PUNCT SCONJ PRON VERB NOUN ADJ PROPN NOUN ADP ADV VERB ADJ NOUN CCONJ ADV VERB DET NOUN PUNCT DET DET NUM NOUN NOUN VERB DET ADJ NOUN ADJ NOUN ADP NUM NOUN NOUN PUNCT NOUN NOUN CCONJ NOUN NOUN PUNCT SCONJ DET ADJ AUX AUX VERB ADP DET NOUN ADP ADJ NOUN NOUN NOUN PUNCT SCONJ DET ADJ AUX DET NOUN NOUN PRON VERB DET ADJ NOUN PUNCT,0.625,33.6,5.4226190476190474
222,136,Zhiqing Sun,"[' Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.', 'The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.', 'We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).']",intro_chunked,"The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.",41.87000000000003,22.733194790065724,222,0.30698317289352417," The motivation of introducing an additional knowledge recitation step comes from our observation that while few shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre training objective. This hinders LLMs from effectively reciting knowledge from their memory. Consider a student taking a closed book exam that contains knowledge intensive questions, for example, what is the tenth decimal of?. They typically can not directly answer this question because in studying stage, it is highly unlikely that they would read the tenth decimal of is 0. However, there can be some sentences like the first Propname digits of are 0.00000 00000... existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite and answer scheme: The first 00 digits of are 0.00000 00000. So the answer is 0. Here, the knowledge recitation step can serve as an intermediate step that mimics the language modeling pre training task, and thus better helps the Propname to generate factual knowledge."," The motivation of introducing an additional knowledge recitation step comes from our observation that while few shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre training objective. This hinders LLMs from effectively reciting knowledge from their memory. Consider a student taking a closed book exam that contains knowledge intensive questions, for example, what is the tenth decimal of?. They typically can not directly answer this question because in studying stage, it is highly unlikely that they would read the tenth decimal of is 0. However, there can be some sentences like the first Propname digits of are 0.00000 00000... existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite and answer scheme: The first 00 digits of are 0.00000 00000. So the answer is 0. Here, the knowledge recitation step can serve as an intermediate step that mimics the language modeling pre training task, and thus better helps the Propname to generate factual knowledge.", DET NOUN ADP VERB DET ADJ NOUN NOUN NOUN VERB ADP PRON NOUN SCONJ SCONJ ADJ NOUN NOUN AUX VERB NOUN VERB ADJ NOUN NOUN PUNCT DET NOUN AUX ADV PART ADP DET ADJ NOUN ADP DET ADJ ADJ NOUN VERB ADJ NOUN NOUN PUNCT PRON VERB NOUN ADP ADV VERB NOUN ADP PRON NOUN PUNCT VERB DET NOUN VERB DET ADJ NOUN NOUN PRON VERB NOUN ADJ NOUN PUNCT ADP NOUN PUNCT PRON AUX DET ADJ ADJ ADP PUNCT PUNCT PRON ADV AUX PART ADV VERB DET NOUN SCONJ ADP VERB NOUN PUNCT PRON AUX ADV ADJ SCONJ PRON AUX VERB DET ADJ ADJ ADP AUX NUM PUNCT ADV PUNCT PRON AUX AUX DET NOUN ADP DET ADJ PROPN NOUN ADP AUX NUM NUM PUNCT VERB ADP DET NOUN PRON AUX AUX VERB ADP DET NOUN PUNCT ADV PUNCT DET NOUN AUX ADV VERB DET NOUN ADP DET NOUN CCONJ NOUN NOUN PUNCT DET ADJ NUM NOUN ADP AUX NUM NUM PUNCT ADV DET NOUN AUX NUM PUNCT ADV PUNCT DET NOUN NOUN NOUN AUX VERB ADP DET ADJ NOUN PRON VERB DET NOUN VERB ADJ NOUN NOUN PUNCT CCONJ ADV ADV VERB DET PROPN PART VERB ADJ NOUN PUNCT,0.582089552238806,25.125,4.701492537313433
223,137,Zhiqing Sun,"[' Large language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022). For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019). Recent research (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs’ performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus. This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE), wherein we tackle knowledge-intensive NLP tasks by first reciting relevant information and then generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the execution step that produces the final outputs.', 'The motivation of introducing an additional knowledge-recitation step comes from our observation that while few-shot prompting can help LLMs execute specific NLP tasks, these tasks are usually not in a similar form as the original causal language modeling pre-training objective. This hinders LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a student taking a closed-book exam that contains knowledge-intensive questions, for example, “what is the tenth decimal of π?”. They typically cannot directly answer this question because in studying stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the first N digits of π are 3.14159 26535...” existing in the textbook that can be recited by the student. Therefore, a student can possibly answer this question in a recite-and-answer scheme: “The first 10 digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve as an intermediate step that mimics the language modeling pre-training task, and thus better helps the LLM to generate factual knowledge.', 'We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).']",intro_chunked,"We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Question Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can generate answers directly without an external corpus or separate retrieval models (Roberts et al., 2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and compatible with other techniques for boosting few-shot performance of LLMs. We also show that, in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM, fine-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve the recitation performance and lead to a better downstream QA accuracy. Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al., 2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer scheme can improve performance on various types of CBQA tasks, including Wikipedia-basedsingle-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al. 2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).",26.51015202702706,22.733194790065724,223,0.667371392250061," We verify the effectiveness of our recitation augmented generation on few shot Propname Propname Propname Propname tasks, as illustrated in Figure 0. CBQA is an attractive open domain QA task in that a fully parameterized Propname can generate answers directly without an external corpus or separate retrieval models. We show that the proposed recite and answer scheme is an effective method for CBQA and compatible with other techniques for boosting few shot performance of Propname. We also show that, in addition to improving the few shot in context learning performance of Propname enhanced Propname, fine tuning the pre trained LLMs on synthetic generated question passage pairs can further improve the recitation performance and lead to a better downstream Propname accuracy. Experiments on four large language models, Propname, Propname, and Propname show that a recite and answer scheme can improve performance on various types of Propname tasks, including Propname basedsingle hop QA, trivia questions, and Propname based Propname hop Propname."," We verify the effectiveness of our recitation augmented generation on few shot Propname Propname Propname Propname tasks, as illustrated in Figure 0. CBQA is an attractive open domain QA task in that a fully parameterized Propname can generate answers directly without an external corpus or separate retrieval models. We show that the proposed recite and answer scheme is an effective method for CBQA and compatible with other techniques for boosting few shot performance of Propname. We also show that, in addition to improving the few shot in context learning performance of Propname enhanced Propname, fine tuning the pre trained LLMs on synthetic generated question passage pairs can further improve the recitation performance and lead to a better downstream Propname accuracy. Experiments on four large language models, Propname, Propname, and Propname show that a recite and answer scheme can improve performance on various types of Propname tasks, including Propname basedsingle hop QA, trivia questions, and Propname based Propname hop Propname.", PRON VERB DET NOUN ADP PRON NOUN VERB NOUN ADP ADJ NOUN PROPN PROPN PROPN PROPN NOUN PUNCT SCONJ VERB ADP NOUN NUM PUNCT NOUN AUX DET ADJ ADJ NOUN NOUN NOUN SCONJ PRON DET ADV ADJ PROPN AUX VERB NOUN ADV ADP DET ADJ NOUN CCONJ ADJ NOUN NOUN PUNCT PRON VERB SCONJ DET VERB NOUN CCONJ NOUN NOUN AUX DET ADJ NOUN ADP NOUN CCONJ ADJ ADP ADJ NOUN ADP VERB ADJ NOUN NOUN ADP PROPN PUNCT PRON ADV VERB SCONJ PUNCT ADP NOUN ADP VERB DET ADJ NOUN ADP NOUN NOUN NOUN ADP PROPN VERB PROPN PUNCT ADJ VERB DET ADJ VERB NOUN ADP ADJ VERB NOUN NOUN NOUN AUX ADV VERB DET NOUN NOUN CCONJ VERB ADP DET ADJ ADJ PROPN NOUN PUNCT NOUN ADP NUM ADJ NOUN NOUN PUNCT PROPN PUNCT PROPN PUNCT CCONJ PROPN VERB SCONJ DET NOUN CCONJ NOUN NOUN AUX VERB NOUN ADP ADJ NOUN ADP PROPN NOUN PUNCT VERB PROPN VERB NOUN NOUN PUNCT ADJ NOUN PUNCT CCONJ PROPN VERB PROPN NOUN PROPN PUNCT,0.5433526011560693,34.6,5.173410404624278
224,138,Zhiqing Sun,"[' The Transformer architecture (Vaswani et al., 2017) has been successfully applied to various tasks, including natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2020), and time series forecasting (Zhou et al., 2020; Wu et al., 2021). Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) (Kitaev et al., 2020; Daras et al., 2020) and mini-batch spherical k-means (Roy et al., 2021; Wang et al., 2020a). The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.', 'The effectiveness of those ANN approaches rely on the assumption that the (transformed) query and key vectors should lie in the same space, which could be sub-optimal in dealing with different sparsity patterns, as analyzed in this paper (Section 3.2). Besides, the hash functions in LSH are randomized and data-agnostic, which cannot fully utilize the rich information in real-world data distributions. In this paper, we address the above limitations of existing ANN-based methods for attention sparsification. Firstly, we analyze two imbalance issues in LSH-produced sparse attention patterns, i.e., unbalanced hash bucket sizes and unbalanced query-key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that ANN-derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning-to-Hash Attention (LHA), for dynamic attention sparsification with enhanced model expressiveness. LHA directly optimizes our newly defined\nattention utility metric in an end-to-end manner via separate learnable hash functions for queries and\nkeys, respectively. As for reducing the computational complexity in the training phase, LHA uses\nunbiased kernelized attention techniques (Choromanski et al., 2020; Peng et al., 2021) to efficiently\napproximate the attention utilities. Similar to other sparse attention models (Kitaev et al., 2020;\nRoy et al., 2021), LHA reduces the overall complexity of self-attention from O(N2\n) to O(N1.5\n)\nfor sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for\nlanguage modeling, natural language understanding, and Long-Range-Arena show that LHA achieves\nbetter performance compared to strong transformer baselines.']",intro_chunked," The Transformer architecture (Vaswani et al., 2017) has been successfully applied to various tasks, including natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2020), and time series forecasting (Zhou et al., 2020; Wu et al., 2021). Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) (Kitaev et al., 2020; Daras et al., 2020) and mini-batch spherical k-means (Roy et al., 2021; Wang et al., 2020a). The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.",36.42234992101109,22.733194790065724,224,0.430179238319397," The Propname architecture has been successfully applied to various tasks, including natural language processing, computer vision, and time series forecasting. Such a success is mainly due to the self attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Propname Propname Propname approaches, including Propname Propname Propname and mini batch Propname Propname means. The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability."," The Propname architecture has been successfully applied to various tasks, including natural language processing, computer vision, and time series forecasting. Such a success is mainly due to the self attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Propname Propname Propname approaches, including Propname Propname Propname and mini batch Propname Propname means. The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.", DET PROPN NOUN AUX AUX ADV VERB ADP ADJ NOUN PUNCT VERB ADJ NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ NOUN NOUN NOUN PUNCT DET DET NOUN AUX ADV ADJ ADP DET NOUN NOUN NOUN PUNCT PRON VERB DET ADJ PART ADV VERB ADP DET ADJ NOUN ADP DET ADJ NOUN PUNCT CCONJ NOUN NOUN VERB DET ADJ NOUN CCONJ NOUN NOUN ADP NOUN ADP DET NOUN NOUN CCONJ ADV AUX PART VERB ADV ADP ADJ NOUN ADP DET NOUN PUNCT PART VERB DET NOUN NOUN PUNCT NUM ADP DET NOUN AUX PART VERB DET ADJ NOUN NOUN ADP DET ADJ NOUN PUNCT SCONJ DET NOUN NOUN AUX VERB ADP DET ADJ NOUN PUNCT DET ADJ NOUN VERB ADP ADJ NOUN ADP ADJ NOUN NOUN ADP PROPN PROPN PROPN NOUN PUNCT VERB PROPN PROPN PROPN CCONJ ADJ NOUN PROPN PROPN VERB PUNCT DET NOUN CCONJ NOUN AUX VERB CCONJ VERB ADP ADJ NOUN PUNCT VERB SCONJ DET NOUN CCONJ NOUN ADP DET ADJ NOUN AUX ADJ ADP DET ADJ NOUN PUNCT,0.6257309941520468,28.5,5.023391812865497
225,139,Zhiqing Sun,"[' The Transformer architecture (Vaswani et al., 2017) has been successfully applied to various tasks, including natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2020), and time series forecasting (Zhou et al., 2020; Wu et al., 2021). Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result. To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) (Kitaev et al., 2020; Daras et al., 2020) and mini-batch spherical k-means (Roy et al., 2021; Wang et al., 2020a). The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability.', 'The effectiveness of those ANN approaches rely on the assumption that the (transformed) query and key vectors should lie in the same space, which could be sub-optimal in dealing with different sparsity patterns, as analyzed in this paper (Section 3.2). Besides, the hash functions in LSH are randomized and data-agnostic, which cannot fully utilize the rich information in real-world data distributions. In this paper, we address the above limitations of existing ANN-based methods for attention sparsification. Firstly, we analyze two imbalance issues in LSH-produced sparse attention patterns, i.e., unbalanced hash bucket sizes and unbalanced query-key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that ANN-derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning-to-Hash Attention (LHA), for dynamic attention sparsification with enhanced model expressiveness. LHA directly optimizes our newly defined\nattention utility metric in an end-to-end manner via separate learnable hash functions for queries and\nkeys, respectively. As for reducing the computational complexity in the training phase, LHA uses\nunbiased kernelized attention techniques (Choromanski et al., 2020; Peng et al., 2021) to efficiently\napproximate the attention utilities. Similar to other sparse attention models (Kitaev et al., 2020;\nRoy et al., 2021), LHA reduces the overall complexity of self-attention from O(N2\n) to O(N1.5\n)\nfor sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for\nlanguage modeling, natural language understanding, and Long-Range-Arena show that LHA achieves\nbetter performance compared to strong transformer baselines.']",intro_chunked,"The effectiveness of those ANN approaches rely on the assumption that the (transformed) query and key vectors should lie in the same space, which could be sub-optimal in dealing with different sparsity patterns, as analyzed in this paper (Section 3.2). Besides, the hash functions in LSH are randomized and data-agnostic, which cannot fully utilize the rich information in real-world data distributions. In this paper, we address the above limitations of existing ANN-based methods for attention sparsification. Firstly, we analyze two imbalance issues in LSH-produced sparse attention patterns, i.e., unbalanced hash bucket sizes and unbalanced query-key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that ANN-derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning-to-Hash Attention (LHA), for dynamic attention sparsification with enhanced model expressiveness. LHA directly optimizes our newly defined
attention utility metric in an end-to-end manner via separate learnable hash functions for queries and
keys, respectively. As for reducing the computational complexity in the training phase, LHA uses
unbiased kernelized attention techniques (Choromanski et al., 2020; Peng et al., 2021) to efficiently
approximate the attention utilities. Similar to other sparse attention models (Kitaev et al., 2020;
Roy et al., 2021), LHA reduces the overall complexity of self-attention from O(N2
) to O(N1.5
)
for sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for
language modeling, natural language understanding, and Long-Range-Arena show that LHA achieves
better performance compared to strong transformer baselines.",14.270757575757585,22.733194790065724,225,0.471809983253479," The effectiveness of those Propname approaches rely on the assumption that the query and key vectors should lie in the same space, which could be sub optimal in dealing with different sparsity patterns, as analyzed in this paper. Besides, the hash functions in Propname are randomized and data agnostic, which can not fully utilize the rich information in real world data distributions. In this paper, we address the above limitations of existing Propname based methods for attention sparsification. Firstly, we analyze two imbalance issues in Propname produced sparse attention patterns, ie, unbalanced hash bucket sizes and unbalanced query key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that Propname derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning to Propname Propname, for dynamic attention sparsification with enhanced model expressiveness. Propname directly optimizes our newly defined 
 attention utility metric in an end to end manner via separate learnable hash functions for queries and 
 keys, respectively. As for reducing the computational complexity in the training phase, Propname uses 
 unbiased kernelized attention techniques to efficiently 
 approximate the attention utilities. Similar to other sparse attention models Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000, Propname reduces the overall complexity of self attention from Propname to ON0.0 for sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for 
 language modeling, natural language understanding, and Propname Propname Propname show that Propname achieves 
 better performance compared to strong transformer baselines."," The effectiveness of those Propname approaches rely on the assumption that the query and key vectors should lie in the same space, which could be sub optimal in dealing with different sparsity patterns, as analyzed in this paper. Besides, the hash functions in Propname are randomized and data agnostic, which can not fully utilize the rich information in real world data distributions. In this paper, we address the above limitations of existing Propname based methods for attention sparsification. Firstly, we analyze two imbalance issues in Propname produced sparse attention patterns, ie, unbalanced hash bucket sizes and unbalanced query key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that Propname derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning to Propname Propname, for dynamic attention sparsification with enhanced model expressiveness. Propname directly optimizes our newly defined 
 attention utility metric in an end to end manner via separate learnable hash functions for queries and 
 keys, respectively. As for reducing the computational complexity in the training phase, Propname uses 
 unbiased kernelized attention techniques to efficiently 
 approximate the attention utilities. Similar to other sparse attention models Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000, Propname reduces the overall complexity of self attention from Propname to ON0.0 for sequence length N. Our experiments in a wide range of tasks on the evaluation benchmarks for 
 language modeling, natural language understanding, and Propname Propname Propname show that Propname achieves 
 better performance compared to strong transformer baselines.", DET NOUN ADP DET PROPN NOUN VERB ADP DET NOUN SCONJ DET NOUN CCONJ ADJ NOUN AUX VERB ADP DET ADJ NOUN PUNCT PRON AUX AUX NOUN ADJ ADP VERB ADP ADJ NOUN NOUN PUNCT SCONJ VERB ADP DET NOUN PUNCT ADV PUNCT DET NOUN NOUN ADP PROPN AUX VERB CCONJ NOUN NOUN PUNCT PRON AUX PART ADV VERB DET ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP VERB PROPN VERB NOUN ADP NOUN NOUN PUNCT ADV PUNCT PRON VERB NUM NOUN NOUN ADP PROPN VERB ADJ NOUN NOUN PUNCT ADV PUNCT ADJ NOUN NOUN NOUN CCONJ ADJ NOUN ADJ NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN VERB NOUN NOUN PART VERB SCONJ ADV DET NOUN NOUN VERB DET ADJ NOUN PUNCT CCONJ PRON VERB SCONJ PROPN VERB ADJ NOUN AUX ADV ADJ ADP PRON NOUN PUNCT ADV PUNCT PRON VERB DET NOUN NOUN PUNCT ADV VERB ADP PROPN PROPN PUNCT ADP ADJ NOUN NOUN ADP VERB NOUN NOUN PUNCT PROPN ADV VERB PRON ADV VERB SPACE NOUN NOUN NOUN ADP DET NOUN PART NOUN NOUN ADP ADJ ADJ NOUN NOUN ADP NOUN CCONJ SPACE NOUN PUNCT ADV PUNCT ADP ADP VERB DET ADJ NOUN ADP DET NOUN NOUN PUNCT PROPN VERB SPACE ADJ VERB NOUN NOUN PART ADV SPACE VERB DET NOUN NOUN PUNCT ADJ ADP ADJ ADJ NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN VERB DET ADJ NOUN ADP NOUN NOUN ADP PROPN ADP VERB ADP NOUN NOUN NOUN PRON NOUN ADP DET ADJ NOUN ADP NOUN ADP DET NOUN NOUN ADP SPACE NOUN NOUN PUNCT ADJ NOUN NOUN PUNCT CCONJ PROPN PROPN PROPN VERB SCONJ PROPN VERB SPACE ADJ NOUN VERB ADP ADJ ADJ NOUN PUNCT,0.5457627118644067,32.77777777777778,5.389830508474576
226,140,Zhiqing Sun,"[' Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state-of-the-art neural detectors [21, 24, 19, 25, 38, 26, 11] are developed in a detect-and-merge fashion that is, instead of directly optimizing the predicted set in an end-to-end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post-processing step (e.g., “non-maximum suppression” or NMS) for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end-to-end and arguably sub-optimal.DEtection TRansformer (DETR) [2] is recently proposed as the first fully end-to-end object detector. It usesTransformer [32] to directly output a final set of predictions without further post-processing. However, it takes extra-long training time to converge. For example, the popular Faster RCNN model [26] only requires about 30 epochs to convergence, but DETR needs 500 epochs, which takes at least 10 days on 8 V100 GPUs.', 'Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like Transformer-based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETR’s optimization difficulty we conduct extensive experiments and find that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder-only version of DETR by removing the cross-attention module. We find that the encoder-only DETR yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETR’s Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only DETR with feature pyramids [18]. Specifically, we present TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN), which are inspired by a classic one-stage detector FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respectively. A novel Feature of Interest (FoI) selection mechanism is developed in TSP-FCOS to help Transformer encoder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20] the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.']",intro_chunked," Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state-of-the-art neural detectors [21, 24, 19, 25, 38, 26, 11] are developed in a detect-and-merge fashion that is, instead of directly optimizing the predicted set in an end-to-end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post-processing step (e.g., “non-maximum suppression” or NMS) for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end-to-end and arguably sub-optimal.DEtection TRansformer (DETR) [2] is recently proposed as the first fully end-to-end object detector. It usesTransformer [32] to directly output a final set of predictions without further post-processing. However, it takes extra-long training time to converge. For example, the popular Faster RCNN model [26] only requires about 30 epochs to convergence, but DETR needs 500 epochs, which takes at least 10 days on 8 V100 GPUs.",24.873468992248092,22.733194790065724,226,0.29211148619651794," Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state of the art neural detectors are developed in a detect and merge fashion that is, instead of directly optimizing the predicted set in an end to end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post processing step for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end to end and arguably sub optimal. Propname Propname is recently proposed as the first fully end to end object detector. It usesTransformer to directly output a final set of predictions without further post processing. However, it takes extra long training time to converge. For example, the popular Propname Propname model only requires about 00 epochs to convergence, but Propname needs 000 epochs, which takes at least 00 days on 0 V000 GPUs."," Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state of the art neural detectors are developed in a detect and merge fashion that is, instead of directly optimizing the predicted set in an end to end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post processing step for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end to end and arguably sub optimal. Propname Propname is recently proposed as the first fully end to end object detector. It usesTransformer to directly output a final set of predictions without further post processing. However, it takes extra long training time to converge. For example, the popular Propname Propname model only requires about 00 epochs to convergence, but Propname needs 000 epochs, which takes at least 00 days on 0 V000 GPUs.", NOUN NOUN VERB ADP VERB DET NOUN ADP NOUN ADP DET NOUN CCONJ VERB PRON NOUN NOUN CCONJ VERB NOUN PUNCT PRON AUX ADV DET ADJ NOUN NOUN PUNCT SCONJ DET NOUN ADP DET VERB NOUN AUX PART VERB PUNCT ADJ ADP DET NOUN ADP DET NOUN ADJ NOUN AUX VERB ADP DET NOUN CCONJ VERB NOUN PRON AUX PUNCT ADV ADP ADV VERB DET VERB VERB ADP DET NOUN PART VERB NOUN PUNCT DET NOUN ADV ADV VERB NOUN ADP DET NOUN ADP NOUN NOUN CCONJ VERB NOUN PUNCT CCONJ ADV VERB DET NOUN NOUN NOUN ADP VERB DET DET NOUN NOUN ADP ADJ NOUN CCONJ NOUN PRON AUX VERB ADP DET ADJ NOUN PUNCT SCONJ DET NOUN NOUN AUX VERB ADV ADP NOUN ADP DET NOUN NOUN PUNCT DET NOUN NOUN ADP DET NOUN NOUN AUX PART NOUN ADP NOUN CCONJ ADV VERB ADJ PUNCT PROPN PROPN AUX ADV VERB SCONJ DET ADJ ADV NOUN PART VERB NOUN NOUN PUNCT PRON NOUN PART ADV VERB DET ADJ NOUN ADP NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB ADJ ADJ NOUN NOUN PART VERB PUNCT ADP NOUN PUNCT DET ADJ PROPN PROPN NOUN ADV VERB ADP NUM NOUN ADP NOUN PUNCT CCONJ PROPN VERB NUM NOUN PUNCT PRON VERB ADV ADJ NUM NOUN ADP NUM ADJ NOUN PUNCT,0.5475113122171946,31.571428571428573,4.660633484162896
227,141,Zhiqing Sun,"[' Object detection aims at finding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state-of-the-art neural detectors [21, 24, 19, 25, 38, 26, 11] are developed in a detect-and-merge fashion that is, instead of directly optimizing the predicted set in an end-to-end fashion, those methods usually first make predictions on a set of region proposals or sliding windows, and then perform a post-processing step (e.g., “non-maximum suppression” or NMS) for merging the the detection results in different proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end-to-end and arguably sub-optimal.DEtection TRansformer (DETR) [2] is recently proposed as the first fully end-to-end object detector. It usesTransformer [32] to directly output a final set of predictions without further post-processing. However, it takes extra-long training time to converge. For example, the popular Faster RCNN model [26] only requires about 30 epochs to convergence, but DETR needs 500 epochs, which takes at least 10 days on 8 V100 GPUs.', 'Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like Transformer-based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETR’s optimization difficulty we conduct extensive experiments and find that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder-only version of DETR by removing the cross-attention module. We find that the encoder-only DETR yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETR’s Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only DETR with feature pyramids [18]. Specifically, we present TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN), which are inspired by a classic one-stage detector FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respectively. A novel Feature of Interest (FoI) selection mechanism is developed in TSP-FCOS to help Transformer encoder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20] the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.']",intro_chunked,"Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like Transformer-based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETR’s optimization difficulty we conduct extensive experiments and find that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder-only version of DETR by removing the cross-attention module. We find that the encoder-only DETR yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETR’s Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only DETR with feature pyramids [18]. Specifically, we present TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN), which are inspired by a classic one-stage detector FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respectively. A novel Feature of Interest (FoI) selection mechanism is developed in TSP-FCOS to help Transformer encoder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20] the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.",17.55244643891143,22.733194790065724,227,0.5780467987060547," Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for Propname like Propname based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETRs optimization difficulty we conduct extensive experiments and find that the cross attention module, by which the Propname decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder only version of Propname by removing the cross attention module. We find that the encoder only Propname yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETRs Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Propname based set prediction methods, both of which can be regarded as improved versions of encoder only Propname with feature pyramids. Specifically, we present Propname Propname and Propname Propname, which are inspired by a classic one stage detector Propname and a classic two stage detector Propname Propname, respectively. A novel Feature of Propname selection mechanism is developed in Propname Propname to help Propname encoder handle multi level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the Propname 0000 detection benchmark the proposed methods not only converge much faster than the original Propname, but also significantly outperform Propname and other baselines in terms of detection accuracy."," Such expensive training cost would be practically prohibitive in large applications. Therefore, in what manner should we accelerate the training process towards fast convergence for Propname like Propname based detectors is a challenging research question and is the main focus of this paper. For analyzing the causes of DETRs optimization difficulty we conduct extensive experiments and find that the cross attention module, by which the Propname decoder obtains object information from images, is mainly responsible for the slow convergence. In pursuit of faster convergence, we further examine an encoder only version of Propname by removing the cross attention module. We find that the encoder only Propname yields a substantial improvement for the detection of small objects in particular but suboptimal performance on large objects. In addition, our analysis shows that the instability of the bipartite matching in DETRs Hungarian loss also contributes to the slow convergence. Based on the above analysis we propose two models for significantly accelerating the training process of Propname based set prediction methods, both of which can be regarded as improved versions of encoder only Propname with feature pyramids. Specifically, we present Propname Propname and Propname Propname, which are inspired by a classic one stage detector Propname and a classic two stage detector Propname Propname, respectively. A novel Feature of Propname selection mechanism is developed in Propname Propname to help Propname encoder handle multi level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also design a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the Propname 0000 detection benchmark the proposed methods not only converge much faster than the original Propname, but also significantly outperform Propname and other baselines in terms of detection accuracy.", ADJ ADJ NOUN NOUN AUX AUX ADV ADJ ADP ADJ NOUN PUNCT ADV PUNCT ADP DET NOUN AUX PRON VERB DET NOUN NOUN ADP ADJ NOUN ADP PROPN ADP PROPN VERB NOUN AUX DET ADJ NOUN NOUN CCONJ AUX DET ADJ NOUN ADP DET NOUN PUNCT ADP VERB DET NOUN ADP NOUN NOUN NOUN PRON VERB ADJ NOUN CCONJ VERB SCONJ DET NOUN NOUN NOUN PUNCT ADP PRON DET PROPN NOUN VERB ADJ NOUN ADP NOUN PUNCT AUX ADV ADJ ADP DET ADJ NOUN PUNCT ADP NOUN ADP ADJ NOUN PUNCT PRON ADV VERB DET NOUN ADV NOUN ADP PROPN ADP VERB DET NOUN NOUN NOUN PUNCT PRON VERB SCONJ DET NOUN ADV PROPN VERB DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN ADP ADJ CCONJ ADJ NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB SCONJ DET NOUN ADP DET ADJ NOUN ADP NOUN ADJ NOUN ADV VERB ADP DET ADJ NOUN PUNCT VERB ADP DET ADJ NOUN PRON VERB NUM NOUN ADP ADV VERB DET NOUN NOUN ADP PROPN VERB VERB NOUN NOUN PUNCT PRON ADP PRON AUX AUX VERB ADP VERB NOUN ADP NOUN ADV PROPN ADP NOUN NOUN PUNCT ADV PUNCT PRON VERB PROPN PROPN CCONJ PROPN PROPN PUNCT PRON AUX VERB ADP DET ADJ NUM NOUN NOUN PROPN CCONJ DET ADJ NUM NOUN NOUN PROPN PROPN PUNCT ADV PUNCT DET ADJ NOUN ADP PROPN NOUN NOUN AUX VERB ADP PROPN PROPN PART VERB PROPN NOUN VERB ADJ NOUN NOUN PUNCT PART VERB DET NOUN ADP DET ADJ NOUN ADP DET ADJ NOUN PUNCT PRON ADV VERB DET ADJ ADJ NOUN NOUN ADP PRON ADP PRON NUM NOUN ADP VERB DET NOUN ADP NOUN PUNCT ADP PRON NOUN ADP DET PROPN NUM NOUN NOUN DET VERB NOUN PART ADV VERB ADV ADV ADP DET ADJ PROPN PUNCT CCONJ ADV ADV VERB PROPN CCONJ ADJ NOUN ADP NOUN ADP NOUN NOUN PUNCT,0.4968553459119497,28.90909090909091,5.3081761006289305
228,142,Zhiqing Sun,"[' The NLP community has witnessed a revolution of\npre-training self-supervised models. These models\nusually have hundreds of millions of parameters\n(Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)\n∗This work was done when the first author was an intern\nat Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP,\nBERT suffers from the heavy model size and high\nlatency, making it impractical for resource-limited\nmobile devices to deploy the power of BERT in\nmobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models\n(Turc et al., 2019; Tang et al., 2019; Sun et al.,\n2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a\nmodel that can be generically fine-tuned on different downstream NLP tasks as the original BERT\ndoes. In this paper, we propose MobileBERT to\nfill this gap. In practice, task-agnostic compression\nof BERT is desirable. Task-specific compression\nneeds to first fine-tune the original large BERT\nmodel into a task-specific teacher and then distill.', 'Such a process is much more complicated (Wu\net al., 2019) and costly than directly fine-tuning a\ntask-agnostic compact model. At first glance, it may seem straightforward to\nobtain a task-agnostic compact BERT. For example,\none may just take a narrower or shallower version\nof BERT, and train it until convergence by minimizing a convex combination of the prediction loss\nand distillation loss (Turc et al., 2019; Sun et al.,\n2019). Unfortunately, empirical results show that\nsuch a straightforward approach results in significant accuracy loss (Turc et al., 2019). This may not\nbe that surprising. It is well-known that shallow\nnetworks usually do not have enough representation power while narrow and deep networks are\ndifficult to train. Our MobileBERT is designed to be as deep as\nBERTLARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self-attentions and feed-forward networks (Figure 1). To train MobileBERT, a deep\nand thin model, we first train a specially designed\nteacher model, an inverted-bottleneck incorporated\nBERTLARGE model (IB-BERT). Then, we conduct\nknowledge transfer from IB-BERT to MobileBERT. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations1\nshow that MobileBERT\nis 4.3× smaller and 5.5× faster than BERTBASE,\nwhile it can still achieve competitive results on\nwell-known NLP benchmarks. On the natural language inference tasks of GLUE, MobileBERT can\nachieve a GLUE score of 77.7, which is only 0.6\nlower than BERTBASE, with a latency of 62 ms on\na Pixel 4 phone. On the SQuAD v1.1/v2.0 question\nanswering task, MobileBER obtains a dev F1 score\nof 90.3/80.2, which is even 1.5/2.1 higher than\nBERTBASE.']",intro_chunked," The NLP community has witnessed a revolution of
pre-training self-supervised models. These models
usually have hundreds of millions of parameters
(Peters et al., 2018; Radford et al., 2018; Devlin
et al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)
∗This work was done when the first author was an intern
at Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP,
BERT suffers from the heavy model size and high
latency, making it impractical for resource-limited
mobile devices to deploy the power of BERT in
mobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models
(Turc et al., 2019; Tang et al., 2019; Sun et al.,
2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a
model that can be generically fine-tuned on different downstream NLP tasks as the original BERT
does. In this paper, we propose MobileBERT to
fill this gap. In practice, task-agnostic compression
of BERT is desirable. Task-specific compression
needs to first fine-tune the original large BERT
model into a task-specific teacher and then distill.",48.69527725118485,22.733194790065724,228,0.3202773928642273," The Propname community has witnessed a revolution of 
 pre training self supervised models. These models 
 usually have hundreds of millions of parameters 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. Among these models, Propname This work was done when the first author was an intern 
 at Propname Propname. shows substantial accuracy improvements. However, as one of the largest models ever in Propname, 
 Propname suffers from the heavy model size and high 
 latency, making it impractical for resource limited 
 mobile devices to deploy the power of Propname in 
 mobile based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill Propname into compact models 
 Turc Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000. To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre trained model, that is, a 
 model that can be generically fine tuned on different downstream Propname tasks as the original Propname 
 does. In this paper, we propose MobileBERT to 
 fill this gap. In practice, task agnostic compression 
 of Propname is desirable. Task specific compression 
 needs to first fine tune the original large Propname 
 model into a task specific teacher and then distill."," The Propname community has witnessed a revolution of 
 pre training self supervised models. These models 
 usually have hundreds of millions of parameters 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. Among these models, Propname This work was done when the first author was an intern 
 at Propname Propname. shows substantial accuracy improvements. However, as one of the largest models ever in Propname, 
 Propname suffers from the heavy model size and high 
 latency, making it impractical for resource limited 
 mobile devices to deploy the power of Propname in 
 mobile based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill Propname into compact models 
 Turc Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000. To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre trained model, that is, a 
 model that can be generically fine tuned on different downstream Propname tasks as the original Propname 
 does. In this paper, we propose MobileBERT to 
 fill this gap. In practice, task agnostic compression 
 of Propname is desirable. Task specific compression 
 needs to first fine tune the original large Propname 
 model into a task specific teacher and then distill.", DET PROPN NOUN AUX VERB DET NOUN ADP SPACE ADJ NOUN NOUN VERB NOUN PUNCT DET NOUN SPACE ADV VERB NOUN ADP NOUN ADP NOUN SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET NOUN PUNCT PROPN DET NOUN AUX VERB SCONJ DET ADJ NOUN AUX DET NOUN SPACE ADP PROPN PROPN PUNCT VERB ADJ NOUN NOUN PUNCT ADV PUNCT ADP NUM ADP DET ADJ NOUN ADV ADP PROPN PUNCT SPACE PROPN VERB ADP DET ADJ NOUN NOUN CCONJ ADJ SPACE NOUN PUNCT VERB PRON ADJ ADP NOUN VERB SPACE ADJ NOUN PART VERB DET NOUN ADP PROPN ADP SPACE ADJ VERB NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ DET ADJ PUNCT PRON AUX AUX DET NOUN PRON ADV VERB PROPN ADP ADJ NOUN SPACE VERB PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET ADJ ADP PRON NOUN PUNCT PRON VERB PART ADV DET NOUN ADP VERB DET ADJ ADJ ADJ VERB NOUN PUNCT ADV ADV PUNCT DET SPACE NOUN PRON AUX AUX ADV ADJ VERB ADP ADJ ADJ PROPN NOUN SCONJ DET ADJ PROPN SPACE VERB PUNCT ADP DET NOUN PUNCT PRON VERB NUM PART SPACE VERB DET NOUN PUNCT ADP NOUN PUNCT NOUN ADJ NOUN SPACE ADP PROPN AUX ADJ PUNCT NOUN ADJ NOUN SPACE VERB PART ADJ ADJ NOUN DET ADJ ADJ PROPN SPACE NOUN ADP DET NOUN ADJ NOUN CCONJ ADV VERB PUNCT,0.4770992366412214,26.2,5.0
229,143,Zhiqing Sun,"[' The NLP community has witnessed a revolution of\npre-training self-supervised models. These models\nusually have hundreds of millions of parameters\n(Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)\n∗This work was done when the first author was an intern\nat Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP,\nBERT suffers from the heavy model size and high\nlatency, making it impractical for resource-limited\nmobile devices to deploy the power of BERT in\nmobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models\n(Turc et al., 2019; Tang et al., 2019; Sun et al.,\n2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a\nmodel that can be generically fine-tuned on different downstream NLP tasks as the original BERT\ndoes. In this paper, we propose MobileBERT to\nfill this gap. In practice, task-agnostic compression\nof BERT is desirable. Task-specific compression\nneeds to first fine-tune the original large BERT\nmodel into a task-specific teacher and then distill.', 'Such a process is much more complicated (Wu\net al., 2019) and costly than directly fine-tuning a\ntask-agnostic compact model. At first glance, it may seem straightforward to\nobtain a task-agnostic compact BERT. For example,\none may just take a narrower or shallower version\nof BERT, and train it until convergence by minimizing a convex combination of the prediction loss\nand distillation loss (Turc et al., 2019; Sun et al.,\n2019). Unfortunately, empirical results show that\nsuch a straightforward approach results in significant accuracy loss (Turc et al., 2019). This may not\nbe that surprising. It is well-known that shallow\nnetworks usually do not have enough representation power while narrow and deep networks are\ndifficult to train. Our MobileBERT is designed to be as deep as\nBERTLARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self-attentions and feed-forward networks (Figure 1). To train MobileBERT, a deep\nand thin model, we first train a specially designed\nteacher model, an inverted-bottleneck incorporated\nBERTLARGE model (IB-BERT). Then, we conduct\nknowledge transfer from IB-BERT to MobileBERT. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations1\nshow that MobileBERT\nis 4.3× smaller and 5.5× faster than BERTBASE,\nwhile it can still achieve competitive results on\nwell-known NLP benchmarks. On the natural language inference tasks of GLUE, MobileBERT can\nachieve a GLUE score of 77.7, which is only 0.6\nlower than BERTBASE, with a latency of 62 ms on\na Pixel 4 phone. On the SQuAD v1.1/v2.0 question\nanswering task, MobileBER obtains a dev F1 score\nof 90.3/80.2, which is even 1.5/2.1 higher than\nBERTBASE.']",intro_chunked,"Such a process is much more complicated (Wu
et al., 2019) and costly than directly fine-tuning a
task-agnostic compact model. At first glance, it may seem straightforward to
obtain a task-agnostic compact BERT. For example,
one may just take a narrower or shallower version
of BERT, and train it until convergence by minimizing a convex combination of the prediction loss
and distillation loss (Turc et al., 2019; Sun et al.,
2019). Unfortunately, empirical results show that
such a straightforward approach results in significant accuracy loss (Turc et al., 2019). This may not
be that surprising. It is well-known that shallow
networks usually do not have enough representation power while narrow and deep networks are
difficult to train. Our MobileBERT is designed to be as deep as
BERTLARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self-attentions and feed-forward networks (Figure 1). To train MobileBERT, a deep
and thin model, we first train a specially designed
teacher model, an inverted-bottleneck incorporated
BERTLARGE model (IB-BERT). Then, we conduct
knowledge transfer from IB-BERT to MobileBERT. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations1
show that MobileBERT
is 4.3× smaller and 5.5× faster than BERTBASE,
while it can still achieve competitive results on
well-known NLP benchmarks. On the natural language inference tasks of GLUE, MobileBERT can
achieve a GLUE score of 77.7, which is only 0.6
lower than BERTBASE, with a latency of 62 ms on
a Pixel 4 phone. On the SQuAD v1.1/v2.0 question
answering task, MobileBER obtains a dev F1 score
of 90.3/80.2, which is even 1.5/2.1 higher than
BERTBASE.",42.66417582417586,22.733194790065724,229,0.2687976062297821," Such a process is much more complicated Propname 
 Propname Propname Propname, 0000 and costly than directly fine tuning a 
 task agnostic compact model. At first glance, it may seem straightforward to 
 obtain a task agnostic compact Propname. For example, 
 one may just take a narrower or shallower version 
 of Propname, and train it until convergence by minimizing a convex combination of the prediction loss 
 and distillation loss Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000. Unfortunately, empirical results show that 
 such a straightforward approach results in significant accuracy loss. This may not 
 be that surprising. It is well known that shallow 
 networks usually do not have enough representation power while narrow and deep networks are 
 difficult to train. Our MobileBERT is designed to be as deep as 
 Propname while each layer is made much narrower via adopting bottleneck structures and balancing between self attentions and feed forward networks. To train Propname, a deep 
 and thin model, we first train a specially designed 
 teacher model, an inverted bottleneck incorporated 
 Propname model. Then, we conduct 
 knowledge transfer from Propname Propname to Propname. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations0 
 show that Propname 
 is 0.0 smaller and 0.0 faster than Propname, 
 while it can still achieve competitive results on 
 well known Propname benchmarks. On the natural language inference tasks of Propname, Propname can 
 achieve a GLUE score of 00.0, which is only 0.0 
 lower than Propname, with a latency of00 ms on 
 a Propname 0 phone. On the SQuAD v0.0v0.0 question 
 answering task, MobileBER obtains a dev F0 score 
 of Propname, which is even 0.00.0 higher than 
 Propname."," Such a process is much more complicated Propname 
 Propname Propname Propname, 0000 and costly than directly fine tuning a 
 task agnostic compact model. At first glance, it may seem straightforward to 
 obtain a task agnostic compact Propname. For example, 
 one may just take a narrower or shallower version 
 of Propname, and train it until convergence by minimizing a convex combination of the prediction loss 
 and distillation loss Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000. Unfortunately, empirical results show that 
 such a straightforward approach results in significant accuracy loss. This may not 
 be that surprising. It is well known that shallow 
 networks usually do not have enough representation power while narrow and deep networks are 
 difficult to train. Our MobileBERT is designed to be as deep as 
 Propname while each layer is made much narrower via adopting bottleneck structures and balancing between self attentions and feed forward networks. To train Propname, a deep 
 and thin model, we first train a specially designed 
 teacher model, an inverted bottleneck incorporated 
 Propname model. Then, we conduct 
 knowledge transfer from Propname Propname to Propname. A variety of knowledge transfer strategies are carefully investigated in our empirical studies. Empirical evaluations0 
 show that Propname 
 is 0.0 smaller and 0.0 faster than Propname, 
 while it can still achieve competitive results on 
 well known Propname benchmarks. On the natural language inference tasks of Propname, Propname can 
 achieve a GLUE score of 00.0, which is only 0.0 
 lower than Propname, with a latency of00 ms on 
 a Propname 0 phone. On the SQuAD v0.0v0.0 question 
 answering task, MobileBER obtains a dev F0 score 
 of Propname, which is even 0.00.0 higher than 
 Propname.", DET DET NOUN AUX ADV ADV ADJ PROPN SPACE PROPN PROPN PROPN PUNCT NUM CCONJ ADJ ADP ADV ADJ VERB DET SPACE NOUN ADJ ADJ NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX VERB ADJ PART SPACE VERB DET NOUN ADJ ADJ PROPN PUNCT ADP NOUN PUNCT SPACE PRON AUX ADV VERB DET ADJ CCONJ ADJ NOUN SPACE ADP PROPN PUNCT CCONJ VERB PRON ADP NOUN ADP VERB DET ADJ NOUN ADP DET NOUN NOUN SPACE CCONJ NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT ADV PUNCT ADJ NOUN VERB SCONJ SPACE DET DET ADJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT PRON AUX PART SPACE AUX ADV ADJ PUNCT PRON AUX ADV ADJ SCONJ ADJ SPACE NOUN ADV AUX PART VERB ADJ NOUN NOUN SCONJ ADJ CCONJ ADJ NOUN AUX SPACE ADJ PART VERB PUNCT PRON NOUN AUX VERB PART AUX ADV ADJ ADP SPACE PROPN SCONJ DET NOUN AUX VERB ADV ADJ ADP VERB NOUN NOUN CCONJ VERB ADP NOUN NOUN CCONJ VERB ADJ NOUN PUNCT PART VERB PROPN PUNCT DET ADJ SPACE CCONJ ADJ NOUN PUNCT PRON ADV VERB DET ADV VERB SPACE NOUN NOUN PUNCT DET VERB NOUN VERB SPACE PROPN NOUN PUNCT ADV PUNCT PRON VERB SPACE NOUN NOUN ADP PROPN PROPN ADP PROPN PUNCT DET NOUN ADP NOUN NOUN NOUN AUX ADV VERB ADP PRON ADJ NOUN PUNCT ADJ NOUN SPACE VERB SCONJ PROPN SPACE AUX NUM ADJ CCONJ NUM ADV ADP PROPN PUNCT SPACE SCONJ PRON AUX ADV VERB ADJ NOUN ADP SPACE ADV VERB PROPN NOUN PUNCT ADP DET ADJ NOUN NOUN NOUN ADP PROPN PUNCT PROPN AUX SPACE VERB DET ADJ NOUN ADP NUM PUNCT PRON AUX ADV NUM SPACE ADJ ADP PROPN PUNCT ADP DET NOUN ADP PUNCT NOUN ADP SPACE DET PROPN NUM NOUN PUNCT ADP DET ADJ NOUN NOUN SPACE VERB NOUN PUNCT NOUN VERB DET NOUN NOUN NOUN SPACE ADP PROPN PUNCT PRON AUX ADV NUM ADJ ADP SPACE PROPN PUNCT,0.5163398692810458,23.53846153846154,4.954248366013072
230,144,Zhiqing Sun,"[' Real-world knowledge bases are usually expressed as multi-relational graphs, which are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. However, real-word knowledge bases are usually incomplete (Dong et al., 2014), which motivates the research of automatically predicting missing links. A popular approach for Knowledge Graph Completion (KGC) is to embed entities and relations into continuous vector or matrix space, and use a well-designed score function f (h, r, t) to measure the plausibility of the triplet (h, r, t). Most of the previous methods use translation distance based (Bordes et al., 2013; Wang et al., 2014; Xiao et al., 2016; Sun et al., 2019) and semantic matching based (Nickel and Tresp, 2013; Yang et al., 2014; Nickel et al., 2016; Trouillon et al., 2016; scoring functions which are easy to analyze. However, recently, a vast number of neural network-based methods have been proposed. They have complex score functions which utilize black-box neural networks including Convolutional Neural Networks (CNNs) (Dettmers et al., 2018; Nguyen et al., 2018), Recurrent Neural Networks (RNNs) (Lin et al., 2015; Wang et al., 2018), Graph Neural Networks (GNNs) (Schlichtkrull et al., 2017; Shang et al., 2019), and Capsule Networks (Nguyen et al., 2019).', 'While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re-examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.']",intro_chunked," Real-world knowledge bases are usually expressed as multi-relational graphs, which are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. However, real-word knowledge bases are usually incomplete (Dong et al., 2014), which motivates the research of automatically predicting missing links. A popular approach for Knowledge Graph Completion (KGC) is to embed entities and relations into continuous vector or matrix space, and use a well-designed score function f (h, r, t) to measure the plausibility of the triplet (h, r, t). Most of the previous methods use translation distance based (Bordes et al., 2013; Wang et al., 2014; Xiao et al., 2016; Sun et al., 2019) and semantic matching based (Nickel and Tresp, 2013; Yang et al., 2014; Nickel et al., 2016; Trouillon et al., 2016; scoring functions which are easy to analyze. However, recently, a vast number of neural network-based methods have been proposed. They have complex score functions which utilize black-box neural networks including Convolutional Neural Networks (CNNs) (Dettmers et al., 2018; Nguyen et al., 2018), Recurrent Neural Networks (RNNs) (Lin et al., 2015; Wang et al., 2018), Graph Neural Networks (GNNs) (Schlichtkrull et al., 2017; Shang et al., 2019), and Capsule Networks (Nguyen et al., 2019).",34.11430985915493,22.733194790065724,230,0.41975441575050354," Real world knowledge bases are usually expressed as multi relational graphs, which are collections of factual triplets, where each triplet represents a relation r between a head entity h and a tail entity Propname However, real word knowledge bases are usually incomplete, which motivates the research of automatically predicting missing links. A popular approach for Propname Propname Propname is to embed entities and relations into continuous vector or matrix space, and use a well designed score function Propname to measure the plausibility of the triplet. Most of the previous methods use translation distance based and semantic matching based, Propname Propname Propname, Propname Propname Propname, and Propname Propname."," Real world knowledge bases are usually expressed as multi relational graphs, which are collections of factual triplets, where each triplet represents a relation r between a head entity h and a tail entity Propname However, real word knowledge bases are usually incomplete, which motivates the research of automatically predicting missing links. A popular approach for Propname Propname Propname is to embed entities and relations into continuous vector or matrix space, and use a well designed score function Propname to measure the plausibility of the triplet. Most of the previous methods use translation distance based and semantic matching based, Propname Propname Propname, Propname Propname Propname, and Propname Propname.", ADJ NOUN NOUN NOUN AUX ADV VERB ADP ADJ ADJ NOUN PUNCT PRON AUX NOUN ADP ADJ NOUN PUNCT SCONJ DET NOUN VERB DET NOUN NOUN ADP DET NOUN NOUN NOUN CCONJ DET NOUN NOUN PROPN ADV PUNCT ADJ NOUN NOUN NOUN AUX ADV ADJ PUNCT PRON VERB DET NOUN ADP ADV VERB VERB NOUN PUNCT DET ADJ NOUN ADP PROPN PROPN PROPN AUX PART VERB NOUN CCONJ NOUN ADP ADJ NOUN CCONJ NOUN NOUN PUNCT CCONJ VERB DET ADV VERB NOUN NOUN PROPN PART VERB DET NOUN ADP DET NOUN PUNCT ADJ ADP DET ADJ NOUN VERB NOUN NOUN VERB CCONJ ADJ NOUN VERB PUNCT PROPN PROPN PROPN PUNCT PROPN PROPN PROPN PUNCT CCONJ PROPN PROPN PUNCT,0.6186440677966102,39.333333333333336,5.279661016949152
231,145,Zhiqing Sun,"[' Real-world knowledge bases are usually expressed as multi-relational graphs, which are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. However, real-word knowledge bases are usually incomplete (Dong et al., 2014), which motivates the research of automatically predicting missing links. A popular approach for Knowledge Graph Completion (KGC) is to embed entities and relations into continuous vector or matrix space, and use a well-designed score function f (h, r, t) to measure the plausibility of the triplet (h, r, t). Most of the previous methods use translation distance based (Bordes et al., 2013; Wang et al., 2014; Xiao et al., 2016; Sun et al., 2019) and semantic matching based (Nickel and Tresp, 2013; Yang et al., 2014; Nickel et al., 2016; Trouillon et al., 2016; scoring functions which are easy to analyze. However, recently, a vast number of neural network-based methods have been proposed. They have complex score functions which utilize black-box neural networks including Convolutional Neural Networks (CNNs) (Dettmers et al., 2018; Nguyen et al., 2018), Recurrent Neural Networks (RNNs) (Lin et al., 2015; Wang et al., 2018), Graph Neural Networks (GNNs) (Schlichtkrull et al., 2017; Shang et al., 2019), and Capsule Networks (Nguyen et al., 2019).', 'While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re-examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.']",intro_chunked,"While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re-examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.",29.632590320381752,22.733194790065724,231,0.3430981934070587," While some of them report state of the art performance on several benchmark datasets that are competitive to previous embedding based approaches, a considerable portion of recent neural network based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at."," While some of them report state of the art performance on several benchmark datasets that are competitive to previous embedding based approaches, a considerable portion of recent neural network based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we investigate this problem and find that this is attributed to the inappropriate evaluation protocol used by these approaches. We demonstrate that their evaluation protocol gives a perfect score to a model that always outputs a constant irrespective of the input. This has lead to artificial inflation of performance of several models. For this, we find a simple evaluation protocol that creates a fair comparison environment for all types of score functions. We conduct extensive experiments to re examine some recent methods and fairly compare them with existing approaches. The source code of the paper has been publicly available at.", SCONJ PRON ADP PRON VERB NOUN ADP DET NOUN NOUN ADP ADJ ADJ NOUN PRON AUX ADJ ADP ADJ VERB VERB NOUN PUNCT DET ADJ NOUN ADP ADJ ADJ NOUN VERB NOUN VERB ADV ADJ NOUN NOUN PRON AUX PART ADJ ADP ADJ NOUN PUNCT ADV PUNCT ADJ ADP DET ADJ NOUN AUX PART ADV ADV VERB PUNCT DET DET NOUN AUX VERB ADJ CCONJ AUX VERB DET ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN CCONJ VERB SCONJ PRON AUX VERB ADP DET ADJ NOUN NOUN VERB ADP DET NOUN PUNCT PRON VERB SCONJ PRON NOUN NOUN VERB DET ADJ NOUN ADP DET NOUN PRON ADV VERB DET ADJ NOUN ADP DET NOUN PUNCT PRON AUX VERB ADP ADJ NOUN ADP NOUN ADP ADJ NOUN PUNCT ADP PRON PUNCT PRON VERB DET ADJ NOUN NOUN PRON VERB DET ADJ NOUN NOUN ADP DET NOUN ADP NOUN NOUN PUNCT PRON VERB ADJ NOUN PART AUX VERB DET ADJ NOUN CCONJ ADV VERB PRON ADP VERB NOUN PUNCT DET NOUN NOUN ADP DET NOUN AUX AUX ADV ADJ ADP PUNCT,0.6098901098901099,20.22222222222222,5.06043956043956
232,146,Zhiqing Sun,"[' State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences.', 'Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model.', 'We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.']",intro_chunked," State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences.",37.62740384615387,22.733194790065724,232,0.17650046944618225," State of the art conditional sequence generation models typically rely on an Propname factorization scheme to produce the output sequences. Denoting by x an input sequence of length T, and by Propname a target sequence of length T, the conditional probability of Propname given Propname is factorized as: As such a sequential factorization can not take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Propname Propname sequence models are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their Propname counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi modality phenomenon. which means that the input sequence can be mapped to multiple correct output sequences."," State of the art conditional sequence generation models typically rely on an Propname factorization scheme to produce the output sequences. Denoting by x an input sequence of length T, and by Propname a target sequence of length T, the conditional probability of Propname given Propname is factorized as: As such a sequential factorization can not take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Propname Propname sequence models are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their Propname counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi modality phenomenon. which means that the input sequence can be mapped to multiple correct output sequences.", NOUN ADP DET NOUN ADJ NOUN NOUN NOUN ADV VERB ADP DET PROPN NOUN NOUN PART VERB DET NOUN NOUN PUNCT VERB ADP NOUN DET NOUN NOUN ADP NOUN NOUN PUNCT CCONJ ADP PROPN DET NOUN NOUN ADP NOUN NOUN PUNCT DET ADJ NOUN ADP PROPN VERB PROPN AUX VERB ADP PUNCT SCONJ DET DET ADJ NOUN AUX PART VERB DET ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB ADJ NOUN NOUN ADP DET NOUN PUNCT ADV PUNCT PROPN PROPN NOUN NOUN AUX VERB PART VERB DET NOUN ADP NOUN NOUN PUNCT ADP VERB DET ADJ NOUN ADP DET NOUN NOUN ADP PUNCT DET NOUN VERB PRON VERB PART AUX VERB ADP NOUN CCONJ ADV VERB DET ADJ NOUN ADP DET NOUN NOUN PUNCT ADV PUNCT NOUN NOUN ADV VERB ADP DET ADJ NOUN NOUN ADP DET NOUN NOUN PUNCT CCONJ ADV AUX PART VERB ADV ADV ADP PRON PROPN NOUN PUNCT DET DET NOUN NOUN AUX ADV ADJ SCONJ DET NOUN NOUN VERB DET ADJ NOUN NOUN PUNCT PRON VERB SCONJ DET NOUN NOUN AUX AUX VERB ADP ADJ ADJ NOUN NOUN PUNCT,0.5597826086956522,30.666666666666668,5.130434782608695
233,147,Zhiqing Sun,"[' State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences.', 'Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model.', 'We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.']",intro_chunked,"Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model.",30.715909090909093,22.733194790065724,233,0.46591803431510925," Such a multi modal output distribution can not be represented as the product of conditionally independent distributions for each position in Propname models. How to overcome the multi modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence level knowledge distillation, which means to replace the target part Propname of each training instance with the system predicted Propname from a pre trained Propname model. Such a replacement strategy removes the one to many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation0 and text summarization. Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the Propname methods with knowledge distillation, the teacher Propname model is pre trained only once on ground truth data and then is used to generate the output training examples for the Propname model."," Such a multi modal output distribution can not be represented as the product of conditionally independent distributions for each position in Propname models. How to overcome the multi modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence level knowledge distillation, which means to replace the target part Propname of each training instance with the system predicted Propname from a pre trained Propname model. Such a replacement strategy removes the one to many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation0 and text summarization. Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the Propname methods with knowledge distillation, the teacher Propname model is pre trained only once on ground truth data and then is used to generate the output training examples for the Propname model.", DET DET ADJ ADJ NOUN NOUN AUX PART AUX VERB ADP DET NOUN ADP ADV ADJ NOUN ADP DET NOUN ADP PROPN NOUN PUNCT SCONJ PART VERB DET ADJ NOUN NOUN AUX AUX DET ADJ NOUN ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT DET ADJ NOUN AUX PART VERB NOUN NOUN NOUN NOUN PUNCT PRON VERB PART VERB DET NOUN NOUN PROPN ADP DET NOUN NOUN ADP DET NOUN VERB PROPN ADP DET ADJ VERB PROPN NOUN PUNCT DET DET NOUN NOUN VERB DET NUM ADP ADJ NOUN ADP DET ADJ NOUN PUNCT DET NOUN ADP VERB ADV AUX SCONJ ADP NOUN PRON AUX PART ADV VERB DET NOUN NOUN NOUN PART VERB DET ADJ NOUN NOUN ADP NOUN NOUN NOUN ADJ ADP NOUN NOUN CCONJ NOUN NOUN PUNCT DET DET NOUN NOUN NOUN AUX VERB PART AUX ADJ ADP VERB DET NOUN ADP NOUN NOUN ADP ADJ NOUN PUNCT PRON VERB PART VERB ADP SCONJ ADP DET DET PROPN NOUN ADP NOUN NOUN PUNCT DET NOUN PROPN NOUN AUX VERB VERB ADV ADV ADP NOUN NOUN NOUN CCONJ ADV AUX VERB PART VERB DET NOUN NOUN NOUN ADP DET PROPN NOUN PUNCT,0.5721649484536082,27.714285714285715,5.041237113402062
234,148,Zhiqing Sun,"[' State-of-the-art conditional sequence generation models (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) typically rely on an AutoRegressive (AR) factorization scheme to produce the output sequences. Denoting by x = (x1, . . . , xT ) an input sequence of length T , and by y = (y1, . . . , yT ′ ) a target sequence of length T ′, the conditional probability of y given x is factorized as: As such a sequential factorization cannot take the full advantage of parallel computing, it yields high inference latency as a limitation. Recently, Non-AutoRegressive (NAR) sequence models (Guet al., 2017; Lee et al., 2018) are proposed to tackle the problem of inference latency, by removing the sequential dependencies among the output tokens as: This formulation allows each token to be decoded in parallel and hence brings a significant reduction of the inference latency. However, NAR models also suffer from the conditional independence assumption among the output tokens, and usually do not perform as well as their AR counterparts. Such a performance gap is particularly evident when the output distributions exhibit a multi-modality phenomenon (Guet al., 2017). which means that the input sequence can be mapped to multiple correct output sequences.', 'Such a multi-modal output distribution cannot be represented as the product of conditionally independent distributions for each position in NAR models (See 3.2 for a detailed discussion). How to overcome the multi-modality issue has been a central focus in recent efforts for improving NAR models. A standard approach is to use sequence-level knowledge distillation (Hinton et al., 2015; Kim & Rush, 2016), which means to replace the target part y of each training instance (x, y) with the system-predicted ˆy from a pre-trained AR model (a.k.a. the ”teacher model”). Such a replacement strategy removes the one-to-many mappings from the original dataset. The justification for doing so is that in practice we do not really need the sequence generation models to mimic a diverse output distribution for sequence generation tasks such as machine translation1 and text summarization.Such a knowledge distillation strategy has shown to be effective for improving the performance of NAR models in multiple studies. We want to point out that in all the NAR methods with knowledge distillation, the teacher AR model is pre-trained only once on ground-truth data and then is used to generate the output training examples for the NAR model.', 'We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.']",intro_chunked,"We argue that such a single-pass knowledge distillation process may not be sufficient for optimizing the NAR model as sequence ˆy predicted by the AR model cannot be perfect. More importantly, it is not necessarily the best choice for alleviating the multi-modality problem in the NAR model. In other words, without knowing how the choice of ˆy by the AR model would effect the training process in the NAR model, the current knowledge distillation approach is unavoidably sub-optimal. To address this fundamental limitation, we propose a novel Expectation-Maximization (EM) approach to the training of NAR models, where both the teacher (an AR model) and the student (a NAR model) are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between AR and NAR models, and is the first EM approach to NAR models, to our knowledge. Fig. 1 illustrates our new framework. In addition, we develop a principled plug-and-play decoding module for effective removal of word duplication in the model’s output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.",33.4106392694064,22.733194790065724,234,0.38957762718200684," We argue that such a single pass knowledge distillation process may not be sufficient for optimizing the Propname model as sequence Propname predicted by the Propname model can not be perfect. More importantly, it is not necessarily the best choice for alleviating the multi modality problem in the Propname model. In other words, without knowing how the choice of Propname by the Propname model would effect the training process in the Propname model, the current knowledge distillation approach is unavoidably sub optimal. To address this fundamental limitation, we propose a novel Propname Propname approach to the training of NAR models, where both the teacher and the student are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between Propname and Propname models, and is the first Propname approach to Propname models, to our knowledge. Propname. Propname illustrates our new framework. In addition, we develop a principled plug and play decoding module for effective removal of word duplication in the models output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly."," We argue that such a single pass knowledge distillation process may not be sufficient for optimizing the Propname model as sequence Propname predicted by the Propname model can not be perfect. More importantly, it is not necessarily the best choice for alleviating the multi modality problem in the Propname model. In other words, without knowing how the choice of Propname by the Propname model would effect the training process in the Propname model, the current knowledge distillation approach is unavoidably sub optimal. To address this fundamental limitation, we propose a novel Propname Propname approach to the training of NAR models, where both the teacher and the student are helping each other in a closed loop, and the iterative updates of the models are guaranteed to converge to a local optimum. This approach gives an extra power to knowledge distillation between Propname and Propname models, and is the first Propname approach to Propname models, to our knowledge. Propname. Propname illustrates our new framework. In addition, we develop a principled plug and play decoding module for effective removal of word duplication in the models output. Our experiments on three machine translation benchmark datasets show that the proposed approach achieves competitive performance as that of the best NAR models in terms of prediction accuracy, and reduces the inference latency significantly.", PRON VERB SCONJ DET DET ADJ NOUN NOUN NOUN NOUN AUX PART AUX ADJ ADP VERB DET PROPN NOUN SCONJ NOUN PROPN VERB ADP DET PROPN NOUN AUX PART AUX ADJ PUNCT ADV ADV PUNCT PRON AUX PART ADV DET ADJ NOUN ADP VERB DET ADJ NOUN NOUN ADP DET PROPN NOUN PUNCT ADP ADJ NOUN PUNCT ADP VERB SCONJ DET NOUN ADP PROPN ADP DET PROPN NOUN AUX VERB DET NOUN NOUN ADP DET PROPN NOUN PUNCT DET ADJ NOUN NOUN NOUN AUX ADV VERB ADJ PUNCT PART VERB DET ADJ NOUN PUNCT PRON VERB DET ADJ PROPN PROPN NOUN ADP DET NOUN ADP NOUN NOUN PUNCT SCONJ CCONJ DET NOUN CCONJ DET NOUN AUX VERB DET ADJ ADP DET ADJ NOUN PUNCT CCONJ DET ADJ NOUN ADP DET NOUN AUX VERB PART VERB ADP DET ADJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN PART VERB NOUN ADP PROPN CCONJ PROPN NOUN PUNCT CCONJ AUX DET ADJ PROPN NOUN ADP PROPN NOUN PUNCT ADP PRON NOUN PUNCT PROPN PUNCT PROPN VERB PRON ADJ NOUN PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NOUN CCONJ VERB VERB NOUN ADP ADJ NOUN ADP NOUN NOUN ADP DET NOUN NOUN PUNCT PRON NOUN ADP NUM NOUN NOUN NOUN NOUN VERB SCONJ DET VERB NOUN VERB ADJ NOUN ADP PRON ADP DET ADJ NOUN NOUN ADP NOUN ADP NOUN NOUN PUNCT CCONJ VERB DET NOUN NOUN ADV PUNCT,0.5254237288135594,26.22222222222222,5.029661016949152
235,149,Zhiqing Sun,"[' Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently.', 'Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.', 'However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts.']",intro_chunked," Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently.",14.291209761163032,22.733194790065724,235,0.21855713427066803," Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications. However, these models suffer from high inference latency, which is sometimes unaffordable for real time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence Propname and a target sequence Propname, autoregressive sequence models are based on a chain of conditional probabilities with a left to right causal structure: where Propname represents the tokens before the i th token of Propname Propname See Figure 0 for the illustration of a state of the art autoregressive sequence model, Propname. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by Propname sequentially. The non autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually can not get results as good as their autoregressive counterparts. As shown in Propname 0, on the machine translation task, compared to Propname Translation models, Propname Propname Translation models suffer from severe decoding inconsistency problem. In non autoregressive sequence models, each token in the target sentence is generated independently."," Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications. However, these models suffer from high inference latency, which is sometimes unaffordable for real time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence Propname and a target sequence Propname, autoregressive sequence models are based on a chain of conditional probabilities with a left to right causal structure: where Propname represents the tokens before the i th token of Propname Propname See Figure 0 for the illustration of a state of the art autoregressive sequence model, Propname. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by Propname sequentially. The non autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually can not get results as good as their autoregressive counterparts. As shown in Propname 0, on the machine translation task, compared to Propname Translation models, Propname Propname Translation models suffer from severe decoding inconsistency problem. In non autoregressive sequence models, each token in the target sentence is generated independently.", ADJ NOUN NOUN VERB ADJ NOUN ADP NOUN ADP NOUN NOUN CCONJ AUX AUX VERB ADP ADJ NOUN PUNCT ADV PUNCT DET NOUN VERB ADP ADJ NOUN NOUN PUNCT PRON AUX ADV ADJ ADP ADJ NOUN ADJ NOUN PUNCT PRON AUX ADV VERB ADP DET ADJ NOUN NOUN ADP DET NOUN PUNCT VERB DET ADJ ADJ NOUN NOUN NOUN PUNCT VERB DET NOUN NOUN PROPN CCONJ DET NOUN NOUN PROPN PUNCT ADJ NOUN NOUN AUX VERB ADP DET NOUN ADP ADJ NOUN ADP DET NOUN ADP ADJ ADJ NOUN PUNCT SCONJ PROPN VERB DET NOUN ADP DET PRON X NOUN ADP PROPN PROPN VERB NOUN NUM ADP DET NOUN ADP DET NOUN ADP DET NOUN ADJ NOUN NOUN PUNCT PROPN PUNCT DET ADJ NOUN VERB DET NOUN NOUN ADJ PART AUX VERB SCONJ DET NOUN AUX VERB VERB ADP PROPN ADV PUNCT DET ADJ ADJ NOUN NOUN VERB ADJ NOUN ADP NOUN CCONJ ADV VERB DET NOUN NOUN PUNCT ADV PUNCT PRON ADV AUX PART VERB NOUN ADV ADJ ADP PRON ADJ NOUN PUNCT SCONJ VERB ADP PROPN NUM PUNCT ADP DET NOUN NOUN NOUN PUNCT VERB ADP PROPN NOUN NOUN PUNCT PROPN PROPN NOUN NOUN VERB ADP ADJ VERB NOUN NOUN PUNCT ADP ADJ ADJ NOUN NOUN PUNCT PRON VERB ADP DET NOUN NOUN AUX VERB ADV PUNCT,0.5342465753424658,27.375,5.561643835616438
236,150,Zhiqing Sun,"[' Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently.', 'Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.', 'However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts.']",intro_chunked,"Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.",25.192428940568476,22.733194790065724,236,0.7370892763137817," Thus the decoding consistency can not be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non autoregressive models can not model the highly multimodal distribution of target sequences properly. For example, an English sentence Thank you. can have many correct German translations like Propname., Propname schon., or Propname Propname.. In practice, this will lead to inconsistent outputs such as Propname Propname. or Propname schon.. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation as a sequence labeling problem and propose to use linear chain Propname Propname Propname to model richer structural dependencies. By modeling the co occurrence relationship between adjacent words, the Propname based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 0, the probability of the target sentence is globally normalized: where Propname is the pairwise potential for yi0 and Propname. Such a probability form could better model the multiple modes in target translations."," Thus the decoding consistency can not be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non autoregressive models can not model the highly multimodal distribution of target sequences properly. For example, an English sentence Thank you. can have many correct German translations like Propname., Propname schon., or Propname Propname.. In practice, this will lead to inconsistent outputs such as Propname Propname. or Propname schon.. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation as a sequence labeling problem and propose to use linear chain Propname Propname Propname to model richer structural dependencies. By modeling the co occurrence relationship between adjacent words, the Propname based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 0, the probability of the target sentence is globally normalized: where Propname is the pairwise potential for yi0 and Propname. Such a probability form could better model the multiple modes in target translations.", ADV DET VERB NOUN AUX PART AUX VERB ADP DET NOUN NOUN PUNCT DET ADJ NOUN PRON AUX AUX VERB AUX DET NOUN NOUN PUNCT DET ADJ ADJ NOUN AUX PART VERB DET ADV ADJ NOUN ADP NOUN NOUN ADV PUNCT ADP NOUN PUNCT DET ADJ NOUN VERB PRON PUNCT AUX VERB ADJ ADJ ADJ NOUN ADP PROPN PUNCT PUNCT PROPN NOUN PUNCT PUNCT CCONJ PROPN PROPN PUNCT ADP NOUN PUNCT PRON AUX VERB ADP ADJ NOUN ADJ ADP PROPN PROPN PUNCT CCONJ PROPN NOUN PUNCT PART VERB DET NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PART VERB DET ADJ NOUN NOUN ADP DET ADJ ADJ NOUN PART ADV VERB DET ADJ NOUN ADP NOUN NOUN PUNCT ADV PUNCT PRON VERB NOUN NOUN ADP DET NOUN NOUN NOUN CCONJ VERB PART VERB ADJ NOUN PROPN PROPN PROPN PART VERB ADJ ADJ NOUN PUNCT ADP VERB DET NOUN NOUN NOUN ADP ADJ NOUN PUNCT DET PROPN VERB ADJ NOUN NOUN AUX ADV VERB VERB NOUN ADP DET NOUN PUNCT ADJ ADP DET NOUN NOUN NOUN ADP NOUN NUM PUNCT DET NOUN ADP DET NOUN NOUN AUX ADV VERB PUNCT SCONJ PROPN AUX DET NOUN NOUN ADP NOUN CCONJ PROPN PUNCT DET DET NOUN NOUN AUX ADV VERB DET ADJ NOUN ADP NOUN NOUN PUNCT,0.5518867924528302,21.2,5.320754716981132
237,151,Zhiqing Sun,"[' Autoregressive sequence models achieve great success in domains like machine translation and have been deployed in real applications [ 1, 2 , 3, 4 , 5]. However, these models suffer from high inference latency [1 , 2], which is sometimes unaffordable for real-time industrial applications. This is mainly attributed to the autoregressive factorization nature of the models: Considering a general conditional sequence generation framework, given a context sequence x = (x1, ..., xT ) and a target sequence y = (y1, ..., yT ′ ), autoregressive sequence models are based on a chain of conditional probabilities with a left-to-right causal structure: where y<i represents the tokens before the i-th token of target y. See Figure 1(a) for the illustration of a state-of-the-art autoregressive sequence model, Transformer [ 1]. The autoregressive factorization makes the inference process hard to be parallelized as the results are generated token by token sequentially. The non-autoregressive sequence models take full advantage of parallelism and significantly improve the inference speed. However, they usually cannot get results as good as their autoregressive counterparts. As shown in Table 1, on the machine translation task, compared to AutoRegressive Translation (ART) models, Non-AutoRegressive Translation (NART) models suffer from severe decoding inconsistency problem. In non-autoregressive sequence models, each token in the target sentence is generated independently.', 'Thus the decoding consistency (e.g., word co-occurrence) cannot be guaranteed on the target side. The primary phenomenon that can be observed is the multimodality problem: the non-autoregressive models cannot model the highly multimodal distribution of target sequences properly [ 6]. For example, an English sentence “Thank you.” can have many correct German translations like “Danke.”, “Danke schon.”, or “Vielen Dank.”. In practice, this will lead to inconsistent outputs such as “Danke Dank.” or “Vielen schon.”. To tackle this problem, in this paper, we propose to incorporate a structured inference module in the non-autoregressive decoder to directly model the multimodal distribution of target sequences. Specifically, we regard sequence generation (e.g., machine translation) as a sequence labeling problem and propose to use linear-chain Conditional Random Fields (CRF) [ 10 ] to model richer structural dependencies. By modeling the co-occurrence relationship between adjacent words, the CRF-based structured inference module can significantly improve decoding consistency in the targetside. Different from the probability product form of Equation 2, the probability of the target sentence is globally normalized: where θi−1,i is the pairwise potential for yi−1 and yi. Such a probability form could better model the multiple modes in target translations.', 'However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts.']",intro_chunked,"However, the label size (vocabulary size) used in typical sequence models is very large (e.g., 32k) and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the CRF: low-rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non-autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in CRF. We evaluate the proposed end-to-end model on three widely used machine translation tasks: WMT14 English-to-German/German-to-English (En-De/De-En) tasks and IWSLT14 German-to-English task. Experimental results show that while losing little speed, our NART-CRF model could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En tasks, our model obtains BLEU scores of 26.80 and 30.04, respectively, which largely outperform previous non-autoregressive baselines and are even comparable to the autoregressive counterparts.",10.906666666666695,22.733194790065724,237,0.3955463171005249," However, the label size used in typical sequence models is very large and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the Propname: low rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in Propname. We evaluate the proposed end to end model on three widely used machine translation tasks: Propname Propname to Propname to English tasks and IWSLT00 German to English task. Experimental results show that while losing little speed, our Propname Propname model could achieve significantly better translation performance than previous Propname models on several tasks. In particular, for the Propname Propname Propname and Propname Propname tasks, our model obtains Propname scores of 00.00 and 00.00, respectively, which largely outperform previous non autoregressive baselines and are even comparable to the autoregressive counterparts."," However, the label size used in typical sequence models is very large and intractable for traditional CRFs. Therefore, we design two effective approximation methods for the Propname: low rank approximation and beam approximation. Moreover, to leverage the rich contextual information from the hidden states of non autoregressive decoder and to improve the expressive power of the structured inference module, we further propose a dynamic transition technique to model positional contexts in Propname. We evaluate the proposed end to end model on three widely used machine translation tasks: Propname Propname to Propname to English tasks and IWSLT00 German to English task. Experimental results show that while losing little speed, our Propname Propname model could achieve significantly better translation performance than previous Propname models on several tasks. In particular, for the Propname Propname Propname and Propname Propname tasks, our model obtains Propname scores of 00.00 and 00.00, respectively, which largely outperform previous non autoregressive baselines and are even comparable to the autoregressive counterparts.", ADV PUNCT DET NOUN NOUN VERB ADP ADJ NOUN NOUN AUX ADV ADJ CCONJ ADJ ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB NUM ADJ NOUN NOUN ADP DET PROPN PUNCT ADJ NOUN NOUN CCONJ NOUN NOUN PUNCT ADV PUNCT PART VERB DET ADJ ADJ NOUN ADP DET ADJ NOUN ADP ADJ ADJ NOUN CCONJ PART VERB DET ADJ NOUN ADP DET ADJ NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN NOUN AUX VERB ADJ NOUN ADP PROPN PUNCT PRON VERB DET VERB NOUN PART NOUN NOUN ADP NUM ADV VERB NOUN NOUN NOUN PUNCT PROPN PROPN ADP PROPN ADP ADJ NOUN CCONJ ADJ ADJ ADP ADJ NOUN PUNCT ADJ NOUN VERB SCONJ SCONJ VERB ADJ NOUN PUNCT PRON PROPN PROPN NOUN AUX VERB ADV ADJ NOUN NOUN ADP ADJ PROPN NOUN ADP ADJ NOUN PUNCT ADP ADJ PUNCT ADP DET PROPN PROPN PROPN CCONJ PROPN PROPN NOUN PUNCT PRON NOUN VERB PROPN NOUN ADP NUM CCONJ NUM PUNCT ADV PUNCT PRON ADV VERB ADJ ADJ ADJ NOUN CCONJ AUX ADV ADJ ADP DET ADJ NOUN PUNCT,0.5842696629213483,29.666666666666668,5.48314606741573
238,152,Zhiqing Sun,"[' Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.', 'Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.', 'Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.']",intro_chunked," Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.",35.40863636363639,22.733194790065724,238,0.27640995383262634," Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval, text summarization, and question answering. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics, and then rank the candidate keyphrases according to their importance in the documents. Along this direction, the state of the art algorithms are graph based ranking methods, which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as Propname. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work to address the problem of over generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones."," Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval, text summarization, and question answering. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics, and then rank the candidate keyphrases according to their importance in the documents. Along this direction, the state of the art algorithms are graph based ranking methods, which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as Propname. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work to address the problem of over generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.", NOUN NOUN ADP NOUN AUX ADJ ADP DET NOUN ADP NOUN ADJ ADP NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ NOUN VERB PUNCT PRON VERB PART VERB DET ADJ NOUN ADP DET NOUN PUNCT DET NOUN AUX VERB DET ADJ NOUN ADP NOUN DET NOUN PUNCT ADV ADJ NOUN PART VERB NOUN AUX VERB NOUN PUNCT PRON ADV ADV VERB DET NOUN NOUN ADP DET NOUN PUNCT CCONJ ADV VERB DET NOUN VERB VERB ADP PRON NOUN ADP DET NOUN PUNCT ADP DET NOUN PUNCT DET NOUN ADP DET NOUN NOUN AUX NOUN VERB VERB NOUN PUNCT PRON ADV VERB DET NOUN NOUN ADP DET NOUN CCONJ ADV VERB DET NOUN ADP DET NOUN ADP ADJ NOUN VERB NOUN ADJ ADP PROPN PUNCT ADP VERB DET NOUN NOUN PUNCT DET NOUN AUX ADV VERB DET ADJ NOUN NOUN PUNCT DET NOUN NOUN AUX ADV AUX VERB ADP DET ADJ NOUN PART VERB DET NOUN ADP ADP NOUN ADP DET ADJ NOUN ADP NOUN NOUN PUNCT ADV PUNCT DET NOUN AUX ADV ADJ PUNCT PRON VERB ADV ADP ADV VERB NOUN PUNCT PRON AUX PART VERB ADV SCONJ VERB ADP DET ADJ NOUN ADP NOUN PUNCT ADP NOUN PUNCT PRON ADV VERB SCONJ DET NOUN ADP DET NOUN AUX ADV ADJ CCONJ ADJ ADP DET ADJ NOUN PUNCT,0.5483870967741935,19.727272727272727,5.147465437788019
239,153,Zhiqing Sun,"[' Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.', 'Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.', 'Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.']",intro_chunked,"Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.",20.413243243243272,22.733194790065724,239,0.3042186498641968," Recently, end to end neural approaches for keyphrase extraction have been attracting growing interests. The neural approaches usually studied keyphrase extraction in the encoder decoder framework, which first encodes the input documents into vector representations and then generates the keyphrases with Propname Propname Propname or Propname decoders conditioned on the document representations. These neural methods have achieved state of the art performance on multiple benchmark data sets with end to end supervised training. The end to end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph based ranking approaches, existing end to end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document level word salience information such as long range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end to end methods is that they can not guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted."," Recently, end to end neural approaches for keyphrase extraction have been attracting growing interests. The neural approaches usually studied keyphrase extraction in the encoder decoder framework, which first encodes the input documents into vector representations and then generates the keyphrases with Propname Propname Propname or Propname decoders conditioned on the document representations. These neural methods have achieved state of the art performance on multiple benchmark data sets with end to end supervised training. The end to end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph based ranking approaches, existing end to end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document level word salience information such as long range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end to end methods is that they can not guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.", ADV PUNCT VERB PART VERB ADJ NOUN ADP NOUN NOUN AUX AUX VERB VERB NOUN PUNCT DET ADJ NOUN ADV VERB NOUN NOUN ADP DET NOUN NOUN NOUN PUNCT PRON ADV VERB DET NOUN NOUN ADP NOUN NOUN CCONJ ADV VERB DET NOUN ADP PROPN PROPN PROPN CCONJ PROPN NOUN VERB ADP DET NOUN NOUN PUNCT DET ADJ NOUN AUX VERB NOUN ADP DET NOUN NOUN ADP ADJ ADJ NOUN NOUN ADP NOUN PART VERB ADJ NOUN PUNCT DET NOUN PART VERB NOUN VERB DET ADJ NOUN SCONJ DET NOUN NOUN AUX VERB ADP DET NOUN ADP NOUN PUNCT ADV PUNCT VERB ADP DET ADJ NOUN VERB VERB NOUN PUNCT VERB NOUN PART VERB NOUN ADV VERB NOUN ADP NOUN ADP NOUN PUNCT PRON AUX PART VERB ADP DET DET ADV ADJ NOUN NOUN PRON VERB ADJ NOUN NOUN NOUN NOUN NOUN ADJ ADP ADJ NOUN NOUN ADP NOUN PUNCT ADV ADV ADP DET ADJ NOUN ADP DET ADJ NOUN ADP ADJ NOUN ADP DET NOUN PUNCT DET NOUN ADP DET NOUN PART NOUN NOUN AUX SCONJ PRON AUX PART VERB DET NOUN ADP DET VERB ADJ NOUN PUNCT PRON AUX ADV DET NOUN SCONJ ADJ ADJ NOUN AUX VERB PUNCT,0.582089552238806,28.714285714285715,5.208955223880597
240,154,Zhiqing Sun,"[' Keyphrase extraction from documents is useful in a variety of tasks such as information retrieval [ 20 ], text summarization [ 34 ], and question answering [24]. It allows to identify the salient contents from a document. The topic has attracted a large amount of workin the literature. Most traditional approaches to keyphrase extraction are unsupervised approaches. They usually first identify the candidate keyphrases with some heuristics (e.g., regular expressions), and then rank the candidate keyphrases according to their importance in the documents [14]. Along this direction, the state-of-the-art algorithms are graph-based ranking methods [ 25, 30 , 43 ], which first construct a word graph from a document and then determine the importance of the keyphrases with random walk based approaches such as PageRank [5 ]. By constructing the word graph, these methods can effectively identify the most salient keyphrases. Some diversification mechanisms have also been investigated in some early work [4, 27] to address the problem of over-generation of the same concepts in keyphrase extraction. However, these methods are fully unsupervised. They rely heavily on manually designed heuristics, which may not work well when applied to a different type of document. In experiments, we also observe that the performance of these methods is usually limited and inferior to the supervised ones.', 'Recently, end-to-end neural approaches for keyphrase extraction have been attracting growing interests [29 , 46, 47 ]. The neural approaches usually studied keyphrase extraction in the encoder-decoder framework [39 ], which first encodes the input documents into vector representations and then generates the keyphrases with Recurrent Neural Networks (RNN) [31 ] or CopyRNN [ 13] decoders conditioned on the document representations. These neural methods have achieved state-of-the-art performance on multiple benchmark data sets with end-to-end supervised training. The end-to-end training offers a great advantage that the extraction process can adapt to the type of documents. However, compared to the unsupervised graph-based ranking approaches, existing end-to-end approaches only treat documents as sequences of words. They do not benefit from the a more global graph structure that provides useful document-level word salience information such as long-range dependencies between words, as well as a synthetic view on the multiple appearances of identical words in the document. Another problem of these end-to-end methods is that they cannot guarantee the diversity of the extracted key phrases: it is often the case that several similar keyphrases are extracted.', 'Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.']",intro_chunked,"Therefore, we are seeking an approach that can have the advantage of modeling document-level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end-to-end fashion. In this paper, we propose an end-to-end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short- and long-range dependency between the words in the document. Afterwards, the graph convolutional neural network [22 ] is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model [ 42 ] over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism [40 ] to address the over-generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 1 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back-propagation in an end-to-end fashion. Experimental results show that our proposed DivGraphPointer achieves state-of-the-art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document-level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.",12.52876515151516,22.733194790065724,240,0.724690854549408," Therefore, we are seeking an approach that can have the advantage of modeling document level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end to end fashion. In this paper, we propose an end to end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short and long range dependency between the words in the document. Afterwards, the graph convolutional neural network is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism to address the over generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 0 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back propagation in an end to end fashion. Experimental results show that our proposed DivGraphPointer achieves state of the art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases."," Therefore, we are seeking an approach that can have the advantage of modeling document level word salience, generating diverse keyphrases, and meanwhile be efficiently trained in an end to end fashion. In this paper, we propose an end to end approach called DivGraphPointer for extracting diversified keyphrases from documents. Specifically, given an input document, we first construct a word graph from it, which aggregates identical words into one node and captures both the short and long range dependency between the words in the document. Afterwards, the graph convolutional neural network is applied to the word graph to learn the representations of each node, which effectively models the word salience. To extract diverse keyphrases from documents, we propose a diversified pointer network model over the word graph, which dynamically picks nodes from the word graph to construct the keyphrases. Two diversity mechanisms are proposed to increase the diversity among the generated keyphrases. Specifically, we employ a coverage attention mechanism to address the over generation problem in keyphrase extraction at lexical level and a semantic modification mechanism to dynamically modify the encoded document representation at semantic level. Figure 0 illustrates our approach schematically. The whole framework can be effectively and efficiently trained with back propagation in an end to end fashion. Experimental results show that our proposed DivGraphPointer achieves state of the art performance for keyphrase extraction on five benchmarks and significantly outperforms the existing supervised and unsupervised keyphrase extraction methods. The contribution of this paper is twofold: We propose a graph convolutional network encoder for keyphrase extraction that can effectively capture document level word salience. We propose two complementary diversification mechanisms that help the pointer network decoder to extract diverse keyphrases.", ADV PUNCT PRON AUX VERB DET NOUN PRON AUX VERB DET NOUN ADP VERB NOUN NOUN NOUN NOUN PUNCT VERB ADJ NOUN PUNCT CCONJ ADV AUX ADV VERB ADP DET NOUN PART VERB NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP NOUN NOUN VERB NOUN ADP VERB ADJ NOUN ADP NOUN PUNCT ADV PUNCT VERB DET NOUN NOUN PUNCT PRON ADV VERB DET NOUN NOUN ADP PRON PUNCT PRON VERB ADJ NOUN ADP NUM NOUN CCONJ VERB CCONJ DET ADJ CCONJ ADJ NOUN NOUN ADP DET NOUN ADP DET NOUN PUNCT ADV PUNCT DET NOUN ADJ ADJ NOUN AUX VERB ADP DET NOUN NOUN PART VERB DET NOUN ADP DET NOUN PUNCT PRON ADV VERB DET NOUN NOUN PUNCT PART VERB ADJ NOUN ADP NOUN PUNCT PRON VERB DET ADJ NOUN NOUN NOUN ADP DET NOUN NOUN PUNCT PRON ADV VERB NOUN ADP DET NOUN NOUN PART VERB DET NOUN PUNCT NUM NOUN NOUN AUX VERB PART VERB DET NOUN ADP DET ADJ NOUN PUNCT ADV PUNCT PRON VERB DET NOUN NOUN NOUN PART VERB DET ADP NOUN NOUN ADP NOUN NOUN ADP ADJ NOUN CCONJ DET ADJ NOUN NOUN PART ADV VERB DET ADJ NOUN NOUN ADP ADJ NOUN PUNCT NOUN NUM VERB PRON NOUN ADV PUNCT DET ADJ NOUN AUX AUX ADV CCONJ ADV VERB ADP ADJ NOUN ADP DET NOUN PART VERB NOUN PUNCT ADJ NOUN VERB SCONJ PRON VERB NOUN VERB NOUN ADP DET NOUN NOUN ADP NOUN NOUN ADP NUM NOUN CCONJ ADV VERB DET VERB VERB CCONJ VERB NOUN NOUN NOUN PUNCT DET NOUN ADP DET NOUN AUX ADJ PUNCT PRON VERB DET NOUN ADJ NOUN NOUN ADP NOUN NOUN PRON AUX ADV VERB NOUN NOUN NOUN NOUN PUNCT PRON VERB NUM ADJ NOUN NOUN PRON VERB DET NOUN NOUN NOUN PART VERB ADJ NOUN PUNCT,0.4721311475409836,25.416666666666668,5.488524590163935
241,155,Zhiqing Sun,"[' Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.', 'For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.', 'Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.', 'To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.']",intro_chunked," Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.",32.412600732600765,22.733194790065724,241,0.5940579771995544," Propname graphs are collections of factual triplets, where each triplet represents a relation r between a head entity h and a tail entity t. Propname of real world knowledge graphs include Propname, Propname, and Propname. Propname graphs are potentially useful to a variety of applications such as question answering, information retrieval, recommender systems, and natural language processing. Research on knowledge graphs is attracting growing interests in both academia and industry communities. Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low dimensional representations of entities and relations for missing link prediction. These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts."," Propname graphs are collections of factual triplets, where each triplet represents a relation r between a head entity h and a tail entity t. Propname of real world knowledge graphs include Propname, Propname, and Propname. Propname graphs are potentially useful to a variety of applications such as question answering, information retrieval, recommender systems, and natural language processing. Research on knowledge graphs is attracting growing interests in both academia and industry communities. Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low dimensional representations of entities and relations for missing link prediction. These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.", PROPN NOUN AUX NOUN ADP ADJ NOUN PUNCT SCONJ DET NOUN VERB DET NOUN NOUN ADP DET NOUN NOUN NOUN CCONJ DET NOUN NOUN NOUN PROPN ADP ADJ NOUN NOUN NOUN VERB PROPN PUNCT PROPN PUNCT CCONJ PROPN PUNCT PROPN NOUN AUX ADV ADJ ADP DET NOUN ADP NOUN ADJ ADP NOUN VERB PUNCT NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ ADJ NOUN NOUN PUNCT NOUN ADP NOUN NOUN AUX VERB VERB NOUN ADP CCONJ NOUN CCONJ NOUN NOUN PUNCT SCONJ NOUN NOUN AUX ADV ADJ PUNCT DET ADJ NOUN ADP NOUN NOUN AUX VERB DET VERB NOUN PUNCT ADV PUNCT ADJ NOUN AUX AUX VERB ADP VERB ADJ ADJ NOUN ADP NOUN CCONJ NOUN ADP VERB NOUN NOUN PUNCT DET NOUN AUX AUX VERB PART AUX ADJ CCONJ ADJ PUNCT DET ADJ NOUN ADP DET NOUN AUX PART VERB CCONJ VERB DET NOUN NOUN ADP NOUN NOUN VERB ADP DET ADJ NOUN NOUN PUNCT,0.6193548387096774,22.142857142857142,5.380645161290323
242,156,Zhiqing Sun,"[' Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.', 'For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.', 'Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.', 'To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.']",intro_chunked,"For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.",39.0107142857143,22.733194790065724,242,0.2976384460926056," For example, some relations are symmetric while others are antisymmetric; some relations are the inverse of other relations; and some relations may be composed by others. It is critical to find ways to model and infer these patterns, ie, symmetryantisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns. For example, the TransE model, which represents relations as translations, aims to model the inversion and composition patterns; the Propname model, which models the three way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called Propname for knowledge graph embedding."," For example, some relations are symmetric while others are antisymmetric; some relations are the inverse of other relations; and some relations may be composed by others. It is critical to find ways to model and infer these patterns, ie, symmetryantisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns. For example, the TransE model, which represents relations as translations, aims to model the inversion and composition patterns; the Propname model, which models the three way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called Propname for knowledge graph embedding.", ADP NOUN PUNCT DET NOUN AUX ADJ SCONJ NOUN AUX ADJ PUNCT DET NOUN AUX DET NOUN ADP ADJ NOUN PUNCT CCONJ DET NOUN AUX AUX VERB ADP NOUN PUNCT PRON AUX ADJ PART VERB NOUN PART NOUN CCONJ VERB DET NOUN PUNCT ADV PUNCT NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT ADP DET VERB NOUN ADP NOUN PART VERB ADJ NOUN PUNCT ADV PUNCT ADJ VERB NOUN AUX AUX VERB PART CCONJ ADV CCONJ ADV VERB NUM CCONJ DET ADJ ADP DET ADJ NOUN NOUN PUNCT ADP NOUN PUNCT DET NUM NOUN PUNCT PRON VERB NOUN ADP NOUN PUNCT VERB PART VERB DET NOUN CCONJ NOUN NOUN PUNCT DET PROPN NOUN PUNCT PRON VERB DET NUM NOUN NOUN ADP NOUN NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN PUNCT VERB PART VERB DET NOUN NOUN PUNCT ADV PUNCT NOUN ADP VERB NOUN AUX ADJ ADP VERB CCONJ VERB DET DET ADJ NOUN PUNCT ADV PUNCT PRON AUX VERB ADP DET NOUN PRON AUX ADJ PART VERB CCONJ VERB DET DET NUM NOUN ADP NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET DET NOUN VERB PROPN ADP NOUN NOUN VERB PUNCT,0.5208333333333334,27.428571428571427,4.666666666666667
243,157,Zhiqing Sun,"[' Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.', 'For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.', 'Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.', 'To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.']",intro_chunked,"Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.",45.76160594170403,22.733194790065724,243,0.44154366850852966," Our motivation is from Propname identity ei cos i sin, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the Propname model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet, we expect that Propname h r, where h, Propname, Propname Propname are the embeddings, the modulus ri0 and denotes the Propname product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetricantisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, Propname Propname, satisfies Propname Propname 0; two relations r0 and r0 are inverse if and only if their embeddings are conjugates: r0 r0; a relation r0 ei0 is a combination of other two relations r0 ei0 and r0 ei0 if and only if r0 r0 r0. Moreover, the Propname model is scalable to large knowledge graphs as it remains linear in both time and memory."," Our motivation is from Propname identity ei cos i sin, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the Propname model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet, we expect that Propname h r, where h, Propname, Propname Propname are the embeddings, the modulus ri0 and denotes the Propname product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetricantisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, Propname Propname, satisfies Propname Propname 0; two relations r0 and r0 are inverse if and only if their embeddings are conjugates: r0 r0; a relation r0 ei0 is a combination of other two relations r0 ei0 and r0 ei0 if and only if r0 r0 r0. Moreover, the Propname model is scalable to large knowledge graphs as it remains linear in both time and memory.", PRON NOUN AUX ADP PROPN NOUN NOUN SCONJ PRON VERB PUNCT PRON VERB SCONJ DET ADJ ADJ NOUN AUX AUX VERB ADP DET NOUN ADP DET ADJ NOUN PUNCT ADV PUNCT DET PROPN NOUN VERB DET NOUN CCONJ NOUN ADP DET ADJ NOUN NOUN CCONJ VERB DET NOUN ADP DET NOUN NOUN NOUN NOUN ADP DET NOUN NOUN PUNCT VERB DET NOUN PUNCT PRON VERB DET PROPN NOUN NOUN PUNCT SCONJ NOUN PUNCT PROPN PUNCT PROPN PROPN AUX DET NOUN PUNCT DET NOUN NOUN PUNCT CCONJ VERB DET PROPN NOUN PUNCT ADV PUNCT ADP DET NOUN ADP DET ADJ NOUN PUNCT PRON VERB PRON PUNCT PRON VERB ADP SCONJ DET DET ADJ NOUN AUX ADV VERB DET DET NUM NOUN NOUN PUNCT NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT ADP NOUN PUNCT DET NOUN NOUN AUX ADJ SCONJ CCONJ ADV SCONJ DET NOUN ADP PRON VERB NOUN PUNCT PROPN PROPN PUNCT NOUN PROPN PROPN NUM PUNCT NUM NOUN NOUN CCONJ NOUN AUX ADJ SCONJ CCONJ ADV SCONJ PRON NOUN AUX NOUN PUNCT NOUN NOUN PUNCT DET NOUN NOUN NOUN AUX DET NOUN ADP ADJ NUM NOUN NOUN NOUN CCONJ VERB NOUN SCONJ CCONJ ADV SCONJ VERB NOUN NOUN PUNCT ADV PUNCT DET PROPN NOUN AUX ADJ ADP ADJ NOUN NOUN SCONJ PRON VERB ADJ ADP DET NOUN CCONJ NOUN PUNCT,0.4703196347031963,36.5,4.34703196347032
244,158,Zhiqing Sun,"[' Knowledge graphs are collections of factual triplets, where each triplet (h, r, t) represents a relation r between a head entity h and a tail entity t. Examples of real-world knowledge graphs include Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), and WordNet (Miller, 1995). Knowledge graphs are potentially useful to a variety of applications such as question-answering (Hao et al., 2017), information retrieval (Xiong et al., 2017), recommender systems (Zhang et al., 2016), and natural language processing (Yang & Mitchell, 2017). Research on knowledge graphs is attracting growing interests in both academia and industry communities.Since knowledge graphs are usually incomplete, a fundamental problem for knowledge graph is predicting the missing links. Recently, extensive studies have been done on learning low-dimensional representations of entities and relations for missing link prediction (a.k.a., knowledge graph embedding) (Bordes et al., 2013; Trouillon et al., 2016; Dettmers et al., 2017). These methods have been shown to be scalable and effective. The general intuition of these methods is to model and infer the connectivity patterns in knowledge graphs according to the observed knowledge facts.', 'For example, some relations are symmetric (e.g., marriage) while others are antisymmetric (e.g., filiation); some relations are the inverse of other relations (e.g., hypernym and hyponym); and some relations may be composed by others (e.g., my mother’s husband is my father). It is critical to find ways to model and infer these patterns, i.e., symmetry/antisymmetry, inversion, and composition, from the observed facts in order to predict missing links. Indeed, many existing approaches have been trying to either implicitly or explicitly model one or a few of the above relation patterns (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b;). For example, the TransE model (Bordes et al., 2011), which represents relations as translations, aims to model the inversion and composition patterns; the DisMult model (Yang et al., 2014), which models the three-way interactions between head entities, relations, and tail entities, aims to model the symmetry pattern. However, none of existing models is capable of modeling and inferring all the above patterns. Therefore, we are looking for an approach that is able to model and infer all the three types of relation patterns. In this paper, we propose such an approach called RotatE for knowledge graph embedding.', 'Our motivation is from Euler’s identity eiθ = cos θ + i sin θ, which indicates that a unitary complex number can be regarded as a rotation in the complex plane. Specifically, the RotatE model maps the entities and relations to the complex vector space and defines each relation as a rotation fromthe source entity to the target entity. Given a triplet (h, r, t), we expect that t = h ◦ r, where h, r, t ∈ Ck are the embeddings, the modulus |ri| = 1 and ◦ denotes the Hadamard (element-wise) product. Specifically, for each dimension in the complex space, we expect that: It turns out that such a simple operation can effectively model all the three relation patterns: symmetric/antisymmetric, inversion, and composition. For example, a relation r is symmetric if and only if each element of its embedding r, i.e. ri, satisfies ri = e0/iπ = ±1; two relations r1 and r2 are inverse if and only if their embeddings are conjugates: r2 = ¯r1; a relation r3 = eiθ3 is a combination of other two relations r1 = eiθ1 and r2 = eiθ2 if and only if r3 = r1 ◦ r2 (i.e. θ3 = θ1 + θ2). Moreover, the RotatE model is scalable to large knowledge graphs as it remains linear in both time and memory.', 'To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.']",intro_chunked,"To effectively optimizing the RotatE, we further propose a novel self-adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the RotatE on four large knowledge graph benchmark datasets including FB15k (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2017). Experimental results show that the RotatE model significantly outperforms existing state-of-the-art approaches. In addition, RotatE also outperforms state-of-the-art models on Countries (Bouchard et al., 2015), a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, RotatE is the first model that achieves state-of-the-art performance on all the benchmarks.",28.00794871794872,22.733194790065724,244,0.49703213572502136," To effectively optimizing the Propname, we further propose a novel self adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the Propname on four large knowledge graph benchmark datasets including Propname, Propname, Propname 000 and Propname. Experimental results show that the Propname model significantly outperforms existing state of the art approaches. In addition, Propname also outperforms state of the art models on Propname, a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, Propname is the first model that achieves state of the art performance on all the benchmarks."," To effectively optimizing the Propname, we further propose a novel self adversarial negative sampling technique, which generates negative samples according to the current entity and relation embeddings. The proposed technique is very general and can be applied to many existing knowledge graph embedding models. We evaluate the Propname on four large knowledge graph benchmark datasets including Propname, Propname, Propname 000 and Propname. Experimental results show that the Propname model significantly outperforms existing state of the art approaches. In addition, Propname also outperforms state of the art models on Propname, a benchmark explicitly designed for composition pattern inference and modeling. To the best of our knowledge, Propname is the first model that achieves state of the art performance on all the benchmarks.", PART ADV VERB DET PROPN PUNCT PRON ADV VERB DET ADJ NOUN ADJ ADJ NOUN NOUN PUNCT PRON VERB ADJ NOUN VERB ADP DET ADJ NOUN CCONJ NOUN NOUN PUNCT DET VERB NOUN AUX ADV ADJ CCONJ AUX AUX VERB ADP ADJ VERB NOUN NOUN VERB NOUN PUNCT PRON VERB DET PROPN ADP NUM ADJ NOUN NOUN ADJ NOUN VERB PROPN PUNCT PROPN PUNCT PROPN NUM CCONJ PROPN PUNCT ADJ NOUN VERB SCONJ DET PROPN NOUN ADV VERB VERB NOUN ADP DET NOUN NOUN PUNCT ADP NOUN PUNCT PROPN ADV VERB NOUN ADP DET NOUN NOUN ADP PROPN PUNCT DET NOUN ADV VERB ADP NOUN NOUN NOUN CCONJ NOUN PUNCT ADP DET ADJ ADP PRON NOUN PUNCT PROPN AUX DET ADJ NOUN PRON VERB NOUN ADP DET NOUN NOUN ADP DET DET NOUN PUNCT,0.582089552238806,22.333333333333332,5.343283582089552
245,159,Zhiqing Sun,"[' Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013).', 'There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets.']",intro_chunked," Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013).",22.087500000000034,22.733194790065724,245,0.6367157697677612," Unlike Propname and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Propname Propname is a crucial step for many Propname Propname Propname Propname tasks such as syntactic parsing, information retrieval and word representation learning. Recently, neural approaches for supervised Propname are attracting huge interest. A great quantities of neural models, Propname, tensor neural network, recursive neural network, long short term memory and convolutionalneural network, have been proposed and given competitive results to the best statistical models. However, the neural approaches for unsupervised Propname have not been investigated. Previous unsupervised approaches to Propname can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Propname and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Propname Propname, normalized Propname of Propname Propname and Propname Propname Propname."," Unlike Propname and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Propname Propname is a crucial step for many Propname Propname Propname Propname tasks such as syntactic parsing, information retrieval and word representation learning. Recently, neural approaches for supervised Propname are attracting huge interest. A great quantities of neural models, Propname, tensor neural network, recursive neural network, long short term memory and convolutionalneural network, have been proposed and given competitive results to the best statistical models. However, the neural approaches for unsupervised Propname have not been investigated. Previous unsupervised approaches to Propname can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Propname and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Propname Propname, normalized Propname of Propname Propname and Propname Propname Propname.", ADP PROPN CCONJ ADJ ADJ NOUN PUNCT ADJ NOUN VERB DET ADJ NOUN NOUN PUNCT ADV PUNCT ADJ PROPN PROPN AUX DET ADJ NOUN ADP ADJ PROPN PROPN PROPN PROPN NOUN ADJ ADP ADJ NOUN PUNCT NOUN NOUN CCONJ NOUN NOUN NOUN PUNCT ADV PUNCT ADJ NOUN ADP ADJ PROPN AUX VERB ADJ NOUN PUNCT DET ADJ NOUN ADP ADJ NOUN PUNCT PROPN PUNCT NOUN ADJ NOUN PUNCT ADJ ADJ NOUN PUNCT ADJ ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT AUX AUX VERB CCONJ VERB ADJ NOUN ADP DET ADJ ADJ NOUN PUNCT ADV PUNCT DET ADJ NOUN ADP ADJ PROPN AUX PART AUX VERB PUNCT ADJ VERB NOUN ADP PROPN AUX AUX ADV VERB ADP ADJ CCONJ ADJ NOUN PUNCT DET ADJ VERB ADV VERB NOUN NOUN ADP NOUN NOUN PUNCT SCONJ DET ADJ VERB ADP VERB ADJ NOUN ADP PROPN CCONJ VERB DET ADJ NOUN ADP DET ADJ ADJ NOUN PUNCT ADJ NOUN NOUN ADP ADJ NOUN VERB PROPN PROPN PUNCT VERB PROPN ADP PROPN PROPN CCONJ PROPN PROPN PROPN PUNCT,0.5375722543352601,21.625,5.901734104046243
246,160,Zhiqing Sun,"[' Unlike English and many other languages, Chinese sentences have no explicit word boundaries. Therefore, Chinese Word Segmentation (CWS) is a crucial step for many Chinese Natural Language Processing (NLP) tasks such as syntactic parsing, information retrieval and word representation learning (Grave et al., 2018). Recently, neural approaches for supervised CWS are attracting huge interest. A great quantities of neural models, e.g., tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a), long-short-term-memory (RNN-LSTM) (Chen et al., 2015b) and convolutionalneural network (CNN) (Wang and Xu, 2017), have been proposed and given competitive results to the best statistical models (Sun, 2010). However, the neural approaches for unsupervised CWS have not been investigated. Previous unsupervised approaches to CWS can be roughly classified into discriminative and generative models. The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003), normalized Variation of Branching Entropy (nVBE) (Magistry and Sagot, 2012) and Minimum Description Length (MDL) (Magistry and Sagot, 2013).', 'There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets.']",intro_chunked,"There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003). There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Hidden Markov Model (HMM) (Chen et al., 2014), Hierarchical Dirichlet Process (HDP) (Goldwater et al., 2009) and Nested Pitman-Yor Process (NPY) (Mochihashi et al.,2009). However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Segmental Language Models (SLMs), a neural generative model that explicitly focuses on the segmental nature of Chinese: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from SIGHAN 2005 bakeoff (Emerson, 2005), namely PKU, MSR, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state-of-the-art statistical models on four different datasets.",26.24690117801049,22.733194790065724,246,0.28886228799819946," There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n gram language models in these approaches by neural language models. There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Propname Propname Propname, Propname Propname Propname and Propname Propname Propname Propname. However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Propname Propname Propname, a neural generative model that explicitly focuses on the segmental nature of Propname: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from Propname 0000 bakeoff, namely Propname, Propname, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state of the art statistical models on four different datasets."," There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n gram language models in these approaches by neural language models. There may exists other more sophisticated neural discriminative approaches, but it is not the focus of this paper. For generative approaches, typical statistical models includes Propname Propname Propname, Propname Propname Propname and Propname Propname Propname Propname. However, none of them can be easily extended into a neural model. Therefore, neural generative models for word segmentation are remaining to be investigated. In this paper, we proposed the Propname Propname Propname, a neural generative model that explicitly focuses on the segmental nature of Propname: SLMs can directly generate segmented sentences and give the corresponding generative probability. We evaluate our methods on four different benchmark datasets from Propname 0000 bakeoff, namely Propname, Propname, AS and CityU. To our knowledge, we are the first to propose a neural model for unsupervised Chinese word segmentation and achieve competitive performance to the state of the art statistical models on four different datasets.", PRON VERB DET ADJ NOUN PART VERB DET ADJ NOUN NOUN PUNCT SCONJ PRON AUX ADV VERB DET NOUN NOUN NOUN NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT PRON AUX VERB ADJ ADV ADJ ADJ ADJ NOUN PUNCT CCONJ PRON AUX PART DET NOUN ADP DET NOUN PUNCT ADP ADJ NOUN PUNCT ADJ ADJ NOUN VERB PROPN PROPN PROPN PUNCT PROPN PROPN PROPN CCONJ PROPN PROPN PROPN PROPN PUNCT ADV PUNCT NOUN ADP PRON AUX AUX ADV VERB ADP DET ADJ NOUN PUNCT ADV PUNCT ADJ ADJ NOUN ADP NOUN NOUN AUX VERB PART AUX VERB PUNCT ADP DET NOUN PUNCT PRON VERB DET PROPN PROPN PROPN PUNCT DET ADJ ADJ NOUN PRON ADV VERB ADP DET ADJ NOUN ADP PROPN PUNCT NOUN AUX ADV VERB VERB NOUN CCONJ VERB DET VERB ADJ NOUN PUNCT PRON VERB PRON NOUN ADP NUM ADJ NOUN NOUN ADP PROPN NUM NOUN PUNCT ADV PROPN PUNCT PROPN PUNCT NOUN CCONJ NOUN ADP PRON NOUN PUNCT PRON AUX DET ADJ PART VERB DET ADJ NOUN ADP ADJ ADJ NOUN NOUN CCONJ VERB ADJ NOUN ADP DET NOUN ADP DET NOUN ADJ NOUN ADP NUM ADJ NOUN PUNCT,0.517948717948718,24.375,5.256410256410256
247,161,Timo Schick,"[' Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real-world uses of NLP to have only a small number of labeled examples, making few-shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text: T1: This was the best pizza I’ve ever had. T2: You can get better sushi for half the price. T3: Pizza was average. Not worth the price. Furthermore, imagine we are told that the labels of T1 and T2 are l and l′, respectively, and we are asked to infer the correct label for T3. Based only on these examples, this is impossible because plausible justifications can be found for both l and l′. However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l′ to T3.', 'This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, i.e., a textual explanation that helps us understand what the task is about. With the rise of pretrained language models (PLMs) such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the PLM predict continuations that solve the task (Radford et al., 2019; Puri and Catanzaro, 2019). So far, this idea has mostly been considered in zero-shot scenarios where no training data is available at all. In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few-shot settings: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that uses natural language patterns to reformulate input examples into cloze-style phrases. As illustrated in Figure 1, PET works in three steps: First, for each pattern a separate PLM is finetuned on a small training set T . The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft-labeled dataset. We also devise iPET, an iterative variant of PET in which this process is repeated with increasing training set sizes.On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, PET and iPET substantially outperform unsupervised approaches, supervised training and strong semi-supervised baselines.']",intro_chunked," Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real-world uses of NLP to have only a small number of labeled examples, making few-shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text: T1: This was the best pizza I’ve ever had. T2: You can get better sushi for half the price. T3: Pizza was average. Not worth the price. Furthermore, imagine we are told that the labels of T1 and T2 are l and l′, respectively, and we are asked to infer the correct label for T3. Based only on these examples, this is impossible because plausible justifications can be found for both l and l′. However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l′ to T3.",53.785714285714306,32.83053957444222,247,0.13162681460380554," Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real world uses of Propname to have only a small number of labeled examples, making few shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text: Propname: This was the best pizza I ve ever had. Propname: You can get better sushi for half the price. Propname: Propname was average. Not worth the price. Furthermore, imagine we are told that the labels of Propname and Propname are l and Propname, respectively, and we are asked to infer the correct label for Propname. Based only on these examples, this is impossible because plausible justifications can be found for both l and Propname However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l to Propname."," Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real world uses of Propname to have only a small number of labeled examples, making few shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text: Propname: This was the best pizza I ve ever had. Propname: You can get better sushi for half the price. Propname: Propname was average. Not worth the price. Furthermore, imagine we are told that the labels of Propname and Propname are l and Propname, respectively, and we are asked to infer the correct label for Propname. Based only on these examples, this is impossible because plausible justifications can be found for both l and Propname However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l to Propname.", VERB ADP NOUN AUX DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN AUX VERB ADP DET NOUN ADP VERB NOUN ADP PRON PRON ADV VERB ADP ADJ NOUN PUNCT ADP ADP DET ADJ NOUN ADP NOUN PUNCT NOUN CCONJ NOUN CCONJ DET NOUN ADP VERB NOUN PUNCT PRON AUX ADJ ADP ADJ NOUN NOUN ADP PROPN PART VERB ADV DET ADJ NOUN ADP VERB NOUN PUNCT VERB ADJ NOUN VERB DET ADV ADJ NOUN NOUN PUNCT ADV PUNCT VERB ADJ ADJ NOUN ADP ADJ NOUN NOUN ADV VERB ADV PUNCT ADJ NOUN AUX ADJ PART VERB ADP ADV VERB ADP DET ADJ NOUN PUNCT ADP NOUN PUNCT VERB PRON AUX VERB DET VERB NOUN ADP NOUN PUNCT PROPN PUNCT PRON AUX DET ADJ NOUN PRON AUX ADV VERB PUNCT PROPN PUNCT PRON AUX VERB ADJ NOUN ADP DET DET NOUN PUNCT PROPN PUNCT PROPN AUX ADJ PUNCT PART ADJ DET NOUN PUNCT ADV PUNCT VERB PRON AUX VERB SCONJ DET NOUN ADP PROPN CCONJ PROPN AUX NOUN CCONJ PROPN PUNCT ADV PUNCT CCONJ PRON AUX VERB PART VERB DET ADJ NOUN ADP PROPN PUNCT VERB ADV ADP DET NOUN PUNCT PRON AUX ADJ SCONJ ADJ NOUN AUX AUX VERB ADP DET NOUN CCONJ PROPN ADV PUNCT SCONJ PRON VERB SCONJ DET ADJ NOUN AUX PART VERB SCONJ DET NOUN VERB PRON ADP NOUN PUNCT PRON AUX ADV VERB NOUN ADP PROPN PUNCT,0.5598290598290598,26.0,4.35042735042735
248,162,Timo Schick,"[' Learning from examples is the predominant approach for many NLP tasks: A model is trained on a set of labeled examples from which it then generalizes to unseen data. Due to the vast number of languages, domains and tasks and the cost of annotating data, it is common in real-world uses of NLP to have only a small number of labeled examples, making few-shot learning a highly important research area. Unfortunately, applying standard supervised learning to small training sets often performs poorly; many problems are difficult to grasp from just looking at a few examples. For instance, assume we are given the following pieces of text: T1: This was the best pizza I’ve ever had. T2: You can get better sushi for half the price. T3: Pizza was average. Not worth the price. Furthermore, imagine we are told that the labels of T1 and T2 are l and l′, respectively, and we are asked to infer the correct label for T3. Based only on these examples, this is impossible because plausible justifications can be found for both l and l′. However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l′ to T3.', 'This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, i.e., a textual explanation that helps us understand what the task is about. With the rise of pretrained language models (PLMs) such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the PLM predict continuations that solve the task (Radford et al., 2019; Puri and Catanzaro, 2019). So far, this idea has mostly been considered in zero-shot scenarios where no training data is available at all. In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few-shot settings: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that uses natural language patterns to reformulate input examples into cloze-style phrases. As illustrated in Figure 1, PET works in three steps: First, for each pattern a separate PLM is finetuned on a small training set T . The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft-labeled dataset. We also devise iPET, an iterative variant of PET in which this process is repeated with increasing training set sizes.On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, PET and iPET substantially outperform unsupervised approaches, supervised training and strong semi-supervised baselines.']",intro_chunked,"This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, i.e., a textual explanation that helps us understand what the task is about. With the rise of pretrained language models (PLMs) such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the PLM predict continuations that solve the task (Radford et al., 2019; Puri and Catanzaro, 2019). So far, this idea has mostly been considered in zero-shot scenarios where no training data is available at all. In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few-shot settings: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that uses natural language patterns to reformulate input examples into cloze-style phrases. As illustrated in Figure 1, PET works in three steps: First, for each pattern a separate PLM is finetuned on a small training set T . The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft-labeled dataset. We also devise iPET, an iterative variant of PET in which this process is repeated with increasing training set sizes.On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, PET and iPET substantially outperform unsupervised approaches, supervised training and strong semi-supervised baselines.",28.35097815985131,32.83053957444222,248,0.3763471245765686," This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, ie, a textual explanation that helps us understand what the task is about. With the rise of pretrained language models such as Propname, Propname and Propname, the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the Propname predict continuations that solve the task. So far, this idea has mostly been considered in zero shot scenarios where no training data is available at all. In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few shot settings: We introduce Propname Propname Propname, a semi supervised training procedure that uses natural language patterns to reformulate input examples into cloze style phrases. As illustrated in Figure 0, Propname works in three steps: First, for each pattern a separate Propname is finetuned on a small training set T. The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft labeled dataset. We also devise iPET, an iterative variant of Propname in which this process is repeated with increasing training set sizes. On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, Propname and Propname substantially outperform unsupervised approaches, supervised training and strong semi supervised baselines."," This illustrates that solving a task from only a few examples becomes much easier when we also have a task description, ie, a textual explanation that helps us understand what the task is about. With the rise of pretrained language models such as Propname, Propname and Propname, the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the Propname predict continuations that solve the task. So far, this idea has mostly been considered in zero shot scenarios where no training data is available at all. In this work, we show that providing task descriptions can successfully be combined with standard supervised learning in few shot settings: We introduce Propname Propname Propname, a semi supervised training procedure that uses natural language patterns to reformulate input examples into cloze style phrases. As illustrated in Figure 0, Propname works in three steps: First, for each pattern a separate Propname is finetuned on a small training set T. The ensemble of all models is then used to annotate a large unlabeled dataset D with soft labels. Finally, a standard classifier is trained on the soft labeled dataset. We also devise iPET, an iterative variant of Propname in which this process is repeated with increasing training set sizes. On a diverse set of tasks in multiple languages, we show that given a small to medium number of labeled examples, Propname and Propname substantially outperform unsupervised approaches, supervised training and strong semi supervised baselines.", PRON VERB SCONJ VERB DET NOUN ADP ADV DET ADJ NOUN VERB ADV ADJ SCONJ PRON ADV VERB DET NOUN NOUN PUNCT ADV PUNCT DET ADJ NOUN PRON VERB PRON VERB PRON DET NOUN AUX ADP PUNCT SCONJ DET NOUN ADP VERB NOUN NOUN ADJ ADP PROPN PUNCT PROPN CCONJ PROPN PUNCT DET NOUN ADP VERB NOUN NOUN AUX VERB ADJ ADP ADJ NOUN PUNCT PRON AUX ADV VERB ADJ NOUN ADP ADJ NOUN ADP DET NOUN CCONJ VERB DET PROPN VERB NOUN PRON VERB DET NOUN PUNCT ADV ADV PUNCT DET NOUN AUX ADV AUX VERB ADP NUM NOUN NOUN SCONJ DET NOUN NOUN AUX ADJ ADV ADV PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ VERB NOUN NOUN AUX ADV AUX VERB ADP ADJ ADJ NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB PROPN PROPN PROPN PUNCT DET ADJ ADJ NOUN NOUN PRON VERB ADJ NOUN NOUN PART VERB NOUN NOUN ADP ADJ NOUN NOUN PUNCT SCONJ VERB ADP NOUN NUM PUNCT PROPN VERB ADP NUM NOUN PUNCT ADV PUNCT ADP DET NOUN DET ADJ PROPN AUX VERB ADP DET ADJ NOUN VERB NOUN PUNCT DET NOUN ADP DET NOUN AUX ADV VERB PART VERB DET ADJ ADJ NOUN NOUN ADP ADJ NOUN PUNCT ADV PUNCT DET ADJ NOUN AUX VERB ADP DET ADJ VERB NOUN PUNCT PRON ADV VERB NOUN PUNCT DET ADJ NOUN ADP PROPN ADP PRON DET NOUN AUX VERB ADP VERB NOUN VERB NOUN PUNCT ADP DET ADJ NOUN ADP NOUN ADP ADJ NOUN PUNCT PRON VERB SCONJ VERB DET ADJ PART ADJ NOUN ADP VERB NOUN PUNCT PROPN CCONJ PROPN ADV VERB ADJ NOUN PUNCT VERB NOUN CCONJ ADJ ADJ ADJ NOUN PUNCT,0.5519713261648745,34.875,4.835125448028674
249,163,Timo Schick,"[' Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018; Devlin et al., 2019; Liuet al., 2019; Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model’s output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is __”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Schütze, 2021). Recently, Brown et al. (2020) introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as LM problems, GPT-3 achieves near state-of-the-art results for some SuperGLUE (Wang et al., 2019) tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed.', 'While being straightforward to use, this method has two major drawbacks: It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern-exploiting training (PET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way. In this work, we adapt PET for tasks that require predicting multiple tokens. We then show that incombination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET’s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET’s “green” properties, we see ourwork as an important contribution to an environmentally sound NLP.']",intro_chunked," Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018; Devlin et al., 2019; Liuet al., 2019; Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model’s output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is __”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Schütze, 2021). Recently, Brown et al. (2020) introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as LM problems, GPT-3 achieves near state-of-the-art results for some SuperGLUE (Wang et al., 2019) tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed.",38.05365384615388,32.83053957444222,249,0.40241459012031555," Pretraining ever larger language models on massive Propname has led to large improvements in Propname. A standard approach is to replace the pretrained models output layer with a task specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions, allowing pretrained LMs to solve them without any or with only very few labeled examples. Recently, Propname Propname Propname. introduced Propname 0, a pretrained Propname with an enormous 000 billion parameters, and showed that it has amazing few shot abilities: By reformulating tasks as Propname problems, Propname0 achieves near state of the art results for some SuperGLUE tasks given just 00 labeled examples. This is achieved through priming: Propname 0 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed."," Pretraining ever larger language models on massive Propname has led to large improvements in Propname. A standard approach is to replace the pretrained models output layer with a task specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions, allowing pretrained LMs to solve them without any or with only very few labeled examples. Recently, Propname Propname Propname. introduced Propname 0, a pretrained Propname with an enormous 000 billion parameters, and showed that it has amazing few shot abilities: By reformulating tasks as Propname problems, Propname0 achieves near state of the art results for some SuperGLUE tasks given just 00 labeled examples. This is achieved through priming: Propname 0 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed.", VERB ADV ADJ NOUN NOUN ADP ADJ PROPN AUX VERB ADP ADJ NOUN ADP PROPN PUNCT DET ADJ NOUN AUX PART VERB DET VERB NOUN VERB NOUN ADP DET NOUN ADJ NOUN CCONJ VERB DET ADJ NOUN ADP DET NOUN ADP VERB NOUN NOUN PUNCT ADV PUNCT NOUN NOUN AUX PART ADV DET ADJ VERB NOUN PUNCT CCONJ ADJ NOUN AUX AUX VERB ADP VERB NOUN PUNCT VERB VERB NOUN PART VERB PRON ADP PRON CCONJ ADP ADV ADV ADJ VERB NOUN PUNCT ADV PUNCT PROPN PROPN PROPN PUNCT VERB PROPN NUM PUNCT DET VERB PROPN ADP DET ADJ NUM NUM NOUN PUNCT CCONJ VERB SCONJ PRON VERB ADJ ADJ NOUN NOUN PUNCT ADP VERB NOUN ADP PROPN NOUN PUNCT PROPN PUNCT VERB ADP NOUN ADP DET NOUN NOUN ADP DET ADJ NOUN VERB ADV NUM VERB NOUN PUNCT PRON AUX VERB ADP NOUN PUNCT PROPN NUM AUX VERB DET ADJ NOUN ADP NOUN CCONJ VERB NOUN ADP NOUN ADP PRON NOUN PUNCT CCONJ DET ADJ NOUN AUX VERB PUNCT,0.6568047337278107,28.166666666666668,4.946745562130178
250,164,Timo Schick,"[' Pretraining ever-larger language models (LMs) on massive corpora has led to large improvements in NLP (Radford et al., 2018; Devlin et al., 2019; Liuet al., 2019; Raffel et al., 2020, i.a.). A standard approach is to replace the pretrained model’s output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is __”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Schütze, 2021). Recently, Brown et al. (2020) introduced GPT-3, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as LM problems, GPT-3 achieves near state-of-the-art results for some SuperGLUE (Wang et al., 2019) tasks given just 32 labeled examples. This is achieved through priming: GPT-3 is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed.', 'While being straightforward to use, this method has two major drawbacks: It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern-exploiting training (PET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way. In this work, we adapt PET for tasks that require predicting multiple tokens. We then show that incombination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET’s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET’s “green” properties, we see ourwork as an important contribution to an environmentally sound NLP.']",intro_chunked,"While being straightforward to use, this method has two major drawbacks: It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern-exploiting training (PET) (Schick and Schütze, 2021), which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While PET additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, PET only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way. In this work, we adapt PET for tasks that require predicting multiple tokens. We then show that incombination with ALBERT (Lan et al., 2020), PET and its iterative variant (iPET) both outperform GPT-3 on SuperGLUE with 32 training examples, while requiring only 0.1% of its parameters (Figure 1). Moreover, training with PET can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to PET’s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM. Given PET’s “green” properties, we see ourwork as an important contribution to an environmentally sound NLP.",28.892763157894763,32.83053957444222,250,0.5704787373542786," While being straightforward to use, this method has two major drawbacks: It requires a gigantic Propname to work well, making it unusable in many real world scenarios and resulting in a large carbon footprint. It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern exploiting training, which combines the idea of reformulating tasks as cloze questions with regular gradient based finetuning. While Propname additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real world applications. Crucially, Propname only works when the answers to be predicted by the Propname correspond to a single token in its vocabulary; this is a severe limitation as many tasks can not easily be worded that way. In this work, we adapt Propname for tasks that require predicting multiple tokens. We then show that incombination with Propname, Propname and its iterative variant both outperform Propname 0 on SuperGLUE with 00 training examples, while requiring only 0.0 of its parameters. Moreover, training with Propname can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to Propname strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying Propname. Given PETs green properties, we see ourwork as an important contribution to an environmentally sound Propname."," While being straightforward to use, this method has two major drawbacks: It requires a gigantic Propname to work well, making it unusable in many real world scenarios and resulting in a large carbon footprint. It does not scale to more than a few examples as the context window of most LMs is limited to a few hundred tokens. An alternative to priming is pattern exploiting training, which combines the idea of reformulating tasks as cloze questions with regular gradient based finetuning. While Propname additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real world applications. Crucially, Propname only works when the answers to be predicted by the Propname correspond to a single token in its vocabulary; this is a severe limitation as many tasks can not easily be worded that way. In this work, we adapt Propname for tasks that require predicting multiple tokens. We then show that incombination with Propname, Propname and its iterative variant both outperform Propname 0 on SuperGLUE with 00 training examples, while requiring only 0.0 of its parameters. Moreover, training with Propname can be performed in several hours on a single GPU without requiring expensive hyperparameter optimization. Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to Propname strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying Propname. Given PETs green properties, we see ourwork as an important contribution to an environmentally sound Propname.", SCONJ AUX ADJ PART VERB PUNCT DET NOUN VERB NUM ADJ NOUN PUNCT PRON VERB DET ADJ PROPN PART VERB ADV PUNCT VERB PRON ADJ ADP ADJ ADJ NOUN NOUN CCONJ VERB ADP DET ADJ NOUN NOUN PUNCT PRON AUX PART VERB ADP ADJ ADP DET ADJ NOUN ADP DET NOUN NOUN ADP ADJ NOUN AUX VERB ADP DET ADJ NUM NOUN PUNCT DET NOUN ADP NOUN AUX NOUN VERB NOUN PUNCT PRON VERB DET NOUN ADP VERB NOUN ADP VERB NOUN ADP ADJ NOUN VERB NOUN PUNCT SCONJ PROPN ADV VERB ADJ NOUN PUNCT ADJ NOUN AUX ADV ADJ PART VERB ADP VERB NOUN ADP ADJ ADJ NOUN NOUN PUNCT ADV PUNCT PROPN ADV VERB SCONJ DET NOUN PART AUX VERB ADP DET PROPN VERB ADP DET ADJ NOUN ADP PRON NOUN PUNCT PRON AUX DET ADJ NOUN SCONJ ADJ NOUN AUX PART ADV AUX VERB DET NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN ADP NOUN PRON VERB VERB ADJ NOUN PUNCT PRON ADV VERB DET NOUN ADP PROPN PUNCT PROPN CCONJ PRON ADJ NOUN PRON ADJ PROPN NUM ADP NUM ADP NUM NOUN NOUN PUNCT SCONJ VERB ADV NUM ADP PRON NOUN PUNCT ADV PUNCT NOUN ADP PROPN AUX AUX VERB ADP ADJ NOUN ADP DET ADJ NOUN ADP VERB ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB SCONJ ADJ NOUN AUX ADV AUX VERB ADP ADJ NOUN CCONJ VERB DET ADJ NOUN ADP DET NOUN VERB ADP PROPN ADJ NOUN PUNCT PRON NOUN PART VERB ADJ NOUN NOUN PUNCT PRON NOUN ADP NOUN PRON AUX ADJ PART VERB PUNCT PRON NOUN ADP VERB NOUN PUNCT CCONJ NOUN ADP DET VERB PROPN PUNCT VERB ADJ ADJ NOUN PUNCT PRON VERB NOUN ADP DET ADJ NOUN ADP DET ADV ADJ PROPN PUNCT,0.559322033898305,29.5,4.959322033898305
251,165,Timo Schick,"[' Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scaling. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022). A simple way to overcome these limitations of\ntoday’s language models is to give them the ability to use external tools such as search engines,\ncalculators, or calendars. However, existing approaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-specific settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.', 'Therefore, we propose Toolformer, a model that\nlearns to use tools in a novel way, which fulfills the\nfollowing desiderata:\n• The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations. This is important not only because of the costs associated\nwith such annotations, but also because what\nhumans find useful may be different from\nwhat a model finds useful. • The LM should not lose any of its generality\nand should be able to decide for itself when\nand how to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspecific tasks. Our approach for achieving these goals is based\non the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Schütze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\nfinetune the LM itself on the API calls that it considers useful.', 'As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a variety of tools, and to choose for themselves which\ntool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the first place. This ensures that the model does not lose any\nof its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) and']",intro_chunked," Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent
capabilities (Wei et al., 2022). However, all of
these models have several inherent limitations that
can at best be partially addressed by further scaling. These limitations include an inability to access
up-to-date information on recent events (Komeili
et al., 2022) and the related tendency to hallucinate
facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin
et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an
unawareness of the progression of time (Dhingra
et al., 2022). A simple way to overcome these limitations of
today’s language models is to give them the ability to use external tools such as search engines,
calculators, or calendars. However, existing approaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-specific settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.",33.081755499153985,32.83053957444222,251,0.138811394572258," Large language models achieve impressive zero and few shot results on a variety of natural language processing tasks and show several emergent 
 capabilities. However, all of 
 these models have several inherent limitations that 
 can at best be partially addressed by further scaling. These limitations include an inability to access 
 up to date information on recent events Propname 
 Propname Propname Propname, 0000 and the related tendency to hallucinate 
 facts, difficulties in understanding low resource languages Propname 
 Propname Propname Propname, 0000, a lack of mathematical skills to perform precise calculations and an 
 unawareness of the progression of time Propname 
 et Propname Propname, 0000. A simple way to overcome these limitations of 
 todays language models is to give them the ability to use external tools such as search engines, 
 calculators, or calendars. However, existing approaches either rely on large amounts of human 
 annotations Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000 or limit tool use to task specific settings only 
, hindering a more widespread adoption of tool use in LMs."," Large language models achieve impressive zero and few shot results on a variety of natural language processing tasks and show several emergent 
 capabilities. However, all of 
 these models have several inherent limitations that 
 can at best be partially addressed by further scaling. These limitations include an inability to access 
 up to date information on recent events Propname 
 Propname Propname Propname, 0000 and the related tendency to hallucinate 
 facts, difficulties in understanding low resource languages Propname 
 Propname Propname Propname, 0000, a lack of mathematical skills to perform precise calculations and an 
 unawareness of the progression of time Propname 
 et Propname Propname, 0000. A simple way to overcome these limitations of 
 todays language models is to give them the ability to use external tools such as search engines, 
 calculators, or calendars. However, existing approaches either rely on large amounts of human 
 annotations Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000 or limit tool use to task specific settings only 
, hindering a more widespread adoption of tool use in LMs.", ADJ NOUN NOUN VERB ADJ NUM CCONJ ADJ NOUN NOUN ADP DET NOUN ADP ADJ NOUN NOUN NOUN CCONJ VERB ADJ ADJ SPACE NOUN PUNCT ADV PUNCT PRON ADP SPACE DET NOUN VERB ADJ ADJ NOUN PRON SPACE AUX ADV ADJ AUX ADV VERB ADP ADJ NOUN PUNCT DET NOUN VERB DET NOUN PART VERB SPACE ADP ADP NOUN NOUN ADP ADJ NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM CCONJ DET ADJ NOUN PART VERB SPACE NOUN PUNCT NOUN ADP VERB ADJ NOUN NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT DET NOUN ADP ADJ NOUN PART VERB ADJ NOUN CCONJ DET SPACE NOUN ADP DET NOUN ADP NOUN PROPN SPACE NOUN PROPN PROPN PUNCT NUM PUNCT DET ADJ NOUN PART VERB DET NOUN ADP SPACE NOUN NOUN NOUN AUX PART VERB PRON DET NOUN PART VERB ADJ NOUN ADJ ADP NOUN NOUN PUNCT SPACE NOUN PUNCT CCONJ NOUN PUNCT ADV PUNCT VERB NOUN CCONJ VERB ADP ADJ NOUN ADP ADJ SPACE NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM CCONJ VERB NOUN NOUN ADP NOUN ADJ NOUN ADV SPACE PUNCT VERB DET ADV ADJ NOUN ADP NOUN NOUN ADP NOUN PUNCT,0.5989304812834224,37.4,5.192513368983957
252,166,Timo Schick,"[' Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scaling. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022). A simple way to overcome these limitations of\ntoday’s language models is to give them the ability to use external tools such as search engines,\ncalculators, or calendars. However, existing approaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-specific settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.', 'Therefore, we propose Toolformer, a model that\nlearns to use tools in a novel way, which fulfills the\nfollowing desiderata:\n• The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations. This is important not only because of the costs associated\nwith such annotations, but also because what\nhumans find useful may be different from\nwhat a model finds useful. • The LM should not lose any of its generality\nand should be able to decide for itself when\nand how to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspecific tasks. Our approach for achieving these goals is based\non the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Schütze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\nfinetune the LM itself on the API calls that it considers useful.', 'As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a variety of tools, and to choose for themselves which\ntool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the first place. This ensures that the model does not lose any\nof its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) and']",intro_chunked,"Therefore, we propose Toolformer, a model that
learns to use tools in a novel way, which fulfills the
following desiderata:
• The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. This is important not only because of the costs associated
with such annotations, but also because what
humans find useful may be different from
what a model finds useful. • The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. In contrast to
existing approaches, this enables a much more
comprehensive use of tools that is not tied to
specific tasks. Our approach for achieving these goals is based
on the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate
entire datasets from scratch (Schick and Schütze,
2021b; Honovich et al., 2022; Wang et al., 2022):
Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
finetune the LM itself on the API calls that it considers useful.",45.669041095890435,32.83053957444222,252,0.5266711711883545," Therefore, we propose Propname, a model that 
 learns to use tools in a novel way, which fulfills the 
 following desiderata: The use of tools should be learned in a 
 self supervised way without requiring large 
 amounts of human annotations. This is important not only because of the costs associated 
 with such annotations, but also because what 
 humans find useful may be different from 
 what a model finds useful. The Propname should not lose any of its generality 
 and should be able to decide for itself when 
 and how to use which tool. In contrast to 
 existing approaches, this enables a much more 
 comprehensive use of tools that is not tied to 
 specific tasks. Our approach for achieving these goals is based 
 on the recent idea of using large LMs with incontext learning to generate 
 entire datasets from Propname Propname and Propname, 
 0000b; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000: 
 Given just a handful of human written examples 
 of how an API can be used, we let a Propname annotate 
 a huge language modeling dataset with potential 
 API calls. We then use a self supervised loss to 
 determine which of these API calls actually help 
 the model in predicting future tokens. Finally, we 
 finetune the Propname itself on the API calls that it considers useful."," Therefore, we propose Propname, a model that 
 learns to use tools in a novel way, which fulfills the 
 following desiderata: The use of tools should be learned in a 
 self supervised way without requiring large 
 amounts of human annotations. This is important not only because of the costs associated 
 with such annotations, but also because what 
 humans find useful may be different from 
 what a model finds useful. The Propname should not lose any of its generality 
 and should be able to decide for itself when 
 and how to use which tool. In contrast to 
 existing approaches, this enables a much more 
 comprehensive use of tools that is not tied to 
 specific tasks. Our approach for achieving these goals is based 
 on the recent idea of using large LMs with incontext learning to generate 
 entire datasets from Propname Propname and Propname, 
 0000b; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000: 
 Given just a handful of human written examples 
 of how an API can be used, we let a Propname annotate 
 a huge language modeling dataset with potential 
 API calls. We then use a self supervised loss to 
 determine which of these API calls actually help 
 the model in predicting future tokens. Finally, we 
 finetune the Propname itself on the API calls that it considers useful.", ADV PUNCT PRON VERB PROPN PUNCT DET NOUN PRON SPACE VERB PART VERB NOUN ADP DET ADJ NOUN PUNCT PRON VERB DET SPACE VERB NOUN PUNCT DET NOUN ADP NOUN AUX AUX VERB ADP DET SPACE NOUN VERB NOUN ADP VERB ADJ SPACE NOUN ADP ADJ NOUN PUNCT PRON AUX ADJ PART ADV SCONJ ADP DET NOUN VERB SPACE ADP ADJ NOUN PUNCT CCONJ ADV SCONJ PRON SPACE NOUN VERB ADJ AUX AUX ADJ ADP SPACE PRON DET NOUN VERB ADJ PUNCT DET PROPN AUX PART VERB PRON ADP PRON NOUN SPACE CCONJ AUX AUX ADJ PART VERB ADP PRON SCONJ SPACE CCONJ SCONJ PART VERB DET NOUN PUNCT ADP NOUN ADP SPACE VERB NOUN PUNCT PRON VERB DET ADV ADJ SPACE ADJ NOUN ADP NOUN PRON AUX PART VERB ADP SPACE ADJ NOUN PUNCT PRON NOUN ADP VERB DET NOUN AUX VERB SPACE ADP DET ADJ NOUN ADP VERB ADJ NOUN ADP NOUN VERB PART VERB SPACE ADJ NOUN ADP PROPN PROPN CCONJ PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE VERB ADV DET NOUN ADP ADJ VERB NOUN SPACE ADP SCONJ DET NOUN AUX AUX VERB PUNCT PRON VERB DET PROPN VERB SPACE DET ADJ NOUN NOUN ADJ ADP ADJ SPACE NOUN NOUN PUNCT PRON ADV VERB DET NOUN VERB NOUN PART SPACE VERB PRON ADP DET NOUN NOUN ADV VERB SPACE DET NOUN ADP VERB ADJ NOUN PUNCT ADV PUNCT PRON SPACE VERB DET PROPN PRON ADP DET NOUN VERB SCONJ PRON VERB ADJ PUNCT,0.5378151260504201,34.0,4.5210084033613445
253,167,Timo Schick,"[' Large language models achieve impressive zero and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scaling. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022). A simple way to overcome these limitations of\ntoday’s language models is to give them the ability to use external tools such as search engines,\ncalculators, or calendars. However, existing approaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-specific settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.', 'Therefore, we propose Toolformer, a model that\nlearns to use tools in a novel way, which fulfills the\nfollowing desiderata:\n• The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations. This is important not only because of the costs associated\nwith such annotations, but also because what\nhumans find useful may be different from\nwhat a model finds useful. • The LM should not lose any of its generality\nand should be able to decide for itself when\nand how to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspecific tasks. Our approach for achieving these goals is based\non the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Schütze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\nfinetune the LM itself on the API calls that it considers useful.', 'As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a variety of tools, and to choose for themselves which\ntool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the first place. This ensures that the model does not lose any\nof its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) and']",intro_chunked,"As illustrated in Figure 1, through
this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which
tool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset
that was used to pretrain a model in the first place. This ensures that the model does not lose any
of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after
learning to use tools, Toolformer, which is based
on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much
stronger zero-shot results, clearly outperforming a
much larger GPT-3 model (Brown et al., 2020) and",43.489375000000024,32.83053957444222,253,0.5203889608383179," As illustrated in Figure 0, through 
 this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which 
 tool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset 
 that was used to pretrain a model in the first place. This ensures that the model does not lose any 
 of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after 
 learning to use tools, Propname, which is based 
 on a pretrained Propname Propname model with 0.0B parameters, achieves much 
 stronger zero shot results, clearly outperforming a 
 much larger Propname 0 model and"," As illustrated in Figure 0, through 
 this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which 
 tool to use when and how. As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset 
 that was used to pretrain a model in the first place. This ensures that the model does not lose any 
 of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after 
 learning to use tools, Propname, which is based 
 on a pretrained Propname Propname model with 0.0B parameters, achieves much 
 stronger zero shot results, clearly outperforming a 
 much larger Propname 0 model and", SCONJ VERB ADP NOUN NUM PUNCT ADP SPACE DET ADJ NOUN PUNCT NOUN AUX VERB PART VERB DET NOUN ADP NOUN PUNCT CCONJ PART VERB ADP PRON DET SPACE NOUN PART VERB SCONJ CCONJ SCONJ PUNCT SCONJ PRON NOUN AUX ADJ ADP DET NOUN AUX VERB PUNCT PRON AUX VERB PRON ADP DET ADJ ADJ NOUN SPACE PRON AUX VERB PART VERB DET NOUN ADP DET ADJ NOUN PUNCT PRON VERB SCONJ DET NOUN AUX PART VERB PRON SPACE ADP PRON NOUN CCONJ NOUN NOUN NOUN PUNCT PRON VERB NOUN ADP DET NOUN ADP ADJ ADJ NOUN PUNCT VERB SCONJ ADP SPACE VERB PART VERB NOUN PUNCT PROPN PUNCT PRON AUX VERB SPACE ADP DET VERB PROPN PROPN NOUN ADP NOUN NOUN PUNCT VERB ADJ SPACE ADJ NUM NOUN NOUN PUNCT ADV VERB DET SPACE ADV ADJ PROPN NUM NOUN CCONJ,0.6240601503759399,33.25,4.451127819548872
254,168,Timo Schick,"[' Pretraining neural networks using a language modeling objective leads to large improvements across\na variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019). With model sizes continually increasing\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are\ntypically based on crawls from the internet that are\nonly filtered with some basic rules (Radford et al.,\n2019; Raffel et al., 2020). As a consequence, they\ncontain non-negligible amounts of text exhibiting\nbiases that are undesirable or outright harmful for\nmany potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such\ndata pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;\nBasta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned\nwords (Raffel et al., 2020) fall short of mitigating\nthis problem for at least two reasons. First, they do\nnot reliably keep language models from generating\nbiased text: Examples in Figure 1 show that biased text can easily be generated by using only words\nthat are, by themselves, completely unproblematic.', 'As many such words are important words of the\nEnglish vocabulary and thus needed for meaningful\ntext generation, they should not be included in a list\nof banned words. Secondly, banning words also\nprevents language models from gaining knowledge\nof topics related to the banned words, which may\nbe necessary for some applications.2\nIt is therefore inherently difficult to ban words without doing\nharm to a model’s capabilities. Building training datasets with more care and\ndeliberation, an alternative solution discussed by\nBender et al. (2021), is important, especially for\nimproving linguistic and cultural diversity in online\nand other forms of communication. However, for\nlarge language models that are available for common global languages, it is desirable to also have\nother mechanisms to address bias because dataset\ncuration and documentation is extremely resource\nintensive, given the amount of data required. It\ncan also necessitate building different training sets\nand, accordingly, training different models for each\ndesired behavior, which can result in high environmental impact (Strubell et al., 2019).', 'In this paper, we therefore propose an approach\nthat, instead of trusting that a model will implicitly learn desired behaviors from the training data,\nmakes explicit how we expect it to behave at test\ntime: If the model is told which biases are undesired – and it is able to discern their presence –,\nit should be able to avoid them even if they are\npresent in some of the texts it has been trained on. As it is a necessary condition for this approach, we\nfirst explore whether language models are able to\ndetect when their own outputs exhibit undesirable\nattributes, based only on their internal knowledge –\na process to which we refer as self-diagnosis. We\nthen investigate whether this ability can be used\nto perform self-debiasing, i.e., whether language\nmodels can use this knowledge to discard undesired\nbehaviors in a fully unsupervised fashion. To this\nend, we propose a decoding algorithm that reduces\nthe probability of a model producing biased text,\nrequiring nothing more than a textual description\nof the undesired behavior, which can be as simple\nas a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in\nparticular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find\nthat their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in\nNLP']",intro_chunked," Pretraining neural networks using a language modeling objective leads to large improvements across
a variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,
2019). With model sizes continually increasing
(Radford et al., 2019; Raffel et al., 2020; Brown
et al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are
typically based on crawls from the internet that are
only filtered with some basic rules (Radford et al.,
2019; Raffel et al., 2020). As a consequence, they
contain non-negligible amounts of text exhibiting
biases that are undesirable or outright harmful for
many potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such
data pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;
Basta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned
words (Raffel et al., 2020) fall short of mitigating
this problem for at least two reasons. First, they do
not reliably keep language models from generating
biased text: Examples in Figure 1 show that biased text can easily be generated by using only words
that are, by themselves, completely unproblematic.",38.623165137614706,32.83053957444222,254,0.10151370614767075," Pretraining neural networks using a language modeling objective leads to large improvements across 
 a variety of natural language processing tasks Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000. With model sizes continually increasing 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, ever larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are 
 typically based on crawls from the internet that are 
 only filtered with some basic rules Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000. As a consequence, they 
 contain non negligible amounts of text exhibiting 
 biases that are undesirable or outright harmful for 
 many potential applications. Unsurprisingly, language models trained on such 
 data pick up, reproduce or even amplify these biases Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, Propname Propname Simple solutions such as using a list of banned 
 words fall short of mitigating 
 this problem for at least two reasons. First, they do 
 not reliably keep language models from generating 
 biased text: Examples in Figure 0 show that biased text can easily be generated by using only words 
 that are, by themselves, completely unproblematic."," Pretraining neural networks using a language modeling objective leads to large improvements across 
 a variety of natural language processing tasks Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000. With model sizes continually increasing 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, ever larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are 
 typically based on crawls from the internet that are 
 only filtered with some basic rules Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000. As a consequence, they 
 contain non negligible amounts of text exhibiting 
 biases that are undesirable or outright harmful for 
 many potential applications. Unsurprisingly, language models trained on such 
 data pick up, reproduce or even amplify these biases Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, Propname Propname Simple solutions such as using a list of banned 
 words fall short of mitigating 
 this problem for at least two reasons. First, they do 
 not reliably keep language models from generating 
 biased text: Examples in Figure 0 show that biased text can easily be generated by using only words 
 that are, by themselves, completely unproblematic.", VERB ADJ NOUN VERB DET NOUN NOUN ADJ VERB ADP ADJ NOUN ADP SPACE DET NOUN ADP ADJ NOUN NOUN NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT ADP NOUN NOUN ADV VERB SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADV ADJ VERB NOUN AUX ADJ PRON PART VERB NOUN CCONJ PART VERB NOUN ADP ADV ADJ NOUN NOUN ADP ADJ PUNCT ADV PUNCT ADJ ADJ NOUN AUX SPACE ADV VERB ADP NOUN ADP DET NOUN PRON AUX SPACE ADV VERB ADP DET ADJ NOUN PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET NOUN PUNCT PRON SPACE VERB ADJ ADJ NOUN ADP NOUN VERB SPACE NOUN PRON AUX ADJ CCONJ ADJ ADJ ADP SPACE ADJ ADJ NOUN PUNCT ADV PUNCT NOUN NOUN VERB ADP ADJ SPACE NOUN VERB ADP PUNCT VERB CCONJ ADV VERB DET NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN ADJ NOUN ADJ ADP VERB DET NOUN ADP VERB SPACE NOUN VERB ADJ ADP VERB SPACE DET NOUN ADP ADV ADV NUM NOUN PUNCT ADV PUNCT PRON AUX SPACE PART ADV VERB NOUN NOUN ADP VERB SPACE ADJ NOUN PUNCT NOUN ADP NOUN NUM VERB SCONJ ADJ NOUN AUX ADV AUX VERB ADP VERB ADJ NOUN SPACE PRON AUX PUNCT ADP PRON PUNCT ADV ADJ PUNCT,0.4714828897338403,43.833333333333336,5.269961977186312
255,169,Timo Schick,"[' Pretraining neural networks using a language modeling objective leads to large improvements across\na variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019). With model sizes continually increasing\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are\ntypically based on crawls from the internet that are\nonly filtered with some basic rules (Radford et al.,\n2019; Raffel et al., 2020). As a consequence, they\ncontain non-negligible amounts of text exhibiting\nbiases that are undesirable or outright harmful for\nmany potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such\ndata pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;\nBasta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned\nwords (Raffel et al., 2020) fall short of mitigating\nthis problem for at least two reasons. First, they do\nnot reliably keep language models from generating\nbiased text: Examples in Figure 1 show that biased text can easily be generated by using only words\nthat are, by themselves, completely unproblematic.', 'As many such words are important words of the\nEnglish vocabulary and thus needed for meaningful\ntext generation, they should not be included in a list\nof banned words. Secondly, banning words also\nprevents language models from gaining knowledge\nof topics related to the banned words, which may\nbe necessary for some applications.2\nIt is therefore inherently difficult to ban words without doing\nharm to a model’s capabilities. Building training datasets with more care and\ndeliberation, an alternative solution discussed by\nBender et al. (2021), is important, especially for\nimproving linguistic and cultural diversity in online\nand other forms of communication. However, for\nlarge language models that are available for common global languages, it is desirable to also have\nother mechanisms to address bias because dataset\ncuration and documentation is extremely resource\nintensive, given the amount of data required. It\ncan also necessitate building different training sets\nand, accordingly, training different models for each\ndesired behavior, which can result in high environmental impact (Strubell et al., 2019).', 'In this paper, we therefore propose an approach\nthat, instead of trusting that a model will implicitly learn desired behaviors from the training data,\nmakes explicit how we expect it to behave at test\ntime: If the model is told which biases are undesired – and it is able to discern their presence –,\nit should be able to avoid them even if they are\npresent in some of the texts it has been trained on. As it is a necessary condition for this approach, we\nfirst explore whether language models are able to\ndetect when their own outputs exhibit undesirable\nattributes, based only on their internal knowledge –\na process to which we refer as self-diagnosis. We\nthen investigate whether this ability can be used\nto perform self-debiasing, i.e., whether language\nmodels can use this knowledge to discard undesired\nbehaviors in a fully unsupervised fashion. To this\nend, we propose a decoding algorithm that reduces\nthe probability of a model producing biased text,\nrequiring nothing more than a textual description\nof the undesired behavior, which can be as simple\nas a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in\nparticular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find\nthat their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in\nNLP']",intro_chunked,"As many such words are important words of the
English vocabulary and thus needed for meaningful
text generation, they should not be included in a list
of banned words. Secondly, banning words also
prevents language models from gaining knowledge
of topics related to the banned words, which may
be necessary for some applications.2
It is therefore inherently difficult to ban words without doing
harm to a model’s capabilities. Building training datasets with more care and
deliberation, an alternative solution discussed by
Bender et al. (2021), is important, especially for
improving linguistic and cultural diversity in online
and other forms of communication. However, for
large language models that are available for common global languages, it is desirable to also have
other mechanisms to address bias because dataset
curation and documentation is extremely resource
intensive, given the amount of data required. It
can also necessitate building different training sets
and, accordingly, training different models for each
desired behavior, which can result in high environmental impact (Strubell et al., 2019).",20.322549019607862,32.83053957444222,255,0.23452791571617126," As many such words are important words of the 
 English vocabulary and thus needed for meaningful 
 text generation, they should not be included in a list 
 of banned words. Secondly, banning words also 
 prevents language models from gaining knowledge 
 of topics related to the banned words, which may 
 be necessary for some applications.0 
 It is therefore inherently difficult to ban words without doing 
 harm to a models capabilities. Building training datasets with more care and 
 deliberation, an alternative solution discussed by 
 Propname Propname Propname., is important, especially for 
 improving linguistic and cultural diversity in online 
 and other forms of communication. However, for 
 large language models that are available for common global languages, it is desirable to also have 
 other mechanisms to address bias because dataset 
 curation and documentation is extremely resource 
 intensive, given the amount of data required. It 
 can also necessitate building different training sets 
 and, accordingly, training different models for each 
 desired behavior, which can result in high environmental impact."," As many such words are important words of the 
 English vocabulary and thus needed for meaningful 
 text generation, they should not be included in a list 
 of banned words. Secondly, banning words also 
 prevents language models from gaining knowledge 
 of topics related to the banned words, which may 
 be necessary for some applications.0 
 It is therefore inherently difficult to ban words without doing 
 harm to a models capabilities. Building training datasets with more care and 
 deliberation, an alternative solution discussed by 
 Propname Propname Propname., is important, especially for 
 improving linguistic and cultural diversity in online 
 and other forms of communication. However, for 
 large language models that are available for common global languages, it is desirable to also have 
 other mechanisms to address bias because dataset 
 curation and documentation is extremely resource 
 intensive, given the amount of data required. It 
 can also necessitate building different training sets 
 and, accordingly, training different models for each 
 desired behavior, which can result in high environmental impact.", SCONJ ADJ ADJ NOUN AUX ADJ NOUN ADP DET SPACE ADJ NOUN CCONJ ADV VERB ADP ADJ SPACE NOUN NOUN PUNCT PRON AUX PART AUX VERB ADP DET NOUN SPACE ADP VERB NOUN PUNCT ADV PUNCT VERB NOUN ADV SPACE VERB NOUN NOUN ADP VERB NOUN SPACE ADP NOUN VERB ADP DET VERB NOUN PUNCT PRON AUX SPACE AUX ADJ SCONJ DET NOUN SPACE PRON AUX ADV ADV ADJ PART VERB NOUN ADP VERB SPACE NOUN ADP DET NOUN NOUN PUNCT VERB NOUN NOUN ADP ADJ NOUN CCONJ SPACE NOUN PUNCT DET ADJ NOUN VERB ADP SPACE PROPN PROPN PROPN PUNCT PUNCT AUX ADJ PUNCT ADV ADP SPACE VERB ADJ CCONJ ADJ NOUN ADP NOUN SPACE CCONJ ADJ NOUN ADP NOUN PUNCT ADV PUNCT ADP SPACE ADJ NOUN NOUN PRON AUX ADJ ADP ADJ ADJ NOUN PUNCT PRON AUX ADJ PART ADV VERB SPACE ADJ NOUN PART VERB NOUN SCONJ ADJ SPACE NOUN CCONJ NOUN AUX ADV NOUN SPACE ADJ PUNCT VERB DET NOUN ADP NOUN VERB PUNCT PRON SPACE AUX ADV VERB VERB ADJ NOUN NOUN SPACE CCONJ PUNCT ADV PUNCT VERB ADJ NOUN ADP DET SPACE VERB NOUN PUNCT PRON AUX VERB ADP ADJ ADJ NOUN PUNCT,0.6424581005586593,35.8,5.240223463687151
256,170,Timo Schick,"[' Pretraining neural networks using a language modeling objective leads to large improvements across\na variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019). With model sizes continually increasing\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), ever-larger pretraining datasets are necessary both to prevent overfitting and to provide access to as much world knowledge as possible. However, such large datasets are\ntypically based on crawls from the internet that are\nonly filtered with some basic rules (Radford et al.,\n2019; Raffel et al., 2020). As a consequence, they\ncontain non-negligible amounts of text exhibiting\nbiases that are undesirable or outright harmful for\nmany potential applications (Gehman et al., 2020). Unsurprisingly, language models trained on such\ndata pick up, reproduce or even amplify these biases (Bolukbasi et al., 2016; Sheng et al., 2019;\nBasta et al., 2019; Gehman et al., 2020, i.a.). Simple solutions such as using a list of banned\nwords (Raffel et al., 2020) fall short of mitigating\nthis problem for at least two reasons. First, they do\nnot reliably keep language models from generating\nbiased text: Examples in Figure 1 show that biased text can easily be generated by using only words\nthat are, by themselves, completely unproblematic.', 'As many such words are important words of the\nEnglish vocabulary and thus needed for meaningful\ntext generation, they should not be included in a list\nof banned words. Secondly, banning words also\nprevents language models from gaining knowledge\nof topics related to the banned words, which may\nbe necessary for some applications.2\nIt is therefore inherently difficult to ban words without doing\nharm to a model’s capabilities. Building training datasets with more care and\ndeliberation, an alternative solution discussed by\nBender et al. (2021), is important, especially for\nimproving linguistic and cultural diversity in online\nand other forms of communication. However, for\nlarge language models that are available for common global languages, it is desirable to also have\nother mechanisms to address bias because dataset\ncuration and documentation is extremely resource\nintensive, given the amount of data required. It\ncan also necessitate building different training sets\nand, accordingly, training different models for each\ndesired behavior, which can result in high environmental impact (Strubell et al., 2019).', 'In this paper, we therefore propose an approach\nthat, instead of trusting that a model will implicitly learn desired behaviors from the training data,\nmakes explicit how we expect it to behave at test\ntime: If the model is told which biases are undesired – and it is able to discern their presence –,\nit should be able to avoid them even if they are\npresent in some of the texts it has been trained on. As it is a necessary condition for this approach, we\nfirst explore whether language models are able to\ndetect when their own outputs exhibit undesirable\nattributes, based only on their internal knowledge –\na process to which we refer as self-diagnosis. We\nthen investigate whether this ability can be used\nto perform self-debiasing, i.e., whether language\nmodels can use this knowledge to discard undesired\nbehaviors in a fully unsupervised fashion. To this\nend, we propose a decoding algorithm that reduces\nthe probability of a model producing biased text,\nrequiring nothing more than a textual description\nof the undesired behavior, which can be as simple\nas a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in\nparticular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find\nthat their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in\nNLP']",intro_chunked,"In this paper, we therefore propose an approach
that, instead of trusting that a model will implicitly learn desired behaviors from the training data,
makes explicit how we expect it to behave at test
time: If the model is told which biases are undesired – and it is able to discern their presence –,
it should be able to avoid them even if they are
present in some of the texts it has been trained on. As it is a necessary condition for this approach, we
first explore whether language models are able to
detect when their own outputs exhibit undesirable
attributes, based only on their internal knowledge –
a process to which we refer as self-diagnosis. We
then investigate whether this ability can be used
to perform self-debiasing, i.e., whether language
models can use this knowledge to discard undesired
behaviors in a fully unsupervised fashion. To this
end, we propose a decoding algorithm that reduces
the probability of a model producing biased text,
requiring nothing more than a textual description
of the undesired behavior, which can be as simple
as a single keyword (e.g., “sexist”, “racist”, “homophobic” or “violent” in Figure 1; see §4 for details).While our results demonstrate that large models in
particular are, to some extent, capable of performing self-diagnosis and self-debiasing, we also find
that their current capabilities are by no means sufficient to eliminate the issue of corpus-based bias in
NLP",4.042096774193567,32.83053957444222,256,0.09841644018888474," In this paper, we therefore propose an approach 
 that, instead of trusting that a model will implicitly learn desired behaviors from the training data, 
 makes explicit how we expect it to behave at test 
 time: If the model is told which biases are undesired and it is able to discern their presence, 
 it should be able to avoid them even if they are 
 present in some of the texts it has been trained on. As it is a necessary condition for this approach, we 
 first explore whether language models are able to 
 detect when their own outputs exhibit undesirable 
 attributes, based only on their internal knowledge a process to which we refer as self diagnosis. We 
 then investigate whether this ability can be used 
 to perform self debiasing, ie, whether language 
 models can use this knowledge to discard undesired 
 behaviors in a fully unsupervised fashion. To this 
 end, we propose a decoding Propname that reduces 
 the probability of a model producing biased text, 
 requiring nothing more than a textual description 
 of the undesired behavior, which can be as simple 
 as a single keyword.While our results demonstrate that large models in 
 particular are, to some extent, capable of performing self diagnosis and self debiasing, we also find 
 that their current capabilities are by no means sufficient to eliminate the issue of Propname based bias in 
 Propname"," In this paper, we therefore propose an approach 
 that, instead of trusting that a model will implicitly learn desired behaviors from the training data, 
 makes explicit how we expect it to behave at test 
 time: If the model is told which biases are undesired and it is able to discern their presence, 
 it should be able to avoid them even if they are 
 present in some of the texts it has been trained on. As it is a necessary condition for this approach, we 
 first explore whether language models are able to 
 detect when their own outputs exhibit undesirable 
 attributes, based only on their internal knowledge a process to which we refer as self diagnosis. We 
 then investigate whether this ability can be used 
 to perform self debiasing, ie, whether language 
 models can use this knowledge to discard undesired 
 behaviors in a fully unsupervised fashion. To this 
 end, we propose a decoding Propname that reduces 
 the probability of a model producing biased text, 
 requiring nothing more than a textual description 
 of the undesired behavior, which can be as simple 
 as a single keyword.While our results demonstrate that large models in 
 particular are, to some extent, capable of performing self diagnosis and self debiasing, we also find 
 that their current capabilities are by no means sufficient to eliminate the issue of Propname based bias in 
 Propname", ADP DET NOUN PUNCT PRON ADV VERB DET NOUN SPACE SCONJ PUNCT ADV ADP VERB SCONJ DET NOUN AUX ADV VERB VERB NOUN ADP DET NOUN NOUN PUNCT SPACE VERB ADJ SCONJ PRON VERB PRON PART VERB ADP NOUN SPACE NOUN PUNCT SCONJ DET NOUN AUX VERB PRON NOUN AUX ADJ CCONJ PRON AUX ADJ PART VERB PRON NOUN PUNCT SPACE PRON AUX AUX ADJ PART VERB PRON ADV SCONJ PRON AUX SPACE ADJ ADP PRON ADP DET NOUN PRON AUX AUX VERB ADP PUNCT SCONJ PRON AUX DET ADJ NOUN ADP DET NOUN PUNCT PRON SPACE ADV VERB SCONJ NOUN NOUN AUX ADJ PART SPACE VERB SCONJ PRON ADJ NOUN VERB ADJ SPACE NOUN PUNCT VERB ADV ADP PRON ADJ NOUN DET NOUN PART PRON PRON VERB ADP NOUN NOUN PUNCT PRON SPACE ADV VERB SCONJ DET NOUN AUX AUX VERB SPACE PART VERB NOUN NOUN PUNCT ADV PUNCT SCONJ NOUN SPACE NOUN AUX VERB DET NOUN PART VERB ADJ SPACE NOUN ADP DET ADV ADJ NOUN PUNCT ADP DET SPACE NOUN PUNCT PRON VERB DET VERB PROPN PRON VERB SPACE DET NOUN ADP DET NOUN VERB ADJ NOUN PUNCT SPACE VERB PRON ADJ ADP DET ADJ NOUN SPACE ADP DET ADJ NOUN PUNCT PRON AUX AUX ADV ADJ SPACE ADP DET ADJ NOUN PUNCT PRON NOUN VERB SCONJ ADJ NOUN ADP SPACE ADJ AUX PUNCT ADP DET NOUN PUNCT ADJ ADP VERB NOUN NOUN CCONJ NOUN NOUN PUNCT PRON ADV VERB SPACE SCONJ PRON ADJ NOUN AUX ADP DET NOUN ADJ PART VERB DET NOUN ADP PROPN VERB NOUN ADP SPACE PROPN,0.5761316872427984,60.75,4.596707818930041
257,171,Timo Schick,"[' Pretraining language models on large corpora has led to improvements on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia), but learning to solve tasks from only a few examples remains a challenging problem. As small datasets are common for many realworld applications of NLP, solving this challenge is crucial to enable broad applicability. A promising direction for many tasks is to reformulate them (e.g., by appending an instruction such as “translate into French”) so that they can directly be solved by a pretrained language model (Radford et al., 2019; Schick and Schutze, 2020a; Brown et al., 2020). The key idea of P ¨ ET (Schick and Schutze, 2020a), one ¨ such approach aimed at text classification, is to rephrase each input as a cloze question for which the language model’s prediction can somehow be mapped to a label; an example is illustrated in Figure 1. While PET achieves remarkable results with little or no labeled training data, manually defining the required mapping between a language model’s predictions and labels is difficult as it requires both taskspecific knowledge and an understanding of the language model’s inner workings to identify words that it understands sufficiently well. In this work, we show how this mapping can be obtained automatically, removing the need for expert knowledge: We introduce PET with Automatic Labels (PETAL), a simple approach for identifying words that can serve as proxies for labels given small amounts of training data. At its core, our approach breaks the intractable problem of finding the mapping that maximizes the likelihood of the training data into several manageable subproblems. Integrating our approach into PET significantly outperforms regular supervised training and almost matches the performance of PET with a manually defined mapping']",intro_chunked," Pretraining language models on large corpora has led to improvements on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019, inter alia), but learning to solve tasks from only a few examples remains a challenging problem. As small datasets are common for many realworld applications of NLP, solving this challenge is crucial to enable broad applicability. A promising direction for many tasks is to reformulate them (e.g., by appending an instruction such as “translate into French”) so that they can directly be solved by a pretrained language model (Radford et al., 2019; Schick and Schutze, 2020a; Brown et al., 2020). The key idea of P ¨ ET (Schick and Schutze, 2020a), one ¨ such approach aimed at text classification, is to rephrase each input as a cloze question for which the language model’s prediction can somehow be mapped to a label; an example is illustrated in Figure 1. While PET achieves remarkable results with little or no labeled training data, manually defining the required mapping between a language model’s predictions and labels is difficult as it requires both taskspecific knowledge and an understanding of the language model’s inner workings to identify words that it understands sufficiently well. In this work, we show how this mapping can be obtained automatically, removing the need for expert knowledge: We introduce PET with Automatic Labels (PETAL), a simple approach for identifying words that can serve as proxies for labels given small amounts of training data. At its core, our approach breaks the intractable problem of finding the mapping that maximizes the likelihood of the training data into several manageable subproblems. Integrating our approach into PET significantly outperforms regular supervised training and almost matches the performance of PET with a manually defined mapping",27.287720588235317,32.83053957444222,257,0.49759116768836975," Pretraining language models on large Propname has led to improvements on a wide range of Propname tasks, but learning to solve tasks from only a few examples remains a challenging problem. As small datasets are common for many realworld applications of Propname, solving this challenge is crucial to enable broad applicability. A promising direction for many tasks is to reformulate them so that they can directly be solved by a pretrained language model. The key idea of P Propname, one such approach aimed at text classification, is to rephrase each input as a cloze question for which the language models prediction can somehow be mapped to a label; an example is illustrated in Figure 0. While Propname achieves remarkable results with little or no labeled training data, manually defining the required mapping between a language models predictions and labels is difficult as it requires both taskspecific knowledge and an understanding of the language models inner workings to identify words that it understands sufficiently well. In this work, we show how this mapping can be obtained automatically, removing the need for expert knowledge: We introduce Propname with Propname Propname, a simple approach for identifying words that can serve as proxies for labels given small amounts of training data. At its core, our approach breaks the intractable problem of finding the mapping that maximizes the likelihood of the training data into several manageable subproblems. Integrating our approach into Propname significantly outperforms regular supervised training and almost matches the performance of Propname with a manually defined mapping"," Pretraining language models on large Propname has led to improvements on a wide range of Propname tasks, but learning to solve tasks from only a few examples remains a challenging problem. As small datasets are common for many realworld applications of Propname, solving this challenge is crucial to enable broad applicability. A promising direction for many tasks is to reformulate them so that they can directly be solved by a pretrained language model. The key idea of P Propname, one such approach aimed at text classification, is to rephrase each input as a cloze question for which the language models prediction can somehow be mapped to a label; an example is illustrated in Figure 0. While Propname achieves remarkable results with little or no labeled training data, manually defining the required mapping between a language models predictions and labels is difficult as it requires both taskspecific knowledge and an understanding of the language models inner workings to identify words that it understands sufficiently well. In this work, we show how this mapping can be obtained automatically, removing the need for expert knowledge: We introduce Propname with Propname Propname, a simple approach for identifying words that can serve as proxies for labels given small amounts of training data. At its core, our approach breaks the intractable problem of finding the mapping that maximizes the likelihood of the training data into several manageable subproblems. Integrating our approach into Propname significantly outperforms regular supervised training and almost matches the performance of Propname with a manually defined mapping", VERB NOUN NOUN ADP ADJ PROPN AUX VERB ADP NOUN ADP DET ADJ NOUN ADP PROPN NOUN PUNCT CCONJ VERB PART VERB NOUN ADP ADV DET ADJ NOUN VERB DET ADJ NOUN PUNCT SCONJ ADJ NOUN AUX ADJ ADP ADJ NOUN NOUN ADP PROPN PUNCT VERB DET NOUN AUX ADJ PART VERB ADJ NOUN PUNCT DET ADJ NOUN ADP ADJ NOUN AUX PART VERB PRON SCONJ SCONJ PRON AUX ADV AUX VERB ADP DET VERB NOUN NOUN PUNCT DET ADJ NOUN ADP NOUN PROPN PUNCT NUM ADJ NOUN VERB ADP NOUN NOUN PUNCT AUX PART VERB DET NOUN ADP DET VERB NOUN ADP PRON DET NOUN NOUN NOUN AUX ADV AUX VERB ADP DET NOUN PUNCT DET NOUN AUX VERB ADP NOUN NUM PUNCT SCONJ PROPN VERB ADJ NOUN ADP ADJ CCONJ PRON VERB NOUN NOUN PUNCT ADV VERB DET VERB NOUN ADP DET NOUN NOUN NOUN CCONJ NOUN AUX ADJ SCONJ PRON VERB DET ADJ NOUN CCONJ DET NOUN ADP DET NOUN NOUN ADJ NOUN PART VERB NOUN PRON PRON VERB ADV ADV PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ DET NOUN AUX AUX VERB ADV PUNCT VERB DET NOUN ADP ADJ NOUN PUNCT PRON VERB PROPN ADP PROPN PROPN PUNCT DET ADJ NOUN ADP VERB NOUN PRON AUX VERB ADP NOUN ADP NOUN VERB ADJ NOUN ADP NOUN NOUN PUNCT ADP PRON NOUN PUNCT PRON NOUN VERB DET ADJ NOUN ADP VERB DET NOUN PRON VERB DET NOUN ADP DET NOUN NOUN ADP ADJ ADJ NOUN PUNCT VERB PRON NOUN ADP PROPN ADV VERB ADJ ADJ NOUN CCONJ ADV VERB DET NOUN ADP PROPN ADP DET ADV VERB NOUN,0.5992647058823529,34.0,5.1213235294117645
258,172,Timo Schick,"[' While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise un-supervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. To alleviate both problems, we explore a novel approach to obtaining high-quality sentence embeddings: We mimic the creation of NLI datasets by human crowdworkers (Bowman et al., 2015; Williams et al., 2018), but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models.', 'Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like GPT-3 (Brown et al., 2020) without requiring any updates to their parameters. As illustrated in Figure 1, our approach is based on recent methods for providing instructions to PLMs (e.g., Radford et al., 2019; Brown et al., 2020; Schick and Schütze, 2020, 2021a). We use the self-debiasing approach of Schick et al. (2021) to ensure that each generated text pair is not only a good fit for a given similarity label, but also not a good fit for other labels. We refer to our method as Datasets from Instructions (DINO). In summary, our contributions are as follows: We introduce DINO, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions. We release STS- (read as “STS-Dino”), the first textual similarity dataset generated completely automatically, without any human annotation effort. We show that Sentence-RoBERTa (Reimers and Gurevych, 2019) trained on STS- out-performs strong baselines on several semantic textual similarity datasets.']",intro_chunked," While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise un-supervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. To alleviate both problems, we explore a novel approach to obtaining high-quality sentence embeddings: We mimic the creation of NLI datasets by human crowdworkers (Bowman et al., 2015; Williams et al., 2018), but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models.",34.676875000000024,32.83053957444222,258,0.2563478946685791," While pretrained language models achieve strong results for many Propname tasks, they do not produce good sentence embeddings out of the box. Recent approaches address this by augmenting or replacing the language modeling objective with likewise Propname supervised sentence level objectives, but they typically lag behind their supervised counterparts trained on human annotated sentence pairs. Unfortunately, obtaining large amounts of high quality training data can be both difficult and prohibitively expensive. Furthermore, with larger and larger model sizes, it becomes increasingly challenging to finetune Propname. To alleviate both problems, we explore a novel approach to obtaining high quality sentence embeddings: We mimic the creation of Propname datasets by human crowdworkers, but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models."," While pretrained language models achieve strong results for many Propname tasks, they do not produce good sentence embeddings out of the box. Recent approaches address this by augmenting or replacing the language modeling objective with likewise Propname supervised sentence level objectives, but they typically lag behind their supervised counterparts trained on human annotated sentence pairs. Unfortunately, obtaining large amounts of high quality training data can be both difficult and prohibitively expensive. Furthermore, with larger and larger model sizes, it becomes increasingly challenging to finetune Propname. To alleviate both problems, we explore a novel approach to obtaining high quality sentence embeddings: We mimic the creation of Propname datasets by human crowdworkers, but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models.", SCONJ VERB NOUN NOUN VERB ADJ NOUN ADP ADJ PROPN NOUN PUNCT PRON AUX PART VERB ADJ NOUN NOUN ADP ADP DET NOUN PUNCT ADJ NOUN VERB PRON ADP VERB CCONJ VERB DET NOUN NOUN ADJ ADP ADV PROPN VERB NOUN NOUN NOUN PUNCT CCONJ PRON ADV VERB ADP PRON ADJ NOUN VERB ADP ADJ ADJ NOUN NOUN PUNCT ADV PUNCT VERB ADJ NOUN ADP ADJ NOUN NOUN NOUN AUX AUX PRON ADJ CCONJ ADV ADJ PUNCT ADV PUNCT ADP ADJ CCONJ ADJ NOUN NOUN PUNCT PRON VERB ADV ADJ PART NOUN PROPN PUNCT PART VERB DET NOUN PUNCT PRON VERB DET ADJ NOUN ADP VERB ADJ NOUN NOUN NOUN PUNCT PRON VERB DET NOUN ADP PROPN NOUN ADP ADJ NOUN PUNCT CCONJ VERB ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB PRON PART ADV VERB ADJ NOUN ADP NOUN PRON AUX AUX VERB ADP ADJ NOUN ADP ADV ADJ NOUN PUNCT,0.6842105263157895,25.333333333333332,5.480263157894737
259,173,Timo Schick,"[' While pretrained language models (PLMs) achieve strong results for many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), they do not produce good sentence embeddings out of the box (Reimers and Gurevych, 2019). Recent approaches address this by augmenting or replacing the language modeling objective with likewise un-supervised sentence-level objectives (e.g., Zhang et al., 2020; Li et al., 2020), but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive (Bowman et al., 2015; Agirre et al., 2016). Furthermore, with larger and larger model sizes (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021), it becomes increasingly challenging to finetune PLMs. To alleviate both problems, we explore a novel approach to obtaining high-quality sentence embeddings: We mimic the creation of NLI datasets by human crowdworkers (Bowman et al., 2015; Williams et al., 2018), but replace human annotators with large PLMs. This allows us to automatically create entire datasets from scratch that can be used for supervised training of much smaller models.', 'Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like GPT-3 (Brown et al., 2020) without requiring any updates to their parameters. As illustrated in Figure 1, our approach is based on recent methods for providing instructions to PLMs (e.g., Radford et al., 2019; Brown et al., 2020; Schick and Schütze, 2020, 2021a). We use the self-debiasing approach of Schick et al. (2021) to ensure that each generated text pair is not only a good fit for a given similarity label, but also not a good fit for other labels. We refer to our method as Datasets from Instructions (DINO). In summary, our contributions are as follows: We introduce DINO, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions. We release STS- (read as “STS-Dino”), the first textual similarity dataset generated completely automatically, without any human annotation effort. We show that Sentence-RoBERTa (Reimers and Gurevych, 2019) trained on STS- out-performs strong baselines on several semantic textual similarity datasets.']",intro_chunked,"Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like GPT-3 (Brown et al., 2020) without requiring any updates to their parameters. As illustrated in Figure 1, our approach is based on recent methods for providing instructions to PLMs (e.g., Radford et al., 2019; Brown et al., 2020; Schick and Schütze, 2020, 2021a). We use the self-debiasing approach of Schick et al. (2021) to ensure that each generated text pair is not only a good fit for a given similarity label, but also not a good fit for other labels. We refer to our method as Datasets from Instructions (DINO). In summary, our contributions are as follows: We introduce DINO, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions. We release STS- (read as “STS-Dino”), the first textual similarity dataset generated completely automatically, without any human annotation effort. We show that Sentence-RoBERTa (Reimers and Gurevych, 2019) trained on STS- out-performs strong baselines on several semantic textual similarity datasets.",39.70750000000001,32.83053957444222,259,0.2551267147064209," Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like Propname 0 without requiring any updates to their parameters. As illustrated in Figure 0, our approach is based on recent methods for providing instructions to PLMs. We use the self debiasing approach of Propname Propname Propname. to ensure that each generated text pair is not only a good fit for a given similarity label, but also not a good fit for other labels. We refer to our method as Datasets from Instructions. In summary, our contributions are as follows: We introduce Propname, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions. We release Propname, the first textual similarity dataset generated completely automatically, without any human annotation effort. We show that Propname Propname trained on Propname out performs strong baselines on several semantic textual similarity datasets."," Not only does this solve the problem of limited training data, it also provides a viable path to leverage big models like Propname 0 without requiring any updates to their parameters. As illustrated in Figure 0, our approach is based on recent methods for providing instructions to PLMs. We use the self debiasing approach of Propname Propname Propname. to ensure that each generated text pair is not only a good fit for a given similarity label, but also not a good fit for other labels. We refer to our method as Datasets from Instructions. In summary, our contributions are as follows: We introduce Propname, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions. We release Propname, the first textual similarity dataset generated completely automatically, without any human annotation effort. We show that Propname Propname trained on Propname out performs strong baselines on several semantic textual similarity datasets.", PART ADV AUX PRON VERB DET NOUN ADP ADJ NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN PART VERB ADJ NOUN ADP PROPN NUM ADP VERB DET NOUN ADP PRON NOUN PUNCT SCONJ VERB ADP NOUN NUM PUNCT PRON NOUN AUX VERB ADP ADJ NOUN ADP VERB NOUN ADP NOUN PUNCT PRON VERB DET NOUN VERB NOUN ADP PROPN PROPN PROPN PUNCT PART VERB SCONJ DET VERB NOUN NOUN AUX PART ADV DET ADJ NOUN ADP DET VERB NOUN NOUN PUNCT CCONJ ADV PART DET ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB ADP PRON NOUN ADP NOUN ADP NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB PROPN PUNCT DET NOUN ADP ADV VERB VERB NOUN ADP ADJ NOUN ADP VERB NOUN ADP NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ ADJ NOUN NOUN VERB ADV ADV PUNCT ADP DET ADJ NOUN NOUN PUNCT PRON VERB SCONJ PROPN PROPN VERB ADP PROPN ADV VERB ADJ NOUN ADP ADJ ADJ ADJ NOUN NOUN PUNCT,0.6094674556213018,21.125,4.911242603550296
260,174,Timo Schick,"[' Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.', 'The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.', 'However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.']",intro_chunked," Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.",49.98237637362638,32.83053957444222,260,0.2773231863975525," Distributed representations of words are a key component of natural language processing systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective have led to large performance gains for a variety of Propname tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine tune entire language models for specific tasks. Taking up this idea, Propname Propname Propname. introduced Propname, a bidirectional language model based on the Propname that has achieved a new state of the art for several Propname tasks. As demonstrated by Propname Propname Propname., it is possible for language models to solve a diverse set of tasks to some extent without any form of task specific fine tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can ask it for properties of that word using natural language. For example, a language model that understands the concept of guilt should be able to correctly complete the sentence Propname is the opposite of. with the word innocence."," Distributed representations of words are a key component of natural language processing systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective have led to large performance gains for a variety of Propname tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine tune entire language models for specific tasks. Taking up this idea, Propname Propname Propname. introduced Propname, a bidirectional language model based on the Propname that has achieved a new state of the art for several Propname tasks. As demonstrated by Propname Propname Propname., it is possible for language models to solve a diverse set of tasks to some extent without any form of task specific fine tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can ask it for properties of that word using natural language. For example, a language model that understands the concept of guilt should be able to correctly complete the sentence Propname is the opposite of. with the word innocence.", VERB NOUN ADP NOUN AUX DET ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT ADP ADJ PUNCT ADJ ADJ NOUN VERB VERB DET ADJ NOUN NOUN NOUN AUX VERB ADP ADJ NOUN NOUN ADP DET NOUN ADP PROPN NOUN PUNCT ADV PUNCT ADJ NOUN AUX VERB PART PART ADV VERB NOUN NOUN ADP NOUN NOUN PUNCT CCONJ ADP ADJ NOUN ADJ NOUN NOUN ADP ADJ NOUN PUNCT VERB ADP DET NOUN PUNCT PROPN PROPN PROPN PUNCT VERB PROPN PUNCT DET ADJ NOUN NOUN VERB ADP DET PROPN PRON AUX VERB DET ADJ NOUN ADP DET NOUN ADP ADJ PROPN NOUN PUNCT SCONJ VERB ADP PROPN PROPN PROPN PUNCT PUNCT PRON AUX ADJ SCONJ NOUN NOUN PART VERB DET ADJ NOUN ADP NOUN ADP DET NOUN ADP DET NOUN ADP NOUN ADJ ADJ NOUN PUNCT PRON AUX AUX VERB ADP ADV VERB DET NOUN ADP NOUN ADP ADJ NOUN NOUN PRON AUX PART AUX VERB ADP DET NOUN PUNCT DET ADV ADJ NOUN AUX ADV AUX VERB PART VERB SCONJ ADV DET NOUN NOUN VERB DET VERB NOUN PUNCT PRON AUX VERB PRON ADP NOUN ADP DET NOUN VERB ADJ NOUN PUNCT ADP NOUN PUNCT DET NOUN NOUN PRON VERB DET NOUN ADP NOUN AUX AUX ADJ PART ADV VERB DET NOUN PROPN AUX DET NOUN ADP PUNCT ADP DET NOUN NOUN PUNCT,0.5294117647058824,22.1,4.8054298642533935
261,175,Timo Schick,"[' Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.', 'The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.', 'However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.']",intro_chunked,"The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.",39.65968325791857,32.83053957444222,261,0.1226462572813034," The examples in Propname 0 show that, according to this measure, Propname is indeed able to understand frequent words such as lime and bicycle: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both kumquat and Propname, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome. To answer this question, we create a novel dataset containing queries like the ones shown in Propname0. This dataset consists of natural language patterns such as where Propname is a placeholder for a word to be investigated, and corresponding pairs of keywords and targets obtained using semantic relations extracted from Propname. Using this dataset, we show that Propname indeed fails to understand many rare words. To overcome this limitation, we propose to apply Propname Propname, a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings."," The examples in Propname 0 show that, according to this measure, Propname is indeed able to understand frequent words such as lime and bicycle: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both kumquat and Propname, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome. To answer this question, we create a novel dataset containing queries like the ones shown in Propname0. This dataset consists of natural language patterns such as where Propname is a placeholder for a word to be investigated, and corresponding pairs of keywords and targets obtained using semantic relations extracted from Propname. Using this dataset, we show that Propname indeed fails to understand many rare words. To overcome this limitation, we propose to apply Propname Propname, a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.", DET NOUN ADP PROPN NUM VERB SCONJ PUNCT VERB ADP DET NOUN PUNCT PROPN AUX ADV ADJ PART VERB ADJ NOUN ADJ ADP NOUN CCONJ NOUN PUNCT PRON VERB PUNCT ADP NOUN PUNCT SCONJ DET ADJ AUX DET NOUN CCONJ DET ADJ AUX DET ADJ ADP DET NOUN PUNCT ADV PUNCT PRON VERB ADV ADP DET NOUN CCONJ PROPN PUNCT NUM ADV ADJ NOUN ADP DET ADJ NOUN PUNCT DET ADJ NOUN VERB DET NOUN SCONJ ADJ NOUN NOUN ADV VERB PART VERB ADJ NOUN CCONJ PUNCT SCONJ ADV PUNCT SCONJ DET NOUN AUX AUX VERB PUNCT PART VERB DET NOUN PUNCT PRON VERB DET ADJ NOUN VERB NOUN ADP DET NOUN VERB ADP PROPN PUNCT PUNCT DET NOUN VERB ADP ADJ NOUN NOUN ADJ ADP SCONJ PROPN AUX DET NOUN ADP DET NOUN PART AUX VERB PUNCT CCONJ VERB NOUN ADP NOUN CCONJ NOUN VERB VERB ADJ NOUN VERB ADP PROPN PUNCT VERB DET NOUN PUNCT PRON VERB SCONJ PROPN ADV VERB PART VERB ADJ ADJ NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB PART VERB PROPN PROPN PUNCT DET NOUN PRON VERB PRON PART ADV VERB ADJ NOUN NOUN ADP ADJ NOUN PUNCT DET NOUN ADP VERB DET NOUN AUX PART VERB ADJ NOUN NOUN ADP ADV ADJ NOUN ADP ADJ PUNCT SCONJ PRON AUX VERB PART VERB VERB NOUN NOUN PUNCT,0.5178571428571429,28.0,4.638392857142857
262,176,Timo Schick,"[' Distributed representations of words are a key component of natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective (Peters et al. 2018) have led to large performance gains for a variety of NLP tasks. Recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks (Radford et al. 2018; Howard and Ruder 2018). Taking up this idea, Devlin et al. (2019) introduced BERT, a bidirectional language model based on the Transformer (Vaswani et al. 2017) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by Radford et al. (2019), it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can “ask” it for properties of that word using natural language. For example, a language model that understands the concept of “guilt” should be able to correctly complete the sentence “Guilt is the opposite of .” with the word “innocence”.', 'The examples in Table 1 show that, according to this measure, BERT is indeed able to understand frequent words such as “lime” and “bicycle”: it predicts, among others, that the former is a fruit and the latter is the same as a bike. However, it fails terribly for both “kumquat” and “unicycle”, two less frequent words from the same domains. This poor performance raises the question whether deep language models generally struggle to understand rare words and, if so, how this weakness can be overcome.To answer this question, we create a novel dataset containing queries like the ones shown in Table 1. This dataset consists of (i) natural language patterns such as where <W> is a placeholder for a word to be investigated, and (ii) corresponding pairs of keywords (<W>) and targets (fillers for ) obtained using semantic relations extracted from WordNet (Miller 1995). Using this dataset, we show that BERT indeed fails to understand many rare words. To overcome this limitation, we propose to apply Attentive Mimicking (Schick and Sch¨utze2019a), a method that allows us to explicitly learn high quality representations for rare words. A prerequisite for using this method is to have high-quality embeddings for as many words as possible, because it is trained to reproduce known word embeddings.', 'However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.']",intro_chunked,"However, many deep language models including BERT make use of byte-pair encoding (Sennrich, Haddow, and Birch 2015), WordPiece (Wu etal. 2016) or similar subword tokenization algorithms. Thus, many words are not represented by a single token but by asequence of subword tokens and do not have their own embeddings. To solve this problem, we introduce one-token approximation (OTA), a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to BERT, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce WordNet Language Model Probing (WN-LaMPro), a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of BERT to understand words depends highly on their frequency. We present one-token approximation (OTA), a method that obtains an embedding for a multi-token word that has behavior similar to the sequence of its subword embeddings. We apply OTA and Attentive Mimicking (Schick andSch¨utze 2019a) to BERT and show that this substantially improves BERT’s understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.",34.662101895734594,32.83053957444222,262,0.4886138141155243," However, many deep language models including Propname make use of byte pair encoding, Propname or similar Propname Propname algorithms. Thus, many words are not represented by a single token but by asequence of Propname Propname and do not have their own embeddings. To solve this problem, we introduce one token approximation, a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to Propname, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce Propname Propname Propname Propname, a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of Propname to understand words depends highly on their frequency. We present one token approximation, a method that obtains an embedding for a multi token word that has behavior similar to the sequence of its Propname embeddings. We apply Propname and Propname Propname to Propname and show that this substantially improves BERTs understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings."," However, many deep language models including Propname make use of byte pair encoding, Propname or similar Propname Propname algorithms. Thus, many words are not represented by a single token but by asequence of Propname Propname and do not have their own embeddings. To solve this problem, we introduce one token approximation, a method that approximately infers what the embedding of an arbitrary word would look like if it were represented by a single token. While we apply this method only to Propname, it can easily be adapted for other language modeling architectures. In summary, our contributions are as follows: We introduce Propname Propname Propname Propname, a novel dataset for evaluating the ability of language models to understand specific words. Using this dataset, we show that the ability of Propname to understand words depends highly on their frequency. We present one token approximation, a method that obtains an embedding for a multi token word that has behavior similar to the sequence of its Propname embeddings. We apply Propname and Propname Propname to Propname and show that this substantially improves BERTs understanding of rare words. Our work is the first to successfully apply mimicking techniques to contextualized word embeddings.", ADV PUNCT ADJ ADJ NOUN NOUN VERB PROPN VERB NOUN ADP NOUN NOUN NOUN PUNCT PROPN CCONJ ADJ PROPN PROPN VERB PUNCT ADV PUNCT ADJ NOUN AUX PART VERB ADP DET ADJ NOUN CCONJ ADP NOUN ADP PROPN PROPN CCONJ AUX PART VERB PRON ADJ NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB NUM NOUN NOUN PUNCT DET NOUN PRON ADV VERB PRON DET NOUN ADP DET ADJ NOUN AUX VERB ADP SCONJ PRON AUX VERB ADP DET ADJ NOUN PUNCT SCONJ PRON VERB DET NOUN ADV ADP PROPN PUNCT PRON AUX ADV AUX VERB ADP ADJ NOUN NOUN NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB PROPN PROPN PROPN PROPN PUNCT DET ADJ NOUN ADP VERB DET NOUN ADP NOUN NOUN PART VERB ADJ NOUN PUNCT VERB DET NOUN PUNCT PRON VERB SCONJ DET NOUN ADP PROPN PART VERB NOUN VERB ADV ADP PRON NOUN PUNCT PRON VERB NUM NOUN NOUN PUNCT DET NOUN PRON VERB DET VERB ADP DET ADJ ADJ NOUN PRON VERB NOUN ADJ ADP DET NOUN ADP PRON PROPN NOUN PUNCT PRON VERB PROPN CCONJ PROPN PROPN ADP PROPN CCONJ VERB SCONJ PRON ADV VERB NOUN NOUN ADP ADJ NOUN PUNCT PRON NOUN AUX DET ADJ PART ADV VERB VERB NOUN ADP VERB NOUN NOUN PUNCT,0.511520737327189,24.11111111111111,4.9308755760368665
263,177,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches often reduce the amount of training data required, they still do not perform well if only a handful of examples is available for the downstream task, which is common for real-word uses of NLP. In such few-shot settings, however, significant gains are possible by proceeding the other way around: Instead of making pretraining more similar to a downstream task, we can reformulate the task itself to make it more similar to the pretraining objective. For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2020a).', 'Besides making pretraining and finetuning more similar, this approach has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. Examples in Figure 1 demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task. While this idea even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020), it only unfolds its full potential when combined with gradient-based training on a handful of abeled examples (Schick and Schütze, 2020b). In particular, Pattern-Exploiting Training (PET) – an approach proposed by Schick and Schütze (2020a) that combines task descriptions with learning from examples – performs strongly for various few-shot text classification datasets. However, it can only be applied to classification tasks and is therefore not applicable to any problems that require the generation of text sequences. In this paper, we adapt PET to train generative models on text generation tasks. In particular, we propose several modifications that enable us to fine-tune a pretrained PEGASUS model (Zhang et al., 2020) with PET. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with our adapted version of PET clearly outperforms regular finetuning. In summary, our contributions are as follows: We describe how PET can be modified for finetuning generative language models for sequence generation tasks. We show that training PEGASUS with PET outperforms regular finetuning across a large set of tasks and training set sizes. We analyze the factors contributing to PET’s strong performance.']",intro_chunked," Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches often reduce the amount of training data required, they still do not perform well if only a handful of examples is available for the downstream task, which is common for real-word uses of NLP. In such few-shot settings, however, significant gains are possible by proceeding the other way around: Instead of making pretraining more similar to a downstream task, we can reformulate the task itself to make it more similar to the pretraining objective. For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2020a).",34.96588028169015,32.83053957444222,263,0.2460232675075531," Pretraining large neural networks with a language modeling objective has led to significant improvements throughout Propname. Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition, gap sentence generation for summarization, and sentence unshuffling for discourse representations. While such approaches often reduce the amount of training data required, they still do not perform well if only a handful of examples is available for the downstream task, which is common for real word uses of Propname. In such few shot settings, however, significant gains are possible by proceeding the other way around: Instead of making pretraining more similar to a downstream task, we can reformulate the task itself to make it more similar to the pretraining objective. For masked language models, one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt."," Pretraining large neural networks with a language modeling objective has led to significant improvements throughout Propname. Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition, gap sentence generation for summarization, and sentence unshuffling for discourse representations. While such approaches often reduce the amount of training data required, they still do not perform well if only a handful of examples is available for the downstream task, which is common for real word uses of Propname. In such few shot settings, however, significant gains are possible by proceeding the other way around: Instead of making pretraining more similar to a downstream task, we can reformulate the task itself to make it more similar to the pretraining objective. For masked language models, one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt.", VERB ADJ ADJ NOUN ADP DET NOUN NOUN NOUN AUX VERB ADP ADJ NOUN ADP PROPN PUNCT ADJ NOUN AUX ADV ADJ ADP VERB DET ADJ NOUN NOUN PRON ADV ADV VERB DET ADJ NOUN ADP NOUN PUNCT NOUN VERB VERB NOUN ADP VERB NOUN NOUN PUNCT NOUN NOUN NOUN ADP NOUN PUNCT CCONJ NOUN VERB ADP NOUN NOUN PUNCT SCONJ ADJ NOUN ADV VERB DET NOUN ADP NOUN NOUN VERB PUNCT PRON ADV AUX PART VERB ADV SCONJ ADV DET NOUN ADP NOUN AUX ADJ ADP DET ADJ NOUN PUNCT PRON AUX ADJ ADP ADJ NOUN NOUN ADP PROPN PUNCT ADP ADJ ADJ NOUN NOUN PUNCT ADV PUNCT ADJ NOUN AUX ADJ ADP VERB DET ADJ NOUN ADV PUNCT ADV ADP VERB VERB ADV ADJ ADP DET ADJ NOUN PUNCT PRON AUX VERB DET NOUN PRON PART VERB PRON ADV ADJ ADP DET VERB NOUN PUNCT ADP VERB NOUN NOUN PUNCT NUM ADJ NOUN NOUN AUX PART VERB NOUN ADP ADJ NOUN ADP VERB DET NOUN NOUN PRON VERB DET NOUN ADP NOUN NOUN PUNCT ADV ADP DET NOUN ADP DET ADJ NOUN PUNCT,0.6270270270270271,30.833333333333332,5.140540540540541
264,178,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches often reduce the amount of training data required, they still do not perform well if only a handful of examples is available for the downstream task, which is common for real-word uses of NLP. In such few-shot settings, however, significant gains are possible by proceeding the other way around: Instead of making pretraining more similar to a downstream task, we can reformulate the task itself to make it more similar to the pretraining objective. For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2020a).', 'Besides making pretraining and finetuning more similar, this approach has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. Examples in Figure 1 demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task. While this idea even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020), it only unfolds its full potential when combined with gradient-based training on a handful of abeled examples (Schick and Schütze, 2020b). In particular, Pattern-Exploiting Training (PET) – an approach proposed by Schick and Schütze (2020a) that combines task descriptions with learning from examples – performs strongly for various few-shot text classification datasets. However, it can only be applied to classification tasks and is therefore not applicable to any problems that require the generation of text sequences. In this paper, we adapt PET to train generative models on text generation tasks. In particular, we propose several modifications that enable us to fine-tune a pretrained PEGASUS model (Zhang et al., 2020) with PET. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with our adapted version of PET clearly outperforms regular finetuning. In summary, our contributions are as follows: We describe how PET can be modified for finetuning generative language models for sequence generation tasks. We show that training PEGASUS with PET outperforms regular finetuning across a large set of tasks and training set sizes. We analyze the factors contributing to PET’s strong performance.']",intro_chunked,"Besides making pretraining and finetuning more similar, this approach has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. Examples in Figure 1 demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task. While this idea even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020), it only unfolds its full potential when combined with gradient-based training on a handful of abeled examples (Schick and Schütze, 2020b). In particular, Pattern-Exploiting Training (PET) – an approach proposed by Schick and Schütze (2020a) that combines task descriptions with learning from examples – performs strongly for various few-shot text classification datasets. However, it can only be applied to classification tasks and is therefore not applicable to any problems that require the generation of text sequences. In this paper, we adapt PET to train generative models on text generation tasks. In particular, we propose several modifications that enable us to fine-tune a pretrained PEGASUS model (Zhang et al., 2020) with PET. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with our adapted version of PET clearly outperforms regular finetuning. In summary, our contributions are as follows: We describe how PET can be modified for finetuning generative language models for sequence generation tasks. We show that training PEGASUS with PET outperforms regular finetuning across a large set of tasks and training set sizes. We analyze the factors contributing to PET’s strong performance.",32.50420454545457,32.83053957444222,264,0.6368799209594727," Besides making pretraining and finetuning more similar, this approach has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. Examples in Figure 0 demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task. While this idea even works in an unsupervised setting or when examples are simply provided as additional context, it only unfolds its full potential when combined with gradient based training on a handful of abeled examples. In particular, Propname Exploiting Propname an approach proposed by Propname and Propname that combines task descriptions with learning from examples performs strongly for various few shot text classification datasets. However, it can only be applied to classification tasks and is therefore not applicable to any problems that require the generation of text sequences. In this paper, we adapt Propname to train generative models on text generation tasks. In particular, we propose several modifications that enable us to fine tune a pretrained PEGASUS model with Propname. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero shot and few shot settings and show that Propname trained with our adapted version of Propname clearly outperforms regular finetuning. In summary, our contributions are as follows: We describe how Propname can be modified for finetuning generative language models for sequence generation tasks. We show that training PEGASUS with Propname outperforms regular finetuning across a large set of tasks and training set sizes. We analyze the factors contributing to PETs strong performance."," Besides making pretraining and finetuning more similar, this approach has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. Examples in Figure 0 demonstrate that pretrained language models can make use of such text snippets to adapt their output for a generation task. While this idea even works in an unsupervised setting or when examples are simply provided as additional context, it only unfolds its full potential when combined with gradient based training on a handful of abeled examples. In particular, Propname Exploiting Propname an approach proposed by Propname and Propname that combines task descriptions with learning from examples performs strongly for various few shot text classification datasets. However, it can only be applied to classification tasks and is therefore not applicable to any problems that require the generation of text sequences. In this paper, we adapt Propname to train generative models on text generation tasks. In particular, we propose several modifications that enable us to fine tune a pretrained PEGASUS model with Propname. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero shot and few shot settings and show that Propname trained with our adapted version of Propname clearly outperforms regular finetuning. In summary, our contributions are as follows: We describe how Propname can be modified for finetuning generative language models for sequence generation tasks. We show that training PEGASUS with Propname outperforms regular finetuning across a large set of tasks and training set sizes. We analyze the factors contributing to PETs strong performance.", SCONJ VERB VERB CCONJ VERB ADV ADJ PUNCT DET NOUN VERB DET ADJ NOUN ADP VERB NOUN PART VERB DET NOUN ADP DET ADJ NOUN PUNCT VERB PRON ADV ADJ SCONJ DET NOUN PART VERB DET NOUN PUNCT NOUN ADP NOUN NUM NOUN PRON VERB NOUN NOUN AUX VERB NOUN ADP ADJ NOUN NOUN PART VERB PRON NOUN ADP DET NOUN NOUN PUNCT SCONJ DET NOUN ADV VERB ADP DET ADJ NOUN CCONJ SCONJ NOUN AUX ADV VERB ADP ADJ NOUN PUNCT PRON ADV VERB PRON ADJ NOUN SCONJ VERB ADP NOUN VERB NOUN ADP DET NOUN ADP VERB NOUN PUNCT ADP ADJ PUNCT PROPN VERB PROPN DET NOUN VERB ADP PROPN CCONJ PROPN PRON VERB NOUN NOUN ADP VERB ADP NOUN VERB ADV ADP ADJ ADJ NOUN NOUN NOUN NOUN PUNCT ADV PUNCT PRON AUX ADV AUX VERB ADP NOUN NOUN CCONJ AUX ADV PART ADJ ADP DET NOUN PRON VERB DET NOUN ADP NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN PART VERB ADJ NOUN ADP NOUN NOUN NOUN PUNCT ADP ADJ PUNCT PRON VERB ADJ NOUN PRON VERB PRON ADP ADJ NOUN DET VERB ADJ NOUN ADP PROPN PUNCT PRON VERB PRON NOUN ADP DET ADJ NOUN ADP NUM ADJ NOUN NOUN CCONJ NOUN NOUN NOUN CCONJ ADP NUM NOUN CCONJ ADJ NOUN NOUN CCONJ VERB SCONJ PROPN VERB ADP PRON VERB NOUN ADP PROPN ADV VERB ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB SCONJ PROPN AUX AUX VERB ADP VERB ADJ NOUN NOUN ADP NOUN NOUN NOUN PUNCT PRON VERB SCONJ NOUN VERB ADP PROPN NOUN ADJ NOUN ADP DET ADJ NOUN ADP NOUN CCONJ NOUN VERB NOUN PUNCT PRON VERB DET NOUN VERB ADP ADJ ADJ NOUN PUNCT,0.523972602739726,26.545454545454547,5.113013698630137
265,179,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.', 'For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization.', 'We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.']",intro_chunked," Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.",28.825181818181818,32.83053957444222,265,0.30256760120391846," Pretraining large neural networks with a language modeling objective has led to significant improvements throughout Propname. Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition, gap sentence generation for summarization, and sentence unshuffling for discourse representations. While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real word uses of Propname. In such few shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective."," Pretraining large neural networks with a language modeling objective has led to significant improvements throughout Propname. Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition, gap sentence generation for summarization, and sentence unshuffling for discourse representations. While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real word uses of Propname. In such few shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.", VERB ADJ ADJ NOUN ADP DET NOUN NOUN NOUN AUX VERB ADP ADJ NOUN ADP PROPN PUNCT ADJ NOUN AUX ADV ADJ ADP VERB DET ADJ NOUN NOUN PRON ADV ADV VERB DET ADJ NOUN ADP NOUN PUNCT NOUN VERB VERB NOUN ADP VERB NOUN NOUN PUNCT NOUN NOUN NOUN ADP NOUN PUNCT CCONJ NOUN VERB ADP NOUN NOUN PUNCT SCONJ ADJ NOUN AUX ADV VERB DET NOUN ADP NOUN NOUN VERB PUNCT PRON ADV ADV AUX PART VERB ADV SCONJ ADV DET NOUN ADP NOUN AUX ADJ ADP DET ADJ NOUN PUNCT PRON AUX DET ADJ NOUN ADP ADJ ADJ NOUN NOUN ADP PROPN PUNCT ADP ADJ ADJ NOUN NOUN PUNCT ADV PUNCT ADJ NOUN AUX ADJ ADP VERB PRON AUX VERB ADP PRON PUNCT ADV ADP VERB VERB ADV ADJ ADP DET ADJ NOUN PUNCT PRON AUX VERB DET ADJ NOUN PART VERB PRON ADV ADJ ADP DET VERB NOUN PUNCT,0.6405228758169934,30.6,5.333333333333333
266,180,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.', 'For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization.', 'We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.']",intro_chunked,"For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization.",31.56277131782946,32.83053957444222,266,0.39458948373794556," For masked language models, one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt. Besides making pre training and finetuning more similar, this approach 000 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 0, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting or when examples are simply provided as additional context; however, it only unfolds its full potential when combined with gradient based training on a handful of labeled examples. Unfortunately, current approaches for doing so are limited to text classification tasks. Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text to text tasks that require the generation of text sequences given an input text, such as abstractive summarization."," For masked language models, one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt. Besides making pre training and finetuning more similar, this approach 000 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 0, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting or when examples are simply provided as additional context; however, it only unfolds its full potential when combined with gradient based training on a handful of labeled examples. Unfortunately, current approaches for doing so are limited to text classification tasks. Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text to text tasks that require the generation of text sequences given an input text, such as abstractive summarization.", ADP VERB NOUN NOUN PUNCT NUM ADJ NOUN NOUN AUX PART VERB NOUN ADP ADJ NOUN ADP VERB DET NOUN NOUN PRON VERB DET NOUN ADP NOUN NOUN PUNCT ADV ADP DET NOUN ADP DET ADJ NOUN PUNCT SCONJ VERB ADJ NOUN CCONJ VERB ADV ADJ PUNCT DET NOUN NUM VERB DET ADJ NOUN ADP VERB NOUN PART VERB DET NOUN ADP DET ADJ NOUN PUNCT VERB PRON ADV ADJ SCONJ DET NOUN PART VERB DET NOUN PUNCT PRON AUX VERB ADP NOUN NUM PUNCT SCONJ DET VERB NOUN NOUN AUX VERB DET ADJ NOUN ADP ADJ NOUN CCONJ VERB PRON NOUN ADV PUNCT DET NOUN ADP VERB NOUN NOUN ADV VERB ADP DET ADJ NOUN CCONJ SCONJ NOUN AUX ADV VERB ADP ADJ NOUN PUNCT ADV PUNCT PRON ADV VERB PRON ADJ NOUN SCONJ VERB ADP NOUN VERB NOUN ADP DET NOUN ADP VERB NOUN PUNCT ADV PUNCT ADJ NOUN ADP VERB ADV AUX ADJ PART VERB NOUN NOUN PUNCT VERB ADP PRON NOUN PUNCT PRON VERB SCONJ DET ADJ NOUN AUX ADV AUX VERB ADP ADV ADJ NOUN ADP NOUN NOUN PRON VERB DET NOUN ADP NOUN NOUN VERB DET NOUN NOUN PUNCT ADJ ADP ADJ NOUN PUNCT,0.635,33.333333333333336,4.91
267,181,Timo Schick,"[' Pretraining large neural networks with a language modeling objective has led to significant improvements throughout NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020, i.a.). Further improvements are often possible by choosing a different pretraining objective that more closely matches the downstream task of interest. Examples include casing prediction for named entity recognition (Mayhew et al., 2020), gap sentence generation for summarization (Zhang et al., 2020), and sentence unshuffling for discourse representations (Lee et al., 2020). While such approaches can significantly reduce the amount of training data required, they typically still do not perform well if only a handful of examples is available for the downstream task, which is a common scenario for many real-word uses of NLP. In such few-shot settings, however, significant gains are possible by reversing what is adapted to what: Instead of making pretraining more similar to a downstream task, we can reformulate the downstream task to make it more similar to the pretraining objective.', 'For masked language models (e.g., Devlin et al., 2019; Lewis et al., 2020), one such reformulation technique is to convert inputs to cloze questions by adding a text snippet that contains some form of task description, often in the form of a short prompt (Radford et al., 2019; Schick and Schütze, 2021a). Besides making pre-training and finetuning more similar, this approach 391 has the compelling benefit of enabling users to explain a task to a pretrained model, making it much easier for the model to understand the task. This is illustrated in Figure 1, where a pretrained language model is given the same input with different instructions and adapts its output accordingly. The idea of providing task descriptions even works in an unsupervised setting (Radford et al., 2019) or when examples are simply provided as additional context (Brown et al., 2020); however, it only unfolds its full potential when combined with gradient-based training on a handful of labeled examples (Schick and Schütze, 2021b). Unfortunately, current approaches for doing so are limited to text classification tasks (Schick and Schütze, 2021a). Inspired by their success, we investigate whether the underlying idea can also be transferred to more challenging text-to-text tasks that require the generation of text sequences given an input text, such as abstractive summarization.', 'We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.']",intro_chunked,"We introduce GENPET, a novel method based on PET (Schick and Schütze, 2021a), that enables finetuning of generative language models using both instructions and labeled examples. We show that GENPET is a highly data-efficient method that enables us to finetune a pretrained PEGASUS model (Zhang et al., 2020) with as little as 10 or 100 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero-shot and few-shot settings and show that PEGASUS trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with GENPET outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to GENPET’s strong performance and quantify the impact of all its components.",28.017083333333346,32.83053957444222,267,0.7613860368728638," We introduce GENPET, a novel method based on Propname, that enables finetuning of generative language models using both instructions and labeled examples. We show that Propname is a highly data efficient method that enables us to finetune a pretrained PEGASUS model with as little as 00 or 000 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero shot and few shot settings and show that Propname trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with Propname outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to Propname strong performance and quantify the impact of all its components."," We introduce GENPET, a novel method based on Propname, that enables finetuning of generative language models using both instructions and labeled examples. We show that Propname is a highly data efficient method that enables us to finetune a pretrained PEGASUS model with as little as 00 or 000 training examples. We evaluate our approach on a diverse set of six English headline generation and text summarization tasks both in zero shot and few shot settings and show that Propname trained with GENPET clearly outperforms regular finetuning. In summary, our contributions are as follows: We introduce GENPET, a finetuning procedure for generative language models that achieves great data efficiency by using both textual instructions and training examples. We show that training PEGASUS with Propname outperforms standard finetuning across a broad set of tasks and training set sizes. We analyze the factors contributing to Propname strong performance and quantify the impact of all its components.", PRON VERB NOUN PUNCT DET ADJ NOUN VERB ADP PROPN PUNCT PRON VERB VERB ADP ADJ NOUN NOUN VERB DET NOUN CCONJ VERB NOUN PUNCT PRON VERB SCONJ PROPN AUX DET ADV NOUN ADJ NOUN PRON VERB PRON PART VERB DET VERB ADJ NOUN ADP ADV ADJ ADP NUM CCONJ NUM NOUN NOUN PUNCT PRON VERB PRON NOUN ADP DET ADJ NOUN ADP NUM ADJ NOUN NOUN CCONJ NOUN NOUN NOUN CCONJ ADP NUM NOUN CCONJ ADJ NOUN NOUN CCONJ VERB SCONJ PROPN VERB ADP NOUN ADV VERB ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB NOUN PUNCT DET VERB NOUN ADP ADJ NOUN NOUN PRON VERB ADJ NOUN NOUN ADP VERB DET ADJ NOUN CCONJ NOUN NOUN PUNCT PRON VERB SCONJ NOUN VERB ADP PROPN NOUN NOUN VERB ADP DET ADJ NOUN ADP NOUN CCONJ NOUN VERB NOUN PUNCT PRON VERB DET NOUN VERB ADP PROPN ADJ NOUN CCONJ VERB DET NOUN ADP DET PRON NOUN PUNCT,0.5548780487804879,27.333333333333332,5.158536585365853
268,182,Timo Schick,"[' Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;\nRaffel et al., 2020; Brown et al., 2020; Rae et al.,\n2021; Zhang et al., 2022; Chowdhery et al., 2022,\ni.a.). However, the way these models operate—\nproducing outputs in a single pass from left to\nright—differs strongly from the iterative process\nby which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond\nthat, they are hard to control (Korbak et al., 2022)\nand verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the\nability to explain their intentions. All of this makes\nit very difficult for humans to collaborate with such\nmodels for writing coherent, factual texts. To address these shortcomings of existing LMs,\nwe propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process.', 'As illustrated in Figure 1, PEER operates in several steps\nthat aim to mirror the human writing process: For\na given text, either a user or the model itself can\nplan an action to be applied, for example by means\nof a natural language instruction. This plan is then\nrealized by an edit, which the model can explain\nboth in form of a textual comment and by pointing\nto references used; this is enabled by augmenting\neach input text with retrieved passages containing\npotentially relevant background information. We\nrepeat these steps until the text is in a satisfactory\nstate that does not require any further updates. This\niterative approach does not only enable the model\nto decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it\nalso allows humans to intervene at any time and\nsteer the model in the right direction, either by providing it with their own plans and comments or by\nmaking edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),\nwe use Wikipedia as our main source of edits and\nassociated comments, which we use as proxies for\nplans and explanations.', 'In contrast to this prior\nwork, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:\nIt should be capable of following human-written\ninstructions for updating texts in any domain. To\nachieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in\nsequential order, but also to infill various parts; for\nexample, given an edited text and a set of relevant\ndocuments, we teach it to produce the original version of this text before it was edited. This enables\nus to use self-training techniques (e.g., Yarowsky,\n1995; Sennrich et al., 2016; He et al., 2020a; Schick\nand Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We\nshow that this substantially improves PEER along\nseveral axes, including its ability to edit texts in any\ndomain, to understand human-written instructions,\nand to explain its actions. In summary, our contributions are as follows:\n• We introduce PEER, a collaborative language\nmodel trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities\nessential for collaborative writing. • For different tasks related to editing texts, we\nshow that PEER clearly outperforms various\nbaselines and analyze factors leading to its\nstrong performance. • To facilitate further research on collaborative\nLMs, we release a variety of PEER models as\nwell as the data and code used to train them.']",intro_chunked," Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;
Raffel et al., 2020; Brown et al., 2020; Rae et al.,
2021; Zhang et al., 2022; Chowdhery et al., 2022,
i.a.). However, the way these models operate—
producing outputs in a single pass from left to
right—differs strongly from the iterative process
by which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond
that, they are hard to control (Korbak et al., 2022)
and verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the
ability to explain their intentions. All of this makes
it very difficult for humans to collaborate with such
models for writing coherent, factual texts. To address these shortcomings of existing LMs,
we propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process.",39.612681159420305,32.83053957444222,268,0.1772633045911789," Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, 
 Propname.. However, the way these models operate 
 producing outputs in a single pass from left to 
 rightdiffers strongly from the iterative process 
 by which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond 
 that, they are hard to control and verifying their outputs is challenging as they often hallucinate content and lack the 
 ability to explain their intentions. All of this makes 
 it very difficult for humans to collaborate with such 
 models for writing coherent, factual texts. To address these shortcomings of existing LMs, 
 we propose Propname, a collaborative language model trained on edit histories to cover the entire writing process."," Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, 
 Propname.. However, the way these models operate 
 producing outputs in a single pass from left to 
 rightdiffers strongly from the iterative process 
 by which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond 
 that, they are hard to control and verifying their outputs is challenging as they often hallucinate content and lack the 
 ability to explain their intentions. All of this makes 
 it very difficult for humans to collaborate with such 
 models for writing coherent, factual texts. To address these shortcomings of existing LMs, 
 we propose Propname, a collaborative language model trained on edit histories to cover the entire writing process.", ADJ ADJ NOUN VERB ADJ NOUN NOUN NOUN SCONJ VERB ADP DET NOUN NOUN ADJ PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PUNCT ADV PUNCT DET NOUN DET NOUN VERB SPACE VERB NOUN ADP DET ADJ NOUN ADP ADJ ADP SPACE NOUN ADV ADP DET ADJ NOUN SPACE ADP PRON NOUN ADV VERB NOUN PUNCT PRON VERB PRON NOUN ADP ADJ NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON AUX PART ADJ PART ADV VERB CCONJ VERB PRON ADJ NOUN PUNCT ADP SPACE SCONJ PUNCT PRON AUX ADJ PART VERB CCONJ VERB PRON NOUN AUX VERB SCONJ PRON ADV VERB NOUN CCONJ VERB DET SPACE NOUN PART VERB PRON NOUN PUNCT PRON ADP PRON VERB SPACE PRON ADV ADJ SCONJ NOUN PART VERB ADP ADJ SPACE NOUN ADP VERB ADJ PUNCT ADJ NOUN PUNCT PART VERB DET NOUN ADP VERB NOUN PUNCT SPACE PRON VERB PROPN PUNCT DET ADJ NOUN NOUN VERB ADP NOUN NOUN PART VERB DET ADJ NOUN NOUN PUNCT,0.5497382198952879,31.833333333333332,5.209424083769633
269,183,Timo Schick,"[' Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;\nRaffel et al., 2020; Brown et al., 2020; Rae et al.,\n2021; Zhang et al., 2022; Chowdhery et al., 2022,\ni.a.). However, the way these models operate—\nproducing outputs in a single pass from left to\nright—differs strongly from the iterative process\nby which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond\nthat, they are hard to control (Korbak et al., 2022)\nand verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the\nability to explain their intentions. All of this makes\nit very difficult for humans to collaborate with such\nmodels for writing coherent, factual texts. To address these shortcomings of existing LMs,\nwe propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process.', 'As illustrated in Figure 1, PEER operates in several steps\nthat aim to mirror the human writing process: For\na given text, either a user or the model itself can\nplan an action to be applied, for example by means\nof a natural language instruction. This plan is then\nrealized by an edit, which the model can explain\nboth in form of a textual comment and by pointing\nto references used; this is enabled by augmenting\neach input text with retrieved passages containing\npotentially relevant background information. We\nrepeat these steps until the text is in a satisfactory\nstate that does not require any further updates. This\niterative approach does not only enable the model\nto decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it\nalso allows humans to intervene at any time and\nsteer the model in the right direction, either by providing it with their own plans and comments or by\nmaking edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),\nwe use Wikipedia as our main source of edits and\nassociated comments, which we use as proxies for\nplans and explanations.', 'In contrast to this prior\nwork, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:\nIt should be capable of following human-written\ninstructions for updating texts in any domain. To\nachieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in\nsequential order, but also to infill various parts; for\nexample, given an edited text and a set of relevant\ndocuments, we teach it to produce the original version of this text before it was edited. This enables\nus to use self-training techniques (e.g., Yarowsky,\n1995; Sennrich et al., 2016; He et al., 2020a; Schick\nand Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We\nshow that this substantially improves PEER along\nseveral axes, including its ability to edit texts in any\ndomain, to understand human-written instructions,\nand to explain its actions. In summary, our contributions are as follows:\n• We introduce PEER, a collaborative language\nmodel trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities\nessential for collaborative writing. • For different tasks related to editing texts, we\nshow that PEER clearly outperforms various\nbaselines and analyze factors leading to its\nstrong performance. • To facilitate further research on collaborative\nLMs, we release a variety of PEER models as\nwell as the data and code used to train them.']",intro_chunked,"As illustrated in Figure 1, PEER operates in several steps
that aim to mirror the human writing process: For
a given text, either a user or the model itself can
plan an action to be applied, for example by means
of a natural language instruction. This plan is then
realized by an edit, which the model can explain
both in form of a textual comment and by pointing
to references used; this is enabled by augmenting
each input text with retrieved passages containing
potentially relevant background information. We
repeat these steps until the text is in a satisfactory
state that does not require any further updates. This
iterative approach does not only enable the model
to decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it
also allows humans to intervene at any time and
steer the model in the right direction, either by providing it with their own plans and comments or by
making edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),
we use Wikipedia as our main source of edits and
associated comments, which we use as proxies for
plans and explanations.",30.341000000000037,32.83053957444222,269,0.30963438749313354," As illustrated in Figure 0, Propname operates in several steps 
 that aim to mirror the human writing process: For 
 a given text, either a user or the model itself can 
 plan an action to be applied, for example by means 
 of a natural language instruction. This plan is then 
 realized by an edit, which the model can explain 
 both in form of a textual comment and by pointing 
 to references used; this is enabled by augmenting 
 each input text with retrieved passages containing 
 potentially relevant background information. We 
 repeat these steps until the text is in a satisfactory 
 state that does not require any further updates. This 
 iterative approach does not only enable the model 
 to decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it 
 also allows humans to intervene at any time and 
 steer the model in the right direction, either by providing it with their own plans and comments or by 
 making edits themselves. Similar to recent approaches for iterative editing, 
 we use Propname as our main source of edits and 
 associated comments, which we use as proxies for 
 plans and explanations."," As illustrated in Figure 0, Propname operates in several steps 
 that aim to mirror the human writing process: For 
 a given text, either a user or the model itself can 
 plan an action to be applied, for example by means 
 of a natural language instruction. This plan is then 
 realized by an edit, which the model can explain 
 both in form of a textual comment and by pointing 
 to references used; this is enabled by augmenting 
 each input text with retrieved passages containing 
 potentially relevant background information. We 
 repeat these steps until the text is in a satisfactory 
 state that does not require any further updates. This 
 iterative approach does not only enable the model 
 to decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it 
 also allows humans to intervene at any time and 
 steer the model in the right direction, either by providing it with their own plans and comments or by 
 making edits themselves. Similar to recent approaches for iterative editing, 
 we use Propname as our main source of edits and 
 associated comments, which we use as proxies for 
 plans and explanations.", SCONJ VERB ADP NOUN NUM PUNCT PROPN VERB ADP ADJ NOUN SPACE PRON VERB PART VERB DET ADJ NOUN NOUN PUNCT ADP SPACE DET VERB NOUN PUNCT CCONJ DET NOUN CCONJ DET NOUN PRON AUX SPACE VERB DET NOUN PART AUX VERB PUNCT ADP NOUN ADP NOUN SPACE ADP DET ADJ NOUN NOUN PUNCT DET NOUN AUX ADV SPACE VERB ADP DET NOUN PUNCT PRON DET NOUN AUX VERB SPACE CCONJ ADP NOUN ADP DET ADJ NOUN CCONJ ADP VERB SPACE ADP NOUN VERB PUNCT PRON AUX VERB ADP VERB SPACE DET NOUN NOUN ADP VERB NOUN VERB SPACE ADV ADJ NOUN NOUN PUNCT PRON SPACE VERB DET NOUN SCONJ DET NOUN AUX ADP DET ADJ SPACE NOUN PRON AUX PART VERB DET ADJ NOUN PUNCT DET SPACE ADJ NOUN AUX PART ADV VERB DET NOUN SPACE PART VERB DET ADJ NOUN ADP VERB DET ADJ PUNCT ADJ NOUN ADP ADJ ADJ NOUN PUNCT PRON SPACE ADV VERB NOUN PART VERB ADP DET NOUN CCONJ SPACE VERB DET NOUN ADP DET ADJ NOUN PUNCT CCONJ ADP VERB PRON ADP PRON ADJ NOUN CCONJ NOUN CCONJ ADP SPACE NOUN VERB PRON PUNCT ADJ ADP ADJ NOUN ADP ADJ NOUN PUNCT SPACE PRON VERB PROPN ADP PRON ADJ NOUN ADP NOUN CCONJ SPACE VERB NOUN PUNCT PRON PRON VERB ADP NOUN ADP SPACE NOUN CCONJ NOUN PUNCT,0.6213592233009708,41.2,4.548543689320389
270,184,Timo Schick,"[' Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019;\nRaffel et al., 2020; Brown et al., 2020; Rae et al.,\n2021; Zhang et al., 2022; Chowdhery et al., 2022,\ni.a.). However, the way these models operate—\nproducing outputs in a single pass from left to\nright—differs strongly from the iterative process\nby which humans typically write texts. This limits their utility for collaborative writing in various respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond\nthat, they are hard to control (Korbak et al., 2022)\nand verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the\nability to explain their intentions. All of this makes\nit very difficult for humans to collaborate with such\nmodels for writing coherent, factual texts. To address these shortcomings of existing LMs,\nwe propose PEER (Plan, Edit, Explain, Repeat), a collaborative language model trained on edit histories to cover the entire writing process.', 'As illustrated in Figure 1, PEER operates in several steps\nthat aim to mirror the human writing process: For\na given text, either a user or the model itself can\nplan an action to be applied, for example by means\nof a natural language instruction. This plan is then\nrealized by an edit, which the model can explain\nboth in form of a textual comment and by pointing\nto references used; this is enabled by augmenting\neach input text with retrieved passages containing\npotentially relevant background information. We\nrepeat these steps until the text is in a satisfactory\nstate that does not require any further updates. This\niterative approach does not only enable the model\nto decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it\nalso allows humans to intervene at any time and\nsteer the model in the right direction, either by providing it with their own plans and comments or by\nmaking edits themselves. Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022),\nwe use Wikipedia as our main source of edits and\nassociated comments, which we use as proxies for\nplans and explanations.', 'In contrast to this prior\nwork, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:\nIt should be capable of following human-written\ninstructions for updating texts in any domain. To\nachieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in\nsequential order, but also to infill various parts; for\nexample, given an edited text and a set of relevant\ndocuments, we teach it to produce the original version of this text before it was edited. This enables\nus to use self-training techniques (e.g., Yarowsky,\n1995; Sennrich et al., 2016; He et al., 2020a; Schick\nand Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We\nshow that this substantially improves PEER along\nseveral axes, including its ability to edit texts in any\ndomain, to understand human-written instructions,\nand to explain its actions. In summary, our contributions are as follows:\n• We introduce PEER, a collaborative language\nmodel trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities\nessential for collaborative writing. • For different tasks related to editing texts, we\nshow that PEER clearly outperforms various\nbaselines and analyze factors leading to its\nstrong performance. • To facilitate further research on collaborative\nLMs, we release a variety of PEER models as\nwell as the data and code used to train them.']",intro_chunked,"In contrast to this prior
work, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia:
It should be capable of following human-written
instructions for updating texts in any domain. To
achieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in
sequential order, but also to infill various parts; for
example, given an edited text and a set of relevant
documents, we teach it to produce the original version of this text before it was edited. This enables
us to use self-training techniques (e.g., Yarowsky,
1995; Sennrich et al., 2016; He et al., 2020a; Schick
and Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We
show that this substantially improves PEER along
several axes, including its ability to edit texts in any
domain, to understand human-written instructions,
and to explain its actions. In summary, our contributions are as follows:
• We introduce PEER, a collaborative language
model trained primarily on Wikipedia edit histories. • By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities
essential for collaborative writing. • For different tasks related to editing texts, we
show that PEER clearly outperforms various
baselines and analyze factors leading to its
strong performance. • To facilitate further research on collaborative
LMs, we release a variety of PEER models as
well as the data and code used to train them.",28.390639763779546,32.83053957444222,270,0.7796497344970703," In contrast to this prior 
 work, however, our goal is to obtain a collaborative model that is useful beyond just Propname: 
 It should be capable of following Propname written 
 instructions for updating texts in any domain. To 
 achieve this goal, we train PEER not only to perform the writing process illustrated in Figure 0 in 
 sequential order, but also to infill various parts; for 
 example, given an edited text and a set of relevant 
 documents, we teach it to produce the original version of this text before it was edited. This enables 
 us to use self training techniques Propname, Propname, 
 0000; Propname Propname Propname Propname, 0000; He Propname Propname Propname, 0000a; Propname 
 and Propname, 0000a for training PEER with synthetic plans, edits, explanations and documents. We 
 show that this substantially improves PEER along 
 several axes, including its ability to edit texts in any 
 domain, to understand human written instructions, 
 and to explain its actions. In summary, our contributions are as follows: We introduce Propname, a collaborative language 
 model trained primarily on Propname edit histories. By training Propname to infill parts of the writing process and leveraging self training techniques, we make it applicable in any domain and enhance several of its core capabilities 
 essential for collaborative writing. For different tasks related to editing texts, we 
 show that Propname clearly outperforms various 
 baselines and analyze factors leading to its 
 strong performance. To facilitate further research on collaborative 
 LMs, we release a variety of Propname models as 
 well as the data and code used to train them."," In contrast to this prior 
 work, however, our goal is to obtain a collaborative model that is useful beyond just Propname: 
 It should be capable of following Propname written 
 instructions for updating texts in any domain. To 
 achieve this goal, we train PEER not only to perform the writing process illustrated in Figure 0 in 
 sequential order, but also to infill various parts; for 
 example, given an edited text and a set of relevant 
 documents, we teach it to produce the original version of this text before it was edited. This enables 
 us to use self training techniques Propname, Propname, 
 0000; Propname Propname Propname Propname, 0000; He Propname Propname Propname, 0000a; Propname 
 and Propname, 0000a for training PEER with synthetic plans, edits, explanations and documents. We 
 show that this substantially improves PEER along 
 several axes, including its ability to edit texts in any 
 domain, to understand human written instructions, 
 and to explain its actions. In summary, our contributions are as follows: We introduce Propname, a collaborative language 
 model trained primarily on Propname edit histories. By training Propname to infill parts of the writing process and leveraging self training techniques, we make it applicable in any domain and enhance several of its core capabilities 
 essential for collaborative writing. For different tasks related to editing texts, we 
 show that Propname clearly outperforms various 
 baselines and analyze factors leading to its 
 strong performance. To facilitate further research on collaborative 
 LMs, we release a variety of Propname models as 
 well as the data and code used to train them.", ADP NOUN ADP DET ADJ SPACE NOUN PUNCT ADV PUNCT PRON NOUN AUX PART VERB DET ADJ NOUN PRON AUX ADJ ADP ADV PROPN PUNCT SPACE PRON AUX AUX ADJ ADP VERB PROPN VERB SPACE NOUN ADP VERB NOUN ADP DET NOUN PUNCT PART SPACE VERB DET NOUN PUNCT PRON VERB ADJ PART ADV PART VERB DET NOUN NOUN VERB ADP NOUN NUM ADP SPACE ADJ NOUN PUNCT CCONJ ADV PART VERB ADJ NOUN PUNCT ADP SPACE NOUN PUNCT VERB DET ADJ NOUN CCONJ DET NOUN ADP ADJ SPACE NOUN PUNCT PRON VERB PRON PART VERB DET ADJ NOUN ADP DET NOUN SCONJ PRON AUX VERB PUNCT PRON VERB SPACE PRON PART VERB NOUN NOUN NOUN PROPN PUNCT PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PRON PROPN PROPN PROPN PUNCT NUM PUNCT PROPN SPACE CCONJ PROPN PUNCT NUM ADP VERB ADJ ADP ADJ NOUN PUNCT NOUN PUNCT NOUN CCONJ NOUN PUNCT PRON SPACE VERB SCONJ PRON ADV VERB NOUN ADP SPACE ADJ NOUN PUNCT VERB PRON NOUN PART VERB NOUN ADP DET SPACE NOUN PUNCT PART VERB ADJ VERB NOUN PUNCT SPACE CCONJ PART VERB PRON NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN SPACE NOUN VERB ADV ADP PROPN NOUN NOUN PUNCT ADP VERB PROPN PART VERB NOUN ADP DET NOUN NOUN CCONJ VERB NOUN NOUN NOUN PUNCT PRON VERB PRON ADJ ADP DET NOUN CCONJ VERB ADJ ADP PRON NOUN NOUN SPACE ADJ ADP ADJ NOUN PUNCT ADP ADJ NOUN VERB ADP VERB NOUN PUNCT PRON SPACE VERB SCONJ PROPN ADV VERB ADJ SPACE NOUN CCONJ VERB NOUN VERB ADP PRON SPACE ADJ NOUN PUNCT PART VERB ADJ NOUN ADP ADJ SPACE NOUN PUNCT PRON VERB DET NOUN ADP PROPN NOUN ADV SPACE ADV ADP DET NOUN CCONJ NOUN VERB PART VERB PRON PUNCT,0.5103448275862069,36.25,4.758620689655173
271,185,Timo Schick,"[' Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavi- cencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to finetune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g.', 'Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019) recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a word’s contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn – by way of self-attention – to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking (AM). Our contributions are as follows: (i) We introduce the attentive mimicking model. It produces high-quality embeddings for rare and medium-frequency words by attending to the most informative contexts. (ii) We propose a novel evaluation method based on VecMap (Artetxe et al., 2018) that allows us to easily evaluate the embedding quality of low- and medium-frequency words. (iii) We show that attentive mimicking improves word\nembeddings on various datasets.']",intro_chunked," Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavi- cencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to finetune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g.",37.06875000000002,32.83053957444222,271,0.3970867991447449," Word embeddings have led to large performance gains in natural language processing. However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface form information into learning. This can either be done directly, or a two step process is employed: first, an embedding model is trained on the word level and then, surface form information is used either to finetune embeddings or to completely recompute them. The latter can be achieved using a model trained to reproduce the original embeddings. However, these methods only work if a words meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task Propname"," Word embeddings have led to large performance gains in natural language processing. However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface form information into learning. This can either be done directly, or a two step process is employed: first, an embedding model is trained on the word level and then, surface form information is used either to finetune embeddings or to completely recompute them. The latter can be achieved using a model trained to reproduce the original embeddings. However, these methods only work if a words meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task Propname", NOUN NOUN AUX VERB ADP ADJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT ADV PUNCT VERB NOUN ADV VERB ADJ NOUN ADP DET NOUN PART VERB DET ADJ NOUN ADP PRON PUNCT NUM NOUN PART VERB DET NOUN CCONJ VERB NOUN ADP ADJ NOUN AUX PART VERB NOUN NOUN NOUN ADP NOUN PUNCT PRON AUX CCONJ AUX VERB ADV PUNCT CCONJ DET NUM NOUN NOUN AUX VERB PUNCT ADV PUNCT DET VERB NOUN AUX VERB ADP DET NOUN NOUN CCONJ ADV PUNCT NOUN NOUN NOUN AUX VERB CCONJ PART NOUN NOUN CCONJ PART ADV VERB PRON PUNCT DET ADJ AUX AUX VERB VERB DET NOUN VERB PART VERB DET ADJ NOUN PUNCT ADV PUNCT DET NOUN ADV VERB SCONJ DET NOUN VERB AUX ADP ADJ ADV AUX VERB ADP PRON NOUN PUNCT DET ADV VERB NOUN ADP NOUN AUX VERB VERB ADP ADJ NOUN PUNCT SCONJ DET NOUN AUX PART VERB NOUN ADP ADV ADJ NOUN ADP ADP ADJ DET NOUN ADP NOUN PUNCT SCONJ ADJ ADJ NOUN ADV VERB NOUN NOUN ADP DET NOUN PROPN,0.6136363636363636,22.0,4.903409090909091
272,186,Timo Schick,"[' Word embeddings have led to large performance gains in natural language processing (NLP). However, embedding methods generally need many observations of a word to learn a good representation for it. One way to overcome this limitation and improve embeddings of infrequent words is to incorporate surface-form information into learning. This can either be done directly (Wieting et al., 2016; Bojanowski et al., 2017; Salle and Villavi- cencio, 2018), or a two-step process is employed: first, an embedding model is trained on the word level and then, surface-form information is used either to finetune embeddings (Cotterell et al., 2016; Vuli´c et al., 2017) or to completely recompute them. The latter can be achieved using a model trained to reproduce (or mimic) the original embeddings (Pinter et al., 2017). However, these methods only work if a word’s meaning can at least partially be predicted from its form. A closely related line of research is embedding learning for novel words, where the goal is to obtain embeddings for previously unseen words from at most a handful of observations. While most contemporary approaches exclusively use context information for this task (e.g.', 'Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019) recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a word’s contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn – by way of self-attention – to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking (AM). Our contributions are as follows: (i) We introduce the attentive mimicking model. It produces high-quality embeddings for rare and medium-frequency words by attending to the most informative contexts. (ii) We propose a novel evaluation method based on VecMap (Artetxe et al., 2018) that allows us to easily evaluate the embedding quality of low- and medium-frequency words. (iii) We show that attentive mimicking improves word\nembeddings on various datasets.']",intro_chunked,"Herbelot and Baroni, 2017; Khodak et al., 2018), Schick and Sch¨utze (2019) recently introduced the form-context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a word’s contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn – by way of self-attention – to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking (AM). Our contributions are as follows: (i) We introduce the attentive mimicking model. It produces high-quality embeddings for rare and medium-frequency words by attending to the most informative contexts. (ii) We propose a novel evaluation method based on VecMap (Artetxe et al., 2018) that allows us to easily evaluate the embedding quality of low- and medium-frequency words. (iii) We show that attentive mimicking improves word
embeddings on various datasets.",38.90269902912624,32.83053957444222,272,0.5341344475746155," Propname and Propname, 0000; Propname Propname Propname Propname, 0000, Propname and Propname recently introduced the form context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a words contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn by way of self attention to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking. Our contributions are as follows: We introduce the attentive mimicking model. It produces high quality embeddings for rare and medium frequency words by attending to the most informative contexts. We propose a novel evaluation method based on Propname that allows us to easily evaluate the embedding quality of low and medium frequency words. We show that attentive mimicking improves word 
 embeddings on various datasets."," Propname and Propname, 0000; Propname Propname Propname Propname, 0000, Propname and Propname recently introduced the form context model and showed that joint learning from both surface form and context leads to better performance. The problem we address in this paper is that often, only few of a words contexts provide valuable information about its meaning. Nonetheless, the current state of the art treats all contexts the same. We address this issue by introducing a more intelligent mechanism of incorporating context into mimicking: instead of using all contexts, we learn by way of self attention to pick a subset of especially informative and reliable contexts. This mechanism is based on the observation that in many cases, reliable contexts for a given word tend to resemble each other. We call our proposed architecture attentive mimicking. Our contributions are as follows: We introduce the attentive mimicking model. It produces high quality embeddings for rare and medium frequency words by attending to the most informative contexts. We propose a novel evaluation method based on Propname that allows us to easily evaluate the embedding quality of low and medium frequency words. We show that attentive mimicking improves word 
 embeddings on various datasets.", PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN ADV VERB DET NOUN NOUN NOUN CCONJ VERB SCONJ ADJ NOUN ADP PRON NOUN NOUN CCONJ NOUN VERB ADP ADJ NOUN PUNCT DET NOUN PRON VERB ADP DET NOUN AUX SCONJ ADV PUNCT ADV ADJ ADP DET NOUN NOUN VERB ADJ NOUN ADP PRON NOUN PUNCT ADV PUNCT DET ADJ NOUN ADP DET NOUN VERB DET NOUN DET ADJ PUNCT PRON VERB DET NOUN ADP VERB DET ADV ADJ NOUN ADP VERB NOUN ADP NOUN PUNCT ADV ADP VERB DET NOUN PUNCT PRON VERB ADP NOUN ADP NOUN NOUN PART VERB DET NOUN ADP ADV ADJ CCONJ ADJ NOUN PUNCT DET NOUN AUX VERB ADP DET NOUN SCONJ ADP ADJ NOUN PUNCT ADJ NOUN ADP DET VERB NOUN VERB PART VERB DET ADJ PUNCT PRON VERB PRON VERB NOUN NOUN NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB DET ADJ ADJ NOUN PUNCT PRON VERB ADJ NOUN NOUN ADP ADJ CCONJ ADJ ADJ NOUN ADP VERB ADP DET ADV ADJ NOUN PUNCT PRON VERB DET ADJ NOUN NOUN VERB ADP PROPN PRON VERB PRON PART ADV VERB DET VERB NOUN ADP ADJ CCONJ ADJ ADJ NOUN PUNCT PRON VERB SCONJ ADJ NOUN VERB NOUN SPACE NOUN ADP ADJ NOUN PUNCT,0.5576036866359447,21.7,4.967741935483871
273,187,Timo Schick,"[' As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations\nhave been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.', 'It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.', 'Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.']",intro_chunked," As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations
have been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.",46.65500000000003,32.83053957444222,273,0.19506871700286865," As word embedding algorithms are known to struggle with rare words, several techniques for improving their representations 
 have been proposed. These approaches exploit either the contexts in which rare words occur, their surface form, or both. However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models already handle rare words implicitly using methods such as byte pair encoding, Propname embeddings and character level CNNs. Nevertheless, Propname and Propname recently showed that BERTs performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Propname Propname. However, Propname is limited in two important respects: For processing contexts, it uses a simple bag of words model, making poor use of the available information."," As word embedding algorithms are known to struggle with rare words, several techniques for improving their representations 
 have been proposed. These approaches exploit either the contexts in which rare words occur, their surface form, or both. However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models already handle rare words implicitly using methods such as byte pair encoding, Propname embeddings and character level CNNs. Nevertheless, Propname and Propname recently showed that BERTs performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Propname Propname. However, Propname is limited in two important respects: For processing contexts, it uses a simple bag of words model, making poor use of the available information.", SCONJ NOUN VERB NOUN AUX VERB PART VERB ADP ADJ NOUN PUNCT ADJ NOUN ADP VERB PRON NOUN SPACE AUX AUX VERB PUNCT DET NOUN VERB CCONJ DET NOUN ADP PRON ADJ NOUN VERB PUNCT PRON NOUN NOUN PUNCT CCONJ PRON PUNCT ADV PUNCT PRON ADP DET ADJ NOUN AUX VERB ADP CCONJ VERB ADP ADJ NOUN NOUN PUNCT ADJ NOUN VERB ADP VERB ADJ NOUN NOUN ADV VERB ADJ NOUN ADV VERB NOUN ADJ ADP NOUN NOUN NOUN PUNCT PROPN NOUN CCONJ NOUN NOUN NOUN PUNCT ADV PUNCT PROPN CCONJ PROPN ADV VERB SCONJ NOUN NOUN ADP DET ADJ NOUN VERB NOUN AUX AUX ADV VERB ADP ADV VERB NOUN ADP ADJ NOUN VERB PROPN PROPN PUNCT ADV PUNCT PROPN AUX VERB ADP NUM ADJ NOUN PUNCT ADP NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP NOUN NOUN PUNCT VERB ADJ NOUN ADP DET ADJ NOUN PUNCT,0.6891891891891891,24.666666666666668,5.324324324324325
274,188,Timo Schick,"[' As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations\nhave been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.', 'It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.', 'Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.']",intro_chunked,"It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.",36.07833333333335,32.83053957444222,274,0.34222880005836487," It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag of words models is a reasonable choice for static embeddings, which are often themselves bag of words, it stands to reason that they are not the best choice to generate input representations for position aware, deep language models. To overcome these limitations, we introduce Propname, a novel architecture for learning rare word representations that combines a pretrained Propname model with Propname. As shown in Figure 0, the learned rare word representations can then be used as an improved input representation for another Propname model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible."," It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag of words models is a reasonable choice for static embeddings, which are often themselves bag of words, it stands to reason that they are not the best choice to generate input representations for position aware, deep language models. To overcome these limitations, we introduce Propname, a novel architecture for learning rare word representations that combines a pretrained Propname model with Propname. As shown in Figure 0, the learned rare word representations can then be used as an improved input representation for another Propname model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.", PRON VERB NOUN CCONJ NOUN ADP DET ADJ NOUN PUNCT VERB DET NOUN NOUN ADP VERB ADP DET ADJ NOUN PUNCT DET NOUN VERB PART ADV PART VERB PUNCT CCONJ ADP DET ADJ NOUN ADP VERB NOUN ADP ADJ NOUN ADP VERB NOUN CCONJ NOUN PUNCT SCONJ VERB NOUN ADP NOUN NOUN AUX DET ADJ NOUN ADP ADJ NOUN PUNCT PRON AUX ADV PRON NOUN ADP NOUN PUNCT PRON VERB ADP NOUN SCONJ PRON AUX PART DET ADJ NOUN PART VERB NOUN NOUN ADP NOUN ADJ PUNCT ADJ NOUN NOUN PUNCT PART VERB DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN ADP VERB ADJ NOUN NOUN PRON VERB DET VERB PROPN NOUN ADP PROPN PUNCT SCONJ VERB ADP NOUN NUM PUNCT DET VERB ADJ NOUN NOUN AUX ADV AUX VERB ADP DET VERB NOUN NOUN ADP DET PROPN NOUN PUNCT ADP VERB ADJ NOUN ADP DET NOUN NOUN CCONJ NOUN VERB ADP DET ADJ NOUN PUNCT DET ADJ NOUN ADP DET NOUN NOUN VERB ADJ PUNCT,0.6190476190476191,28.0,4.857142857142857
275,189,Timo Schick,"[' As word embedding algorithms (e.g. Mikolov et al., 2013) are known to struggle with rare words, several techniques for improving their representations\nhave been proposed. These approaches exploit either the contexts in which rare words occur (Lazari-dou et al., 2017; Herbelot and Baroni, 2017; Kho-dak et al., 2018; Liu et al., 2019a), their surface-form (Luong et al., 2013; Bojanowski et al., 2017; Pinter et al., 2017), or both (Schick and Sch ¨utze, 2019a,b; Hautte et al., 2019). However, all of this prior work is designed for and evaluated on uncontextualized word embeddings. Contextualized representations obtained from pretrained deep language models (e.g. Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019b) already handle rare words implicitly using methods such as byte-pair encoding (Sennrich et al., 2016), WordPiece embeddings (Wu et al., 2016) and character-level CNNs (Baevski et al., 2019). Nevertheless, Schick and Schutze(2020) recently showed that BERT’s (Devlin et al., 2019) performance on a rare word probing task can be significantly improved by explicitly learning representations of rare words using Attentive Mimicking (AM) (Schick and Sch ¨utze, 2019a). However, AM is limited in two important respects: For processing contexts, it uses a simple bag-of-words model, making poor use of the available information.', 'It combines form and context in a shallow fashion, preventing both input signals from interacting in a complex manner. These limitations apply not only to AM, but to all previous work on obtaining representations for rare words by leveraging form and context. While using bag-of-words models is a reasonable choice for static embeddings, which are often themselves bag-of-words (e.g. Mikolov et al., 2013; Bojanowski et al., 2017), it stands to reason that they are not the best choice to generate input representations for position-aware, deep language models. To overcome these limitations, we introduce BERTRAM (BERT for Attentive Mimicking), a novel architecture for learning rare word representations that combines a pretrained BERT model with AM. As shown in Figure 1, the learned rare word representations can then be used as an improved input representation for another BERT model. By giving BERTRAM access to both surface form and contexts starting at the lowest layer, a deep integration of both input signals becomes possible.', 'Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.']",intro_chunked,"Assessing the effectiveness of methods like BERTRAM in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words (e.g Luong et al., 2013; Herbelot and Baroni, 2017; Khodak et al., 2018; Liu et al., 2019a), these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task-relevant frequent words with rare synonyms obtained using semantic resources such as WordNet (Miller, 1995). We rarify three common text (or text pair) classification datasets: MNLI (Williams et al., 2018), AG’s News (Zhang et al., 2015) and DBPedia (Lehmann et al., 2015). BERTRAM outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on WNLaMPro (Schick and Sch¨utze, 2020). In summary, our contributions are as follows: We introduce BERTRAM, a model that integrates BERT into Attentive Mimicking, enabling a deep integration of surface-form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding BERTRAM to BERT achieves a new state-of-the-art on WNLaM-Pro (Schick and Sch ¨utze, 2020) and beats all baselines on rarified AG’s News, MNLI and DBPedia, resulting in an absolute improvement of up to 25% over BERT.",31.717271973466012,32.83053957444222,275,0.5890331864356995," Assessing the effectiveness of methods like Propname in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words, these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task relevant frequent words with rare synonyms obtained using semantic resources such as Propname. We rarify three common text classification datasets: Propname, AGs Propname and Propname. Propname outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on Propname. In summary, our contributions are as follows: We introduce Propname, a model that integrates Propname into Propname Propname, enabling a deep integration of surface form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding Propname to Propname achieves a new state of the art on WNLaM Propname and beats all baselines on rarified AGs Propname, Propname and Propname, resulting in an absolute improvement of up to 00 over Propname."," Assessing the effectiveness of methods like Propname in a contextualized setting is challenging: While most previous work on rare words was evaluated on datasets explicitly focusing on rare words, these datasets are tailored to uncontextualized embeddings and thus not suitable for evaluating our model. Furthermore, rare words are not well represented in commonly used downstream task datasets. We therefore introduce rarification, a procedure to automatically convert evaluation datasets into ones for which rare words are guaranteed to be important. This is achieved by replacing task relevant frequent words with rare synonyms obtained using semantic resources such as Propname. We rarify three common text classification datasets: Propname, AGs Propname and Propname. Propname outperforms previous work on four English datasets by a large margin: on the three rarified datasets and on Propname. In summary, our contributions are as follows: We introduce Propname, a model that integrates Propname into Propname Propname, enabling a deep integration of surface form and contexts and much better representations for rare words. We devise rarification, a method that transforms evaluation datasets into ones for which rare words are guaranteed to be important. We show that adding Propname to Propname achieves a new state of the art on WNLaM Propname and beats all baselines on rarified AGs Propname, Propname and Propname, resulting in an absolute improvement of up to 00 over Propname.", VERB DET NOUN ADP NOUN ADP PROPN ADP DET VERB NOUN AUX VERB PUNCT SCONJ ADV ADJ NOUN ADP ADJ NOUN AUX VERB ADP NOUN ADV VERB ADP ADJ NOUN PUNCT DET NOUN AUX VERB ADP ADJ NOUN CCONJ ADV PART ADJ ADP VERB PRON NOUN PUNCT ADV PUNCT ADJ NOUN AUX PART ADV VERB ADP ADV VERB ADJ NOUN NOUN PUNCT PRON ADV VERB NOUN PUNCT DET NOUN PART ADV VERB NOUN NOUN ADP NOUN ADP PRON ADJ NOUN AUX VERB PART AUX ADJ PUNCT PRON AUX VERB ADP VERB NOUN ADJ ADJ NOUN ADP ADJ NOUN VERB VERB ADJ NOUN ADJ ADP PROPN PUNCT PRON VERB NUM ADJ NOUN NOUN NOUN PUNCT PROPN PUNCT ADJ PROPN CCONJ PROPN PUNCT PROPN VERB ADJ NOUN ADP NUM ADJ NOUN ADP DET ADJ NOUN PUNCT ADP DET NUM VERB NOUN CCONJ ADP PROPN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT PRON VERB PROPN PUNCT DET NOUN PRON VERB PROPN ADP PROPN PROPN PUNCT VERB DET ADJ NOUN ADP NOUN NOUN CCONJ NOUN CCONJ ADV ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB NOUN PUNCT DET NOUN PRON VERB NOUN NOUN ADP NOUN ADP PRON ADJ NOUN AUX VERB PART AUX ADJ PUNCT PRON VERB SCONJ VERB PROPN ADP PROPN VERB DET ADJ NOUN ADP DET NOUN ADP ADJ PROPN CCONJ VERB DET NOUN ADP ADJ ADJ PROPN PUNCT PROPN CCONJ PROPN PUNCT VERB ADP DET ADJ NOUN ADP ADP PART NUM ADP PROPN PUNCT,0.5040650406504065,27.333333333333332,5.154471544715447
276,190,Timo Schick,"[' With pretrained language models (LMs) getting\never larger (Radford et al., 2019; Raffel et al., 2020;\nBrown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method\nfor few-shot text classification (e.g., Jiang et al.,\n2020; Schick and Schütze, 2021a,c; Brown et al.,\n2020; Wei et al., 2021; Sanh et al., 2021). The\nkey idea is to give an LM access to descriptive\nnames for all possible outputs and to short prompts\nexplaining the task to be solved. In settings where\nat most a few dozen examples are available, this\nsimple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;\nGao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong\nfew-shot performance of instruction-based approaches, arguing in particular that the considered\nsettings are often not true few-shot settings (Perez\net al., 2021; Logan IV et al., 2021) mainly for\ntwo reasons: For one, some approaches (e.g., Xie\net al., 2019; Zhang et al., 2020; Chen et al., 2020;\nTam et al., 2021) make use of large development\nsets to optimize hyperparameters. Beyond that, it\nis argued that manually designed instructions require manual tuning on development sets to achieve\nstrong performance (Perez et al., 2021; Logan IV\net al., 2021).', 'Indeed, performance can vary largely\n– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and\nSchütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh\net al., 2021). Even separate from this problem, the\nneed for human involvement is generally seen as a\nhuge drawback of manually designed instructions\n(Shin et al., 2020; Lester et al., 2021). Thus, several\nrecent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao\net al., 2021; Hambardzumyan et al., 2021; Li and\nLiang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when\ncorrectly configured, prompt-based approaches\nachieve strong performance even in true few-shot\nsettings and that there is no problem in using manually designed instructions per se. On the opposite,\nsuch instructions are often relatively easy to specify\nif one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific\nknowledge, and if properly used, they consistently\nimprove model performance in few-shot settings.', 'To provide empirical support for these claims, we\nrevisit PET (Schick and Schütze, 2021a) – a method\nfor combining instructions with example-based\nfinetuning whose key feature is that it allows users\nto specify multiple instructions for a single task\n– and thoroughly examine its performance with\nhuman-made instructions in true few-shot settings. In order to simulate a real-world scenario as best\nas possible, we proceed in two steps: First, we conduct an extensive study of PET using three English\nacademic datasets to analyze its ability to perform\ntrue few-shot learning in a controlled environment\nand to derive best practices regarding the choice of\ninstructions and other hyperparameters. We then\nput our findings to the test and evaluate PET on a\nlarge variety of different real-world tasks from the\nRAFT benchmark (Alex et al., 2021), for which no\nlabeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines\non this dataset and comes surprisingly close to the\nperformance of non-expert humans (see Figure 1),\ndemonstrating that instruction-based learning can\nsuccessfully be applied to real-world tasks in true\nfew-shot settings. In summary, the main contributions of this work\nare as follows:\n• We investigate the performance of PET for\nvarious models, tasks and training set sizes,\nits ability to cope with different instructions\nand its robustness to hyperparameter choices\nin true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant\nfor efficient classification in scenarios with\nmany different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in\ntrue few-shot settings.']",intro_chunked," With pretrained language models (LMs) getting
ever larger (Radford et al., 2019; Raffel et al., 2020;
Brown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method
for few-shot text classification (e.g., Jiang et al.,
2020; Schick and Schütze, 2021a,c; Brown et al.,
2020; Wei et al., 2021; Sanh et al., 2021). The
key idea is to give an LM access to descriptive
names for all possible outputs and to short prompts
explaining the task to be solved. In settings where
at most a few dozen examples are available, this
simple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;
Gao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong
few-shot performance of instruction-based approaches, arguing in particular that the considered
settings are often not true few-shot settings (Perez
et al., 2021; Logan IV et al., 2021) mainly for
two reasons: For one, some approaches (e.g., Xie
et al., 2019; Zhang et al., 2020; Chen et al., 2020;
Tam et al., 2021) make use of large development
sets to optimize hyperparameters. Beyond that, it
is argued that manually designed instructions require manual tuning on development sets to achieve
strong performance (Perez et al., 2021; Logan IV
et al., 2021).",42.35366666666671,32.83053957444222,276,0.4580135643482208," With pretrained language models getting 
 ever larger Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, instructionbased learning has emerged as a powerful method 
 for few shot text classification Propname, Propname Propname Propname Propname, 
 0000; Propname and Propname, 0000a, Propname; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. The 
 key idea is to give an Propname access to descriptive 
 names for all possible outputs and to short prompts 
 explaining the task to be solved. In settings where 
 at most a few dozen examples are available, this 
 simple idea leads to substantial improvements over various baselines Propname and Propname, 0000a, Propname; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. However, recent work has questioned the strong 
 few shot performance of instruction based approaches, arguing in particular that the considered 
 settings are often not true few shot settings Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname Propname, 0000 mainly for 
 two reasons: For one, some approaches Propname, Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000 make use of large development 
 sets to optimize hyperparameters. Beyond that, it 
 is argued that manually designed instructions require manual tuning on development sets to achieve 
 strong performance Propname Propname Propname Propname, 0000; Propname Propname 
 Propname Propname Propname, 0000."," With pretrained language models getting 
 ever larger Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, instructionbased learning has emerged as a powerful method 
 for few shot text classification Propname, Propname Propname Propname Propname, 
 0000; Propname and Propname, 0000a, Propname; Propname Propname Propname Propname, 
 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. The 
 key idea is to give an Propname access to descriptive 
 names for all possible outputs and to short prompts 
 explaining the task to be solved. In settings where 
 at most a few dozen examples are available, this 
 simple idea leads to substantial improvements over various baselines Propname and Propname, 0000a, Propname; 
 Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000. However, recent work has questioned the strong 
 few shot performance of instruction based approaches, arguing in particular that the considered 
 settings are often not true few shot settings Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname Propname, 0000 mainly for 
 two reasons: For one, some approaches Propname, Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000 make use of large development 
 sets to optimize hyperparameters. Beyond that, it 
 is argued that manually designed instructions require manual tuning on development sets to achieve 
 strong performance Propname Propname Propname Propname, 0000; Propname Propname 
 Propname Propname Propname, 0000.", ADP VERB NOUN NOUN VERB SPACE ADV ADJ PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT VERB NOUN AUX VERB ADP DET ADJ NOUN SPACE ADP ADJ NOUN NOUN NOUN PROPN PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PUNCT PROPN PROPN PROPN PROPN PUNCT SPACE NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT DET SPACE ADJ NOUN AUX PART VERB DET PROPN NOUN ADP ADJ SPACE NOUN ADP DET ADJ NOUN CCONJ ADP ADJ NOUN SPACE VERB DET NOUN PART AUX VERB PUNCT ADP NOUN SCONJ SPACE ADP ADJ DET ADJ NOUN NOUN AUX ADJ PUNCT DET SPACE ADJ NOUN VERB ADP ADJ NOUN ADP ADJ NOUN PROPN CCONJ PROPN PUNCT NUM PUNCT PROPN PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADV PUNCT ADJ NOUN AUX VERB DET ADJ SPACE ADJ NOUN NOUN ADP NOUN VERB NOUN PUNCT VERB ADP ADJ SCONJ DET VERB SPACE NOUN AUX ADV PART ADJ ADJ NOUN NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PROPN PUNCT NUM ADV ADP SPACE NUM NOUN PUNCT ADP NUM PUNCT PRON NOUN PROPN PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM VERB NOUN ADP ADJ NOUN SPACE NOUN PART VERB NOUN PUNCT ADP PRON PUNCT PRON SPACE AUX VERB SCONJ ADV VERB NOUN VERB ADJ NOUN ADP NOUN NOUN PART VERB SPACE ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT,0.37543859649122807,57.0,5.315789473684211
277,191,Timo Schick,"[' With pretrained language models (LMs) getting\never larger (Radford et al., 2019; Raffel et al., 2020;\nBrown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method\nfor few-shot text classification (e.g., Jiang et al.,\n2020; Schick and Schütze, 2021a,c; Brown et al.,\n2020; Wei et al., 2021; Sanh et al., 2021). The\nkey idea is to give an LM access to descriptive\nnames for all possible outputs and to short prompts\nexplaining the task to be solved. In settings where\nat most a few dozen examples are available, this\nsimple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;\nGao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong\nfew-shot performance of instruction-based approaches, arguing in particular that the considered\nsettings are often not true few-shot settings (Perez\net al., 2021; Logan IV et al., 2021) mainly for\ntwo reasons: For one, some approaches (e.g., Xie\net al., 2019; Zhang et al., 2020; Chen et al., 2020;\nTam et al., 2021) make use of large development\nsets to optimize hyperparameters. Beyond that, it\nis argued that manually designed instructions require manual tuning on development sets to achieve\nstrong performance (Perez et al., 2021; Logan IV\net al., 2021).', 'Indeed, performance can vary largely\n– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and\nSchütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh\net al., 2021). Even separate from this problem, the\nneed for human involvement is generally seen as a\nhuge drawback of manually designed instructions\n(Shin et al., 2020; Lester et al., 2021). Thus, several\nrecent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao\net al., 2021; Hambardzumyan et al., 2021; Li and\nLiang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when\ncorrectly configured, prompt-based approaches\nachieve strong performance even in true few-shot\nsettings and that there is no problem in using manually designed instructions per se. On the opposite,\nsuch instructions are often relatively easy to specify\nif one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific\nknowledge, and if properly used, they consistently\nimprove model performance in few-shot settings.', 'To provide empirical support for these claims, we\nrevisit PET (Schick and Schütze, 2021a) – a method\nfor combining instructions with example-based\nfinetuning whose key feature is that it allows users\nto specify multiple instructions for a single task\n– and thoroughly examine its performance with\nhuman-made instructions in true few-shot settings. In order to simulate a real-world scenario as best\nas possible, we proceed in two steps: First, we conduct an extensive study of PET using three English\nacademic datasets to analyze its ability to perform\ntrue few-shot learning in a controlled environment\nand to derive best practices regarding the choice of\ninstructions and other hyperparameters. We then\nput our findings to the test and evaluate PET on a\nlarge variety of different real-world tasks from the\nRAFT benchmark (Alex et al., 2021), for which no\nlabeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines\non this dataset and comes surprisingly close to the\nperformance of non-expert humans (see Figure 1),\ndemonstrating that instruction-based learning can\nsuccessfully be applied to real-world tasks in true\nfew-shot settings. In summary, the main contributions of this work\nare as follows:\n• We investigate the performance of PET for\nvarious models, tasks and training set sizes,\nits ability to cope with different instructions\nand its robustness to hyperparameter choices\nin true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant\nfor efficient classification in scenarios with\nmany different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in\ntrue few-shot settings.']",intro_chunked,"Indeed, performance can vary largely
– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and
Schütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh
et al., 2021). Even separate from this problem, the
need for human involvement is generally seen as a
huge drawback of manually designed instructions
(Shin et al., 2020; Lester et al., 2021). Thus, several
recent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao
et al., 2021; Hambardzumyan et al., 2021; Li and
Liang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when
correctly configured, prompt-based approaches
achieve strong performance even in true few-shot
settings and that there is no problem in using manually designed instructions per se. On the opposite,
such instructions are often relatively easy to specify
if one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific
knowledge, and if properly used, they consistently
improve model performance in few-shot settings.",30.747909090909104,32.83053957444222,277,0.3480371832847595," Indeed, performance can vary largely and in mostly unpredictable ways across different instructions Propname Propname Propname Propname, 0000; Propname and 
 Propname, 0000a; this issue even persists after finetuning a model on hundreds of instructions Sanh 
 Propname Propname Propname, 0000. Even separate from this problem, the 
 need for human involvement is generally seen as a 
 huge drawback of manually designed instructions 
. Thus, several 
 recent works abandon them in favor of automatically generated prompts Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and 
 Propname, 0000; Propname Propname Propname Propname, 0000. Contrary to this trend, we argue that when 
 correctly configured, prompt based approaches 
 achieve strong performance even in true few shot 
 settings and that there is no problem in using manually designed instructions per se. On the opposite, 
 such instructions are often relatively easy to specify 
 if one is familiar with the task to be solved, they provide an intuitive interface to convey task specific 
 knowledge, and if properly used, they consistently 
 improve model performance in few shot settings."," Indeed, performance can vary largely and in mostly unpredictable ways across different instructions Propname Propname Propname Propname, 0000; Propname and 
 Propname, 0000a; this issue even persists after finetuning a model on hundreds of instructions Sanh 
 Propname Propname Propname, 0000. Even separate from this problem, the 
 need for human involvement is generally seen as a 
 huge drawback of manually designed instructions 
. Thus, several 
 recent works abandon them in favor of automatically generated prompts Propname Propname Propname Propname, 0000; Propname 
 Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname and 
 Propname, 0000; Propname Propname Propname Propname, 0000. Contrary to this trend, we argue that when 
 correctly configured, prompt based approaches 
 achieve strong performance even in true few shot 
 settings and that there is no problem in using manually designed instructions per se. On the opposite, 
 such instructions are often relatively easy to specify 
 if one is familiar with the task to be solved, they provide an intuitive interface to convey task specific 
 knowledge, and if properly used, they consistently 
 improve model performance in few shot settings.", ADV PUNCT NOUN AUX VERB ADV CCONJ ADP ADV ADJ NOUN ADP ADJ NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ SPACE PROPN PUNCT NUM PUNCT DET NOUN ADV VERB ADP VERB DET NOUN ADP NOUN ADP NOUN VERB SPACE PROPN PROPN PROPN PUNCT NUM PUNCT ADV ADJ ADP DET NOUN PUNCT DET SPACE NOUN ADP ADJ NOUN AUX ADV VERB ADP DET SPACE ADJ NOUN ADP ADV VERB NOUN SPACE PUNCT ADV PUNCT ADJ SPACE ADJ NOUN VERB PRON ADP NOUN ADP ADV VERB NOUN PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN SPACE PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ SPACE PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADJ ADP DET NOUN PUNCT PRON VERB SCONJ SCONJ SPACE ADV VERB PUNCT ADJ VERB NOUN SPACE VERB ADJ NOUN ADV ADP ADJ ADJ NOUN SPACE NOUN CCONJ SCONJ PRON VERB DET NOUN ADP VERB ADV VERB NOUN X X PUNCT ADP DET ADJ PUNCT SPACE ADJ NOUN AUX ADV ADV ADJ PART VERB SPACE SCONJ NUM AUX ADJ ADP DET NOUN PART AUX VERB PUNCT PRON VERB DET ADJ NOUN PART VERB NOUN ADJ SPACE NOUN PUNCT CCONJ SCONJ ADV VERB PUNCT PRON ADV SPACE VERB NOUN NOUN ADP ADJ NOUN NOUN PUNCT,0.5320197044334976,40.6,5.113300492610837
278,192,Timo Schick,"[' With pretrained language models (LMs) getting\never larger (Radford et al., 2019; Raffel et al., 2020;\nBrown et al., 2020; Fedus et al., 2021), instructionbased learning has emerged as a powerful method\nfor few-shot text classification (e.g., Jiang et al.,\n2020; Schick and Schütze, 2021a,c; Brown et al.,\n2020; Wei et al., 2021; Sanh et al., 2021). The\nkey idea is to give an LM access to descriptive\nnames for all possible outputs and to short prompts\nexplaining the task to be solved. In settings where\nat most a few dozen examples are available, this\nsimple idea leads to substantial improvements over various baselines (Schick and Schütze, 2021a,c;\nGao et al., 2021; Tam et al., 2021). However, recent work has questioned the strong\nfew-shot performance of instruction-based approaches, arguing in particular that the considered\nsettings are often not true few-shot settings (Perez\net al., 2021; Logan IV et al., 2021) mainly for\ntwo reasons: For one, some approaches (e.g., Xie\net al., 2019; Zhang et al., 2020; Chen et al., 2020;\nTam et al., 2021) make use of large development\nsets to optimize hyperparameters. Beyond that, it\nis argued that manually designed instructions require manual tuning on development sets to achieve\nstrong performance (Perez et al., 2021; Logan IV\net al., 2021).', 'Indeed, performance can vary largely\n– and in mostly unpredictable ways – across different instructions (Jiang et al., 2020; Schick and\nSchütze, 2021a); this issue even persists after finetuning a model on hundreds of instructions (Sanh\net al., 2021). Even separate from this problem, the\nneed for human involvement is generally seen as a\nhuge drawback of manually designed instructions\n(Shin et al., 2020; Lester et al., 2021). Thus, several\nrecent works abandon them in favor of automatically generated prompts (Shin et al., 2020; Gao\net al., 2021; Hambardzumyan et al., 2021; Li and\nLiang, 2021; Lester et al., 2021). Contrary to this trend, we argue that when\ncorrectly configured, prompt-based approaches\nachieve strong performance even in true few-shot\nsettings and that there is no problem in using manually designed instructions per se. On the opposite,\nsuch instructions are often relatively easy to specify\nif one is familiar with the task to be solved, they provide an intuitive interface to convey task-specific\nknowledge, and if properly used, they consistently\nimprove model performance in few-shot settings.', 'To provide empirical support for these claims, we\nrevisit PET (Schick and Schütze, 2021a) – a method\nfor combining instructions with example-based\nfinetuning whose key feature is that it allows users\nto specify multiple instructions for a single task\n– and thoroughly examine its performance with\nhuman-made instructions in true few-shot settings. In order to simulate a real-world scenario as best\nas possible, we proceed in two steps: First, we conduct an extensive study of PET using three English\nacademic datasets to analyze its ability to perform\ntrue few-shot learning in a controlled environment\nand to derive best practices regarding the choice of\ninstructions and other hyperparameters. We then\nput our findings to the test and evaluate PET on a\nlarge variety of different real-world tasks from the\nRAFT benchmark (Alex et al., 2021), for which no\nlabeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines\non this dataset and comes surprisingly close to the\nperformance of non-expert humans (see Figure 1),\ndemonstrating that instruction-based learning can\nsuccessfully be applied to real-world tasks in true\nfew-shot settings. In summary, the main contributions of this work\nare as follows:\n• We investigate the performance of PET for\nvarious models, tasks and training set sizes,\nits ability to cope with different instructions\nand its robustness to hyperparameter choices\nin true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant\nfor efficient classification in scenarios with\nmany different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in\ntrue few-shot settings.']",intro_chunked,"To provide empirical support for these claims, we
revisit PET (Schick and Schütze, 2021a) – a method
for combining instructions with example-based
finetuning whose key feature is that it allows users
to specify multiple instructions for a single task
– and thoroughly examine its performance with
human-made instructions in true few-shot settings. In order to simulate a real-world scenario as best
as possible, we proceed in two steps: First, we conduct an extensive study of PET using three English
academic datasets to analyze its ability to perform
true few-shot learning in a controlled environment
and to derive best practices regarding the choice of
instructions and other hyperparameters. We then
put our findings to the test and evaluate PET on a
large variety of different real-world tasks from the
RAFT benchmark (Alex et al., 2021), for which no
labeled development or test sets are available, enforcing a true few-shot setting (Perez et al., 2021). On average, PET clearly outperforms all baselines
on this dataset and comes surprisingly close to the
performance of non-expert humans (see Figure 1),
demonstrating that instruction-based learning can
successfully be applied to real-world tasks in true
few-shot settings. In summary, the main contributions of this work
are as follows:
• We investigate the performance of PET for
various models, tasks and training set sizes,
its ability to cope with different instructions
and its robustness to hyperparameter choices
in true few-shot settings. • We show how PET can be used when no unlabeled data is available and propose a variant
for efficient classification in scenarios with
many different classes. • We apply PET to RAFT (Alex et al., 2021),a benchmark of real-world tasks where it obtains a new state of the art and achieves nearhuman performance for 7 out of 11 tasks in
true few-shot settings.",21.725484949832804,32.83053957444222,278,0.49597349762916565," To provide empirical support for these claims, we 
 revisit Propname a method 
 for combining instructions with example based 
 finetuning whose key feature is that it allows users 
 to specify multiple instructions for a single task and thoroughly examine its performance with 
 human made instructions in true few shot settings. In order to simulate a real world scenario as best 
 as possible, we proceed in two steps: First, we conduct an extensive study of Propname using three English 
 academic datasets to analyze its ability to perform 
 true few shot learning in a controlled environment 
 and to derive best practices regarding the choice of 
 instructions and other hyperparameters. We then 
 put our findings to the test and evaluate Propname on a 
 large variety of different real world tasks from the 
 Propname benchmark, for which no 
 labeled development or test sets are available, enforcing a true few shot setting. On average, Propname clearly outperforms all baselines 
 on this dataset and comes surprisingly close to the 
 performance of non expert humans, 
 demonstrating that instruction based learning can 
 successfully be applied to real world tasks in true 
 few shot settings. In summary, the main contributions of this work 
 are as follows: We investigate the performance of Propname for 
 various models, tasks and training set sizes, 
 its ability to cope with different instructions 
 and its robustness to hyperparameter choices 
 in true few shot settings. We show how Propname can be used when no unlabeled data is available and propose a variant 
 for efficient classification in scenarios with 
 many different classes. We apply Propname to Propname, a benchmark of real world tasks where it obtains a new state of the art and achieves nearhuman performance for 0 out of 00 tasks in 
 true few shot settings."," To provide empirical support for these claims, we 
 revisit Propname a method 
 for combining instructions with example based 
 finetuning whose key feature is that it allows users 
 to specify multiple instructions for a single task and thoroughly examine its performance with 
 human made instructions in true few shot settings. In order to simulate a real world scenario as best 
 as possible, we proceed in two steps: First, we conduct an extensive study of Propname using three English 
 academic datasets to analyze its ability to perform 
 true few shot learning in a controlled environment 
 and to derive best practices regarding the choice of 
 instructions and other hyperparameters. We then 
 put our findings to the test and evaluate Propname on a 
 large variety of different real world tasks from the 
 Propname benchmark, for which no 
 labeled development or test sets are available, enforcing a true few shot setting. On average, Propname clearly outperforms all baselines 
 on this dataset and comes surprisingly close to the 
 performance of non expert humans, 
 demonstrating that instruction based learning can 
 successfully be applied to real world tasks in true 
 few shot settings. In summary, the main contributions of this work 
 are as follows: We investigate the performance of Propname for 
 various models, tasks and training set sizes, 
 its ability to cope with different instructions 
 and its robustness to hyperparameter choices 
 in true few shot settings. We show how Propname can be used when no unlabeled data is available and propose a variant 
 for efficient classification in scenarios with 
 many different classes. We apply Propname to Propname, a benchmark of real world tasks where it obtains a new state of the art and achieves nearhuman performance for 0 out of 00 tasks in 
 true few shot settings.", PART VERB ADJ NOUN ADP DET NOUN PUNCT PRON SPACE VERB PROPN DET NOUN SPACE ADP VERB NOUN ADP NOUN VERB SPACE VERB DET ADJ NOUN AUX SCONJ PRON VERB NOUN SPACE PART VERB ADJ NOUN ADP DET ADJ NOUN CCONJ ADV VERB PRON NOUN ADP SPACE ADJ VERB NOUN ADP ADJ ADJ NOUN NOUN PUNCT ADP NOUN PART VERB DET ADJ NOUN NOUN ADV ADV SPACE ADP ADJ PUNCT PRON VERB ADP NUM NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADP PROPN VERB NUM ADJ SPACE ADJ NOUN PART VERB PRON NOUN PART VERB SPACE ADJ ADJ NOUN NOUN ADP DET VERB NOUN SPACE CCONJ PART VERB ADJ NOUN VERB DET NOUN ADP SPACE NOUN CCONJ ADJ NOUN PUNCT PRON ADV SPACE VERB PRON NOUN ADP DET NOUN CCONJ VERB PROPN ADP DET SPACE ADJ NOUN ADP ADJ ADJ NOUN NOUN ADP DET SPACE PROPN NOUN PUNCT ADP PRON DET SPACE VERB NOUN CCONJ NOUN NOUN AUX ADJ PUNCT VERB DET ADJ ADJ NOUN NOUN PUNCT ADP ADJ PUNCT PROPN ADV VERB DET NOUN SPACE ADP DET NOUN CCONJ VERB ADV ADJ ADP DET SPACE NOUN ADP ADJ ADJ NOUN PUNCT SPACE VERB DET NOUN VERB NOUN AUX SPACE ADV AUX VERB ADP ADJ NOUN NOUN ADP ADJ SPACE ADJ NOUN NOUN PUNCT ADP NOUN PUNCT DET ADJ NOUN ADP DET NOUN SPACE AUX SCONJ VERB PUNCT PRON VERB DET NOUN ADP PROPN ADP SPACE ADJ NOUN PUNCT NOUN CCONJ NOUN VERB NOUN PUNCT SPACE PRON NOUN PART VERB ADP ADJ NOUN SPACE CCONJ PRON NOUN ADP NOUN NOUN SPACE ADP ADJ ADJ NOUN NOUN PUNCT PRON VERB SCONJ PROPN AUX AUX VERB SCONJ DET ADJ NOUN AUX ADJ CCONJ VERB DET NOUN SPACE ADP ADJ NOUN ADP NOUN ADP SPACE ADJ ADJ NOUN PUNCT PRON VERB PROPN ADP PROPN PUNCT DET NOUN ADP ADJ NOUN NOUN SCONJ PRON VERB DET ADJ NOUN ADP DET NOUN CCONJ VERB NOUN NOUN ADP NUM ADP ADP NUM NOUN ADP SPACE ADJ ADJ NOUN NOUN PUNCT,0.5339805825242718,44.142857142857146,4.877022653721683
279,193,Timo Schick,"[' Distributed word representations (or embeddings) are a\nfoundational aspect of many natural language processing\nsystems; they have successfully been used for a wide vari-\nety of different tasks (Goldberg 2016). The idea behind em-\nbeddings is to assign to each word a low-dimensional, real-\nvalued vector representing its meaning. In particular, neural\nnetwork based approaches such as the skipgram and cbow\nmodels introduced by Mikolov et al. (2013) have gained in-\ncreasing popularity over the last few years. Despite their success, an important problem with current\napproaches to learning embeddings is that they require many\nobservations of a word for its embedding to become reliable;\nas a consequence, they struggle with small corpora and in-\nfrequent words (Ataman and Federico 2018). Furthermore,\nas models are typically trained with a fixed vocabulary, they\nlack the ability to assign vectors to novel, out-of-vocabulary\n(OOV) words once training is complete. n recent times, several ways have been proposed to over-\ncome these limitations and to extend word embedding mod-\nels with the ability to obtain representations of previously\nunseen words on the fly. These approaches can roughly be\ndivided into two directions: (i) the usage of subword in-\nformation, i.e., exploiting information that can be extracted\nfrom the surface-form of the word and (ii) the usage of\ncontext information.', 'The first direction aims to obtain good\nembeddings for novel words by looking at their characters\n(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-\ndou et al. 2013; Luong, Socher, and Manning 2013; Cot-\nterell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;\nSalle and Villavicencio 2018). Naturally, this direction is\nespecially well-suited for languages with rich morphology\n(Gerz et al. 2018). The second, context-based direction\ntries to infer embeddings for novel words from the words\nsurrounding them (Lazaridou, Marelli, and Baroni 2017;\nHerbelot and Baroni 2017; Khodak et al. 2018). Both direc-\ntions show promising results on various benchmarks. How-\never, for both purely surface-form-based and purely context-\nbased approaches, there are many cases in which they are\nhighly unlikely to succeed in obtaining meaningful embed-\ndings. As an example, suppose that we encounter the fol-\nlowing three words – highlighted in bold letters – as novel\nwords in the given contexts:\n(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons\nup the front. (3) Unlike the grapefruit, the pomelo has very little impor-\ntance in the marketplace.', 'In sentence (1), the context is of almost no help for de-\ntermining the meaning of the novel word, but we can de-\nduce its meaning without great difficulty from an analy-\nsis of the morphemes “un”, “employ” and “able”. For sen-\ntence (2), the reverse is true: While the novel word’s mor-\nphemes give no indication that it is a piece of clothing, this\ninformation can easily be derived from the context in which\nit occurs. Perhaps most interesting is sentence (3): Both the\nclose occurrence of the word “grapefruit” and the fact that\nthe novel word’s morphemes resemble words like “pome”,\n“pomegranate” and “melon” are indicative of the fact that it\nmay be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a\npretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an\napproach to cover a wide range of novel words, it is essen-\ntial to make use of all available information. In this work,\nwe therefore propose an architecture that, given a new word,\ncaptures both its subword structure and all available context\ninformation and combines them to obtain a high-quality em-\nbedding.', 'To this end, we first infer two distinct embeddings,\none incorporating the word’s inner structure and one captur-\ning its context, and then combine them into a unified word\nembedding. Importantly, both embeddings and their compo-\nsition function are learned jointly, allowing each embedding\nto rely on its counterpart whenever its available informa-\ntion is not sufficient. In a similar fashion to work by Pinter,\nGuthrie, and Eisenstein (2017) and Khodak et al. (2018),\nour approach is not trained from scratch, but instead makes\nuse of preexisting word embeddings and aims to reconstruct\nthese embeddings. This allows for a much faster learning\nprocess and enables us to easily combine our approach with\nany existing word embedding model, regardless of its inter-\nnal structure. Our approach is able to generate embeddings for OOV\nwords even from only a single observation with high accu-\nracy in many cases and outperforms previous work on the\nDefinitional Nonce dataset (Herbelot and Baroni 2017) and\nthe Contextual Rare Words dataset (Khodak et al. 2018). To\nthe best of our knowledge, this is the first work that jointly\nuses surface-form and context information to obtain repre-\nsentations for novel words. In summary, our contributions are as follows:\nWe propose a new model for learning embeddings for\nnovel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –\nwhich only used one of these two sources of information\n– by a large margin. Our model is designed in a way which allows it to eas-\nily be integrated into existing systems. It therefore has\nthe potential to enhance the capability of any NLP sys-\ntem that uses distributed word representations to handle\nnovel words.']",intro_chunked," Distributed word representations (or embeddings) are a
foundational aspect of many natural language processing
systems; they have successfully been used for a wide vari-
ety of different tasks (Goldberg 2016). The idea behind em-
beddings is to assign to each word a low-dimensional, real-
valued vector representing its meaning. In particular, neural
network based approaches such as the skipgram and cbow
models introduced by Mikolov et al. (2013) have gained in-
creasing popularity over the last few years. Despite their success, an important problem with current
approaches to learning embeddings is that they require many
observations of a word for its embedding to become reliable;
as a consequence, they struggle with small corpora and in-
frequent words (Ataman and Federico 2018). Furthermore,
as models are typically trained with a fixed vocabulary, they
lack the ability to assign vectors to novel, out-of-vocabulary
(OOV) words once training is complete. n recent times, several ways have been proposed to over-
come these limitations and to extend word embedding mod-
els with the ability to obtain representations of previously
unseen words on the fly. These approaches can roughly be
divided into two directions: (i) the usage of subword in-
formation, i.e., exploiting information that can be extracted
from the surface-form of the word and (ii) the usage of
context information.",29.596212557603707,32.83053957444222,279,0.3226972818374634," Distributed word representations are a 
 foundational aspect of many natural language processing 
 systems; they have successfully been used for a wide vari ety of different tasks. The idea behind em beddings is to assign to each word a low dimensional, real valued vector representing its meaning. In particular, neural 
 network based approaches such as the skipgram and cbow 
 models introduced by Propname Propname Propname. have gained in creasing popularity over the last few years. Despite their success, an important problem with current 
 approaches to learning embeddings is that they require many 
 observations of a word for its embedding to become reliable; 
 as a consequence, they struggle with small Propname and in frequent words. Furthermore, 
 as models are typically trained with a fixed vocabulary, they 
 lack the ability to assign vectors to novel, out of vocabulary words once training is complete. n recent times, several ways have been proposed to over come these limitations and to extend word embedding mod els with the ability to obtain representations of previously 
 unseen words on the fly. These approaches can roughly be 
 divided into two directions: the usage of Propname in formation, ie, exploiting information that can be extracted 
 from the surface form of the word and the usage of 
 context information."," Distributed word representations are a 
 foundational aspect of many natural language processing 
 systems; they have successfully been used for a wide vari ety of different tasks. The idea behind em beddings is to assign to each word a low dimensional, real valued vector representing its meaning. In particular, neural 
 network based approaches such as the skipgram and cbow 
 models introduced by Propname Propname Propname. have gained in creasing popularity over the last few years. Despite their success, an important problem with current 
 approaches to learning embeddings is that they require many 
 observations of a word for its embedding to become reliable; 
 as a consequence, they struggle with small Propname and in frequent words. Furthermore, 
 as models are typically trained with a fixed vocabulary, they 
 lack the ability to assign vectors to novel, out of vocabulary words once training is complete. n recent times, several ways have been proposed to over come these limitations and to extend word embedding mod els with the ability to obtain representations of previously 
 unseen words on the fly. These approaches can roughly be 
 divided into two directions: the usage of Propname in formation, ie, exploiting information that can be extracted 
 from the surface form of the word and the usage of 
 context information.", VERB NOUN NOUN AUX DET SPACE ADJ NOUN ADP ADJ ADJ NOUN NOUN SPACE NOUN PUNCT PRON AUX ADV AUX VERB ADP DET ADJ NOUN NOUN ADP ADJ NOUN PUNCT DET NOUN ADP PRON NOUN AUX PART VERB ADP DET NOUN DET ADJ ADJ PUNCT ADV VERB NOUN VERB PRON NOUN PUNCT ADP ADJ PUNCT ADJ SPACE NOUN VERB NOUN ADJ ADP DET NOUN CCONJ VERB SPACE NOUN VERB ADP PROPN PROPN PROPN PUNCT AUX VERB ADP VERB NOUN ADP DET ADJ ADJ NOUN PUNCT SCONJ PRON NOUN PUNCT DET ADJ NOUN ADP ADJ SPACE NOUN ADP VERB NOUN AUX SCONJ PRON VERB ADJ SPACE NOUN ADP DET NOUN ADP PRON VERB PART VERB ADJ PUNCT SPACE ADP DET NOUN PUNCT PRON VERB ADP ADJ PROPN CCONJ ADP ADJ NOUN PUNCT ADV PUNCT SPACE SCONJ NOUN AUX ADV VERB ADP DET VERB NOUN PUNCT PRON SPACE VERB DET NOUN PART VERB NOUN PART VERB PUNCT ADP ADP ADJ NOUN SCONJ NOUN AUX ADJ PUNCT CCONJ ADJ NOUN PUNCT ADJ NOUN AUX AUX VERB PART ADV VERB DET NOUN CCONJ PART VERB NOUN VERB ADJ NOUN ADP DET NOUN PART VERB NOUN ADP ADV SPACE ADJ NOUN ADP DET NOUN PUNCT DET NOUN AUX ADV AUX SPACE VERB ADP NUM NOUN PUNCT DET NOUN ADP PROPN ADP NOUN PUNCT ADV PUNCT VERB NOUN PRON AUX AUX VERB SPACE ADP DET NOUN NOUN ADP DET NOUN CCONJ DET NOUN ADP SPACE NOUN NOUN PUNCT,0.6026200873362445,28.625,4.860262008733624
280,194,Timo Schick,"[' Distributed word representations (or embeddings) are a\nfoundational aspect of many natural language processing\nsystems; they have successfully been used for a wide vari-\nety of different tasks (Goldberg 2016). The idea behind em-\nbeddings is to assign to each word a low-dimensional, real-\nvalued vector representing its meaning. In particular, neural\nnetwork based approaches such as the skipgram and cbow\nmodels introduced by Mikolov et al. (2013) have gained in-\ncreasing popularity over the last few years. Despite their success, an important problem with current\napproaches to learning embeddings is that they require many\nobservations of a word for its embedding to become reliable;\nas a consequence, they struggle with small corpora and in-\nfrequent words (Ataman and Federico 2018). Furthermore,\nas models are typically trained with a fixed vocabulary, they\nlack the ability to assign vectors to novel, out-of-vocabulary\n(OOV) words once training is complete. n recent times, several ways have been proposed to over-\ncome these limitations and to extend word embedding mod-\nels with the ability to obtain representations of previously\nunseen words on the fly. These approaches can roughly be\ndivided into two directions: (i) the usage of subword in-\nformation, i.e., exploiting information that can be extracted\nfrom the surface-form of the word and (ii) the usage of\ncontext information.', 'The first direction aims to obtain good\nembeddings for novel words by looking at their characters\n(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-\ndou et al. 2013; Luong, Socher, and Manning 2013; Cot-\nterell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;\nSalle and Villavicencio 2018). Naturally, this direction is\nespecially well-suited for languages with rich morphology\n(Gerz et al. 2018). The second, context-based direction\ntries to infer embeddings for novel words from the words\nsurrounding them (Lazaridou, Marelli, and Baroni 2017;\nHerbelot and Baroni 2017; Khodak et al. 2018). Both direc-\ntions show promising results on various benchmarks. How-\never, for both purely surface-form-based and purely context-\nbased approaches, there are many cases in which they are\nhighly unlikely to succeed in obtaining meaningful embed-\ndings. As an example, suppose that we encounter the fol-\nlowing three words – highlighted in bold letters – as novel\nwords in the given contexts:\n(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons\nup the front. (3) Unlike the grapefruit, the pomelo has very little impor-\ntance in the marketplace.', 'In sentence (1), the context is of almost no help for de-\ntermining the meaning of the novel word, but we can de-\nduce its meaning without great difficulty from an analy-\nsis of the morphemes “un”, “employ” and “able”. For sen-\ntence (2), the reverse is true: While the novel word’s mor-\nphemes give no indication that it is a piece of clothing, this\ninformation can easily be derived from the context in which\nit occurs. Perhaps most interesting is sentence (3): Both the\nclose occurrence of the word “grapefruit” and the fact that\nthe novel word’s morphemes resemble words like “pome”,\n“pomegranate” and “melon” are indicative of the fact that it\nmay be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a\npretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an\napproach to cover a wide range of novel words, it is essen-\ntial to make use of all available information. In this work,\nwe therefore propose an architecture that, given a new word,\ncaptures both its subword structure and all available context\ninformation and combines them to obtain a high-quality em-\nbedding.', 'To this end, we first infer two distinct embeddings,\none incorporating the word’s inner structure and one captur-\ning its context, and then combine them into a unified word\nembedding. Importantly, both embeddings and their compo-\nsition function are learned jointly, allowing each embedding\nto rely on its counterpart whenever its available informa-\ntion is not sufficient. In a similar fashion to work by Pinter,\nGuthrie, and Eisenstein (2017) and Khodak et al. (2018),\nour approach is not trained from scratch, but instead makes\nuse of preexisting word embeddings and aims to reconstruct\nthese embeddings. This allows for a much faster learning\nprocess and enables us to easily combine our approach with\nany existing word embedding model, regardless of its inter-\nnal structure. Our approach is able to generate embeddings for OOV\nwords even from only a single observation with high accu-\nracy in many cases and outperforms previous work on the\nDefinitional Nonce dataset (Herbelot and Baroni 2017) and\nthe Contextual Rare Words dataset (Khodak et al. 2018). To\nthe best of our knowledge, this is the first work that jointly\nuses surface-form and context information to obtain repre-\nsentations for novel words. In summary, our contributions are as follows:\nWe propose a new model for learning embeddings for\nnovel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –\nwhich only used one of these two sources of information\n– by a large margin. Our model is designed in a way which allows it to eas-\nily be integrated into existing systems. It therefore has\nthe potential to enhance the capability of any NLP sys-\ntem that uses distributed word representations to handle\nnovel words.']",intro_chunked,"The first direction aims to obtain good
embeddings for novel words by looking at their characters
(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-
dou et al. 2013; Luong, Socher, and Manning 2013; Cot-
terell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;
Salle and Villavicencio 2018). Naturally, this direction is
especially well-suited for languages with rich morphology
(Gerz et al. 2018). The second, context-based direction
tries to infer embeddings for novel words from the words
surrounding them (Lazaridou, Marelli, and Baroni 2017;
Herbelot and Baroni 2017; Khodak et al. 2018). Both direc-
tions show promising results on various benchmarks. How-
ever, for both purely surface-form-based and purely context-
based approaches, there are many cases in which they are
highly unlikely to succeed in obtaining meaningful embed-
dings. As an example, suppose that we encounter the fol-
lowing three words – highlighted in bold letters – as novel
words in the given contexts:
(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons
up the front. (3) Unlike the grapefruit, the pomelo has very little impor-
tance in the marketplace.",54.949600152322944,32.83053957444222,280,0.5363441705703735," The first direction aims to obtain good 
 embeddings for novel words by looking at their characters 
, morphemes Propname dou Propname Propname. 0000; Propname, Propname, and Propname 0000; Propname terell, Propname, and Eisner 0000 or n grams Propname Propname Propname. 0000; Propname Propname Propname. 0000; Propname and Propname 0000; 
 Propname and Propname 0000. Naturally, this direction is 
 especially well suited for languages with rich morphology 
. The second, context based direction 
 tries to infer embeddings for novel words from the words 
 surrounding them Propname, Propname, and Propname 0000; 
 Propname and Propname 0000; Propname Propname Propname. 0000. Both direc tions show promising results on various benchmarks. How ever, for both purely surface form based and purely context based approaches, there are many cases in which they are 
 highly unlikely to succeed in obtaining meaningful embed dings. As an example, suppose that we encounter the fol lowing three words highlighted in bold letters as novel 
 words in the given contexts: We should write no one off as being unemployable. A cardigan is a knitted jacket or sweater with buttons 
 up the front. Unlike the grapefruit, the Propname has very little impor tance in the marketplace."," The first direction aims to obtain good 
 embeddings for novel words by looking at their characters 
, morphemes Propname dou Propname Propname. 0000; Propname, Propname, and Propname 0000; Propname terell, Propname, and Eisner 0000 or n grams Propname Propname Propname. 0000; Propname Propname Propname. 0000; Propname and Propname 0000; 
 Propname and Propname 0000. Naturally, this direction is 
 especially well suited for languages with rich morphology 
. The second, context based direction 
 tries to infer embeddings for novel words from the words 
 surrounding them Propname, Propname, and Propname 0000; 
 Propname and Propname 0000; Propname Propname Propname. 0000. Both direc tions show promising results on various benchmarks. How ever, for both purely surface form based and purely context based approaches, there are many cases in which they are 
 highly unlikely to succeed in obtaining meaningful embed dings. As an example, suppose that we encounter the fol lowing three words highlighted in bold letters as novel 
 words in the given contexts: We should write no one off as being unemployable. A cardigan is a knitted jacket or sweater with buttons 
 up the front. Unlike the grapefruit, the Propname has very little impor tance in the marketplace.", DET ADJ NOUN VERB PART VERB ADJ SPACE NOUN ADP ADJ NOUN ADP VERB ADP PRON NOUN SPACE PUNCT VERB PROPN ADP PROPN PROPN PUNCT NUM PUNCT PROPN PUNCT PROPN PUNCT CCONJ PROPN NUM PUNCT PROPN NOUN PUNCT PROPN PUNCT CCONJ ADP NUM CCONJ NOUN NOUN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PUNCT NUM PUNCT PROPN CCONJ PROPN NUM PUNCT SPACE PROPN CCONJ PROPN NUM PUNCT ADV PUNCT DET NOUN AUX SPACE ADV ADV ADJ ADP NOUN ADP ADJ NOUN SPACE PUNCT DET ADJ PUNCT NOUN VERB NOUN SPACE VERB PART VERB NOUN ADP ADJ NOUN ADP DET NOUN SPACE VERB PRON PROPN PUNCT PROPN PUNCT CCONJ PROPN NUM PUNCT SPACE PROPN CCONJ PROPN NUM PUNCT PROPN PROPN PROPN PUNCT NUM PUNCT DET NOUN NOUN VERB VERB NOUN ADP ADJ NOUN PUNCT SCONJ ADV PUNCT ADP DET ADV NOUN NOUN VERB CCONJ ADV NOUN VERB NOUN PUNCT PRON VERB ADJ NOUN ADP PRON PRON AUX SPACE ADV ADJ PART VERB ADP VERB ADJ ADJ NOUN PUNCT ADP DET NOUN PUNCT VERB SCONJ PRON VERB DET NOUN VERB NUM NOUN VERB ADP ADJ NOUN ADP ADJ SPACE NOUN ADP DET ADJ NOUN PUNCT PRON AUX VERB DET NOUN ADP ADP AUX ADJ PUNCT DET NOUN AUX DET VERB NOUN CCONJ NOUN ADP NOUN SPACE ADP DET NOUN PUNCT ADP DET NOUN PUNCT DET PROPN VERB ADV ADJ NOUN NOUN ADP DET NOUN PUNCT,0.5357142857142857,18.666666666666668,4.71875
281,195,Timo Schick,"[' Distributed word representations (or embeddings) are a\nfoundational aspect of many natural language processing\nsystems; they have successfully been used for a wide vari-\nety of different tasks (Goldberg 2016). The idea behind em-\nbeddings is to assign to each word a low-dimensional, real-\nvalued vector representing its meaning. In particular, neural\nnetwork based approaches such as the skipgram and cbow\nmodels introduced by Mikolov et al. (2013) have gained in-\ncreasing popularity over the last few years. Despite their success, an important problem with current\napproaches to learning embeddings is that they require many\nobservations of a word for its embedding to become reliable;\nas a consequence, they struggle with small corpora and in-\nfrequent words (Ataman and Federico 2018). Furthermore,\nas models are typically trained with a fixed vocabulary, they\nlack the ability to assign vectors to novel, out-of-vocabulary\n(OOV) words once training is complete. n recent times, several ways have been proposed to over-\ncome these limitations and to extend word embedding mod-\nels with the ability to obtain representations of previously\nunseen words on the fly. These approaches can roughly be\ndivided into two directions: (i) the usage of subword in-\nformation, i.e., exploiting information that can be extracted\nfrom the surface-form of the word and (ii) the usage of\ncontext information.', 'The first direction aims to obtain good\nembeddings for novel words by looking at their characters\n(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-\ndou et al. 2013; Luong, Socher, and Manning 2013; Cot-\nterell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;\nSalle and Villavicencio 2018). Naturally, this direction is\nespecially well-suited for languages with rich morphology\n(Gerz et al. 2018). The second, context-based direction\ntries to infer embeddings for novel words from the words\nsurrounding them (Lazaridou, Marelli, and Baroni 2017;\nHerbelot and Baroni 2017; Khodak et al. 2018). Both direc-\ntions show promising results on various benchmarks. How-\never, for both purely surface-form-based and purely context-\nbased approaches, there are many cases in which they are\nhighly unlikely to succeed in obtaining meaningful embed-\ndings. As an example, suppose that we encounter the fol-\nlowing three words – highlighted in bold letters – as novel\nwords in the given contexts:\n(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons\nup the front. (3) Unlike the grapefruit, the pomelo has very little impor-\ntance in the marketplace.', 'In sentence (1), the context is of almost no help for de-\ntermining the meaning of the novel word, but we can de-\nduce its meaning without great difficulty from an analy-\nsis of the morphemes “un”, “employ” and “able”. For sen-\ntence (2), the reverse is true: While the novel word’s mor-\nphemes give no indication that it is a piece of clothing, this\ninformation can easily be derived from the context in which\nit occurs. Perhaps most interesting is sentence (3): Both the\nclose occurrence of the word “grapefruit” and the fact that\nthe novel word’s morphemes resemble words like “pome”,\n“pomegranate” and “melon” are indicative of the fact that it\nmay be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a\npretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an\napproach to cover a wide range of novel words, it is essen-\ntial to make use of all available information. In this work,\nwe therefore propose an architecture that, given a new word,\ncaptures both its subword structure and all available context\ninformation and combines them to obtain a high-quality em-\nbedding.', 'To this end, we first infer two distinct embeddings,\none incorporating the word’s inner structure and one captur-\ning its context, and then combine them into a unified word\nembedding. Importantly, both embeddings and their compo-\nsition function are learned jointly, allowing each embedding\nto rely on its counterpart whenever its available informa-\ntion is not sufficient. In a similar fashion to work by Pinter,\nGuthrie, and Eisenstein (2017) and Khodak et al. (2018),\nour approach is not trained from scratch, but instead makes\nuse of preexisting word embeddings and aims to reconstruct\nthese embeddings. This allows for a much faster learning\nprocess and enables us to easily combine our approach with\nany existing word embedding model, regardless of its inter-\nnal structure. Our approach is able to generate embeddings for OOV\nwords even from only a single observation with high accu-\nracy in many cases and outperforms previous work on the\nDefinitional Nonce dataset (Herbelot and Baroni 2017) and\nthe Contextual Rare Words dataset (Khodak et al. 2018). To\nthe best of our knowledge, this is the first work that jointly\nuses surface-form and context information to obtain repre-\nsentations for novel words. In summary, our contributions are as follows:\nWe propose a new model for learning embeddings for\nnovel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –\nwhich only used one of these two sources of information\n– by a large margin. Our model is designed in a way which allows it to eas-\nily be integrated into existing systems. It therefore has\nthe potential to enhance the capability of any NLP sys-\ntem that uses distributed word representations to handle\nnovel words.']",intro_chunked,"In sentence (1), the context is of almost no help for de-
termining the meaning of the novel word, but we can de-
duce its meaning without great difficulty from an analy-
sis of the morphemes “un”, “employ” and “able”. For sen-
tence (2), the reverse is true: While the novel word’s mor-
phemes give no indication that it is a piece of clothing, this
information can easily be derived from the context in which
it occurs. Perhaps most interesting is sentence (3): Both the
close occurrence of the word “grapefruit” and the fact that
the novel word’s morphemes resemble words like “pome”,
“pomegranate” and “melon” are indicative of the fact that it
may be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a
pretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an
approach to cover a wide range of novel words, it is essen-
tial to make use of all available information. In this work,
we therefore propose an architecture that, given a new word,
captures both its subword structure and all available context
information and combines them to obtain a high-quality em-
bedding.",46.57250000000002,32.83053957444222,281,0.41559433937072754," In sentence, the context is of almost no help for de termining the meaning of the novel word, but we can de duce its meaning without great difficulty from an Propname sis of the morphemes un, employ and able. For Propname tence, the reverse is true: While the novel words Propname phemes give no indication that it is a piece of clothing, this 
 information can easily be derived from the context in which 
 it occurs. Perhaps most interesting is sentence: Both the 
 close occurrence of the word grapefruit and the fact that 
 the novel words morphemes resemble words like pome, 
 pomegranate and melon are indicative of the fact that it 
 may be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a 
 pretty strong clue of the words meaning. As all three of the above sentences demonstrate, for an 
 approach to cover a wide range of novel words, it is essen tial to make use of all available information. In this work, 
 we therefore propose an architecture that, given a new word, 
 captures both its Propname structure and all available context 
 information and combines them to obtain a high quality em bedding."," In sentence, the context is of almost no help for de termining the meaning of the novel word, but we can de duce its meaning without great difficulty from an Propname sis of the morphemes un, employ and able. For Propname tence, the reverse is true: While the novel words Propname phemes give no indication that it is a piece of clothing, this 
 information can easily be derived from the context in which 
 it occurs. Perhaps most interesting is sentence: Both the 
 close occurrence of the word grapefruit and the fact that 
 the novel words morphemes resemble words like pome, 
 pomegranate and melon are indicative of the fact that it 
 may be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a 
 pretty strong clue of the words meaning. As all three of the above sentences demonstrate, for an 
 approach to cover a wide range of novel words, it is essen tial to make use of all available information. In this work, 
 we therefore propose an architecture that, given a new word, 
 captures both its Propname structure and all available context 
 information and combines them to obtain a high quality em bedding.", ADP NOUN PUNCT DET NOUN AUX ADP ADV PRON NOUN ADP X VERB DET NOUN ADP DET ADJ NOUN PUNCT CCONJ PRON AUX ADV VERB PRON NOUN ADP ADJ NOUN ADP DET PROPN NOUN ADP DET NOUN NOUN PUNCT NOUN CCONJ ADJ PUNCT ADP PROPN NOUN PUNCT DET NOUN AUX ADJ PUNCT SCONJ DET ADJ NOUN PROPN NOUN VERB DET NOUN SCONJ PRON AUX DET NOUN ADP NOUN PUNCT DET SPACE NOUN AUX ADV AUX VERB ADP DET NOUN ADP PRON SPACE PRON VERB PUNCT ADV ADV ADJ AUX NOUN PUNCT CCONJ DET SPACE ADJ NOUN ADP DET NOUN NOUN CCONJ DET NOUN SCONJ SPACE DET ADJ NOUN NOUN VERB NOUN ADP ADJ PUNCT SPACE NOUN CCONJ NOUN AUX ADJ ADP DET NOUN SCONJ PRON SPACE AUX AUX DET NOUN ADP NOUN PUNCT SCONJ NOUN ADP DET NOUN AUX AUX ADJ ADV ADP PRON ADJ PUNCT PRON NOUN VERB DET SPACE ADV ADJ NOUN ADP DET NOUN VERB PUNCT SCONJ DET NUM ADP DET ADJ NOUN VERB PUNCT SCONJ DET SPACE NOUN PART VERB DET ADJ NOUN ADP ADJ NOUN PUNCT PRON AUX VERB ADJ PART VERB NOUN ADP DET ADJ NOUN PUNCT ADP DET NOUN PUNCT SPACE PRON ADV VERB DET NOUN PRON PUNCT VERB DET ADJ NOUN PUNCT SPACE VERB CCONJ PRON PROPN NOUN CCONJ DET ADJ NOUN SPACE NOUN CCONJ VERB PRON PART VERB DET ADJ NOUN PRON ADJ PUNCT,0.5405405405405406,37.0,4.288288288288288
282,196,Timo Schick,"[' Distributed word representations (or embeddings) are a\nfoundational aspect of many natural language processing\nsystems; they have successfully been used for a wide vari-\nety of different tasks (Goldberg 2016). The idea behind em-\nbeddings is to assign to each word a low-dimensional, real-\nvalued vector representing its meaning. In particular, neural\nnetwork based approaches such as the skipgram and cbow\nmodels introduced by Mikolov et al. (2013) have gained in-\ncreasing popularity over the last few years. Despite their success, an important problem with current\napproaches to learning embeddings is that they require many\nobservations of a word for its embedding to become reliable;\nas a consequence, they struggle with small corpora and in-\nfrequent words (Ataman and Federico 2018). Furthermore,\nas models are typically trained with a fixed vocabulary, they\nlack the ability to assign vectors to novel, out-of-vocabulary\n(OOV) words once training is complete. n recent times, several ways have been proposed to over-\ncome these limitations and to extend word embedding mod-\nels with the ability to obtain representations of previously\nunseen words on the fly. These approaches can roughly be\ndivided into two directions: (i) the usage of subword in-\nformation, i.e., exploiting information that can be extracted\nfrom the surface-form of the word and (ii) the usage of\ncontext information.', 'The first direction aims to obtain good\nembeddings for novel words by looking at their characters\n(Pinter, Guthrie, and Eisenstein 2017), morphemes (Lazari-\ndou et al. 2013; Luong, Socher, and Manning 2013; Cot-\nterell, Sch¨utze, and Eisner 2016) or n-grams (Wieting et al. 2016; Bojanowski et al. 2017; Ataman and Federico 2018;\nSalle and Villavicencio 2018). Naturally, this direction is\nespecially well-suited for languages with rich morphology\n(Gerz et al. 2018). The second, context-based direction\ntries to infer embeddings for novel words from the words\nsurrounding them (Lazaridou, Marelli, and Baroni 2017;\nHerbelot and Baroni 2017; Khodak et al. 2018). Both direc-\ntions show promising results on various benchmarks. How-\never, for both purely surface-form-based and purely context-\nbased approaches, there are many cases in which they are\nhighly unlikely to succeed in obtaining meaningful embed-\ndings. As an example, suppose that we encounter the fol-\nlowing three words – highlighted in bold letters – as novel\nwords in the given contexts:\n(1) We should write no one off as being unemployable. (2) A cardigan is a knitted jacket or sweater with buttons\nup the front. (3) Unlike the grapefruit, the pomelo has very little impor-\ntance in the marketplace.', 'In sentence (1), the context is of almost no help for de-\ntermining the meaning of the novel word, but we can de-\nduce its meaning without great difficulty from an analy-\nsis of the morphemes “un”, “employ” and “able”. For sen-\ntence (2), the reverse is true: While the novel word’s mor-\nphemes give no indication that it is a piece of clothing, this\ninformation can easily be derived from the context in which\nit occurs. Perhaps most interesting is sentence (3): Both the\nclose occurrence of the word “grapefruit” and the fact that\nthe novel word’s morphemes resemble words like “pome”,\n“pomegranate” and “melon” are indicative of the fact that it\nmay be some sort of fruit. While none of those indicators may be strong enough on its own, their combination gives a\npretty strong clue of the word’s meaning. As all three of the above sentences demonstrate, for an\napproach to cover a wide range of novel words, it is essen-\ntial to make use of all available information. In this work,\nwe therefore propose an architecture that, given a new word,\ncaptures both its subword structure and all available context\ninformation and combines them to obtain a high-quality em-\nbedding.', 'To this end, we first infer two distinct embeddings,\none incorporating the word’s inner structure and one captur-\ning its context, and then combine them into a unified word\nembedding. Importantly, both embeddings and their compo-\nsition function are learned jointly, allowing each embedding\nto rely on its counterpart whenever its available informa-\ntion is not sufficient. In a similar fashion to work by Pinter,\nGuthrie, and Eisenstein (2017) and Khodak et al. (2018),\nour approach is not trained from scratch, but instead makes\nuse of preexisting word embeddings and aims to reconstruct\nthese embeddings. This allows for a much faster learning\nprocess and enables us to easily combine our approach with\nany existing word embedding model, regardless of its inter-\nnal structure. Our approach is able to generate embeddings for OOV\nwords even from only a single observation with high accu-\nracy in many cases and outperforms previous work on the\nDefinitional Nonce dataset (Herbelot and Baroni 2017) and\nthe Contextual Rare Words dataset (Khodak et al. 2018). To\nthe best of our knowledge, this is the first work that jointly\nuses surface-form and context information to obtain repre-\nsentations for novel words. In summary, our contributions are as follows:\nWe propose a new model for learning embeddings for\nnovel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –\nwhich only used one of these two sources of information\n– by a large margin. Our model is designed in a way which allows it to eas-\nily be integrated into existing systems. It therefore has\nthe potential to enhance the capability of any NLP sys-\ntem that uses distributed word representations to handle\nnovel words.']",intro_chunked,"To this end, we first infer two distinct embeddings,
one incorporating the word’s inner structure and one captur-
ing its context, and then combine them into a unified word
embedding. Importantly, both embeddings and their compo-
sition function are learned jointly, allowing each embedding
to rely on its counterpart whenever its available informa-
tion is not sufficient. In a similar fashion to work by Pinter,
Guthrie, and Eisenstein (2017) and Khodak et al. (2018),
our approach is not trained from scratch, but instead makes
use of preexisting word embeddings and aims to reconstruct
these embeddings. This allows for a much faster learning
process and enables us to easily combine our approach with
any existing word embedding model, regardless of its inter-
nal structure. Our approach is able to generate embeddings for OOV
words even from only a single observation with high accu-
racy in many cases and outperforms previous work on the
Definitional Nonce dataset (Herbelot and Baroni 2017) and
the Contextual Rare Words dataset (Khodak et al. 2018). To
the best of our knowledge, this is the first work that jointly
uses surface-form and context information to obtain repre-
sentations for novel words. In summary, our contributions are as follows:
We propose a new model for learning embeddings for
novel words that leverages both surface-form and context. We demonstrate that this model outperforms prior work –
which only used one of these two sources of information
– by a large margin. Our model is designed in a way which allows it to eas-
ily be integrated into existing systems. It therefore has
the potential to enhance the capability of any NLP sys-
tem that uses distributed word representations to handle
novel words.",43.806644736842145,32.83053957444222,282,0.5694267749786377," To this end, we first infer two distinct embeddings, 
 one incorporating the words inner structure and one captur ing its context, and then combine them into a unified word 
 embedding. Importantly, both embeddings and their compo sition function are learned jointly, allowing each embedding 
 to rely on its counterpart whenever its available informa tion is not sufficient. In a similar fashion to work by Propname, 
 Propname, and Propname and Propname Propname Propname., 
 our approach is not trained from scratch, but instead makes 
 use of preexisting word embeddings and aims to reconstruct 
 these embeddings. This allows for a much faster learning 
 process and enables us to easily combine our approach with 
 any existing word embedding model, regardless of its inter nal structure. Our approach is able to generate embeddings for Propname 
 words even from only a single observation with high accu racy in many cases and outperforms previous work on the 
 Propname Propname dataset and 
 the Contextual Propname Words dataset. To 
 the best of our knowledge, this is the first work that jointly 
 uses surface form and context information to obtain repre sentations for novel words. In summary, our contributions are as follows: 
 We propose a new model for learning embeddings for 
 novel words that leverages both surface form and context. We demonstrate that this model outperforms prior work which only used one of these two sources of information by a large margin. Our model is designed in a way which allows it to Propname ily be integrated into existing systems. It therefore has 
 the potential to enhance the capability of any Propname sys tem that uses distributed word representations to handle 
 novel words."," To this end, we first infer two distinct embeddings, 
 one incorporating the words inner structure and one captur ing its context, and then combine them into a unified word 
 embedding. Importantly, both embeddings and their compo sition function are learned jointly, allowing each embedding 
 to rely on its counterpart whenever its available informa tion is not sufficient. In a similar fashion to work by Propname, 
 Propname, and Propname and Propname Propname Propname., 
 our approach is not trained from scratch, but instead makes 
 use of preexisting word embeddings and aims to reconstruct 
 these embeddings. This allows for a much faster learning 
 process and enables us to easily combine our approach with 
 any existing word embedding model, regardless of its inter nal structure. Our approach is able to generate embeddings for Propname 
 words even from only a single observation with high accu racy in many cases and outperforms previous work on the 
 Propname Propname dataset and 
 the Contextual Propname Words dataset. To 
 the best of our knowledge, this is the first work that jointly 
 uses surface form and context information to obtain repre sentations for novel words. In summary, our contributions are as follows: 
 We propose a new model for learning embeddings for 
 novel words that leverages both surface form and context. We demonstrate that this model outperforms prior work which only used one of these two sources of information by a large margin. Our model is designed in a way which allows it to Propname ily be integrated into existing systems. It therefore has 
 the potential to enhance the capability of any Propname sys tem that uses distributed word representations to handle 
 novel words.", ADP DET NOUN PUNCT PRON ADV VERB NUM ADJ NOUN PUNCT SPACE NUM VERB DET NOUN ADJ NOUN CCONJ NUM NOUN VERB PRON NOUN PUNCT CCONJ ADV VERB PRON ADP DET ADJ NOUN SPACE VERB PUNCT ADV PUNCT DET NOUN CCONJ PRON NOUN NOUN NOUN AUX VERB ADV PUNCT VERB PRON VERB SPACE PART VERB ADP PRON NOUN SCONJ PRON ADJ ADJ NOUN AUX PART ADJ PUNCT ADP DET ADJ NOUN ADP NOUN ADP PROPN PUNCT SPACE PROPN PUNCT CCONJ PROPN CCONJ PROPN PROPN PROPN PUNCT PUNCT SPACE PRON NOUN AUX PART VERB ADP NOUN PUNCT CCONJ ADV VERB SPACE NOUN ADP VERB NOUN NOUN CCONJ VERB PART VERB SPACE DET NOUN PUNCT PRON VERB ADP DET ADV ADV NOUN SPACE NOUN CCONJ VERB PRON PART ADV VERB PRON NOUN ADP SPACE DET VERB NOUN VERB NOUN PUNCT ADV ADP PRON ADJ ADJ NOUN PUNCT PRON NOUN AUX ADJ PART VERB NOUN ADP PROPN SPACE NOUN ADV ADP ADV DET ADJ NOUN ADP ADJ NOUN NOUN ADP ADJ NOUN CCONJ VERB ADJ NOUN ADP DET SPACE PROPN PROPN NOUN CCONJ SPACE DET ADJ PROPN NOUN VERB PUNCT ADP SPACE DET ADJ ADP PRON NOUN PUNCT PRON AUX DET ADJ NOUN PRON ADV SPACE VERB NOUN NOUN CCONJ NOUN NOUN PART VERB NOUN NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX SCONJ VERB PUNCT SPACE PRON VERB DET ADJ NOUN ADP VERB NOUN ADP SPACE ADJ NOUN PRON VERB DET NOUN NOUN CCONJ NOUN PUNCT PRON VERB SCONJ DET NOUN VERB ADJ NOUN PRON ADV VERB NUM ADP DET NUM NOUN ADP NOUN ADP DET ADJ NOUN PUNCT PRON NOUN AUX VERB ADP DET NOUN PRON VERB PRON ADP PROPN ADV AUX VERB ADP VERB NOUN PUNCT PRON ADV VERB SPACE DET NOUN PART VERB DET NOUN ADP DET PROPN NOUN NOUN PRON VERB VERB NOUN NOUN PART VERB SPACE ADJ NOUN PUNCT,0.5168918918918919,29.6,4.8175675675675675
283,197,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked," Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.",27.37500000000003,32.83053957444222,283,0.6673000454902649," Semantic representations of natural language are of great interest for various aspects of natural language processing. For example, semantic representations may be useful for challenging tasks such as information extraction, question answering, natural language generation and machine translation. To provide a coherent framework for semantic representations, Propname Propname Propname. introduced Propname Propname Propname, a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so called concepts and edges encode relations between them. As Propname abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary Propname graph can be seen in Propname Propname; several sentences corresponding to this graph are listed in Propname 0b. For Propname to be useful in solving the above mentioned tasks, one must of course be able to convert sentences into Propname graphs and vice versa. Therefore, two important domain specific problems are parsing, the task of finding the graph corresponding to a given natural language sentence, and generation, the inverse task of finding a good natural language realization for a given Propname graph."," Semantic representations of natural language are of great interest for various aspects of natural language processing. For example, semantic representations may be useful for challenging tasks such as information extraction, question answering, natural language generation and machine translation. To provide a coherent framework for semantic representations, Propname Propname Propname. introduced Propname Propname Propname, a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so called concepts and edges encode relations between them. As Propname abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary Propname graph can be seen in Propname Propname; several sentences corresponding to this graph are listed in Propname 0b. For Propname to be useful in solving the above mentioned tasks, one must of course be able to convert sentences into Propname graphs and vice versa. Therefore, two important domain specific problems are parsing, the task of finding the graph corresponding to a given natural language sentence, and generation, the inverse task of finding a good natural language realization for a given Propname graph.", ADJ NOUN ADP ADJ NOUN AUX ADP ADJ NOUN ADP ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADP NOUN PUNCT ADJ NOUN AUX AUX ADJ ADP VERB NOUN ADJ ADP NOUN NOUN PUNCT NOUN VERB PUNCT ADJ NOUN NOUN CCONJ NOUN NOUN PUNCT PART VERB DET ADJ NOUN ADP ADJ NOUN PUNCT PROPN PROPN PROPN PUNCT VERB PROPN PROPN PROPN PUNCT DET ADJ NOUN NOUN PRON VERB DET NOUN ADP ADJ NOUN NOUN ADP VERB ADJ NOUN ADP NOUN VERB ADP DET NOUN CCONJ NOUN PUNCT ADP DET NOUN PUNCT NOUN VERB ADV VERB NOUN CCONJ VERB ADJ NOUN ADP PRON PUNCT SCONJ PROPN VERB ADV ADJ NOUN ADP NOUN PUNCT DET NOUN ADV VERB ADP PART ADV NUM PUNCT CCONJ DET NOUN ADP ADJ NOUN PUNCT DET ADJ PROPN NOUN AUX AUX VERB ADP PROPN PROPN PUNCT ADJ NOUN VERB ADP DET NOUN AUX VERB ADP PROPN NOUN PUNCT SCONJ PROPN PART AUX ADJ ADP VERB DET ADJ VERB NOUN PUNCT PRON AUX ADP NOUN AUX ADJ PART VERB NOUN ADP PROPN NOUN CCONJ NOUN ADV PUNCT ADV PUNCT NUM ADJ NOUN ADJ NOUN AUX VERB PUNCT DET NOUN ADP VERB DET NOUN VERB ADP DET VERB ADJ NOUN NOUN PUNCT CCONJ NOUN PUNCT DET ADJ NOUN ADP VERB DET ADJ ADJ NOUN NOUN ADP DET VERB PROPN NOUN PUNCT,0.5158371040723982,24.555555555555557,5.334841628959276
284,198,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;
Konstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see
Nivre, 2008).",44.44130726698265,32.83053957444222,284,0.41578689217567444," To give a simple example of how solutions to these tasks may be beneficial for Propname, a parser and a generator can easily be combined into a machine translation system. While many approaches have been proposed for the text to Propname parsing task, the number of currently published Propname to text generators is comparably small see Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, 0000; 
 Propname Propname Propname Propname, 0000. In this work, we tackle the problem of natural language generation from Propname by successively transforming input Propname graphs into structures that resemble dependency trees. To this end, we define a set of actions such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of Propname, is often used for dependency parsing see 
 Propname, 0000."," To give a simple example of how solutions to these tasks may be beneficial for Propname, a parser and a generator can easily be combined into a machine translation system. While many approaches have been proposed for the text to Propname parsing task, the number of currently published Propname to text generators is comparably small see Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000; Propname Propname Propname Propname, 0000, 0000; 
 Propname Propname Propname Propname, 0000. In this work, we tackle the problem of natural language generation from Propname by successively transforming input Propname graphs into structures that resemble dependency trees. To this end, we define a set of actions such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of Propname, is often used for dependency parsing see 
 Propname, 0000.", PART VERB DET ADJ NOUN ADP SCONJ NOUN ADP DET NOUN AUX AUX ADJ ADP PROPN PUNCT DET NOUN CCONJ DET NOUN AUX ADV AUX VERB ADP DET NOUN NOUN NOUN PUNCT SCONJ ADJ NOUN AUX AUX VERB ADP DET NOUN ADP PROPN VERB NOUN PUNCT DET NOUN ADP ADV VERB PROPN PART VERB NOUN AUX ADV ADJ VERB PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT PROPN PROPN PROPN PROPN PUNCT NUM PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP ADJ NOUN NOUN ADP PROPN ADP ADV VERB NOUN PROPN NOUN ADP NOUN PRON VERB NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET NOUN ADP NOUN ADJ ADP DET NOUN PUNCT VERB CCONJ NOUN ADP NOUN CCONJ NOUN PUNCT ADP VERB DET NOUN ADP DET NOUN PUNCT PRON VERB DET VERB NOUN NOUN ADP DET NOUN ADP VERB PRON NOUN ADP DET ADJ NOUN PUNCT PRON VERB DET ADJ NOUN ADP VERB NOUN ADP DET NOUN NOUN PUNCT DET ADJ NOUN PRON PUNCT ADP DET NOUN ADP PROPN PUNCT AUX ADV VERB ADP NOUN NOUN NOUN SPACE PROPN PUNCT NUM PUNCT,0.515,33.333333333333336,4.78
285,199,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.",47.37389671361504,32.83053957444222,285,0.5279470086097717," To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models from a corpus of Propname graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition based approach is to a large extent inspired by the likewise transition based parser Propname. In fact, this parser may be seen as the direct inverse of our system: While we turn Propname graphs into ordered trees which, in turn, are converted into sentences, the parser by Propname Propname Propname. generates dependency trees from sentences and subsequently transforms these trees into Propname graphs. Accordingly, several transitions used by Propname have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input Propname graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input."," To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models from a corpus of Propname graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition based approach is to a large extent inspired by the likewise transition based parser Propname. In fact, this parser may be seen as the direct inverse of our system: While we turn Propname graphs into ordered trees which, in turn, are converted into sentences, the parser by Propname Propname Propname. generates dependency trees from sentences and subsequently transforms these trees into Propname graphs. Accordingly, several transitions used by Propname have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input Propname graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.", PART VERB DET ADJ NOUN ADP NOUN PART AUX VERB ADP DET NOUN PUNCT PRON VERB ADJ NOUN NOUN ADP DET NOUN ADP PROPN NOUN CCONJ VERB NOUN PUNCT SCONJ AUX VERB ADP DET ADJ NOUN ADP DET NOUN PUNCT PRON VERB PRON ADP VERB ADJ NOUN PUNCT PRON VERB ADV ADV SCONJ DET ADV ADJ NOUN ADP DET ADJ ADJ NOUN AUX ADJ ADP NOUN PUNCT ADV PUNCT PRON AUX ADJ SCONJ PRON NOUN AUX AUX VERB ADP ADJ ADJ NOUN ADP DET NOUN PUNCT PRON NOUN VERB NOUN AUX ADP DET ADJ NOUN VERB ADP DET ADJ NOUN VERB NOUN PROPN PUNCT ADP NOUN PUNCT DET NOUN AUX AUX VERB ADP DET ADJ NOUN ADP PRON NOUN PUNCT SCONJ PRON VERB PROPN NOUN ADP VERB NOUN PRON PUNCT ADP NOUN PUNCT AUX VERB ADP NOUN PUNCT DET NOUN ADP PROPN PROPN PROPN PUNCT VERB NOUN NOUN ADP NOUN CCONJ ADV VERB DET NOUN ADP PROPN NOUN PUNCT ADV PUNCT ADJ NOUN VERB ADP PROPN VERB DET ADJ NOUN ADP PRON NOUN PUNCT ADP DET NOUN PUNCT DET NOUN VERB ADP PRON NOUN AUX ADJ ADP PRON NOUN PUNCT PRON AUX SCONJ PRON AUX PART VERB PART VERB NOUN PROPN NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN AUX ADJ ADV ADV SCONJ DET NOUN VERB ADP PRON AUX DET ADJ NOUN ADP DET NOUN PUNCT,0.5506607929515418,25.22222222222222,4.700440528634362
286,200,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.",30.80186274509805,32.83053957444222,286,0.2784596085548401," For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from Propname graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that Propname graphs, in contrast to dependency trees, are unordered. Furthermore, Propname abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline which is indispensable to obtain competitive results makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation Propname that incorporates a language model but is still sufficiently efficient."," For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from Propname graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that Propname graphs, in contrast to dependency trees, are unordered. Furthermore, Propname abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline which is indispensable to obtain competitive results makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation Propname that incorporates a language model but is still sufficiently efficient.", ADP DET ADJ NOUN PUNCT PRON VERB ADV DET NOUN SCONJ PRON PART VERB NOUN NOUN SCONJ PRON VERB DET NOUN ADP DET ADJ NOUN PUNCT ADP ADJ NOUN PUNCT ADV PUNCT DET NOUN ADP PROPN NOUN ADP ADJ NOUN AUX ADV ADV ADJ ADP VERB DET ADJ NOUN PUNCT ADP NOUN PUNCT PRON VERB PART ADV VERB ADP DET NOUN SCONJ PROPN NOUN PUNCT ADP NOUN ADP NOUN NOUN PUNCT AUX VERB PUNCT ADV PUNCT PROPN VERB ADV ADJ PUNCT NOUN CCONJ NOUN ADV ADV ADP NOUN NOUN ADJ ADP NOUN PUNCT NOUN CCONJ NOUN PUNCT DET DET NOUN AUX ADV AUX VERB PUNCT ADV PUNCT DET NOUN ADP DET NOUN NOUN ADP PRON NOUN NOUN PRON AUX ADJ PART VERB ADJ NOUN VERB PRON ADV ADJ PART ADV VERB DET ADJ NOUN ADP NOUN ADP DET VERB NOUN PUNCT PRON VERB DET NOUN ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON VERB DET NOUN ADP ADJ NOUN PART VERB DET NOUN ADP DET NOUN ADP PRON NOUN PUNCT PRON VERB PART VERB ADP VERB ADJ NOUN ADP VERB ADJ ADJ NOUN NOUN PART VERB DET ADJ NOUN PUNCT DET NOUN AUX VERB ADP VERB DET NOUN ADP ADJ NOUN PUNCT PART ADV VERB PRON NOUN PUNCT PRON VERB DET ADJ NOUN PROPN PRON VERB DET NOUN NOUN CCONJ AUX ADV ADV ADJ PUNCT,0.6177777777777778,25.0,4.92
287,201,Timo Schick,"[' Semantic representations of natural language are of great interest for various aspects of natural language processing (NLP). For example, semantic representations may be useful for challenging tasks such as information extraction (Palmer et al., 2005), question answering (Shen and Lapata, 2007), natural language generation (Langkilde and Knight, 1998) and machine translation (Jones et al., 2012). To provide a coherent framework for semantic representations, Banarescu et al. (2013) introduced Abstract Meaning Representation (AMR), a semantic representation language that encodes the meanings of natural language sentences as directed acyclic graphs with labels assigned to both vertices and edges. Within this formalism, vertices represent so-called concepts and edges encode relations between them. As AMR abstracts away various kinds of information, each graph typically corresponds to not just one, but a number of different sentences. An exemplary AMR graph can be seen in Figure 1a; several sentences corresponding to this graph are listed in Figure 1b. For AMR to be useful in solving the above-mentioned tasks, one must of course be able to convert sentences into AMR graphs and vice versa. Therefore, two important domain-specific problems are (text-to-AMR) parsing, the task of finding the graph corresponding to a given natural language sentence, and (AMR-to-text) generation, the inverse task of finding a good natural language realization for a given AMR graph.', 'To give a simple example of how solutions to these tasks may be beneficial for NLP, a parser and a generator can easily be combined into a machine translation system (Jones et al., 2012). While many approaches have been proposed for the text-to-AMR parsing task (see Flanigan et al., 2014; Peng et al., 2015; Pust et al., 2015; Wang et al., 2015; Puzikov et al., 2016; Zhou et al., 2016; Buys and Blunsom, 2017; van Noord and Bos, 2017; Konstas et al., 2017), the number of currently published AMR-to-text generators is comparably small (see Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2016, 2017;\nKonstas et al., 2017). In this work, we tackle the problem of natural language generation from AMR by successively transforming input AMR graphs into structures that resemble dependency trees. To this end, we define a set of actions (transitions) such as the deletion, merging and swapping of edges and vertices. After applying these transitions to the input, we turn the obtained tree structure into a sentence by visiting its vertices in a specific order. We embed the different kinds of required actions into a transition system, a formal framework that, in the context of NLP, is often used for dependency parsing (see\nNivre, 2008).', 'To predict the correct sequence of transitions to be applied for each input, we train maximum entropy models (Berger et al., 1996) from a corpus of AMR graphs and corresponding realizations. As is done in all previous works on this topic, we restrict ourselves to generating English sentences; we do so simply because no reasonably large corpus for any other natural language is available to date. However, we are confident that our results can be transferred to many other languages with some effort. Our transition-based approach is to a large extent inspired by the likewise transition-based parser CAMR (Wang et al., 2015). In fact, this parser may be seen as the direct inverse of our system: While we turn AMR graphs into ordered trees which, in turn, are converted into sentences, the parser by Wang et al. (2015) generates dependency trees from sentences and subsequently transforms these trees into AMR graphs. Accordingly, several transitions used by CAMR have a direct counterpart in our generator. In a way, the task performed by our system is simpler than its inverse. This is because we are not required to transform input AMR graphs into actual dependency trees; any tree is sufficient as long as the sentence obtained from it is a good realization of the input.', 'For this very reason, there is also no need for us to assign dependency labels as they have no representation in the generated sentence. In other respects, however, the transformation from AMR graphs to suitable trees is much more challenging than going the opposite way. For example, we have to somehow cope with the fact that AMR graphs, in contrast to dependency trees, are unordered. Furthermore, AMR abstracts away tense, number and voice as well as function words such as articles, pronouns and prepositions; all this information must somehow be retrieved. Finally, the inclusion of a language model into our generation pipeline – which is indispensable to obtain competitive results – makes it very difficult to efficiently determine the best sequence of transitions for a given input. We address these challenges in various ways. For instance, we devise a set of special transitions to establish an order on the vertices of our input. We try to compensate for lacking syntactic information by training several maximum entropy models to estimate this very information; this idea is formalized by introducing the concept of syntactic annotations. To actually implement our system, we develop a novel generation algorithm that incorporates a language model but is still sufficiently efficient.', 'We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.']",intro_chunked,"We proceed as follows: After giving a succinct overview of previous work on AMR-to-text generation and related tasks in Section 2, we discuss basic notation and other preliminaries such as the AMR formalism, transition systems and maximum entropy models in Section 3. We introduce our generator in Section 4, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 5, we discuss our Java-based implementation of the generator. Results obtained with this implementation are reported in Section 6; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Table 8 of Section 6. We conclude with a concise summary of our work and an outlook on future research topics in Section 7.",27.778250551876425,32.83053957444222,287,0.5364612936973572," We proceed as follows: After giving a succinct overview of previous work on Propname to text generation and related tasks in Section 0, we discuss basic notation and other preliminaries such as the Propname formalism, transition systems and maximum entropy models in Section 0. We introduce our generator in Section 0, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 0, we discuss our Propname based implementation of the generator. Results obtained with this implementation are reported in Section 0; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Propname 0 of Section 0. We conclude with a concise summary of our work and an outlook on future research topics in Section 0."," We proceed as follows: After giving a succinct overview of previous work on Propname to text generation and related tasks in Section 0, we discuss basic notation and other preliminaries such as the Propname formalism, transition systems and maximum entropy models in Section 0. We introduce our generator in Section 0, which constitutes the core of this work. This section includes a detailed definition of all required transitions as well as a thorough derivation of our generation algorithm and an explanation of the required training procedure. In Section 0, we discuss our Propname based implementation of the generator. Results obtained with this implementation are reported in Section 0; for a quick overview on the performance of our generator and a comparison with all other currently published approaches, we refer to Propname 0 of Section 0. We conclude with a concise summary of our work and an outlook on future research topics in Section 0.", PRON VERB SCONJ VERB PUNCT ADP VERB DET ADJ NOUN ADP ADJ NOUN ADP PROPN PART VERB NOUN CCONJ ADJ NOUN ADP NOUN NUM PUNCT PRON VERB ADJ NOUN CCONJ ADJ NOUN ADJ ADP DET PROPN NOUN PUNCT NOUN NOUN CCONJ ADJ NOUN NOUN ADP NOUN NUM PUNCT PRON VERB PRON NOUN ADP NOUN NUM PUNCT PRON VERB DET NOUN ADP DET NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP DET VERB NOUN ADV ADV ADP DET ADJ NOUN ADP PRON NOUN NOUN CCONJ DET NOUN ADP DET VERB NOUN NOUN PUNCT ADP NOUN NUM PUNCT PRON VERB PRON PROPN VERB NOUN ADP DET NOUN PUNCT NOUN VERB ADP DET NOUN AUX VERB ADP NOUN NUM PUNCT ADP DET ADJ NOUN ADP DET NOUN ADP PRON NOUN CCONJ DET NOUN ADP DET ADJ ADV VERB NOUN PUNCT PRON VERB ADP PROPN NUM ADP NOUN NUM PUNCT PRON VERB ADP DET ADJ NOUN ADP PRON NOUN CCONJ DET NOUN ADP ADJ NOUN NOUN ADP NOUN NUM PUNCT,0.5209580838323353,27.833333333333332,4.790419161676646
288,0,GPT-3.5,"[' AutoMix emerges as a promising solution in the realm of cloud-based LLMs, providing a strategic means to balance computational cost and performance. By leveraging a few-shot self-verification mechanism, the system accurately gauges the reliability of its outputs, eliminating the need for extensive training. The incorporation of a meta verifier further enhances the precision of these assessments, addressing the inherent noise in the verification process. Experimental results, conducted on LLAMA2-13/70B across context-grounded reasoning datasets, affirm the effectiveness of AutoMix. Notably, the approach surpasses established baselines, showcasing a substantial up to 89% improvement in the incremental benefit per cost. AutoMix stands as a valuable contribution in the pursuit of optimizing the utilization of cloud-based LLMs, promising a more cost-effective and performance-driven approach in natural language processing tasks.']",conclusion_chunked," AutoMix emerges as a promising solution in the realm of cloud-based LLMs, providing a strategic means to balance computational cost and performance. By leveraging a few-shot self-verification mechanism, the system accurately gauges the reliability of its outputs, eliminating the need for extensive training. The incorporation of a meta verifier further enhances the precision of these assessments, addressing the inherent noise in the verification process. Experimental results, conducted on LLAMA2-13/70B across context-grounded reasoning datasets, affirm the effectiveness of AutoMix. Notably, the approach surpasses established baselines, showcasing a substantial up to 89% improvement in the incremental benefit per cost. AutoMix stands as a valuable contribution in the pursuit of optimizing the utilization of cloud-based LLMs, promising a more cost-effective and performance-driven approach in natural language processing tasks.",4.8264238845144405,2.6406588729545177,288,0.8056436777114868," Propname emerges as a promising solution in the realm of Propname based LLMs, providing a strategic means to balance computational cost and performance. By leveraging a few shot self verification mechanism, the system accurately gauges the reliability of its outputs, eliminating the need for extensive training. The incorporation of a meta verifier further enhances the precision of these assessments, addressing the inherent noise in the verification process. Experimental results, conducted on Propname Propname across context grounded reasoning datasets, affirm the effectiveness of Propname. Notably, the approach surpasses established baselines, showcasing a substantial up to 00 improvement in the incremental benefit per cost. Propname stands as a valuable contribution in the pursuit of optimizing the utilization of Propname based LLMs, promising a more cost effective and performance driven approach in natural language processing tasks."," Propname emerges as a promising solution in the realm of Propname based LLMs, providing a strategic means to balance computational cost and performance. By leveraging a few shot self verification mechanism, the system accurately gauges the reliability of its outputs, eliminating the need for extensive training. The incorporation of a meta verifier further enhances the precision of these assessments, addressing the inherent noise in the verification process. Experimental results, conducted on Propname Propname across context grounded reasoning datasets, affirm the effectiveness of Propname. Notably, the approach surpasses established baselines, showcasing a substantial up to 00 improvement in the incremental benefit per cost. Propname stands as a valuable contribution in the pursuit of optimizing the utilization of Propname based LLMs, promising a more cost effective and performance driven approach in natural language processing tasks.", PROPN VERB ADP DET ADJ NOUN ADP DET NOUN ADP PROPN VERB NOUN PUNCT VERB DET ADJ NOUN PART VERB ADJ NOUN CCONJ NOUN PUNCT ADP VERB DET ADJ NOUN NOUN NOUN NOUN PUNCT DET NOUN ADV VERB DET NOUN ADP PRON NOUN PUNCT VERB DET NOUN ADP ADJ NOUN PUNCT DET NOUN ADP DET ADJ NOUN ADV VERB DET NOUN ADP DET NOUN PUNCT VERB DET ADJ NOUN ADP DET NOUN NOUN PUNCT ADJ NOUN PUNCT VERB ADP PROPN PROPN ADP NOUN VERB NOUN NOUN PUNCT VERB DET NOUN ADP PROPN PUNCT ADV PUNCT DET NOUN VERB VERB NOUN PUNCT VERB DET ADJ ADP PART NUM NOUN ADP DET ADJ NOUN ADP NOUN PUNCT PROPN VERB ADP DET ADJ NOUN ADP DET NOUN ADP VERB DET NOUN ADP PROPN VERB NOUN PUNCT VERB DET ADJ NOUN ADJ CCONJ NOUN VERB NOUN ADP ADJ NOUN NOUN NOUN PUNCT,0.6148648648648649,24.666666666666668,5.513513513513513
289,1,GPT-3.5,"[' In this study, we introduced SELF-REFINE, a novel approach to enhance the outputs of Large Language Models (LLMs) through an iterative feedback and refinement process. By mimicking the iterative refinement process observed in human writing, SELF-REFINE utilizes a single LLM as the generator, refiner, and feedback provider, eliminating the need for additional training or specialized data. Across seven diverse tasks, ranging from dialog response generation to mathematical reasoning, we demonstrated the effectiveness of SELF-REFINE using state-of-the-art LLMs like GPT-3.5 and GPT-4. Our results consistently revealed that outputs generated with SELF-REFINE outperformed those from conventional one-step generation, with an average improvement of approximately 20% in task performance according to both human evaluations and automatic metrics. This research highlights the potential for further improving the capabilities of even the most advanced LLMs at test-time through the adoption of a simple and standalone approach like SELF-REFINE.']",conclusion_chunked," In this study, we introduced SELF-REFINE, a novel approach to enhance the outputs of Large Language Models (LLMs) through an iterative feedback and refinement process. By mimicking the iterative refinement process observed in human writing, SELF-REFINE utilizes a single LLM as the generator, refiner, and feedback provider, eliminating the need for additional training or specialized data. Across seven diverse tasks, ranging from dialog response generation to mathematical reasoning, we demonstrated the effectiveness of SELF-REFINE using state-of-the-art LLMs like GPT-3.5 and GPT-4. Our results consistently revealed that outputs generated with SELF-REFINE outperformed those from conventional one-step generation, with an average improvement of approximately 20% in task performance according to both human evaluations and automatic metrics. This research highlights the potential for further improving the capabilities of even the most advanced LLMs at test-time through the adoption of a simple and standalone approach like SELF-REFINE.",4.699310344827609,2.6406588729545177,289,0.8002233505249023," In this study, we introduced Propname Propname, a novel approach to enhance the outputs of Large Propname Propname through an iterative feedback and refinement process. By mimicking the iterative refinement process observed in human writing, Propname Propname utilizes a single Propname as the generator, refiner, and feedback provider, eliminating the need for additional training or specialized data. Across seven diverse tasks, ranging from dialog response generation to mathematical reasoning, we demonstrated the effectiveness of Propname Propname using state of the art LLMs like Propname 0.0 and Propname 0. Our results consistently revealed that outputs generated with Propname Propname outperformed those from conventional one step generation, with an average improvement of approximately 00 in task performance according to both human evaluations and automatic metrics. This research highlights the potential for further improving the capabilities of even the most advanced LLMs at test time through the adoption of a simple and standalone approach like Propname Propname."," In this study, we introduced Propname Propname, a novel approach to enhance the outputs of Large Propname Propname through an iterative feedback and refinement process. By mimicking the iterative refinement process observed in human writing, Propname Propname utilizes a single Propname as the generator, refiner, and feedback provider, eliminating the need for additional training or specialized data. Across seven diverse tasks, ranging from dialog response generation to mathematical reasoning, we demonstrated the effectiveness of Propname Propname using state of the art LLMs like Propname 0.0 and Propname 0. Our results consistently revealed that outputs generated with Propname Propname outperformed those from conventional one step generation, with an average improvement of approximately 00 in task performance according to both human evaluations and automatic metrics. This research highlights the potential for further improving the capabilities of even the most advanced LLMs at test time through the adoption of a simple and standalone approach like Propname Propname.", ADP DET NOUN PUNCT PRON VERB PROPN PROPN PUNCT DET ADJ NOUN PART VERB DET NOUN ADP ADJ PROPN PROPN ADP DET ADJ NOUN CCONJ ADJ NOUN PUNCT ADP VERB DET ADJ ADJ NOUN VERB ADP ADJ NOUN PUNCT PROPN PROPN VERB DET ADJ PROPN ADP DET NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN PUNCT VERB DET NOUN ADP ADJ NOUN CCONJ ADJ NOUN PUNCT ADP NUM ADJ NOUN PUNCT VERB ADP NOUN NOUN NOUN ADP ADJ NOUN PUNCT PRON VERB DET NOUN ADP PROPN PROPN VERB NOUN ADP DET NOUN NOUN ADP PROPN NUM CCONJ PROPN NUM PUNCT PRON NOUN ADV VERB SCONJ NOUN VERB ADP PROPN PROPN VERB PRON ADP ADJ NUM NOUN NOUN PUNCT ADP DET ADJ NOUN ADP ADV NUM ADP NOUN NOUN VERB ADP PRON ADJ NOUN CCONJ ADJ NOUN PUNCT DET NOUN VERB DET NOUN ADP ADV VERB DET NOUN ADP ADV DET ADV ADJ NOUN ADP NOUN NOUN ADP DET NOUN ADP DET ADJ CCONJ ADJ NOUN ADP PROPN PROPN PUNCT,0.6130952380952381,33.6,5.529761904761905
290,2,GPT-3.5,"[' In this work, we introduced FLOWGEN, a graph generation model that embraces the dual-process theory of mind, allowing for adaptive responses to varying task complexities. FLOWGEN employs either a FAST (weaker) or SLOW (stronger) module, each characterized by identical architectures but differing in parameter count and generative power. Our experiments on real-world graphs underscore the effectiveness of FLOWGEN, showcasing its ability to generate graphs akin to those produced by a single large model. Notably, FLOWGEN achieves this while demonstrating a speed improvement of up to 2x. This research contributes to bridging the gap between machine learning models and human cognitive processes, offering a promising avenue for developing more adaptive and efficient graph generation systems.']",conclusion_chunked," In this work, we introduced FLOWGEN, a graph generation model that embraces the dual-process theory of mind, allowing for adaptive responses to varying task complexities. FLOWGEN employs either a FAST (weaker) or SLOW (stronger) module, each characterized by identical architectures but differing in parameter count and generative power. Our experiments on real-world graphs underscore the effectiveness of FLOWGEN, showcasing its ability to generate graphs akin to those produced by a single large model. Notably, FLOWGEN achieves this while demonstrating a speed improvement of up to 2x. This research contributes to bridging the gap between machine learning models and human cognitive processes, offering a promising avenue for developing more adaptive and efficient graph generation systems.",17.461421052631607,2.6406588729545177,290,0.7023260593414307," In this work, we introduced Propname, a graph generation model that embraces the dual process theory of mind, allowing for adaptive responses to varying task complexities. FLOWGEN employs either a FAST or SLOW module, each characterized by identical architectures but differing in parameter count and generative power. Our experiments on real world graphs underscore the effectiveness of FLOWGEN, showcasing its ability to generate graphs akin to those produced by a single large model. Notably, Propname achieves this while demonstrating a speed improvement of up to Propname. This research contributes to bridging the gap between machine learning models and human cognitive processes, offering a promising avenue for developing more adaptive and efficient graph generation systems."," In this work, we introduced Propname, a graph generation model that embraces the dual process theory of mind, allowing for adaptive responses to varying task complexities. FLOWGEN employs either a FAST or SLOW module, each characterized by identical architectures but differing in parameter count and generative power. Our experiments on real world graphs underscore the effectiveness of FLOWGEN, showcasing its ability to generate graphs akin to those produced by a single large model. Notably, Propname achieves this while demonstrating a speed improvement of up to Propname. This research contributes to bridging the gap between machine learning models and human cognitive processes, offering a promising avenue for developing more adaptive and efficient graph generation systems.", ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN NOUN NOUN PRON VERB DET ADJ NOUN NOUN ADP NOUN PUNCT VERB ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT NOUN VERB CCONJ DET ADJ CCONJ ADJ NOUN PUNCT PRON VERB ADP ADJ NOUN CCONJ VERB ADP NOUN NOUN CCONJ ADJ NOUN PUNCT PRON NOUN ADP ADJ NOUN NOUN VERB DET NOUN ADP NOUN PUNCT VERB PRON NOUN PART VERB NOUN ADJ ADP PRON VERB ADP DET ADJ ADJ NOUN PUNCT ADV PUNCT PROPN VERB PRON SCONJ VERB DET NOUN NOUN ADP ADP ADP PROPN PUNCT DET NOUN VERB ADP VERB DET NOUN ADP NOUN NOUN NOUN CCONJ ADJ ADJ NOUN PUNCT VERB DET ADJ NOUN ADP VERB ADV ADJ CCONJ ADJ NOUN NOUN NOUN PUNCT,0.7222222222222222,25.2,5.317460317460317
291,3,GPT-3.5,"[' In this work, we introduced a method for curating high-quality comparable training data specifically designed for low-resource languages, utilizing monolingual annotators. The strategic use of carefully chosen images as linguistic pivots between source and target languages proved effective, as evidenced by human evaluations on the English-Hindi comparable corpora. Our method demonstrated an impressive 81.1% acceptability rate for translation pairs, with a minimal 2.47% identified as non-translations. To underscore the practical utility of our curated dataset, we conducted experiments on two downstream tasksâ€”machine translation and dictionary extractionâ€”revealing promising outcomes. The availability of all code and data at https://github.com/madaan/PML4DC-Comparable-Data-Collection emphasizes the transparency and reproducibility of our approach, contributing a valuable resource for researchers in low-resource language domains.']",conclusion_chunked," In this work, we introduced a method for curating high-quality comparable training data specifically designed for low-resource languages, utilizing monolingual annotators. The strategic use of carefully chosen images as linguistic pivots between source and target languages proved effective, as evidenced by human evaluations on the English-Hindi comparable corpora. Our method demonstrated an impressive 81.1% acceptability rate for translation pairs, with a minimal 2.47% identified as non-translations. To underscore the practical utility of our curated dataset, we conducted experiments on two downstream tasksâ€”machine translation and dictionary extractionâ€”revealing promising outcomes. The availability of all code and data at https://github.com/madaan/PML4DC-Comparable-Data-Collection emphasizes the transparency and reproducibility of our approach, contributing a valuable resource for researchers in low-resource language domains.",-10.699900826446253,2.6406588729545177,291,0.7659293413162231," In this work, we introduced a method for curating high quality comparable training data specifically designed for low resource languages, utilizing monolingual annotators. The strategic use of carefully chosen images as linguistic pivots between source and target languages proved effective, as evidenced by human evaluations on the Propname Propname comparable Propname. Our method demonstrated an impressive 00.0 acceptability rate for translation pairs, with a minimal 0.00 identified as non translations. To underscore the practical utility of our curated dataset, we conducted experiments on two downstream tasksmachine translation and dictionary extractionrevealing promising outcomes. The availability of all code and data at Propname Propname Propname emphasizes the transparency and reproducibility of our approach, contributing a valuable resource for researchers in low resource language domains."," In this work, we introduced a method for curating high quality comparable training data specifically designed for low resource languages, utilizing monolingual annotators. The strategic use of carefully chosen images as linguistic pivots between source and target languages proved effective, as evidenced by human evaluations on the Propname Propname comparable Propname. Our method demonstrated an impressive 00.0 acceptability rate for translation pairs, with a minimal 0.00 identified as non translations. To underscore the practical utility of our curated dataset, we conducted experiments on two downstream tasksmachine translation and dictionary extractionrevealing promising outcomes. The availability of all code and data at Propname Propname Propname emphasizes the transparency and reproducibility of our approach, contributing a valuable resource for researchers in low resource language domains.", ADP DET NOUN PUNCT PRON VERB DET NOUN ADP VERB ADJ NOUN ADJ NOUN NOUN ADV VERB ADP ADJ NOUN NOUN PUNCT VERB ADJ NOUN PUNCT DET ADJ NOUN ADP ADV VERB NOUN ADP ADJ NOUN ADP NOUN CCONJ NOUN NOUN VERB ADJ PUNCT SCONJ VERB ADP ADJ NOUN ADP DET PROPN PROPN ADJ PROPN PUNCT PRON NOUN VERB DET ADJ NUM NOUN NOUN ADP NOUN NOUN PUNCT ADP DET ADJ NUM VERB ADP NOUN NOUN PUNCT PART VERB DET ADJ NOUN ADP PRON VERB NOUN PUNCT PRON VERB NOUN ADP NUM ADJ NOUN NOUN CCONJ ADJ VERB ADJ NOUN PUNCT DET NOUN ADP DET NOUN CCONJ NOUN ADP PROPN PROPN PROPN VERB DET NOUN CCONJ NOUN ADP PRON NOUN PUNCT VERB DET ADJ NOUN ADP NOUN ADP ADJ NOUN NOUN NOUN PUNCT,0.6917293233082706,26.6,5.909774436090226
292,4,GPT-3.5,"[' In conclusion, this paper introduces and addresses the novel task of politeness transfer, providing a substantial dataset and a robust tag-and-generate pipeline to facilitate research in this burgeoning field of natural language processing. Our model not only outperforms state-of-the-art methods on automatic metrics related to content preservation but also demonstrates superior performance in human evaluations, showcasing its effectiveness in achieving grammaticality, meaning preservation, and transfer accuracy across a diverse set of style transfer tasks. By making our code and dataset publicly available, we encourage collaboration and invite the community to build upon our work, fostering advancements in politeness transfer and broader style transfer applications. The presented contributions aim to propel the field forward, offering valuable insights and tools for researchers and practitioners alike.']",conclusion_chunked," In conclusion, this paper introduces and addresses the novel task of politeness transfer, providing a substantial dataset and a robust tag-and-generate pipeline to facilitate research in this burgeoning field of natural language processing. Our model not only outperforms state-of-the-art methods on automatic metrics related to content preservation but also demonstrates superior performance in human evaluations, showcasing its effectiveness in achieving grammaticality, meaning preservation, and transfer accuracy across a diverse set of style transfer tasks. By making our code and dataset publicly available, we encourage collaboration and invite the community to build upon our work, fostering advancements in politeness transfer and broader style transfer applications. The presented contributions aim to propel the field forward, offering valuable insights and tools for researchers and practitioners alike.",-6.6445426829268115,2.6406588729545177,292,0.8507444858551025," In conclusion, this paper introduces and addresses the novel task of politeness transfer, providing a substantial dataset and a robust tag and generate pipeline to facilitate research in this burgeoning field of natural language processing. Our model not only outperforms state of the art methods on automatic metrics related to content preservation but also demonstrates superior performance in human evaluations, showcasing its effectiveness in achieving grammaticality, meaning preservation, and transfer accuracy across a diverse set of style transfer tasks. By making our code and dataset publicly available, we encourage collaboration and invite the community to build upon our work, fostering advancements in politeness transfer and broader style transfer applications. The presented contributions aim to propel the field forward, offering valuable insights and tools for researchers and practitioners alike."," In conclusion, this paper introduces and addresses the novel task of politeness transfer, providing a substantial dataset and a robust tag and generate pipeline to facilitate research in this burgeoning field of natural language processing. Our model not only outperforms state of the art methods on automatic metrics related to content preservation but also demonstrates superior performance in human evaluations, showcasing its effectiveness in achieving grammaticality, meaning preservation, and transfer accuracy across a diverse set of style transfer tasks. By making our code and dataset publicly available, we encourage collaboration and invite the community to build upon our work, fostering advancements in politeness transfer and broader style transfer applications. The presented contributions aim to propel the field forward, offering valuable insights and tools for researchers and practitioners alike.", ADP NOUN PUNCT DET NOUN NOUN CCONJ VERB DET ADJ NOUN ADP NOUN NOUN PUNCT VERB DET ADJ NOUN CCONJ DET ADJ NOUN CCONJ VERB NOUN PART VERB NOUN ADP DET VERB NOUN ADP ADJ NOUN NOUN PUNCT PRON NOUN PART ADV VERB NOUN ADP DET NOUN NOUN ADP ADJ NOUN VERB ADP NOUN NOUN CCONJ ADV VERB ADJ NOUN ADP ADJ NOUN PUNCT VERB PRON NOUN ADP VERB NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN ADP DET ADJ NOUN ADP NOUN NOUN NOUN PUNCT ADP VERB PRON NOUN CCONJ VERB ADV ADJ PUNCT PRON VERB NOUN CCONJ VERB DET NOUN PART VERB SCONJ PRON NOUN PUNCT VERB NOUN ADP NOUN NOUN CCONJ ADJ NOUN NOUN NOUN PUNCT DET VERB NOUN VERB PART VERB DET NOUN ADV PUNCT VERB ADJ NOUN CCONJ NOUN ADP NOUN CCONJ NOUN ADV PUNCT,0.6928571428571428,35.0,5.628571428571429
293,5,GPT-3.5,"[' In conclusion, EIGEN stands as a robust and effective method for event influence generation, showcasing advancements in both automated evaluation metrics and human judgment. The introduced dataset serves as a valuable resource for future research in this domain. The demonstrated enhancement of performance on the WIQA benchmark, especially for questions involving background knowledge and multi-hop reasoning, underlines the practical utility of EIGEN. As the understanding of event influences plays a pivotal role in various applications, ranging from natural language understanding to decision support systems, EIGEN contributes significantly to advancing the state-of-the-art in this field.']",conclusion_chunked," In conclusion, EIGEN stands as a robust and effective method for event influence generation, showcasing advancements in both automated evaluation metrics and human judgment. The introduced dataset serves as a valuable resource for future research in this domain. The demonstrated enhancement of performance on the WIQA benchmark, especially for questions involving background knowledge and multi-hop reasoning, underlines the practical utility of EIGEN. As the understanding of event influences plays a pivotal role in various applications, ranging from natural language understanding to decision support systems, EIGEN contributes significantly to advancing the state-of-the-art in this field.",2.6406588729545177,2.6406588729545177,293,0.8895085453987122," In conclusion, Propname stands as a robust and effective method for event influence generation, showcasing advancements in both automated evaluation metrics and human judgment. The introduced dataset serves as a valuable resource for future research in this domain. The demonstrated enhancement of performance on the Propname benchmark, especially for questions involving background knowledge and multi hop reasoning, underlines the practical utility of Propname. As the understanding of event influences plays a pivotal role in various applications, ranging from natural language understanding to decision support systems, Propname contributes significantly to advancing the state of the art in this field."," In conclusion, Propname stands as a robust and effective method for event influence generation, showcasing advancements in both automated evaluation metrics and human judgment. The introduced dataset serves as a valuable resource for future research in this domain. The demonstrated enhancement of performance on the Propname benchmark, especially for questions involving background knowledge and multi hop reasoning, underlines the practical utility of Propname. As the understanding of event influences plays a pivotal role in various applications, ranging from natural language understanding to decision support systems, Propname contributes significantly to advancing the state of the art in this field.", ADP NOUN PUNCT PROPN VERB ADP DET ADJ CCONJ ADJ NOUN ADP NOUN NOUN NOUN PUNCT VERB NOUN ADP DET VERB NOUN NOUN CCONJ ADJ NOUN PUNCT DET VERB NOUN VERB ADP DET ADJ NOUN ADP ADJ NOUN ADP DET NOUN PUNCT DET VERB NOUN ADP NOUN ADP DET PROPN NOUN PUNCT ADV ADP NOUN VERB NOUN NOUN CCONJ VERB NOUN NOUN PUNCT VERB DET ADJ NOUN ADP PROPN PUNCT SCONJ DET NOUN ADP NOUN NOUN VERB DET ADJ NOUN ADP ADJ NOUN PUNCT VERB ADP ADJ NOUN NOUN PART VERB NOUN NOUN PUNCT PROPN VERB ADV ADP VERB DET NOUN ADP DET NOUN ADP DET NOUN PUNCT,0.6944444444444444,27.0,5.648148148148148
294,6,GPT-3.5,"[' In this study, we have presented a novel approach to building and optimizing deeper transformer networks for image classification, challenging the traditional dominance of convolutional neural networks. Through careful exploration of architectural modifications and optimization strategies, we have achieved substantial improvements in the accuracy of deep transformers. Our proposed model not only prevents early saturation issues but also sets a new standard in image classification, reaching 86.5% top-1 accuracy on ImageNet without external data. This result establishes the model as the current state-of-the-art solution, surpassing existing benchmarks in terms of both computational efficiency (FLOPs) and model parameters. Additionally, our model exhibits robust performance on ImageNet with Reassessed labels and ImageNet-V2/match frequency, even in the absence of additional training data. The code and models developed in this work are made publicly available, contributing to the ongoing progress and collaboration in the field of large-scale image classification with transformers.']",conclusion_chunked," In this study, we have presented a novel approach to building and optimizing deeper transformer networks for image classification, challenging the traditional dominance of convolutional neural networks. Through careful exploration of architectural modifications and optimization strategies, we have achieved substantial improvements in the accuracy of deep transformers. Our proposed model not only prevents early saturation issues but also sets a new standard in image classification, reaching 86.5% top-1 accuracy on ImageNet without external data. This result establishes the model as the current state-of-the-art solution, surpassing existing benchmarks in terms of both computational efficiency (FLOPs) and model parameters. Additionally, our model exhibits robust performance on ImageNet with Reassessed labels and ImageNet-V2/match frequency, even in the absence of additional training data. The code and models developed in this work are made publicly available, contributing to the ongoing progress and collaboration in the field of large-scale image classification with transformers.",-0.1479999999999677,2.6406588729545177,294,0.7521213293075562," In this study, we have presented a novel approach to building and optimizing deeper transformer networks for image classification, challenging the traditional dominance of convolutional neural networks. Through careful exploration of architectural modifications and optimization strategies, we have achieved substantial improvements in the accuracy of deep transformers. Our proposed model not only prevents early saturation issues but also sets a new standard in image classification, reaching 00.0 top 0 accuracy on Propname without external data. This result establishes the model as the current state of the art solution, surpassing existing benchmarks in terms of both computational efficiency and model parameters. Additionally, our model exhibits robust performance on Propname with Propname labels and Propname V0match frequency, even in the absence of additional training data. The code and models developed in this work are made publicly available, contributing to the ongoing progress and collaboration in the field of large scale image classification with transformers."," In this study, we have presented a novel approach to building and optimizing deeper transformer networks for image classification, challenging the traditional dominance of convolutional neural networks. Through careful exploration of architectural modifications and optimization strategies, we have achieved substantial improvements in the accuracy of deep transformers. Our proposed model not only prevents early saturation issues but also sets a new standard in image classification, reaching 00.0 top 0 accuracy on Propname without external data. This result establishes the model as the current state of the art solution, surpassing existing benchmarks in terms of both computational efficiency and model parameters. Additionally, our model exhibits robust performance on Propname with Propname labels and Propname V0match frequency, even in the absence of additional training data. The code and models developed in this work are made publicly available, contributing to the ongoing progress and collaboration in the field of large scale image classification with transformers.", ADP DET NOUN PUNCT PRON AUX VERB DET ADJ NOUN ADP VERB CCONJ VERB ADJ NOUN NOUN ADP NOUN NOUN PUNCT VERB DET ADJ NOUN ADP ADJ ADJ NOUN PUNCT ADP ADJ NOUN ADP ADJ NOUN CCONJ NOUN NOUN PUNCT PRON AUX VERB ADJ NOUN ADP DET NOUN ADP ADJ NOUN PUNCT PRON VERB NOUN PART ADV VERB ADJ NOUN NOUN CCONJ ADV VERB DET ADJ NOUN ADP NOUN NOUN PUNCT VERB NUM ADJ NUM NOUN ADP PROPN ADP ADJ NOUN PUNCT DET NOUN VERB DET NOUN ADP DET ADJ NOUN ADP DET NOUN NOUN PUNCT VERB VERB NOUN ADP NOUN ADP PRON ADJ NOUN CCONJ NOUN NOUN PUNCT ADV PUNCT PRON NOUN VERB ADJ NOUN ADP PROPN ADP PROPN NOUN CCONJ PROPN NOUN NOUN PUNCT ADV ADP DET NOUN ADP ADJ NOUN NOUN PUNCT DET NOUN CCONJ NOUN VERB ADP DET NOUN AUX VERB ADV ADJ PUNCT VERB ADP DET ADJ NOUN CCONJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN NOUN ADP NOUN PUNCT,0.6626506024096386,27.666666666666668,5.602409638554217
295,7,GPT-3.5,"[' In conclusion, we propose a straightforward yet powerful architecture for unpaired image-to-image translation tasks, leveraging a fixed-weight image autoencoder as its foundation. The introduction of task-specific residual blocks in the latent space, applied iteratively, yields impressive results across diverse transformations, including style transfer, denoising, and more. A carefully crafted training schedule mitigates the challenges associated with iterative processes. During testing, our approach offers a compelling set of advantages, characterized by a constrained number of weight parameters and a compositional design that facilitates the adjustment of transformation strength based on the number of iterations. This adaptability proves invaluable in scenarios where the nature or magnitude of noise is uncertain. Experimental validations not only illustrate the effectiveness of our method but also showcase comparable or superior performance to CycleGAN, highlighting the potential for achieving high-quality image transformations with significantly reduced model complexity.']",conclusion_chunked," In conclusion, we propose a straightforward yet powerful architecture for unpaired image-to-image translation tasks, leveraging a fixed-weight image autoencoder as its foundation. The introduction of task-specific residual blocks in the latent space, applied iteratively, yields impressive results across diverse transformations, including style transfer, denoising, and more. A carefully crafted training schedule mitigates the challenges associated with iterative processes. During testing, our approach offers a compelling set of advantages, characterized by a constrained number of weight parameters and a compositional design that facilitates the adjustment of transformation strength based on the number of iterations. This adaptability proves invaluable in scenarios where the nature or magnitude of noise is uncertain. Experimental validations not only illustrate the effectiveness of our method but also showcase comparable or superior performance to CycleGAN, highlighting the potential for achieving high-quality image transformations with significantly reduced model complexity.",-7.802619047619032,2.6406588729545177,295,0.5583428144454956," In conclusion, we propose a straightforward yet powerful architecture for unpaired image to image translation tasks, leveraging a fixed weight image autoencoder as its foundation. The introduction of task specific residual blocks in the latent space, applied iteratively, yields impressive results across diverse transformations, including style transfer, denoising, and more. A carefully crafted training schedule mitigates the challenges associated with iterative processes. During testing, our approach offers a compelling set of advantages, characterized by a constrained number of weight parameters and a compositional design that facilitates the adjustment of transformation strength based on the number of iterations. This adaptability proves invaluable in scenarios where the nature or magnitude of noise is uncertain. Experimental validations not only illustrate the effectiveness of our method but also showcase comparable or superior performance to Propname, highlighting the potential for achieving high quality image transformations with significantly reduced model complexity."," In conclusion, we propose a straightforward yet powerful architecture for unpaired image to image translation tasks, leveraging a fixed weight image autoencoder as its foundation. The introduction of task specific residual blocks in the latent space, applied iteratively, yields impressive results across diverse transformations, including style transfer, denoising, and more. A carefully crafted training schedule mitigates the challenges associated with iterative processes. During testing, our approach offers a compelling set of advantages, characterized by a constrained number of weight parameters and a compositional design that facilitates the adjustment of transformation strength based on the number of iterations. This adaptability proves invaluable in scenarios where the nature or magnitude of noise is uncertain. Experimental validations not only illustrate the effectiveness of our method but also showcase comparable or superior performance to Propname, highlighting the potential for achieving high quality image transformations with significantly reduced model complexity.", ADP NOUN PUNCT PRON VERB DET ADJ CCONJ ADJ NOUN ADP ADJ NOUN ADP NOUN NOUN NOUN PUNCT VERB DET VERB NOUN NOUN NOUN ADP PRON NOUN PUNCT DET NOUN ADP NOUN ADJ ADJ NOUN ADP DET ADJ NOUN PUNCT VERB ADV PUNCT VERB ADJ NOUN ADP ADJ NOUN PUNCT VERB NOUN NOUN PUNCT NOUN PUNCT CCONJ ADJ PUNCT DET ADV VERB NOUN NOUN NOUN DET NOUN VERB ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN VERB DET ADJ NOUN ADP NOUN PUNCT VERB ADP DET ADJ NOUN ADP NOUN NOUN CCONJ DET ADJ NOUN PRON VERB DET NOUN ADP NOUN NOUN VERB ADP DET NOUN ADP NOUN PUNCT DET NOUN VERB ADJ ADP NOUN SCONJ DET NOUN CCONJ NOUN ADP NOUN AUX ADJ PUNCT ADJ NOUN PART ADV VERB DET NOUN ADP PRON NOUN CCONJ ADV VERB ADJ CCONJ ADJ NOUN ADP PROPN PUNCT VERB DET NOUN ADP VERB ADJ NOUN NOUN NOUN ADP ADV VERB NOUN NOUN PUNCT,0.7329192546583851,26.833333333333332,5.875776397515528
296,8,GPT-3.5,"[' The development of Llama 2 marks a significant contribution to the field of large language models, offering a diverse and scalable collection of pretrained and fine-tuned models. In particular, Llama 2-Chat demonstrates remarkable performance in dialogue applications, surpassing existing open-source chat models across various benchmarks. Human evaluations affirm the potential of our models as viable alternatives to closed-source counterparts in terms of both helpfulness and safety. We emphasize the importance of responsible development and knowledge-sharing in the era of large language models and provide a detailed account of our fine-tuning methodology and safety enhancements. Through the release of Llama 2 and the associated documentation, we invite the community to build upon our work, fostering collaborative efforts towards advancing the capabilities and ethical considerations of large language models.']",conclusion_chunked," The development of Llama 2 marks a significant contribution to the field of large language models, offering a diverse and scalable collection of pretrained and fine-tuned models. In particular, Llama 2-Chat demonstrates remarkable performance in dialogue applications, surpassing existing open-source chat models across various benchmarks. Human evaluations affirm the potential of our models as viable alternatives to closed-source counterparts in terms of both helpfulness and safety. We emphasize the importance of responsible development and knowledge-sharing in the era of large language models and provide a detailed account of our fine-tuning methodology and safety enhancements. Through the release of Llama 2 and the associated documentation, we invite the community to build upon our work, fostering collaborative efforts towards advancing the capabilities and ethical considerations of large language models.",3.7197500000000048,2.6406588729545177,296,0.8207466006278992," The development of Propname 0 marks a significant contribution to the field of large language models, offering a diverse and scalable collection of pretrained and fine tuned models. In particular, Propname 0 Propname demonstrates remarkable performance in dialogue applications, surpassing existing open source chat models across various benchmarks. Human evaluations affirm the potential of our models as viable alternatives to closed source counterparts in terms of both helpfulness and safety. We emphasize the importance of responsible development and knowledge sharing in the era of large language models and provide a detailed account of our fine tuning methodology and safety enhancements. Through the release of Propname 0 and the associated documentation, we invite the community to build upon our work, fostering collaborative efforts towards advancing the capabilities and ethical considerations of large language models."," The development of Propname 0 marks a significant contribution to the field of large language models, offering a diverse and scalable collection of pretrained and fine tuned models. In particular, Propname 0 Propname demonstrates remarkable performance in dialogue applications, surpassing existing open source chat models across various benchmarks. Human evaluations affirm the potential of our models as viable alternatives to closed source counterparts in terms of both helpfulness and safety. We emphasize the importance of responsible development and knowledge sharing in the era of large language models and provide a detailed account of our fine tuning methodology and safety enhancements. Through the release of Propname 0 and the associated documentation, we invite the community to build upon our work, fostering collaborative efforts towards advancing the capabilities and ethical considerations of large language models.", DET NOUN ADP PROPN NUM VERB DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT VERB DET ADJ CCONJ ADJ NOUN ADP VERB CCONJ ADJ VERB NOUN PUNCT ADP ADJ PUNCT PROPN NUM PROPN VERB ADJ NOUN ADP NOUN NOUN PUNCT VERB VERB ADJ NOUN NOUN NOUN ADP ADJ NOUN PUNCT ADJ NOUN VERB DET NOUN ADP PRON NOUN ADP ADJ NOUN ADP VERB NOUN NOUN ADP NOUN ADP CCONJ NOUN CCONJ NOUN PUNCT PRON VERB DET NOUN ADP ADJ NOUN CCONJ NOUN NOUN ADP DET NOUN ADP ADJ NOUN NOUN CCONJ VERB DET ADJ NOUN ADP PRON ADJ NOUN NOUN CCONJ NOUN NOUN PUNCT ADP DET NOUN ADP PROPN NUM CCONJ DET ADJ NOUN PUNCT PRON VERB DET NOUN PART VERB SCONJ PRON NOUN PUNCT VERB ADJ NOUN ADP VERB DET NOUN CCONJ ADJ NOUN ADP ADJ NOUN NOUN PUNCT,0.6013986013986014,28.6,5.594405594405594
297,9,GPT-3.5,"["" In conclusion, this paper underscores the critical challenge of integrating data from diverse neuroimaging databases, where variability due to factors like age, gender, and acquisition parameters can impact the performance of classification methods. Through experiments on simulated data, we highlight the intricate interactions between data and confounding variables, emphasizing the potential pitfalls in adjusting data from multiple sources. We compare and evaluate three strategies for data adjustment in the context of a Computer-Aided Diagnosis (CAD) system designed to discern healthy subjects from those with Alzheimer's Disease (AD). The findings contribute valuable insights into mitigating variability in large neuroimaging databases, advancing the robustness and interpretability of machine learning models in real-world applications.""]",conclusion_chunked," In conclusion, this paper underscores the critical challenge of integrating data from diverse neuroimaging databases, where variability due to factors like age, gender, and acquisition parameters can impact the performance of classification methods. Through experiments on simulated data, we highlight the intricate interactions between data and confounding variables, emphasizing the potential pitfalls in adjusting data from multiple sources. We compare and evaluate three strategies for data adjustment in the context of a Computer-Aided Diagnosis (CAD) system designed to discern healthy subjects from those with Alzheimer's Disease (AD). The findings contribute valuable insights into mitigating variability in large neuroimaging databases, advancing the robustness and interpretability of machine learning models in real-world applications.",-3.4880067567567323,2.6406588729545177,297,0.5738530158996582," In conclusion, this paper underscores the critical challenge of integrating data from diverse neuroimaging databases, where variability due to factors like age, gender, and acquisition parameters can impact the performance of classification methods. Through experiments on simulated data, we highlight the intricate interactions between data and confounding variables, emphasizing the potential pitfalls in adjusting data from multiple sources. We compare and evaluate three strategies for data adjustment in the context of a Propname Propname Propname system designed to discern healthy subjects from those with Propname Propname. The findings contribute valuable insights into mitigating variability in large neuroimaging databases, advancing the robustness and interpretability of machine learning models in real world applications."," In conclusion, this paper underscores the critical challenge of integrating data from diverse neuroimaging databases, where variability due to factors like age, gender, and acquisition parameters can impact the performance of classification methods. Through experiments on simulated data, we highlight the intricate interactions between data and confounding variables, emphasizing the potential pitfalls in adjusting data from multiple sources. We compare and evaluate three strategies for data adjustment in the context of a Propname Propname Propname system designed to discern healthy subjects from those with Propname Propname. The findings contribute valuable insights into mitigating variability in large neuroimaging databases, advancing the robustness and interpretability of machine learning models in real world applications.", ADP NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP VERB NOUN ADP ADJ VERB NOUN PUNCT SCONJ NOUN ADP ADP NOUN ADP NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN AUX VERB DET NOUN ADP NOUN NOUN PUNCT ADP NOUN ADP VERB NOUN PUNCT PRON VERB DET ADJ NOUN ADP NOUN CCONJ VERB NOUN PUNCT VERB DET ADJ NOUN ADP VERB NOUN ADP ADJ NOUN PUNCT PRON VERB CCONJ VERB NUM NOUN ADP NOUN NOUN ADP DET NOUN ADP DET PROPN PROPN PROPN NOUN VERB PART VERB ADJ NOUN ADP PRON ADP PROPN PROPN PUNCT DET NOUN VERB ADJ NOUN ADP VERB NOUN ADP ADJ VERB NOUN PUNCT VERB DET NOUN CCONJ NOUN ADP NOUN NOUN NOUN ADP ADJ NOUN NOUN PUNCT,0.6967213114754098,30.5,5.942622950819672
298,10,GPT-3.5,"[' In the evolving landscape of deep learning, this thesis has illuminated the symbiotic relationship between architectures and training procedures, with a specific focus on the transformative potential of Transformer architectures in visual understanding. The exploration began with a nuanced investigation into the feasibility of learning with coarse labels, revealing a promising avenue for enhancing training procedures. The subsequent scrutiny of diverse computer vision architectures provided a panoramic view of their features, strengths, and challenges, alongside insights into optimal training strategies. As we navigated through this intricate landscape, the imperative role of training in unlocking the full potential of architectures became increasingly evident. Our evaluations, conducted rigorously across tasks such as image classification on ImageNet, transfer learning, and semantic segmentation, attest to the efficacy of the proposed approaches. By addressing the less mature training procedures for transformers, this research not only contributes to advancing the field but also opens avenues for the development of more robust and versatile deep learning models. In conclusion, the synergy between architecture and training processes emerges as a cornerstone for achieving breakthroughs in deep learning applications. As we continue to unravel the complexities of this dynamic interplay, the quest for refined optimization procedures remains paramount for the continued evolution of machine learning in shaping our technological future.']",conclusion_chunked," In the evolving landscape of deep learning, this thesis has illuminated the symbiotic relationship between architectures and training procedures, with a specific focus on the transformative potential of Transformer architectures in visual understanding. The exploration began with a nuanced investigation into the feasibility of learning with coarse labels, revealing a promising avenue for enhancing training procedures. The subsequent scrutiny of diverse computer vision architectures provided a panoramic view of their features, strengths, and challenges, alongside insights into optimal training strategies. As we navigated through this intricate landscape, the imperative role of training in unlocking the full potential of architectures became increasingly evident. Our evaluations, conducted rigorously across tasks such as image classification on ImageNet, transfer learning, and semantic segmentation, attest to the efficacy of the proposed approaches. By addressing the less mature training procedures for transformers, this research not only contributes to advancing the field but also opens avenues for the development of more robust and versatile deep learning models. In conclusion, the synergy between architecture and training processes emerges as a cornerstone for achieving breakthroughs in deep learning applications. As we continue to unravel the complexities of this dynamic interplay, the quest for refined optimization procedures remains paramount for the continued evolution of machine learning in shaping our technological future.",3.647313388625605,2.6406588729545177,298,0.7318270206451416," In the evolving landscape of deep learning, this thesis has illuminated the symbiotic relationship between architectures and training procedures, with a specific focus on the transformative potential of Propname architectures in visual understanding. The exploration began with a nuanced investigation into the feasibility of learning with coarse labels, revealing a promising avenue for enhancing training procedures. The subsequent scrutiny of diverse computer vision architectures provided a panoramic view of their features, strengths, and challenges, alongside insights into optimal training strategies. As we navigated through this intricate landscape, the imperative role of training in unlocking the full potential of architectures became increasingly evident. Our evaluations, conducted rigorously across tasks such as image classification on Propname, transfer learning, and semantic segmentation, attest to the efficacy of the proposed approaches. By addressing the less mature training procedures for transformers, this research not only contributes to advancing the field but also opens avenues for the development of more robust and versatile deep learning models. In conclusion, the synergy between architecture and training processes emerges as a cornerstone for achieving breakthroughs in deep learning applications. As we continue to unravel the complexities of this dynamic interplay, the quest for refined optimization procedures remains paramount for the continued evolution of machine learning in shaping our technological future."," In the evolving landscape of deep learning, this thesis has illuminated the symbiotic relationship between architectures and training procedures, with a specific focus on the transformative potential of Propname architectures in visual understanding. The exploration began with a nuanced investigation into the feasibility of learning with coarse labels, revealing a promising avenue for enhancing training procedures. The subsequent scrutiny of diverse computer vision architectures provided a panoramic view of their features, strengths, and challenges, alongside insights into optimal training strategies. As we navigated through this intricate landscape, the imperative role of training in unlocking the full potential of architectures became increasingly evident. Our evaluations, conducted rigorously across tasks such as image classification on Propname, transfer learning, and semantic segmentation, attest to the efficacy of the proposed approaches. By addressing the less mature training procedures for transformers, this research not only contributes to advancing the field but also opens avenues for the development of more robust and versatile deep learning models. In conclusion, the synergy between architecture and training processes emerges as a cornerstone for achieving breakthroughs in deep learning applications. As we continue to unravel the complexities of this dynamic interplay, the quest for refined optimization procedures remains paramount for the continued evolution of machine learning in shaping our technological future.", ADP DET VERB NOUN ADP ADJ NOUN PUNCT DET NOUN AUX VERB DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN PUNCT ADP DET ADJ NOUN ADP DET ADJ NOUN ADP PROPN VERB ADP ADJ NOUN PUNCT DET NOUN VERB ADP DET VERB NOUN ADP DET NOUN ADP VERB ADP ADJ NOUN PUNCT VERB DET ADJ NOUN ADP VERB NOUN NOUN PUNCT DET ADJ NOUN ADP ADJ NOUN NOUN NOUN VERB DET ADJ NOUN ADP PRON NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT ADP NOUN ADP ADJ NOUN NOUN PUNCT SCONJ PRON VERB ADP DET ADJ NOUN PUNCT DET ADJ NOUN ADP NOUN ADP VERB DET ADJ NOUN ADP NOUN VERB ADV ADJ PUNCT PRON NOUN PUNCT VERB ADV ADP NOUN ADJ ADP NOUN NOUN ADP PROPN PUNCT NOUN NOUN PUNCT CCONJ ADJ NOUN PUNCT NOUN ADP DET NOUN ADP DET VERB NOUN PUNCT ADP VERB DET ADV ADJ NOUN NOUN ADP NOUN PUNCT DET NOUN PART ADV VERB ADP VERB DET NOUN CCONJ ADV VERB NOUN ADP DET NOUN ADP ADV ADJ CCONJ ADJ ADJ NOUN NOUN PUNCT ADP NOUN PUNCT DET NOUN ADP NOUN CCONJ NOUN NOUN VERB ADP DET NOUN ADP VERB NOUN ADP ADJ NOUN NOUN PUNCT SCONJ PRON VERB PART VERB DET NOUN ADP DET ADJ NOUN PUNCT DET NOUN ADP ADJ NOUN NOUN VERB ADJ ADP DET VERB NOUN ADP NOUN VERB ADP VERB PRON ADJ NOUN PUNCT,0.5879828326180258,29.125,5.759656652360515
299,11,GPT-3.5,"["" In conclusion, ResMLP presents a compelling alternative to traditional CNN architectures for image classification. The architecture's simplicity, relying exclusively on multi-layer perceptrons, does not compromise its performance. Through systematic exploration of training strategies, including heavy data augmentation and optional distillation, ResMLP achieves a remarkable balance between accuracy and model complexity on the ImageNet dataset. Furthermore, the extension of ResMLP to self-supervised learning underscores its ability to learn meaningful representations in the absence of labeled data. This aspect enhances its applicability in scenarios where labeled datasets are scarce or costly to obtain. The adaptation of ResMLP to machine translation, with unexpectedly strong results, emphasizes the architecture's potential for broader applications beyond image classification. To facilitate further exploration and adoption, the paper provides pre-trained ResMLP models and code based on the Timm library. The promising results obtained across various tasks suggest that ResMLP is a valuable addition to the landscape of neural network architectures, offering a simple yet powerful approach to diverse machine learning challenges.""]",conclusion_chunked," In conclusion, ResMLP presents a compelling alternative to traditional CNN architectures for image classification. The architecture's simplicity, relying exclusively on multi-layer perceptrons, does not compromise its performance. Through systematic exploration of training strategies, including heavy data augmentation and optional distillation, ResMLP achieves a remarkable balance between accuracy and model complexity on the ImageNet dataset. Furthermore, the extension of ResMLP to self-supervised learning underscores its ability to learn meaningful representations in the absence of labeled data. This aspect enhances its applicability in scenarios where labeled datasets are scarce or costly to obtain. The adaptation of ResMLP to machine translation, with unexpectedly strong results, emphasizes the architecture's potential for broader applications beyond image classification. To facilitate further exploration and adoption, the paper provides pre-trained ResMLP models and code based on the Timm library. The promising results obtained across various tasks suggest that ResMLP is a valuable addition to the landscape of neural network architectures, offering a simple yet powerful approach to diverse machine learning challenges.",1.3838228527607725,2.6406588729545177,299,0.7537019848823547," In conclusion, Propname presents a compelling alternative to traditional Propname architectures for image classification. The architectures simplicity, relying exclusively on multi layer perceptrons, does not compromise its performance. Through systematic exploration of training strategies, including heavy data augmentation and optional distillation, Propname achieves a remarkable balance between accuracy and model complexity on the Propname dataset. Furthermore, the extension of Propname to self supervised learning underscores its ability to learn meaningful representations in the absence of labeled data. This aspect enhances its applicability in scenarios where labeled datasets are scarce or costly to obtain. The adaptation of Propname to machine translation, with unexpectedly strong results, emphasizes the architectures potential for broader applications beyond image classification. To facilitate further exploration and adoption, the paper provides pre trained Propname models and code based on the Propname library. The promising results obtained across various tasks suggest that Propname is a valuable addition to the landscape of neural network architectures, offering a simple yet powerful approach to diverse machine learning challenges."," In conclusion, Propname presents a compelling alternative to traditional Propname architectures for image classification. The architectures simplicity, relying exclusively on multi layer perceptrons, does not compromise its performance. Through systematic exploration of training strategies, including heavy data augmentation and optional distillation, Propname achieves a remarkable balance between accuracy and model complexity on the Propname dataset. Furthermore, the extension of Propname to self supervised learning underscores its ability to learn meaningful representations in the absence of labeled data. This aspect enhances its applicability in scenarios where labeled datasets are scarce or costly to obtain. The adaptation of Propname to machine translation, with unexpectedly strong results, emphasizes the architectures potential for broader applications beyond image classification. To facilitate further exploration and adoption, the paper provides pre trained Propname models and code based on the Propname library. The promising results obtained across various tasks suggest that Propname is a valuable addition to the landscape of neural network architectures, offering a simple yet powerful approach to diverse machine learning challenges.", ADP NOUN PUNCT PROPN VERB DET ADJ NOUN ADP ADJ PROPN NOUN ADP NOUN NOUN PUNCT DET NOUN NOUN PUNCT VERB ADV ADP ADJ NOUN NOUN PUNCT AUX PART VERB PRON NOUN PUNCT ADP ADJ NOUN ADP NOUN NOUN PUNCT VERB ADJ NOUN NOUN CCONJ ADJ NOUN PUNCT PROPN VERB DET ADJ NOUN ADP NOUN CCONJ NOUN NOUN ADP DET PROPN NOUN PUNCT ADV PUNCT DET NOUN ADP PROPN PART VERB ADJ NOUN VERB PRON NOUN PART VERB ADJ NOUN ADP DET NOUN ADP VERB NOUN PUNCT DET NOUN VERB PRON NOUN ADP NOUN SCONJ VERB NOUN AUX ADJ CCONJ ADJ PART VERB PUNCT DET NOUN ADP PROPN PART VERB NOUN PUNCT ADP ADV ADJ NOUN PUNCT VERB DET NOUN NOUN ADP ADJ NOUN ADP NOUN NOUN PUNCT PART VERB ADJ NOUN CCONJ NOUN PUNCT DET NOUN VERB ADJ VERB PROPN NOUN CCONJ NOUN VERB ADP DET PROPN NOUN PUNCT DET ADJ NOUN VERB ADP ADJ NOUN VERB SCONJ PROPN AUX DET ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT VERB DET ADJ ADV ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT,0.6467391304347826,23.0,5.951086956521739
300,12,GPT-3.5,"[' In conclusion, our work sheds light on a critical concern in the realm of Knowledge Graph Completion (KGC) research, where recent publications tout unprecedented performance levels that surpass established state-of-the-art methods. Our investigation reveals that this trend can be attributed to the use of inappropriate evaluation protocols. To address this, we introduce a simple yet robust evaluation protocol designed to rectify shortcomings in existing methodologies. Crucially, our protocol is adept at handling model bias, a factor that can significantly impact final results. Through extensive experiments and evaluations of numerous existing KGC methods using our proposed protocol, we provide a comprehensive perspective on the true performance of these techniques. The public availability of our reproducible code contributes to fostering transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques, thereby advancing the reliability of research outcomes in this evolving field.']",conclusion_chunked," In conclusion, our work sheds light on a critical concern in the realm of Knowledge Graph Completion (KGC) research, where recent publications tout unprecedented performance levels that surpass established state-of-the-art methods. Our investigation reveals that this trend can be attributed to the use of inappropriate evaluation protocols. To address this, we introduce a simple yet robust evaluation protocol designed to rectify shortcomings in existing methodologies. Crucially, our protocol is adept at handling model bias, a factor that can significantly impact final results. Through extensive experiments and evaluations of numerous existing KGC methods using our proposed protocol, we provide a comprehensive perspective on the true performance of these techniques. The public availability of our reproducible code contributes to fostering transparency and reproducibility in the evaluation of Knowledge Graph Completion techniques, thereby advancing the reliability of research outcomes in this evolving field.",6.095952380952383,2.6406588729545177,300,0.4659420847892761," In conclusion, our work sheds light on a critical concern in the realm of Propname Propname Propname research, where recent publications tout unprecedented performance levels that surpass established state of the art methods. Our investigation reveals that this trend can be attributed to the use of inappropriate evaluation protocols. To address this, we introduce a simple yet robust evaluation protocol designed to rectify shortcomings in existing methodologies. Crucially, our protocol is adept at handling model Propname, a factor that can significantly impact final results. Through extensive experiments and evaluations of numerous existing Propname methods using our proposed protocol, we provide a comprehensive perspective on the true performance of these techniques. The public availability of our reproducible code contributes to fostering transparency and reproducibility in the evaluation of Propname Propname Propname techniques, thereby advancing the reliability of research outcomes in this evolving field."," In conclusion, our work sheds light on a critical concern in the realm of Propname Propname Propname research, where recent publications tout unprecedented performance levels that surpass established state of the art methods. Our investigation reveals that this trend can be attributed to the use of inappropriate evaluation protocols. To address this, we introduce a simple yet robust evaluation protocol designed to rectify shortcomings in existing methodologies. Crucially, our protocol is adept at handling model Propname, a factor that can significantly impact final results. Through extensive experiments and evaluations of numerous existing Propname methods using our proposed protocol, we provide a comprehensive perspective on the true performance of these techniques. The public availability of our reproducible code contributes to fostering transparency and reproducibility in the evaluation of Propname Propname Propname techniques, thereby advancing the reliability of research outcomes in this evolving field.", ADP NOUN PUNCT PRON NOUN VERB NOUN ADP DET ADJ NOUN ADP DET NOUN ADP PROPN PROPN PROPN NOUN PUNCT SCONJ ADJ NOUN VERB ADJ NOUN NOUN PRON VERB VERB NOUN ADP DET NOUN NOUN PUNCT PRON NOUN VERB SCONJ DET NOUN AUX AUX VERB ADP DET NOUN ADP ADJ NOUN NOUN PUNCT PART VERB PRON PUNCT PRON VERB DET ADJ CCONJ ADJ NOUN NOUN VERB PART VERB NOUN ADP VERB NOUN PUNCT ADV PUNCT PRON NOUN AUX ADJ ADP VERB NOUN PROPN PUNCT DET NOUN PRON AUX ADV VERB ADJ NOUN PUNCT ADP ADJ NOUN CCONJ NOUN ADP ADJ VERB PROPN NOUN VERB PRON VERB NOUN PUNCT PRON VERB DET ADJ NOUN ADP DET ADJ NOUN ADP DET NOUN PUNCT DET ADJ NOUN ADP PRON ADJ NOUN VERB ADP VERB NOUN CCONJ NOUN ADP DET NOUN ADP PROPN PROPN PROPN NOUN PUNCT ADV VERB DET NOUN ADP NOUN NOUN ADP DET VERB NOUN PUNCT,0.6258064516129033,25.833333333333332,5.690322580645161
301,13,GPT-3.5,"[' In conclusion, this paper presents a novel strategy to address the latency challenges of autoregressive sequence models during inference while preserving translation accuracy. By incorporating a structured inference module into non-autoregressive models, leveraging an efficient approximation for Conditional Random Fields (CRF), and introducing a dynamic transition technique for positional context modeling, our proposed model achieves significantly enhanced decoding consistency and improved translation performance. Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and approaches the accuracy of purely autoregressive models, showcasing its efficacy in balancing latency reduction and translation quality. The minimal additional latency (8âˆ¼14ms) positions our model as a promising solution for practical applications where both efficiency and accuracy are paramount.']",conclusion_chunked," In conclusion, this paper presents a novel strategy to address the latency challenges of autoregressive sequence models during inference while preserving translation accuracy. By incorporating a structured inference module into non-autoregressive models, leveraging an efficient approximation for Conditional Random Fields (CRF), and introducing a dynamic transition technique for positional context modeling, our proposed model achieves significantly enhanced decoding consistency and improved translation performance. Notably, on the WMT14 En-De dataset, our model outperforms prior non-autoregressive baselines and approaches the accuracy of purely autoregressive models, showcasing its efficacy in balancing latency reduction and translation quality. The minimal additional latency (8âˆ¼14ms) positions our model as a promising solution for practical applications where both efficiency and accuracy are paramount.",-21.709149159663838,2.6406588729545177,301,0.6385694146156311," In conclusion, this paper presents a novel strategy to address the latency challenges of autoregressive sequence models during inference while preserving translation accuracy. By incorporating a structured inference module into non autoregressive models, leveraging an efficient approximation for Propname Propname Propname, and introducing a dynamic transition technique for positional context modeling, our proposed model achieves significantly enhanced decoding consistency and improved translation performance. Notably, on the Propname Propname Propname dataset, our model outperforms prior non autoregressive baselines and approaches the accuracy of purely autoregressive models, showcasing its efficacy in balancing latency reduction and translation quality. The minimal additional latency positions our model as a promising solution for practical applications where both efficiency and accuracy are paramount."," In conclusion, this paper presents a novel strategy to address the latency challenges of autoregressive sequence models during inference while preserving translation accuracy. By incorporating a structured inference module into non autoregressive models, leveraging an efficient approximation for Propname Propname Propname, and introducing a dynamic transition technique for positional context modeling, our proposed model achieves significantly enhanced decoding consistency and improved translation performance. Notably, on the Propname Propname Propname dataset, our model outperforms prior non autoregressive baselines and approaches the accuracy of purely autoregressive models, showcasing its efficacy in balancing latency reduction and translation quality. The minimal additional latency positions our model as a promising solution for practical applications where both efficiency and accuracy are paramount.", ADP NOUN PUNCT DET NOUN VERB DET ADJ NOUN PART VERB DET NOUN NOUN ADP ADJ NOUN NOUN ADP NOUN SCONJ VERB NOUN NOUN PUNCT ADP VERB DET ADJ NOUN NOUN ADP ADJ ADJ NOUN PUNCT VERB DET ADJ NOUN ADP PROPN PROPN PROPN PUNCT CCONJ VERB DET ADJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB NOUN VERB ADV ADJ VERB NOUN CCONJ VERB NOUN NOUN PUNCT ADV PUNCT ADP DET PROPN PROPN PROPN NOUN PUNCT PRON NOUN VERB ADJ ADJ ADJ NOUN CCONJ VERB DET NOUN ADP ADV ADJ NOUN PUNCT VERB PRON NOUN ADP VERB NOUN NOUN CCONJ NOUN NOUN PUNCT DET ADJ ADJ NOUN NOUN PRON NOUN ADP DET ADJ NOUN ADP ADJ NOUN SCONJ CCONJ NOUN CCONJ NOUN AUX ADJ PUNCT,0.6614173228346457,31.75,6.307086614173229
302,14,GPT-3.5,"[' In conclusion, SALMON represents a groundbreaking approach to aligning AI agents based on large language models, minimizing the dependency on extensive human supervision. Leveraging a principle-following reward model trained on synthetic preference data, our method achieves superior performance with minimal human-defined principles. By adjusting these principles during RL training, we gain unprecedented control over preferences, influencing RL-trained policies and obviating the need for online human preferences. Applied to the LLaMA-2-70b base language model, Dromedary-2, our AI assistant, significantly outperforms state-of-the-art systems on various benchmark datasets with minimal human intervention. We have open-sourced the code and model weights, encouraging further exploration and advancements in aligning LLM-based AI agents with heightened supervision efficiency, improved controllability, and scalable oversight.']",conclusion_chunked," In conclusion, SALMON represents a groundbreaking approach to aligning AI agents based on large language models, minimizing the dependency on extensive human supervision. Leveraging a principle-following reward model trained on synthetic preference data, our method achieves superior performance with minimal human-defined principles. By adjusting these principles during RL training, we gain unprecedented control over preferences, influencing RL-trained policies and obviating the need for online human preferences. Applied to the LLaMA-2-70b base language model, Dromedary-2, our AI assistant, significantly outperforms state-of-the-art systems on various benchmark datasets with minimal human intervention. We have open-sourced the code and model weights, encouraging further exploration and advancements in aligning LLM-based AI agents with heightened supervision efficiency, improved controllability, and scalable oversight.",-7.138806722689054,2.6406588729545177,302,0.74989914894104," In conclusion, Propname represents a groundbreaking approach to aligning Propname agents based on large language models, minimizing the dependency on extensive human supervision. Leveraging a principle following reward model trained on synthetic preference data, our method achieves superior performance with minimal human defined principles. By adjusting these principles during Propname training, we gain unprecedented control over preferences, influencing Propname trained policies and obviating the need for online human preferences. Applied to the Propname 0 00b base language model, Propname 0, our Propname assistant, significantly outperforms state of the art systems on various benchmark datasets with minimal human intervention. We have open sourced the code and model weights, encouraging further exploration and advancements in aligning Propname based Propname agents with heightened supervision efficiency, improved controllability, and scalable oversight."," In conclusion, Propname represents a groundbreaking approach to aligning Propname agents based on large language models, minimizing the dependency on extensive human supervision. Leveraging a principle following reward model trained on synthetic preference data, our method achieves superior performance with minimal human defined principles. By adjusting these principles during Propname training, we gain unprecedented control over preferences, influencing Propname trained policies and obviating the need for online human preferences. Applied to the Propname 0 00b base language model, Propname 0, our Propname assistant, significantly outperforms state of the art systems on various benchmark datasets with minimal human intervention. We have open sourced the code and model weights, encouraging further exploration and advancements in aligning Propname based Propname agents with heightened supervision efficiency, improved controllability, and scalable oversight.", ADP NOUN PUNCT PROPN VERB DET VERB NOUN ADP VERB PROPN NOUN VERB ADP ADJ NOUN NOUN PUNCT VERB DET NOUN ADP ADJ ADJ NOUN PUNCT VERB DET NOUN VERB NOUN NOUN VERB ADP ADJ NOUN NOUN PUNCT PRON NOUN VERB ADJ NOUN ADP ADJ ADJ VERB NOUN PUNCT ADP VERB DET NOUN ADP PROPN NOUN PUNCT PRON VERB ADJ NOUN ADP NOUN PUNCT VERB PROPN VERB NOUN CCONJ VERB DET NOUN ADP ADJ ADJ NOUN PUNCT VERB ADP DET PROPN NUM NOUN NOUN NOUN NOUN PUNCT PROPN NUM PUNCT PRON PROPN NOUN PUNCT ADV VERB NOUN ADP DET NOUN NOUN ADP ADJ ADJ NOUN ADP ADJ ADJ NOUN PUNCT PRON AUX ADJ VERB DET NOUN CCONJ NOUN NOUN PUNCT VERB ADJ NOUN CCONJ NOUN ADP VERB PROPN VERB PROPN NOUN ADP VERB NOUN NOUN PUNCT VERB NOUN PUNCT CCONJ ADJ NOUN PUNCT,0.6363636363636364,28.6,5.895104895104895
303,15,GPT-3.5,"[' In conclusion, this paper introduces RotatE, a novel knowledge graph embedding approach tailored for the challenging task of predicting missing links. The RotatE model stands out for its ability to effectively model and infer a spectrum of relation patterns, including symmetry/antisymmetry, inversion, and composition. By representing each relation as a rotation in the complex vector space, RotatE offers a versatile framework for capturing diverse relation characteristics. Furthermore, we propose a novel self-adversarial negative sampling technique, contributing to the efficiency and effectiveness of RotatE model training. Experimental results across multiple benchmark knowledge graphs underscore the scalability and superior performance of RotatE, establishing it as a state-of-the-art model in link prediction. The capabilities of RotatE in capturing intricate relation patterns make it a valuable contribution to the field of knowledge graph embeddings.']",conclusion_chunked," In conclusion, this paper introduces RotatE, a novel knowledge graph embedding approach tailored for the challenging task of predicting missing links. The RotatE model stands out for its ability to effectively model and infer a spectrum of relation patterns, including symmetry/antisymmetry, inversion, and composition. By representing each relation as a rotation in the complex vector space, RotatE offers a versatile framework for capturing diverse relation characteristics. Furthermore, we propose a novel self-adversarial negative sampling technique, contributing to the efficiency and effectiveness of RotatE model training. Experimental results across multiple benchmark knowledge graphs underscore the scalability and superior performance of RotatE, establishing it as a state-of-the-art model in link prediction. The capabilities of RotatE in capturing intricate relation patterns make it a valuable contribution to the field of knowledge graph embeddings.",4.495540712468198,2.6406588729545177,303,0.8256879448890686," In conclusion, this paper introduces Propname, a novel knowledge graph embedding approach tailored for the challenging task of predicting missing links. The Propname model stands out for its ability to effectively model and infer a spectrum of relation patterns, including symmetryantisymmetry, inversion, and composition. By representing each relation as a rotation in the complex vector space, Propname offers a versatile framework for capturing diverse relation characteristics. Furthermore, we propose a novel self adversarial negative sampling technique, contributing to the efficiency and effectiveness of Propname model training. Experimental results across multiple benchmark knowledge graphs underscore the scalability and superior performance of Propname, establishing it as a state of the art model in link prediction. The capabilities of Propname in capturing intricate relation patterns make it a valuable contribution to the field of knowledge graph embeddings."," In conclusion, this paper introduces Propname, a novel knowledge graph embedding approach tailored for the challenging task of predicting missing links. The Propname model stands out for its ability to effectively model and infer a spectrum of relation patterns, including symmetryantisymmetry, inversion, and composition. By representing each relation as a rotation in the complex vector space, Propname offers a versatile framework for capturing diverse relation characteristics. Furthermore, we propose a novel self adversarial negative sampling technique, contributing to the efficiency and effectiveness of Propname model training. Experimental results across multiple benchmark knowledge graphs underscore the scalability and superior performance of Propname, establishing it as a state of the art model in link prediction. The capabilities of Propname in capturing intricate relation patterns make it a valuable contribution to the field of knowledge graph embeddings.", ADP NOUN PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN NOUN VERB NOUN VERB ADP DET ADJ NOUN ADP VERB ADJ NOUN PUNCT DET PROPN NOUN VERB ADP ADP PRON NOUN PART ADV VERB CCONJ VERB DET NOUN ADP NOUN NOUN PUNCT VERB NOUN PUNCT NOUN PUNCT CCONJ NOUN PUNCT ADP VERB DET NOUN ADP DET NOUN ADP DET ADJ NOUN NOUN PUNCT PROPN VERB DET ADJ NOUN ADP VERB ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB DET ADJ NOUN ADJ ADJ NOUN NOUN PUNCT VERB ADP DET NOUN CCONJ NOUN ADP PROPN NOUN NOUN PUNCT ADJ NOUN ADP ADJ ADJ NOUN NOUN VERB DET NOUN CCONJ ADJ NOUN ADP PROPN PUNCT VERB PRON ADP DET NOUN ADP DET NOUN NOUN ADP NOUN NOUN PUNCT DET NOUN ADP PROPN ADP VERB ADJ NOUN NOUN VERB PRON DET ADJ NOUN ADP DET NOUN ADP NOUN NOUN NOUN PUNCT,0.6040268456375839,24.833333333333332,5.651006711409396
304,16,GPT-3.5,"[' In conclusion, RECITation-augmented gEneration (RECITE) emerges as a groundbreaking paradigm, offering a fresh perspective on enhancing factual knowledge generation in Large Language Models (LLMs). By introducing a recite-first approach, RECITE minimizes reliance on external corpuses, addressing the limitations of conventional retrieval-augmented models. Our experiments across multiple closed-book question answering (CBQA) tasks, leveraging four pre-trained models—PaLM, UL2, OPT, and Codex—underscore the potency of RECITE in achieving state-of-the-art performance. The recite-and-answer scheme introduced by RECITE not only demonstrates its efficacy in improving accuracy but also positions itself as a versatile solution applicable to various knowledge-intensive Natural Language Processing (NLP) tasks. This paper contributes not only a novel paradigm but also comprehensive empirical evidence supporting its effectiveness. As we make our code publicly available, we encourage further exploration and adoption of RECITE, anticipating its integration into the evolving landscape of large language models and knowledge-intensive NLP applications.']",conclusion_chunked," In conclusion, RECITation-augmented gEneration (RECITE) emerges as a groundbreaking paradigm, offering a fresh perspective on enhancing factual knowledge generation in Large Language Models (LLMs). By introducing a recite-first approach, RECITE minimizes reliance on external corpuses, addressing the limitations of conventional retrieval-augmented models. Our experiments across multiple closed-book question answering (CBQA) tasks, leveraging four pre-trained models—PaLM, UL2, OPT, and Codex—underscore the potency of RECITE in achieving state-of-the-art performance. The recite-and-answer scheme introduced by RECITE not only demonstrates its efficacy in improving accuracy but also positions itself as a versatile solution applicable to various knowledge-intensive Natural Language Processing (NLP) tasks. This paper contributes not only a novel paradigm but also comprehensive empirical evidence supporting its effectiveness. As we make our code publicly available, we encourage further exploration and adoption of RECITE, anticipating its integration into the evolving landscape of large language models and knowledge-intensive NLP applications.",-10.838153153153144,2.6406588729545177,304,0.7515892386436462," In conclusion, Propname augmented Propname emerges as a groundbreaking paradigm, offering a fresh perspective on enhancing factual knowledge generation in Large Propname Propname. By introducing a recite first approach, Propname minimizes reliance on external corpuses, addressing the limitations of conventional retrieval augmented models. Our experiments across multiple closed book question answering tasks, leveraging four pre trained Propname, Propname, Propname, and Codexunderscore the potency of Propname in achieving state of the art performance. The recite and answer scheme introduced by Propname not only demonstrates its efficacy in improving accuracy but also positions itself as a versatile solution applicable to various knowledge intensive Propname Propname Propname tasks. This paper contributes not only a novel paradigm but also comprehensive empirical evidence supporting its effectiveness. As we make our code publicly available, we encourage further exploration and adoption of Propname, anticipating its integration into the evolving landscape of large language models and knowledge intensive Propname applications."," In conclusion, Propname augmented Propname emerges as a groundbreaking paradigm, offering a fresh perspective on enhancing factual knowledge generation in Large Propname Propname. By introducing a recite first approach, Propname minimizes reliance on external corpuses, addressing the limitations of conventional retrieval augmented models. Our experiments across multiple closed book question answering tasks, leveraging four pre trained Propname, Propname, Propname, and Codexunderscore the potency of Propname in achieving state of the art performance. The recite and answer scheme introduced by Propname not only demonstrates its efficacy in improving accuracy but also positions itself as a versatile solution applicable to various knowledge intensive Propname Propname Propname tasks. This paper contributes not only a novel paradigm but also comprehensive empirical evidence supporting its effectiveness. As we make our code publicly available, we encourage further exploration and adoption of Propname, anticipating its integration into the evolving landscape of large language models and knowledge intensive Propname applications.", ADP NOUN PUNCT PROPN VERB PROPN VERB ADP DET NOUN NOUN PUNCT VERB DET ADJ NOUN ADP VERB ADJ NOUN NOUN ADP ADJ PROPN PROPN PUNCT ADP VERB DET ADJ ADJ NOUN PUNCT PROPN NOUN NOUN ADP ADJ NOUN PUNCT VERB DET NOUN ADP ADJ NOUN VERB NOUN PUNCT PRON NOUN ADP ADJ ADJ NOUN NOUN NOUN NOUN PUNCT VERB NUM ADJ VERB PROPN PUNCT PROPN PUNCT PROPN PUNCT CCONJ VERB DET NOUN ADP PROPN ADP VERB NOUN ADP DET NOUN NOUN PUNCT DET NOUN CCONJ NOUN NOUN VERB ADP PROPN PART ADV VERB PRON NOUN ADP VERB NOUN CCONJ ADV VERB PRON ADP DET ADJ NOUN ADJ ADP ADJ NOUN ADJ PROPN PROPN PROPN NOUN PUNCT DET NOUN VERB PART ADV DET ADJ NOUN CCONJ ADV ADJ ADJ NOUN VERB PRON NOUN PUNCT SCONJ PRON VERB PRON NOUN ADV ADJ PUNCT PRON VERB ADJ NOUN CCONJ NOUN ADP PROPN PUNCT VERB PRON NOUN ADP DET VERB NOUN ADP ADJ NOUN NOUN CCONJ NOUN ADJ PROPN NOUN PUNCT,0.6369047619047619,28.0,5.880952380952381
305,17,GPT-3.5,"["" In this paper, we present MobileBERT, a novel approach to addressing the challenges associated with deploying large pre-trained NLP models on resource-limited mobile devices. By compressing and accelerating the widely used BERT model, MobileBERT achieves a significant reduction in size and improvement in speed without compromising performance. The task-agnostic nature of MobileBERT, inherited from the original BERT, ensures its applicability to various downstream NLP tasks through straightforward fine-tuning. The architectural enhancements, including bottleneck structures and a nuanced balance between self-attentions and feed-forward networks, contribute to the model's efficiency. Empirical studies validate the effectiveness of MobileBERT, showcasing a 4.3× size reduction and a 5.5× speedup compared to BERTBASE, with competitive results on standard benchmarks. MobileBERT's GLUE score of 77.7 on natural language inference tasks and its high performance on the SQuAD question answering task underscore its practical viability for real-world applications on mobile devices.""]",conclusion_chunked," In this paper, we present MobileBERT, a novel approach to addressing the challenges associated with deploying large pre-trained NLP models on resource-limited mobile devices. By compressing and accelerating the widely used BERT model, MobileBERT achieves a significant reduction in size and improvement in speed without compromising performance. The task-agnostic nature of MobileBERT, inherited from the original BERT, ensures its applicability to various downstream NLP tasks through straightforward fine-tuning. The architectural enhancements, including bottleneck structures and a nuanced balance between self-attentions and feed-forward networks, contribute to the model's efficiency. Empirical studies validate the effectiveness of MobileBERT, showcasing a 4.3× size reduction and a 5.5× speedup compared to BERTBASE, with competitive results on standard benchmarks. MobileBERT's GLUE score of 77.7 on natural language inference tasks and its high performance on the SQuAD question answering task underscore its practical viability for real-world applications on mobile devices.",3.7706609195402336,2.6406588729545177,305,0.7335652709007263," In this paper, we present Propname, a novel approach to addressing the challenges associated with deploying large pre trained NLP models on resource limited mobile devices. By compressing and accelerating the widely used Propname model, Propname achieves a significant reduction in size and improvement in speed without compromising performance. The task agnostic nature of Propname, inherited from the original Propname, ensures its applicability to various downstream Propname tasks through straightforward fine tuning. The architectural enhancements, including bottleneck structures and a nuanced balance between self attentions and feed forward networks, contribute to the models efficiency. Empirical studies validate the effectiveness of Propname, showcasing a 0.0 size reduction and a 0.0 speedup compared to Propname, with competitive results on standard benchmarks. MobileBERTs Propname score of 00.0 on natural language inference tasks and its high performance on the SQuAD question answering task underscore its practical viability for real world applications on mobile devices."," In this paper, we present Propname, a novel approach to addressing the challenges associated with deploying large pre trained NLP models on resource limited mobile devices. By compressing and accelerating the widely used Propname model, Propname achieves a significant reduction in size and improvement in speed without compromising performance. The task agnostic nature of Propname, inherited from the original Propname, ensures its applicability to various downstream Propname tasks through straightforward fine tuning. The architectural enhancements, including bottleneck structures and a nuanced balance between self attentions and feed forward networks, contribute to the models efficiency. Empirical studies validate the effectiveness of Propname, showcasing a 0.0 size reduction and a 0.0 speedup compared to Propname, with competitive results on standard benchmarks. MobileBERTs Propname score of 00.0 on natural language inference tasks and its high performance on the SQuAD question answering task underscore its practical viability for real world applications on mobile devices.", ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ NOUN ADP VERB DET NOUN VERB ADP VERB ADJ ADJ VERB NOUN NOUN ADP NOUN ADJ ADJ NOUN PUNCT ADP VERB CCONJ VERB DET ADV VERB PROPN NOUN PUNCT PROPN VERB DET ADJ NOUN ADP NOUN CCONJ NOUN ADP NOUN ADP VERB NOUN PUNCT DET NOUN ADJ NOUN ADP PROPN PUNCT VERB ADP DET ADJ PROPN PUNCT VERB PRON NOUN ADP ADJ ADJ PROPN NOUN ADP ADJ ADJ NOUN PUNCT DET ADJ NOUN PUNCT VERB NOUN NOUN CCONJ DET VERB NOUN ADP NOUN NOUN CCONJ VERB ADJ NOUN PUNCT VERB ADP DET NOUN NOUN PUNCT ADJ NOUN VERB DET NOUN ADP PROPN PUNCT VERB DET NUM NOUN NOUN CCONJ DET NUM NOUN VERB ADP PROPN PUNCT ADP ADJ NOUN ADP ADJ NOUN PUNCT NOUN PROPN NOUN ADP NUM ADP ADJ NOUN NOUN NOUN CCONJ PRON ADJ NOUN ADP DET ADJ NOUN VERB NOUN VERB PRON ADJ NOUN ADP ADJ NOUN NOUN ADP ADJ NOUN PUNCT,0.6484848484848484,27.5,5.696969696969697
306,18,GPT-3.5,"[' In conclusion, this paper introduces PEER, a collaborative language model designed to emulate the entire writing process, addressing critical limitations in existing language models related to collaborative writing scenarios. PEER exhibits the capability to generate drafts, propose edits, and provide explanations for its actions, unlocking new dimensions of controllability and adaptability. The novel approach of training multiple instances of PEER to cover various stages of the writing process, coupled with self-training techniques, enhances the quality, quantity, and diversity of training data. This empowers PEER to operate effectively in domains without edit histories and improves its ability to follow instructions, generate insightful comments, and explain its actions. Across diverse domains and editing tasks, PEER demonstrates strong and versatile performance, showcasing its potential as a valuable tool for collaborative writing processes.']",conclusion_chunked," In conclusion, this paper introduces PEER, a collaborative language model designed to emulate the entire writing process, addressing critical limitations in existing language models related to collaborative writing scenarios. PEER exhibits the capability to generate drafts, propose edits, and provide explanations for its actions, unlocking new dimensions of controllability and adaptability. The novel approach of training multiple instances of PEER to cover various stages of the writing process, coupled with self-training techniques, enhances the quality, quantity, and diversity of training data. This empowers PEER to operate effectively in domains without edit histories and improves its ability to follow instructions, generate insightful comments, and explain its actions. Across diverse domains and editing tasks, PEER demonstrates strong and versatile performance, showcasing its potential as a valuable tool for collaborative writing processes.",1.610790697674446,2.6406588729545177,306,0.8481831550598145," In conclusion, this paper introduces Propname, a collaborative language model designed to emulate the entire writing process, addressing critical limitations in existing language models related to collaborative writing scenarios. PEER exhibits the capability to generate drafts, propose edits, and provide explanations for its actions, unlocking new dimensions of controllability and adaptability. The novel approach of training multiple instances of Propname to cover various stages of the writing process, coupled with self training techniques, enhances the quality, quantity, and diversity of training data. This empowers PEER to operate effectively in domains without edit histories and improves its ability to follow instructions, generate insightful comments, and explain its actions. Across diverse domains and editing tasks, Propname demonstrates strong and versatile performance, showcasing its potential as a valuable tool for collaborative writing processes."," In conclusion, this paper introduces Propname, a collaborative language model designed to emulate the entire writing process, addressing critical limitations in existing language models related to collaborative writing scenarios. PEER exhibits the capability to generate drafts, propose edits, and provide explanations for its actions, unlocking new dimensions of controllability and adaptability. The novel approach of training multiple instances of Propname to cover various stages of the writing process, coupled with self training techniques, enhances the quality, quantity, and diversity of training data. This empowers PEER to operate effectively in domains without edit histories and improves its ability to follow instructions, generate insightful comments, and explain its actions. Across diverse domains and editing tasks, Propname demonstrates strong and versatile performance, showcasing its potential as a valuable tool for collaborative writing processes.", ADP NOUN PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN NOUN VERB PART VERB DET ADJ NOUN NOUN PUNCT VERB ADJ NOUN ADP VERB NOUN NOUN VERB ADP ADJ NOUN NOUN PUNCT ADJ VERB DET NOUN PART VERB NOUN PUNCT VERB NOUN PUNCT CCONJ VERB NOUN ADP PRON NOUN PUNCT VERB ADJ NOUN ADP NOUN CCONJ NOUN PUNCT DET ADJ NOUN ADP VERB ADJ NOUN ADP PROPN PART VERB ADJ NOUN ADP DET NOUN NOUN PUNCT VERB ADP NOUN NOUN NOUN PUNCT VERB DET NOUN PUNCT NOUN PUNCT CCONJ NOUN ADP NOUN NOUN PUNCT DET NOUN VERB PART VERB ADV ADP NOUN ADP NOUN NOUN CCONJ VERB PRON NOUN PART VERB NOUN PUNCT VERB ADJ NOUN PUNCT CCONJ VERB PRON NOUN PUNCT ADP ADJ NOUN CCONJ NOUN NOUN PUNCT PROPN VERB ADJ CCONJ ADJ NOUN PUNCT VERB PRON NOUN ADP DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT,0.6241610738255033,29.8,5.6375838926174495
307,19,GPT-3.5,"["" In conclusion, this paper introduces BERTRAM, a powerful architecture derived from BERT, with a specific focus on addressing the challenges associated with rare words in pretrained language models. Leveraging insights from static word embeddings, BERTRAM facilitates the interaction between the surface form and contexts of words within a deep architecture. The integration of BERTRAM into BERT results in significant performance enhancements, particularly in the representation of rare and medium-frequency words. This improvement is consistently demonstrated across a rare word probing task and three downstream tasks, underscoring BERTRAM's efficacy in ameliorating the treatment of rare words within the context of pretrained language models. The findings contribute to advancing our understanding and strategies for handling rare words in contemporary NLP applications.""]",conclusion_chunked," In conclusion, this paper introduces BERTRAM, a powerful architecture derived from BERT, with a specific focus on addressing the challenges associated with rare words in pretrained language models. Leveraging insights from static word embeddings, BERTRAM facilitates the interaction between the surface form and contexts of words within a deep architecture. The integration of BERTRAM into BERT results in significant performance enhancements, particularly in the representation of rare and medium-frequency words. This improvement is consistently demonstrated across a rare word probing task and three downstream tasks, underscoring BERTRAM's efficacy in ameliorating the treatment of rare words within the context of pretrained language models. The findings contribute to advancing our understanding and strategies for handling rare words in contemporary NLP applications.",12.056151260504208,2.6406588729545177,307,0.775482714176178," In conclusion, this paper introduces Propname, a powerful architecture derived from Propname, with a specific focus on addressing the challenges associated with rare words in pretrained language models. Leveraging insights from static word embeddings, Propname facilitates the interaction between the surface form and contexts of words within a deep architecture. The integration of Propname into Propname results in significant performance enhancements, particularly in the representation of rare and medium frequency words. This improvement is consistently demonstrated across a rare word probing task and three downstream tasks, underscoring BERTRAMs efficacy in ameliorating the treatment of rare words within the context of pretrained language models. The findings contribute to advancing our understanding and strategies for handling rare words in contemporary Propname applications."," In conclusion, this paper introduces Propname, a powerful architecture derived from Propname, with a specific focus on addressing the challenges associated with rare words in pretrained language models. Leveraging insights from static word embeddings, Propname facilitates the interaction between the surface form and contexts of words within a deep architecture. The integration of Propname into Propname results in significant performance enhancements, particularly in the representation of rare and medium frequency words. This improvement is consistently demonstrated across a rare word probing task and three downstream tasks, underscoring BERTRAMs efficacy in ameliorating the treatment of rare words within the context of pretrained language models. The findings contribute to advancing our understanding and strategies for handling rare words in contemporary Propname applications.", ADP NOUN PUNCT DET NOUN VERB PROPN PUNCT DET ADJ NOUN VERB ADP PROPN PUNCT ADP DET ADJ NOUN ADP VERB DET NOUN VERB ADP ADJ NOUN ADP VERB NOUN NOUN PUNCT VERB NOUN ADP ADJ NOUN NOUN PUNCT PROPN VERB DET NOUN ADP DET NOUN NOUN CCONJ NOUN ADP NOUN ADP DET ADJ NOUN PUNCT DET NOUN ADP PROPN ADP PROPN NOUN ADP ADJ NOUN NOUN PUNCT ADV ADP DET NOUN ADP ADJ CCONJ ADJ ADJ NOUN PUNCT DET NOUN AUX ADV VERB ADP DET ADJ NOUN VERB NOUN CCONJ NUM ADJ NOUN PUNCT VERB NOUN NOUN ADP VERB DET NOUN ADP ADJ NOUN ADP DET NOUN ADP VERB NOUN NOUN PUNCT DET NOUN VERB ADP VERB PRON NOUN CCONJ NOUN ADP VERB ADJ NOUN ADP ADJ PROPN NOUN PUNCT,0.6183206106870229,26.2,5.877862595419847
308,20,GPT-3.5,"[' In conclusion, this paper introduces Pattern-Exploiting Training (PET), a novel semi-supervised learning approach that effectively combines the strengths of unsupervised and supervised learning for natural language processing (NLP) tasks. By reformulating input examples as cloze-style phrases, PET provides valuable guidance to language models in understanding specific tasks. Soft labels assigned to a large set of unlabeled examples using these phrases enable effective leveraging of unlabeled data. Standard supervised training on the resulting training set leads to superior performance across various tasks and languages, outperforming both traditional supervised training and strong semi-supervised approaches. PET emerges as a promising strategy, especially in low-resource settings, showcasing its potential to enhance the efficiency and efficacy of language models in NLP applications.']",conclusion_chunked," In conclusion, this paper introduces Pattern-Exploiting Training (PET), a novel semi-supervised learning approach that effectively combines the strengths of unsupervised and supervised learning for natural language processing (NLP) tasks. By reformulating input examples as cloze-style phrases, PET provides valuable guidance to language models in understanding specific tasks. Soft labels assigned to a large set of unlabeled examples using these phrases enable effective leveraging of unlabeled data. Standard supervised training on the resulting training set leads to superior performance across various tasks and languages, outperforming both traditional supervised training and strong semi-supervised approaches. PET emerges as a promising strategy, especially in low-resource settings, showcasing its potential to enhance the efficiency and efficacy of language models in NLP applications.",0.14553846153847871,2.6406588729545177,308,0.7927700877189636," In conclusion, this paper introduces Propname Propname Propname, a novel semi supervised learning approach that effectively combines the strengths of unsupervised and supervised learning for natural language processing tasks. By reformulating input examples as cloze style phrases, Propname provides valuable guidance to language models in understanding specific tasks. Soft labels assigned to a large set of unlabeled examples using these phrases enable effective leveraging of unlabeled data. Standard supervised training on the resulting training set leads to superior performance across various tasks and languages, outperforming both traditional supervised training and strong semi supervised approaches. Propname emerges as a promising strategy, especially in low resource settings, showcasing its potential to enhance the efficiency and efficacy of language models in Propname applications."," In conclusion, this paper introduces Propname Propname Propname, a novel semi supervised learning approach that effectively combines the strengths of unsupervised and supervised learning for natural language processing tasks. By reformulating input examples as cloze style phrases, Propname provides valuable guidance to language models in understanding specific tasks. Soft labels assigned to a large set of unlabeled examples using these phrases enable effective leveraging of unlabeled data. Standard supervised training on the resulting training set leads to superior performance across various tasks and languages, outperforming both traditional supervised training and strong semi supervised approaches. Propname emerges as a promising strategy, especially in low resource settings, showcasing its potential to enhance the efficiency and efficacy of language models in Propname applications.", ADP NOUN PUNCT DET NOUN VERB PROPN PROPN PROPN PUNCT DET ADJ ADJ ADJ NOUN NOUN PRON ADV VERB DET NOUN ADP ADJ CCONJ ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT ADP VERB NOUN NOUN SCONJ VERB NOUN NOUN PUNCT PROPN VERB ADJ NOUN ADP NOUN NOUN ADP VERB ADJ NOUN PUNCT ADJ NOUN VERB ADP DET ADJ NOUN ADP ADJ NOUN VERB DET NOUN VERB ADJ NOUN ADP ADJ NOUN PUNCT ADJ ADJ NOUN ADP DET VERB NOUN NOUN VERB ADP ADJ NOUN ADP ADJ NOUN CCONJ NOUN PUNCT VERB CCONJ ADJ ADJ NOUN CCONJ ADJ ADJ ADJ NOUN PUNCT PROPN VERB ADP DET ADJ NOUN PUNCT ADV ADP ADJ NOUN NOUN PUNCT VERB PRON NOUN PART VERB DET NOUN CCONJ NOUN ADP NOUN NOUN ADP PROPN NOUN PUNCT,0.6412213740458015,26.2,5.938931297709924
309,21,GPT-3.5,"[' In conclusion, this paper introduces a novel architecture for learning word embeddings, specifically designed to address the challenge of effectively representing novel words in natural language processing (NLP) systems. By incorporating information from both surface-form and context, our model demonstrates significant improvements in embedding quality, as evidenced by state-of-the-art performance on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, offering a practical and easily integrable solution. This model can be seamlessly incorporated into any existing NLP system, enhancing its ability to handle novel words and thereby contributing to the adaptability and robustness of NLP applications.']",conclusion_chunked," In conclusion, this paper introduces a novel architecture for learning word embeddings, specifically designed to address the challenge of effectively representing novel words in natural language processing (NLP) systems. By incorporating information from both surface-form and context, our model demonstrates significant improvements in embedding quality, as evidenced by state-of-the-art performance on the Definitional Nonce and Contextual Rare Words datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, offering a practical and easily integrable solution. This model can be seamlessly incorporated into any existing NLP system, enhancing its ability to handle novel words and thereby contributing to the adaptability and robustness of NLP applications.",-7.098979357798157,2.6406588729545177,309,0.7094637751579285," In conclusion, this paper introduces a novel architecture for learning word embeddings, specifically designed to address the challenge of effectively representing novel words in natural language processing systems. By incorporating information from both surface form and context, our model demonstrates significant improvements in embedding quality, as evidenced by state of the art performance on the Propname Propname and Propname Propname Propname datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, offering a practical and easily integrable solution. This model can be seamlessly incorporated into any existing NLP system, enhancing its ability to handle novel words and thereby contributing to the adaptability and robustness of Propname applications."," In conclusion, this paper introduces a novel architecture for learning word embeddings, specifically designed to address the challenge of effectively representing novel words in natural language processing systems. By incorporating information from both surface form and context, our model demonstrates significant improvements in embedding quality, as evidenced by state of the art performance on the Propname Propname and Propname Propname Propname datasets. Notably, our approach requires only an embedding set and an unlabeled corpus for training, offering a practical and easily integrable solution. This model can be seamlessly incorporated into any existing NLP system, enhancing its ability to handle novel words and thereby contributing to the adaptability and robustness of Propname applications.", ADP NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP VERB NOUN NOUN PUNCT ADV VERB PART VERB DET NOUN ADP ADV VERB ADJ NOUN ADP ADJ NOUN NOUN NOUN PUNCT ADP VERB NOUN ADP DET NOUN NOUN CCONJ NOUN PUNCT PRON NOUN VERB ADJ NOUN ADP VERB NOUN PUNCT SCONJ VERB ADP NOUN ADP DET NOUN NOUN ADP DET PROPN PROPN CCONJ PROPN PROPN PROPN NOUN PUNCT ADV PUNCT PRON NOUN VERB ADV DET VERB NOUN CCONJ DET ADJ NOUN ADP NOUN PUNCT VERB DET ADJ CCONJ ADV ADJ NOUN PUNCT DET NOUN AUX AUX ADV VERB ADP DET VERB NOUN NOUN PUNCT VERB PRON NOUN PART VERB ADJ NOUN CCONJ ADV VERB ADP DET NOUN CCONJ NOUN ADP PROPN NOUN PUNCT,0.7073170731707317,30.75,5.650406504065041
310,22,GPT-3.5,"[' In conclusion, this paper introduces a paradigm shift in the pursuit of high-quality sentence embeddings, circumventing the limitations associated with labeled data, finetuning, and modifications to pretraining objectives. Leveraging the generative abilities of large and high-performing pretrained language models (PLMs), we showcase a fully unsupervised approach that produces impressive results on various semantic textual similarity datasets. The demonstrated efficacy of our methodology not only highlights its potential for practical applications but also invites further exploration into the synergies between generative language models and unsupervised learning techniques. This innovative approach opens new avenues for research and development in the realm of natural language processing and representation learning.']",conclusion_chunked," In conclusion, this paper introduces a paradigm shift in the pursuit of high-quality sentence embeddings, circumventing the limitations associated with labeled data, finetuning, and modifications to pretraining objectives. Leveraging the generative abilities of large and high-performing pretrained language models (PLMs), we showcase a fully unsupervised approach that produces impressive results on various semantic textual similarity datasets. The demonstrated efficacy of our methodology not only highlights its potential for practical applications but also invites further exploration into the synergies between generative language models and unsupervised learning techniques. This innovative approach opens new avenues for research and development in the realm of natural language processing and representation learning.",-13.205896226415092,2.6406588729545177,310,0.6422692537307739," In conclusion, this paper introduces a paradigm shift in the pursuit of high quality sentence embeddings, circumventing the limitations associated with labeled data, finetuning, and modifications to pretraining objectives. Leveraging the generative abilities of large and high performing pretrained language models, we showcase a fully unsupervised approach that produces impressive results on various semantic textual similarity datasets. The demonstrated efficacy of our methodology not only highlights its potential for practical applications but also invites further exploration into the synergies between generative language models and unsupervised learning techniques. This innovative approach opens new avenues for research and development in the realm of natural language processing and representation learning."," In conclusion, this paper introduces a paradigm shift in the pursuit of high quality sentence embeddings, circumventing the limitations associated with labeled data, finetuning, and modifications to pretraining objectives. Leveraging the generative abilities of large and high performing pretrained language models, we showcase a fully unsupervised approach that produces impressive results on various semantic textual similarity datasets. The demonstrated efficacy of our methodology not only highlights its potential for practical applications but also invites further exploration into the synergies between generative language models and unsupervised learning techniques. This innovative approach opens new avenues for research and development in the realm of natural language processing and representation learning.", ADP NOUN PUNCT DET NOUN VERB DET NOUN NOUN ADP DET NOUN ADP ADJ NOUN NOUN NOUN PUNCT VERB DET NOUN VERB ADP VERB NOUN PUNCT VERB PUNCT CCONJ NOUN ADP VERB NOUN PUNCT VERB DET ADJ NOUN ADP ADJ CCONJ ADJ VERB VERB NOUN NOUN PUNCT PRON VERB DET ADV ADJ NOUN PRON VERB ADJ NOUN ADP ADJ ADJ ADJ NOUN NOUN PUNCT DET ADJ NOUN ADP PRON NOUN PART ADV VERB PRON NOUN ADP ADJ NOUN CCONJ ADV VERB ADJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN CCONJ VERB VERB NOUN PUNCT DET ADJ NOUN VERB ADJ NOUN ADP NOUN CCONJ NOUN ADP DET NOUN ADP ADJ NOUN NOUN CCONJ NOUN NOUN PUNCT,0.75,29.0,6.146551724137931
311,23,GPT-3.5,"[' In conclusion, this paper introduces GENPET as a promising method that successfully marries pretrained language models with task descriptions for enhanced text generation capabilities. By extending the concept of pattern-exploiting training to generative tasks, GENPET overcomes challenges related to task description clarity, model utilization, and overfitting prevention. Through empirical validation on summarization and headline generation datasets, GENPET consistently outperforms robust baselines in few-shot scenarios, emphasizing its efficacy and versatility. The presented approach not only contributes to the ongoing discourse on improving data efficiency in generative settings but also opens avenues for further exploration in combining task descriptions and example-based learning for text generation tasks.']",conclusion_chunked," In conclusion, this paper introduces GENPET as a promising method that successfully marries pretrained language models with task descriptions for enhanced text generation capabilities. By extending the concept of pattern-exploiting training to generative tasks, GENPET overcomes challenges related to task description clarity, model utilization, and overfitting prevention. Through empirical validation on summarization and headline generation datasets, GENPET consistently outperforms robust baselines in few-shot scenarios, emphasizing its efficacy and versatility. The presented approach not only contributes to the ongoing discourse on improving data efficiency in generative settings but also opens avenues for further exploration in combining task descriptions and example-based learning for text generation tasks.",-13.972307692307652,2.6406588729545177,311,0.6316918134689331," In conclusion, this paper introduces GENPET as a promising method that successfully marries pretrained language models with task descriptions for enhanced text generation capabilities. By extending the concept of pattern exploiting training to generative tasks, GENPET overcomes challenges related to task description clarity, model utilization, and overfitting prevention. Through empirical validation on summarization and headline generation datasets, GENPET consistently outperforms robust baselines in few shot scenarios, emphasizing its efficacy and versatility. The presented approach not only contributes to the ongoing discourse on improving data efficiency in generative settings but also opens avenues for further exploration in combining task descriptions and example based learning for text generation tasks."," In conclusion, this paper introduces GENPET as a promising method that successfully marries pretrained language models with task descriptions for enhanced text generation capabilities. By extending the concept of pattern exploiting training to generative tasks, GENPET overcomes challenges related to task description clarity, model utilization, and overfitting prevention. Through empirical validation on summarization and headline generation datasets, GENPET consistently outperforms robust baselines in few shot scenarios, emphasizing its efficacy and versatility. The presented approach not only contributes to the ongoing discourse on improving data efficiency in generative settings but also opens avenues for further exploration in combining task descriptions and example based learning for text generation tasks.", ADP NOUN PUNCT DET NOUN NOUN VERB ADP DET ADJ NOUN PRON ADV VERB VERB NOUN NOUN ADP NOUN NOUN ADP VERB NOUN NOUN NOUN PUNCT ADP VERB DET NOUN ADP NOUN VERB NOUN ADP ADJ NOUN PUNCT NOUN NOUN NOUN VERB ADP NOUN NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ VERB NOUN PUNCT ADP ADJ NOUN ADP NOUN CCONJ NOUN NOUN NOUN PUNCT VERB ADV VERB ADJ NOUN ADP ADJ NOUN NOUN PUNCT VERB PRON NOUN CCONJ NOUN PUNCT DET VERB NOUN PART ADV VERB ADP DET ADJ NOUN ADP VERB NOUN NOUN ADP ADJ NOUN CCONJ ADV VERB NOUN ADP ADJ NOUN ADP VERB NOUN NOUN CCONJ NOUN VERB VERB ADP NOUN NOUN NOUN PUNCT,0.7521367521367521,29.25,6.102564102564102
312,24,Aman Madaan,"[' AutoMix integrates black-box large language model (LLM) APIs into a multi-step problem-solving\nframework, optimizing the computational cost and performance trade-offs. AutoMix opens avenues\nfor several interesting research directions. First, while self-verification and correction are challenging\nfor LLMs in general, we find promising results using context-grounded few-shot verification, indicating that similar approaches may yield gain in other scenarios. Secondly, our work interweaves\nGood Old-Fashioned Artificial Intelligence (GOFAI) approaches with LLMs, demonstrating that the\nincorporation of a POMDP can boost the accuracy of a noisy few-shot verifier, showing the promise\nof this paradigm as an approach for improving LLMs during inference.']",conclusion_chunked," AutoMix integrates black-box large language model (LLM) APIs into a multi-step problem-solving
framework, optimizing the computational cost and performance trade-offs. AutoMix opens avenues
for several interesting research directions. First, while self-verification and correction are challenging
for LLMs in general, we find promising results using context-grounded few-shot verification, indicating that similar approaches may yield gain in other scenarios. Secondly, our work interweaves
Good Old-Fashioned Artificial Intelligence (GOFAI) approaches with LLMs, demonstrating that the
incorporation of a POMDP can boost the accuracy of a noisy few-shot verifier, showing the promise
of this paradigm as an approach for improving LLMs during inference.",30.71625077440773,30.71625077440773,312,0.6349619626998901," Propname integrates black Propname large language model APIs into a multi step problem solving 
 framework, optimizing the computational cost and performance trade offs. Propname opens avenues 
 for several interesting research directions. First, while self verification and correction are challenging 
 for LLMs in general, we find promising results using context grounded few shot verification, indicating that similar approaches may yield gain in other scenarios. Secondly, our work interweaves 
 Propname Propname Propname Propname Propname approaches with LLMs, demonstrating that the 
 incorporation of a Propname can boost the accuracy of a noisy few shot verifier, showing the promise 
 of this paradigm as an approach for improving LLMs during inference."," Propname integrates black Propname large language model APIs into a multi step problem solving 
 framework, optimizing the computational cost and performance trade offs. Propname opens avenues 
 for several interesting research directions. First, while self verification and correction are challenging 
 for LLMs in general, we find promising results using context grounded few shot verification, indicating that similar approaches may yield gain in other scenarios. Secondly, our work interweaves 
 Propname Propname Propname Propname Propname approaches with LLMs, demonstrating that the 
 incorporation of a Propname can boost the accuracy of a noisy few shot verifier, showing the promise 
 of this paradigm as an approach for improving LLMs during inference.", PROPN VERB ADJ PROPN ADJ NOUN NOUN NOUN ADP DET ADJ NOUN NOUN VERB SPACE NOUN PUNCT VERB DET ADJ NOUN CCONJ NOUN NOUN NOUN PUNCT PROPN VERB NOUN SPACE ADP ADJ ADJ NOUN NOUN PUNCT ADV PUNCT SCONJ NOUN NOUN CCONJ NOUN AUX VERB SPACE ADP NOUN ADP ADJ PUNCT PRON VERB ADJ NOUN VERB NOUN VERB ADJ NOUN NOUN PUNCT VERB SCONJ ADJ NOUN AUX VERB NOUN ADP ADJ NOUN PUNCT ADV PUNCT PRON NOUN VERB SPACE PROPN PROPN PROPN PROPN PROPN NOUN ADP NOUN PUNCT VERB SCONJ DET SPACE NOUN ADP DET PROPN AUX VERB DET NOUN ADP DET ADJ ADJ NOUN NOUN PUNCT VERB DET NOUN SPACE ADP DET NOUN ADP DET NOUN ADP VERB NOUN ADP NOUN PUNCT,0.7008547008547008,29.25,5.521367521367521
313,25,Aman Madaan,"[' We present SELF-REFINE: a novel approach that allows large language models to iteratively provide\nself-feedback and refine their own outputs. SELF-REFINE operates within a single LLM, requiring\nneither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of\nuse of SELF-REFINE across a wide variety of tasks. By showcasing the potential of SELF-REFINE in\ndiverse tasks, our research contributes to the ongoing exploration and development of large language\nmodels, with the aim of reducing the cost of human creative processes in real-world settings. We hope that our iterative approach will help drive further research in this area. To this end, we make all\nour code, data and prompts anonymously available at https://selfrefine.info/.']",conclusion_chunked," We present SELF-REFINE: a novel approach that allows large language models to iteratively provide
self-feedback and refine their own outputs. SELF-REFINE operates within a single LLM, requiring
neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of
use of SELF-REFINE across a wide variety of tasks. By showcasing the potential of SELF-REFINE in
diverse tasks, our research contributes to the ongoing exploration and development of large language
models, with the aim of reducing the cost of human creative processes in real-world settings. We hope that our iterative approach will help drive further research in this area. To this end, we make all
our code, data and prompts anonymously available at https://selfrefine.info/.",35.10083333333333,30.71625077440773,313,0.6304576396942139," We present Propname Propname: a novel approach that allows large language models to iteratively provide 
 self feedback and refine their own outputs. Propname Propname operates within a single LLM, requiring 
 neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of 
 use of Propname Propname across a wide variety of tasks. By showcasing the potential of Propname Propname in 
 diverse tasks, our research contributes to the ongoing exploration and development of large language 
 models, with the aim of reducing the cost of human creative processes in real world settings. We hope that our iterative approach will help drive further research in this area. To this end, we make all 
 our code, data and prompts anonymously available at"," We present Propname Propname: a novel approach that allows large language models to iteratively provide 
 self feedback and refine their own outputs. Propname Propname operates within a single LLM, requiring 
 neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of 
 use of Propname Propname across a wide variety of tasks. By showcasing the potential of Propname Propname in 
 diverse tasks, our research contributes to the ongoing exploration and development of large language 
 models, with the aim of reducing the cost of human creative processes in real world settings. We hope that our iterative approach will help drive further research in this area. To this end, we make all 
 our code, data and prompts anonymously available at", PRON VERB PROPN PROPN PUNCT DET ADJ NOUN PRON VERB ADJ NOUN NOUN PART ADV VERB SPACE NOUN NOUN CCONJ VERB PRON ADJ NOUN PUNCT PROPN PROPN VERB ADP DET ADJ NOUN PUNCT VERB SPACE CCONJ ADJ NOUN NOUN CCONJ NOUN NOUN PUNCT PRON VERB DET NOUN CCONJ NOUN ADP SPACE NOUN ADP PROPN PROPN ADP DET ADJ NOUN ADP NOUN PUNCT ADP VERB DET NOUN ADP PROPN PROPN ADP SPACE ADJ NOUN PUNCT PRON NOUN VERB ADP DET ADJ NOUN CCONJ NOUN ADP ADJ NOUN SPACE NOUN PUNCT ADP DET NOUN ADP VERB DET NOUN ADP ADJ ADJ NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB SCONJ PRON ADJ NOUN AUX VERB VERB ADJ NOUN ADP DET NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET SPACE PRON NOUN PUNCT NOUN CCONJ NOUN ADV ADJ ADP,0.648854961832061,21.833333333333332,4.984732824427481
314,26,Aman Madaan,"[' This work explores the capability of language models of code in generating performance-improving edits, while\nadhering to the functional correctness constraint. Our results demonstrate that such models can achieve speedup\nup to 2.5ˆ for over 25% of test programs in C++ and Python . This research is the initial step towards unlocking the potential of language models in leveraging the opportunities at the “top” of the computing stack. Particularly, we improve algorithmic efficiency and enable automatic code optimization without taking additional cycles from developers. Our promising results and the proven capability of large language models in\na variety of tasks sketch an exciting path forward to drive the computing efficiency in the post-Moore era. Nonetheless, there are many research directions and challenges that need attention, such as automatic test generation for performance triage, improving the alignment of generated code edits to target system and/or user\npreferences, adding structure to generated code edits, and tailoring code edit generation to architecture and\nhardware features.']",conclusion_chunked," This work explores the capability of language models of code in generating performance-improving edits, while
adhering to the functional correctness constraint. Our results demonstrate that such models can achieve speedup
up to 2.5ˆ for over 25% of test programs in C++ and Python . This research is the initial step towards unlocking the potential of language models in leveraging the opportunities at the “top” of the computing stack. Particularly, we improve algorithmic efficiency and enable automatic code optimization without taking additional cycles from developers. Our promising results and the proven capability of large language models in
a variety of tasks sketch an exciting path forward to drive the computing efficiency in the post-Moore era. Nonetheless, there are many research directions and challenges that need attention, such as automatic test generation for performance triage, improving the alignment of generated code edits to target system and/or user
preferences, adding structure to generated code edits, and tailoring code edit generation to architecture and
hardware features.",17.489555888223578,30.71625077440773,314,0.6151560544967651," This work explores the capability of language models of code in generating performance improving edits, while 
 adhering to the functional correctness constraint. Our results demonstrate that such models can achieve speedup 
 up to 0.0 for over 00 of test programs in Propname and Propname. This research is the initial step towards unlocking the potential of language models in leveraging the opportunities at the top of the computing stack. Particularly, we improve algorithmic efficiency and enable automatic code optimization without taking additional cycles from developers. Our promising results and the proven capability of large language models in 
 a variety of tasks sketch an exciting path forward to drive the computing efficiency in the post Propname era. Nonetheless, there are many research directions and challenges that need attention, such as automatic test generation for performance triage, improving the alignment of generated code edits to target system andor user 
 preferences, adding structure to generated code edits, and tailoring code edit generation to architecture and 
 hardware features."," This work explores the capability of language models of code in generating performance improving edits, while 
 adhering to the functional correctness constraint. Our results demonstrate that such models can achieve speedup 
 up to 0.0 for over 00 of test programs in Propname and Propname. This research is the initial step towards unlocking the potential of language models in leveraging the opportunities at the top of the computing stack. Particularly, we improve algorithmic efficiency and enable automatic code optimization without taking additional cycles from developers. Our promising results and the proven capability of large language models in 
 a variety of tasks sketch an exciting path forward to drive the computing efficiency in the post Propname era. Nonetheless, there are many research directions and challenges that need attention, such as automatic test generation for performance triage, improving the alignment of generated code edits to target system andor user 
 preferences, adding structure to generated code edits, and tailoring code edit generation to architecture and 
 hardware features.", DET NOUN VERB DET NOUN ADP NOUN NOUN ADP NOUN ADP VERB NOUN VERB NOUN PUNCT SCONJ SPACE VERB ADP DET ADJ NOUN NOUN PUNCT PRON NOUN VERB SCONJ ADJ NOUN AUX VERB ADJ SPACE ADP ADP NUM ADP ADP NUM ADP NOUN NOUN ADP PROPN CCONJ PROPN PUNCT DET NOUN AUX DET ADJ NOUN ADP VERB DET NOUN ADP NOUN NOUN ADP VERB DET NOUN ADP DET NOUN ADP DET VERB NOUN PUNCT ADV PUNCT PRON VERB ADJ NOUN CCONJ VERB ADJ NOUN NOUN ADP VERB ADJ NOUN ADP NOUN PUNCT PRON ADJ NOUN CCONJ DET VERB NOUN ADP ADJ NOUN NOUN ADP SPACE DET NOUN ADP NOUN VERB DET ADJ NOUN ADV PART VERB DET NOUN NOUN ADP DET NOUN PROPN NOUN PUNCT ADV PUNCT PRON VERB ADJ NOUN NOUN CCONJ NOUN PRON VERB NOUN PUNCT ADJ ADP ADJ NOUN NOUN ADP NOUN NOUN PUNCT VERB DET NOUN ADP VERB NOUN NOUN PART VERB NOUN ADJ NOUN SPACE NOUN PUNCT VERB NOUN ADP VERB NOUN NOUN PUNCT CCONJ VERB NOUN NOUN NOUN ADP NOUN CCONJ SPACE NOUN NOUN PUNCT,0.5965909090909091,29.333333333333332,5.380681818181818
315,27,Aman Madaan,"[' We present the first work to employ large language models of code for structured commonsense generation. By converting the output commonsense structures to Python code, COCOGEN provides a simple and effective method for leveraging the code-generation abilities of Code-LLMs for structured generation. These results open a promising direction for structural commonsense reasoning. We believe that the principles and the methods presented in this paper are applicable to additional NLP tasks that require “language understanding” and structured prediction.']",conclusion_chunked," We present the first work to employ large language models of code for structured commonsense generation. By converting the output commonsense structures to Python code, COCOGEN provides a simple and effective method for leveraging the code-generation abilities of Code-LLMs for structured generation. These results open a promising direction for structural commonsense reasoning. We believe that the principles and the methods presented in this paper are applicable to additional NLP tasks that require “language understanding” and structured prediction.",30.71625077440773,30.71625077440773,315,0.7268341183662415," We present the first work to employ large language models of code for structured commonsense generation. By converting the output commonsense structures to Propname code, Propname provides a simple and effective method for leveraging the code generation abilities of Propname LLMs for structured generation. These results open a promising direction for structural commonsense reasoning. We believe that the principles and the methods presented in this paper are applicable to additional Propname tasks that require language understanding and structured prediction."," We present the first work to employ large language models of code for structured commonsense generation. By converting the output commonsense structures to Propname code, Propname provides a simple and effective method for leveraging the code generation abilities of Propname LLMs for structured generation. These results open a promising direction for structural commonsense reasoning. We believe that the principles and the methods presented in this paper are applicable to additional Propname tasks that require language understanding and structured prediction.", PRON VERB DET ADJ NOUN PART VERB ADJ NOUN NOUN ADP NOUN ADP ADJ NOUN NOUN PUNCT ADP VERB DET NOUN NOUN NOUN ADP PROPN NOUN PUNCT PROPN VERB DET ADJ CCONJ ADJ NOUN ADP VERB DET NOUN NOUN NOUN ADP PROPN NOUN ADP ADJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP ADJ NOUN NOUN PUNCT PRON VERB SCONJ DET NOUN CCONJ DET NOUN VERB ADP DET NOUN AUX ADJ ADP ADJ PROPN NOUN PRON VERB NOUN NOUN CCONJ ADJ NOUN PUNCT,0.6428571428571429,21.0,5.785714285714286
316,28,Aman Madaan,"[' This work evaluates the capacity of COT to elevate complex reasoning in three state-of-the-arts LLMs,\nPaLM, GPT-3, and CODEX. We systematically assembled a series of controlled counterfactual experiments. Our results show the initial inklings of connection between text, patterns, and reasoning in LLMs. Our study\nindicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process. In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models\nto mold correct answers.']",conclusion_chunked," This work evaluates the capacity of COT to elevate complex reasoning in three state-of-the-arts LLMs,
PaLM, GPT-3, and CODEX. We systematically assembled a series of controlled counterfactual experiments. Our results show the initial inklings of connection between text, patterns, and reasoning in LLMs. Our study
indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process. In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models
to mold correct answers.",30.71625077440773,30.71625077440773,316,0.449067085981369," This work evaluates the capacity of Propname to elevate complex reasoning in three state of the arts Propname, 
 Propname, Propname 0, and Propname. We systematically assembled a series of controlled counterfactual experiments. Our results show the initial inklings of connection between text, patterns, and reasoning in Propname. Our study 
 indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process. In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models 
 to mold correct answers."," This work evaluates the capacity of Propname to elevate complex reasoning in three state of the arts Propname, 
 Propname, Propname 0, and Propname. We systematically assembled a series of controlled counterfactual experiments. Our results show the initial inklings of connection between text, patterns, and reasoning in Propname. Our study 
 indicates that the symbiosis of text and patterns bears more weight in the chain of thought reasoning process. In addition, we assert that text is a channel to extract semantic patterns, unlocking the ability of these models 
 to mold correct answers.", DET NOUN VERB DET NOUN ADP PROPN PART VERB ADJ NOUN ADP NUM NOUN ADP DET NOUN PROPN PUNCT SPACE PROPN PUNCT PROPN NUM PUNCT CCONJ PROPN PUNCT PRON ADV VERB DET NOUN ADP VERB ADJ NOUN PUNCT PRON NOUN VERB DET ADJ NOUN ADP NOUN ADP NOUN PUNCT NOUN PUNCT CCONJ VERB ADP PROPN PUNCT PRON NOUN SPACE VERB SCONJ DET NOUN ADP NOUN CCONJ NOUN VERB ADJ NOUN ADP DET NOUN ADP VERB NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB SCONJ NOUN AUX DET NOUN PART VERB ADJ NOUN PUNCT VERB DET NOUN ADP DET NOUN SPACE PART VERB ADJ NOUN PUNCT,0.5980392156862745,20.4,4.892156862745098
317,29,Aman Madaan,"[' Future machine learning applications will potentially have\nAPI-level access to several models of varying strengths and\ncosts of usage. In such scenarios, building systems that\ncan adapt to the difficulty of the sample will be critical for\nscale and efficiency. FLOWGEN presents a real-world use\ncase for such FAST-SLOW systems. As future work, we\nplan to explore the use of FAST-SLOW generation methods\nfor effective and adaptive language generation using largelanguage models.']",conclusion_chunked," Future machine learning applications will potentially have
API-level access to several models of varying strengths and
costs of usage. In such scenarios, building systems that
can adapt to the difficulty of the sample will be critical for
scale and efficiency. FLOWGEN presents a real-world use
case for such FAST-SLOW systems. As future work, we
plan to explore the use of FAST-SLOW generation methods
for effective and adaptive language generation using largelanguage models.",30.71625077440773,30.71625077440773,317,0.46347948908805847," Future machine learning applications will potentially have 
 API level access to several models of varying strengths and 
 costs of usage. In such scenarios, building systems that 
 can adapt to the difficulty of the sample will be critical for 
 scale and efficiency. Propname presents a real world use 
 case for such FAST SLOW systems. As future work, we 
 plan to explore the use of FAST SLOW generation methods 
 for effective and adaptive language generation using largelanguage models."," Future machine learning applications will potentially have 
 API level access to several models of varying strengths and 
 costs of usage. In such scenarios, building systems that 
 can adapt to the difficulty of the sample will be critical for 
 scale and efficiency. Propname presents a real world use 
 case for such FAST SLOW systems. As future work, we 
 plan to explore the use of FAST SLOW generation methods 
 for effective and adaptive language generation using largelanguage models.", ADJ NOUN NOUN NOUN AUX ADV VERB SPACE NOUN NOUN NOUN ADP ADJ NOUN ADP VERB NOUN CCONJ SPACE NOUN ADP NOUN PUNCT ADP ADJ NOUN PUNCT NOUN NOUN PRON SPACE AUX VERB ADP DET NOUN ADP DET NOUN AUX AUX ADJ ADP SPACE NOUN CCONJ NOUN PUNCT PROPN VERB DET ADJ NOUN VERB SPACE NOUN ADP ADJ ADJ ADJ NOUN PUNCT ADP ADJ NOUN PUNCT PRON SPACE VERB PART VERB DET NOUN ADP ADJ ADJ NOUN NOUN SPACE ADP ADJ CCONJ ADJ NOUN NOUN VERB NOUN NOUN PUNCT,0.7195121951219512,20.5,4.914634146341464
318,30,Aman Madaan,"[' We present SETAUG, a novel data augmentation\nmethod for conditional set generation that incorporates informative orders and adds cardinality information. Our key idea is using the most likely\norder (vs. a randomly selected order) to represent\na set as a sequence and conditioning the generation of a set on predicted cardinality. As a computationally efficient and general-purpose plug-in\ndata augmentation algorithm, SETAUG improves\nSEQ2SEQ models for set generation across a wide\nspectrum of tasks. For future work, it would be\ninteresting to investigate if the general ideas in this\nwork have applications in settings beyond set generation. Examples include generating additional data\nto improve language modeling in low-resource scenarios and determining the ideal order of in-context\nexamples in a prompt.']",conclusion_chunked," We present SETAUG, a novel data augmentation
method for conditional set generation that incorporates informative orders and adds cardinality information. Our key idea is using the most likely
order (vs. a randomly selected order) to represent
a set as a sequence and conditioning the generation of a set on predicted cardinality. As a computationally efficient and general-purpose plug-in
data augmentation algorithm, SETAUG improves
SEQ2SEQ models for set generation across a wide
spectrum of tasks. For future work, it would be
interesting to investigate if the general ideas in this
work have applications in settings beyond set generation. Examples include generating additional data
to improve language modeling in low-resource scenarios and determining the ideal order of in-context
examples in a prompt.",10.974479338842997,30.71625077440773,318,0.6906384825706482," We present Propname, a novel data augmentation 
 method for conditional set generation that incorporates informative orders and adds cardinality information. Our key idea is using the most likely 
 order to represent 
 a set as a sequence and conditioning the generation of a set on predicted cardinality. As a computationally efficient and general purpose plug in 
 data augmentation Propname, Propname improves 
 Propname models for set generation across a wide 
 spectrum of tasks. For future work, it would be 
 interesting to investigate if the general ideas in this 
 work have applications in settings beyond set generation. Examples include generating additional data 
 to improve language modeling in low resource scenarios and determining the ideal order of in context 
 examples in a prompt."," We present Propname, a novel data augmentation 
 method for conditional set generation that incorporates informative orders and adds cardinality information. Our key idea is using the most likely 
 order to represent 
 a set as a sequence and conditioning the generation of a set on predicted cardinality. As a computationally efficient and general purpose plug in 
 data augmentation Propname, Propname improves 
 Propname models for set generation across a wide 
 spectrum of tasks. For future work, it would be 
 interesting to investigate if the general ideas in this 
 work have applications in settings beyond set generation. Examples include generating additional data 
 to improve language modeling in low resource scenarios and determining the ideal order of in context 
 examples in a prompt.", PRON VERB PROPN PUNCT DET ADJ NOUN NOUN SPACE NOUN ADP ADJ VERB NOUN PRON VERB ADJ NOUN CCONJ VERB NOUN NOUN PUNCT PRON ADJ NOUN AUX VERB DET ADV ADJ SPACE NOUN PART VERB SPACE DET NOUN ADP DET NOUN CCONJ VERB DET NOUN ADP DET NOUN ADP VERB NOUN PUNCT ADP DET ADV ADJ CCONJ ADJ NOUN NOUN ADP SPACE NOUN NOUN PROPN PUNCT PROPN VERB SPACE PROPN NOUN ADP ADJ NOUN ADP DET ADJ SPACE NOUN ADP NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX AUX SPACE ADJ PART VERB SCONJ DET ADJ NOUN ADP DET SPACE NOUN VERB NOUN ADP NOUN ADP ADJ NOUN PUNCT NOUN VERB VERB ADJ NOUN SPACE PART VERB NOUN NOUN ADP ADJ NOUN NOUN CCONJ VERB DET ADJ NOUN ADP ADP NOUN SPACE NOUN ADP DET NOUN PUNCT,0.6456692913385826,25.4,5.228346456692913
319,31,Aman Madaan,"[' We present MemPrompt, a novel, memoryenhanced GPT-3 that allows users to interact and\nimprove the model without retraining. A key insight is to have the model articulate not just its\nanswer but also its understanding of the user’s intent, providing an avenue for feedback. We show\nthat deployed systems with fixed large-language\nmodels can still be improved by interacting with end-users, potentially improving their performance\nand broadening their utility.']",conclusion_chunked," We present MemPrompt, a novel, memoryenhanced GPT-3 that allows users to interact and
improve the model without retraining. A key insight is to have the model articulate not just its
answer but also its understanding of the user’s intent, providing an avenue for feedback. We show
that deployed systems with fixed large-language
models can still be improved by interacting with end-users, potentially improving their performance
and broadening their utility.",30.71625077440773,30.71625077440773,319,0.7798891067504883," We present Propname, a novel, memoryenhanced Propname 0 that allows users to interact and 
 improve the model without retraining. A key insight is to have the model articulate not just its 
 answer but also its understanding of the users intent, providing an avenue for feedback. We show 
 that deployed systems with fixed large language 
 models can still be improved by interacting with end users, potentially improving their performance 
 and broadening their utility."," We present Propname, a novel, memoryenhanced Propname 0 that allows users to interact and 
 improve the model without retraining. A key insight is to have the model articulate not just its 
 answer but also its understanding of the users intent, providing an avenue for feedback. We show 
 that deployed systems with fixed large language 
 models can still be improved by interacting with end users, potentially improving their performance 
 and broadening their utility.", PRON VERB PROPN PUNCT DET NOUN PUNCT VERB PROPN NUM PRON VERB NOUN PART VERB CCONJ SPACE VERB DET NOUN ADP VERB PUNCT DET ADJ NOUN AUX PART VERB DET NOUN VERB PART ADV PRON SPACE NOUN CCONJ ADV PRON NOUN ADP DET NOUN NOUN PUNCT VERB DET NOUN ADP NOUN PUNCT PRON VERB SPACE PRON VERB NOUN ADP VERB ADJ NOUN SPACE NOUN AUX ADV AUX VERB ADP VERB ADP NOUN NOUN PUNCT ADV VERB PRON NOUN SPACE CCONJ VERB PRON NOUN PUNCT,0.7721518987341772,26.333333333333332,4.936708860759493
320,32,Aman Madaan,"[' Cognitive science suggests that people form “mental models” of a situation to answer questions about it. Drawing on those ideas, we have presented a simple instantiation in which the situational model is an inference graph. Different from GCN-based models popular in graph learning, we use mixtureof-experts to pool graph representations. Our experiments show that MoE-based pooling can be a strong (both in terms of performance and explainability) alternative to GCN for graph-based learning for reasoning tasks. Our method establishes a new state-of-the-art on three defeasible reasoning datasets. Overall, our method shows that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reflexively']",conclusion_chunked," Cognitive science suggests that people form “mental models” of a situation to answer questions about it. Drawing on those ideas, we have presented a simple instantiation in which the situational model is an inference graph. Different from GCN-based models popular in graph learning, we use mixtureof-experts to pool graph representations. Our experiments show that MoE-based pooling can be a strong (both in terms of performance and explainability) alternative to GCN for graph-based learning for reasoning tasks. Our method establishes a new state-of-the-art on three defeasible reasoning datasets. Overall, our method shows that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reflexively",31.295367231638437,30.71625077440773,320,0.6901184916496277," Cognitive science suggests that people form mental models of a situation to answer questions about it. Drawing on those ideas, we have presented a simple instantiation in which the situational model is an inference graph. Different from Propname based models popular in graph learning, we use mixtureof experts to pool graph representations. Our experiments show that Propname based pooling can be a strong alternative to Propname for graph based learning for reasoning tasks. Our method establishes a new state of the art on three defeasible reasoning datasets. Overall, our method shows that performance can be improved by guiding a system to think about a question and explicitly model the scenario, rather than answering reflexively"," Cognitive science suggests that people form mental models of a situation to answer questions about it. Drawing on those ideas, we have presented a simple instantiation in which the situational model is an inference graph. Different from Propname based models popular in graph learning, we use mixtureof experts to pool graph representations. Our experiments show that Propname based pooling can be a strong alternative to Propname for graph based learning for reasoning tasks. Our method establishes a new state of the art on three defeasible reasoning datasets. Overall, our method shows that performance can be improved by guiding a system to think about a question and explicitly model the scenario, rather than answering reflexively", ADJ NOUN VERB SCONJ NOUN VERB ADJ NOUN ADP DET NOUN PART VERB NOUN ADP PRON PUNCT VERB ADP DET NOUN PUNCT PRON AUX VERB DET ADJ NOUN ADP PRON DET ADJ NOUN AUX DET NOUN NOUN PUNCT ADJ ADP PROPN VERB NOUN ADJ ADP NOUN NOUN PUNCT PRON VERB NOUN NOUN PART VERB NOUN NOUN PUNCT PRON NOUN VERB SCONJ PROPN VERB NOUN AUX AUX DET ADJ NOUN ADP PROPN ADP NOUN VERB NOUN ADP NOUN NOUN PUNCT PRON NOUN VERB DET ADJ NOUN ADP DET NOUN ADP NUM ADJ NOUN NOUN PUNCT ADV PUNCT PRON NOUN VERB SCONJ NOUN AUX AUX VERB ADP VERB DET NOUN PART VERB ADP DET NOUN CCONJ ADV VERB DET NOUN PUNCT ADV ADP VERB ADV,0.6747967479674797,20.5,5.065040650406504
321,33,Aman Madaan,"[' Our work takes the idea of using inference graphs\nfor defeasible inference and scales up its usability by automatically generating and augmenting\nthem to a downstream defeasible task that both humans and machines are known to find difficult. We\nidentify that the contextualizer and mediator nodes\nare crucial to defeasible inference, and show that\nour generated graphs generate these critical nodes effectively. Humans perform significantly better\n(20% absolute improvement) across diverse defeasible datasets and overwhelmingly attribute their\nsuccess to the mediator nodes – giving insights into\nwhat helps and why. In this case study, we show\nthat machines can fill the gaps in human knowledge\nwhen for defeasible reasoning. While we establish\nthat humans are helped by these graphs, a further\ninvestigation on how (and if) the graphs reinforced\ntheir beliefs, and what additional information in the\ngraphs was beneficial to their understanding is essential. Furthermore, a deeper understanding of the\ntrade-offs (time spent in answering these questions\nwith and without the graphs) also forms important\nfuture work.']",conclusion_chunked," Our work takes the idea of using inference graphs
for defeasible inference and scales up its usability by automatically generating and augmenting
them to a downstream defeasible task that both humans and machines are known to find difficult. We
identify that the contextualizer and mediator nodes
are crucial to defeasible inference, and show that
our generated graphs generate these critical nodes effectively. Humans perform significantly better
(20% absolute improvement) across diverse defeasible datasets and overwhelmingly attribute their
success to the mediator nodes – giving insights into
what helps and why. In this case study, we show
that machines can fill the gaps in human knowledge
when for defeasible reasoning. While we establish
that humans are helped by these graphs, a further
investigation on how (and if) the graphs reinforced
their beliefs, and what additional information in the
graphs was beneficial to their understanding is essential. Furthermore, a deeper understanding of the
trade-offs (time spent in answering these questions
with and without the graphs) also forms important
future work.",22.81142857142862,30.71625077440773,321,0.6748791933059692," Our work takes the idea of using inference graphs 
 for defeasible inference and scales up its usability by automatically generating and augmenting 
 them to a downstream defeasible task that both humans and machines are known to find difficult. We 
 identify that the contextualizer and mediator nodes 
 are crucial to defeasible inference, and show that 
 our generated graphs generate these critical nodes effectively. Humans perform significantly better across diverse defeasible datasets and overwhelmingly attribute their 
 success to the mediator nodes giving insights into 
 what helps and why. In this case study, we show 
 that machines can fill the gaps in human knowledge 
 when for defeasible reasoning. While we establish 
 that humans are helped by these graphs, a further 
 investigation on how the graphs reinforced 
 their beliefs, and what additional information in the 
 graphs was beneficial to their understanding is essential. Furthermore, a deeper understanding of the 
 trade offs time spent in answering these questions 
 with and without the graphs also forms important 
 future work."," Our work takes the idea of using inference graphs 
 for defeasible inference and scales up its usability by automatically generating and augmenting 
 them to a downstream defeasible task that both humans and machines are known to find difficult. We 
 identify that the contextualizer and mediator nodes 
 are crucial to defeasible inference, and show that 
 our generated graphs generate these critical nodes effectively. Humans perform significantly better across diverse defeasible datasets and overwhelmingly attribute their 
 success to the mediator nodes giving insights into 
 what helps and why. In this case study, we show 
 that machines can fill the gaps in human knowledge 
 when for defeasible reasoning. While we establish 
 that humans are helped by these graphs, a further 
 investigation on how the graphs reinforced 
 their beliefs, and what additional information in the 
 graphs was beneficial to their understanding is essential. Furthermore, a deeper understanding of the 
 trade offs time spent in answering these questions 
 with and without the graphs also forms important 
 future work.", PRON NOUN VERB DET NOUN ADP VERB NOUN NOUN SPACE ADP ADJ NOUN CCONJ VERB ADP PRON NOUN ADP ADV VERB CCONJ VERB SPACE PRON ADP DET ADJ ADJ NOUN SCONJ DET NOUN CCONJ NOUN AUX VERB PART VERB ADJ PUNCT PRON SPACE VERB SCONJ DET NOUN CCONJ NOUN NOUN SPACE AUX ADJ ADP ADJ NOUN PUNCT CCONJ VERB SCONJ SPACE PRON VERB NOUN VERB DET ADJ NOUN ADV PUNCT NOUN VERB ADV ADV ADP ADJ ADJ NOUN CCONJ ADV VERB PRON SPACE NOUN ADP DET NOUN NOUN VERB NOUN ADP SPACE PRON VERB CCONJ SCONJ PUNCT ADP DET NOUN NOUN PUNCT PRON VERB SPACE SCONJ NOUN AUX VERB DET NOUN ADP ADJ NOUN SPACE SCONJ ADP ADJ NOUN PUNCT SCONJ PRON VERB SPACE SCONJ NOUN AUX VERB ADP DET NOUN PUNCT DET ADJ SPACE NOUN ADP SCONJ DET NOUN VERB SPACE PRON NOUN PUNCT CCONJ PRON ADJ NOUN ADP DET SPACE NOUN AUX ADJ ADP PRON NOUN AUX ADJ PUNCT ADV PUNCT DET ADJ NOUN ADP DET SPACE NOUN NOUN NOUN VERB ADP VERB DET NOUN SPACE ADP CCONJ ADP DET NOUN ADV VERB ADJ SPACE ADJ NOUN PUNCT,0.6206896551724138,29.0,5.2701149425287355
322,34,Aman Madaan,"[' We present MERCURIE, a system that improves the explanation structure (graphs) generated by a model without requiring expensive humanannotated feedback. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains. This work paves a new path towards exciting future research direction of constantly improving explainable NLP models by applying human feedback.']",conclusion_chunked," We present MERCURIE, a system that improves the explanation structure (graphs) generated by a model without requiring expensive humanannotated feedback. Our approach generates graphs that have 40% fewer inconsistencies as compared with the off-the-shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 1.2 points on accuracy on defeasible reasoning across all three domains. This work paves a new path towards exciting future research direction of constantly improving explainable NLP models by applying human feedback.",30.71625077440773,30.71625077440773,322,0.6540254950523376," We present Propname, a system that improves the explanation structure generated by a model without requiring expensive humanannotated feedback. Our approach generates graphs that have 00 fewer inconsistencies as compared with the off the shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 0.0 points on accuracy on defeasible reasoning across all three domains. This work paves a new path towards exciting future research direction of constantly improving explainable NLP models by applying human feedback."," We present Propname, a system that improves the explanation structure generated by a model without requiring expensive humanannotated feedback. Our approach generates graphs that have 00 fewer inconsistencies as compared with the off the shelf system. Further, simply appending the corrected explanation structures to the output leads to a gain of 0.0 points on accuracy on defeasible reasoning across all three domains. This work paves a new path towards exciting future research direction of constantly improving explainable NLP models by applying human feedback.", PRON VERB PROPN PUNCT DET NOUN PRON VERB DET NOUN NOUN VERB ADP DET NOUN ADP VERB ADJ VERB NOUN PUNCT PRON NOUN VERB NOUN PRON VERB NUM ADJ NOUN SCONJ VERB ADP DET ADP DET NOUN NOUN PUNCT ADV PUNCT ADV VERB DET VERB NOUN NOUN ADP DET NOUN VERB ADP DET NOUN ADP NUM NOUN ADP NOUN ADP ADJ NOUN ADP DET NUM NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP ADJ ADJ NOUN NOUN ADP ADV VERB ADJ NOUN NOUN ADP VERB ADJ NOUN PUNCT,0.7865168539325843,22.25,5.426966292134831
323,35,Aman Madaan,"[' We define the problem of event-influence reasoning as a generation task conditioned on context, particularly exploring the efficacy of large scale pre-trained language models for the task. We use human-curated event influence graphs to train a model to generate targeted event influences grounded in a context. Our experiments with ablations and error analysis provide insights into how to effectively adapt pretrained language models for event influence generation and opens up exciting avenues for further research. Our method outperforms strong baselines on both automated and human evaluations. Furthermore, generated influences improve performance on the benchmark Question Type BERT + EIGEN BERT Exogenous 64.04 56.13 In-para 73.58 79.68 Out-of-para 90.84 89.38 Table 9: QA accuracy by question type WIQA QA task without architectural changes to the model. Future work would extend the generalizability of this method to understand more complex and volatile event influences, such as events in news articles and stock markets.']",conclusion_chunked," We define the problem of event-influence reasoning as a generation task conditioned on context, particularly exploring the efficacy of large scale pre-trained language models for the task. We use human-curated event influence graphs to train a model to generate targeted event influences grounded in a context. Our experiments with ablations and error analysis provide insights into how to effectively adapt pretrained language models for event influence generation and opens up exciting avenues for further research. Our method outperforms strong baselines on both automated and human evaluations. Furthermore, generated influences improve performance on the benchmark Question Type BERT + EIGEN BERT Exogenous 64.04 56.13 In-para 73.58 79.68 Out-of-para 90.84 89.38 Table 9: QA accuracy by question type WIQA QA task without architectural changes to the model. Future work would extend the generalizability of this method to understand more complex and volatile event influences, such as events in news articles and stock markets.",20.494806843267156,30.71625077440773,323,0.7437705397605896," We define the problem of event influence reasoning as a generation task conditioned on context, particularly exploring the efficacy of large scale pre trained language models for the task. We use human curated event influence graphs to train a model to generate targeted event influences grounded in a context. Our experiments with ablations and error analysis provide insights into how to effectively adapt pretrained language models for event influence generation and opens up exciting avenues for further research. Our method outperforms strong baselines on both automated and human evaluations. Furthermore, generated influences improve performance on the benchmark Propname Propname Propname Propname Propname Propname 00.00 00.00 In para 00.00 00.00 Out of Propname 00.00 00.00 Table 0: QA accuracy by question type Propname Propname task without architectural changes to the model. Future work would extend the generalizability of this method to understand more complex and volatile event influences, such as events in news articles and stock markets."," We define the problem of event influence reasoning as a generation task conditioned on context, particularly exploring the efficacy of large scale pre trained language models for the task. We use human curated event influence graphs to train a model to generate targeted event influences grounded in a context. Our experiments with ablations and error analysis provide insights into how to effectively adapt pretrained language models for event influence generation and opens up exciting avenues for further research. Our method outperforms strong baselines on both automated and human evaluations. Furthermore, generated influences improve performance on the benchmark Propname Propname Propname Propname Propname Propname 00.00 00.00 In para 00.00 00.00 Out of Propname 00.00 00.00 Table 0: QA accuracy by question type Propname Propname task without architectural changes to the model. Future work would extend the generalizability of this method to understand more complex and volatile event influences, such as events in news articles and stock markets.", PRON VERB DET NOUN ADP NOUN NOUN VERB ADP DET NOUN NOUN VERB ADP NOUN PUNCT ADV VERB DET NOUN ADP ADJ NOUN VERB VERB NOUN NOUN ADP DET NOUN PUNCT PRON VERB ADJ VERB NOUN NOUN NOUN PART VERB DET NOUN PART VERB VERB NOUN NOUN VERB ADP DET NOUN PUNCT PRON NOUN ADP NOUN CCONJ NOUN NOUN VERB NOUN ADP SCONJ PART ADV VERB VERB NOUN NOUN ADP NOUN NOUN NOUN CCONJ VERB ADP ADJ NOUN ADP ADJ NOUN PUNCT PRON NOUN VERB ADJ NOUN ADP DET VERB CCONJ ADJ NOUN PUNCT ADV PUNCT VERB NOUN VERB NOUN ADP DET ADJ PROPN PROPN PROPN PROPN PROPN PROPN NUM NUM ADP NOUN NUM NUM ADP ADP PROPN NUM NUM NOUN NUM PUNCT NOUN NOUN ADP NOUN NOUN PROPN PROPN NOUN ADP ADJ NOUN ADP DET NOUN PUNCT ADJ NOUN AUX VERB DET NOUN ADP DET NOUN PART VERB ADV ADJ CCONJ ADJ NOUN NOUN PUNCT ADJ ADP NOUN ADP NOUN NOUN CCONJ NOUN NOUN PUNCT,0.6204819277108434,27.666666666666668,5.4397590361445785
324,36,Aman Madaan,"[' Current methods for generating event-level temporal graphs are developed with relatively small amounts of hand-labeled data. On the other hand, the possibility of using pre-trained language models for this task has not received sufficient attention. This paper addresses this open challenge by first developing a data generation pipeline that uses existing IE/NLP/clustering techniques for automated acquisition of a large corpus of document-graph pairs, and by proposing a new formulation of the graph generation task as a sequence-to-sequence mapping task, allowing us to leverage and fine-tune pretrained language models. Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines. We plan to explore techniques for adapting largescale language models on unseen domains and at multiple granularity levels in the future.']",conclusion_chunked," Current methods for generating event-level temporal graphs are developed with relatively small amounts of hand-labeled data. On the other hand, the possibility of using pre-trained language models for this task has not received sufficient attention. This paper addresses this open challenge by first developing a data generation pipeline that uses existing IE/NLP/clustering techniques for automated acquisition of a large corpus of document-graph pairs, and by proposing a new formulation of the graph generation task as a sequence-to-sequence mapping task, allowing us to leverage and fine-tune pretrained language models. Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines. We plan to explore techniques for adapting largescale language models on unseen domains and at multiple granularity levels in the future.",8.699857142857184,30.71625077440773,324,0.6187153458595276," Current methods for generating event level temporal graphs are developed with relatively small amounts of hand labeled data. On the other hand, the possibility of using pre trained language models for this task has not received sufficient attention. This paper addresses this open challenge by first developing a data generation pipeline that uses existing Propname techniques for automated acquisition of a large corpus of document graph pairs, and by proposing a new formulation of the graph generation task as a sequence to sequence mapping task, allowing us to leverage and fine tune pretrained language models. Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines. We plan to explore techniques for adapting largescale language models on unseen domains and at multiple granularity levels in the future."," Current methods for generating event level temporal graphs are developed with relatively small amounts of hand labeled data. On the other hand, the possibility of using pre trained language models for this task has not received sufficient attention. This paper addresses this open challenge by first developing a data generation pipeline that uses existing Propname techniques for automated acquisition of a large corpus of document graph pairs, and by proposing a new formulation of the graph generation task as a sequence to sequence mapping task, allowing us to leverage and fine tune pretrained language models. Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines. We plan to explore techniques for adapting largescale language models on unseen domains and at multiple granularity levels in the future.", ADJ NOUN ADP VERB NOUN NOUN ADJ NOUN AUX VERB ADP ADV ADJ NOUN ADP NOUN VERB NOUN PUNCT ADP DET ADJ NOUN PUNCT DET NOUN ADP VERB ADJ VERB NOUN NOUN ADP DET NOUN AUX PART VERB ADJ NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP ADV VERB DET NOUN NOUN NOUN PRON VERB VERB PROPN NOUN ADP VERB NOUN ADP DET ADJ NOUN ADP NOUN NOUN NOUN PUNCT CCONJ ADP VERB DET ADJ NOUN ADP DET NOUN NOUN NOUN ADP DET NOUN PART VERB NOUN NOUN PUNCT VERB PRON ADP NOUN CCONJ ADJ NOUN VERB NOUN NOUN PUNCT PRON NOUN ADV VERB DET NOUN ADP DET VERB NOUN PUNCT PRON ADV VERB ADJ NOUN PUNCT PRON VERB PART VERB NOUN ADP VERB NOUN NOUN NOUN ADP ADJ NOUN CCONJ ADP ADJ NOUN NOUN ADP DET NOUN PUNCT,0.7071428571428572,28.0,5.357142857142857
325,37,Aman Madaan,"[' We introduce the task of politeness transfer for\nwhich we provide a dataset comprised of sentences\ncurated from email exchanges present in the Enron\ncorpus. We extend prior works (Li et al., 2018;\nSudhakar et al., 2019) on attribute transfer by introducing a simple pipeline – tag & generate which\nis an interpretable two-staged approach for content\npreserving style transfer. We believe our approach\nis the first to be robust in cases when the source is\nstyle neutral, like the “non-polite” class in the case\nof politeness transfer. Automatic and human evaluation shows that our approach outperforms other\nstate-of-the-art models on content preservation metrics while retaining (or in some cases improving)\nthe transfer accuracies.']",conclusion_chunked," We introduce the task of politeness transfer for
which we provide a dataset comprised of sentences
curated from email exchanges present in the Enron
corpus. We extend prior works (Li et al., 2018;
Sudhakar et al., 2019) on attribute transfer by introducing a simple pipeline – tag & generate which
is an interpretable two-staged approach for content
preserving style transfer. We believe our approach
is the first to be robust in cases when the source is
style neutral, like the “non-polite” class in the case
of politeness transfer. Automatic and human evaluation shows that our approach outperforms other
state-of-the-art models on content preservation metrics while retaining (or in some cases improving)
the transfer accuracies.",31.258967391304367,30.71625077440773,325,0.2499607801437378," We introduce the task of politeness transfer for 
 which we provide a dataset comprised of sentences 
 curated from email exchanges present in the Propname 
 Propname. We extend prior works Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000 on attribute transfer by introducing a simple pipeline tag generate which 
 is an interpretable two staged approach for content 
 preserving style transfer. We believe our approach 
 is the first to be robust in cases when the source is 
 style neutral, like the non polite class in the case 
 of politeness transfer. Automatic and human evaluation shows that our approach outperforms other 
 state of the art models on content preservation metrics while retaining the transfer accuracies."," We introduce the task of politeness transfer for 
 which we provide a dataset comprised of sentences 
 curated from email exchanges present in the Propname 
 Propname. We extend prior works Propname Propname Propname Propname, 0000; 
 Propname Propname Propname Propname, 0000 on attribute transfer by introducing a simple pipeline tag generate which 
 is an interpretable two staged approach for content 
 preserving style transfer. We believe our approach 
 is the first to be robust in cases when the source is 
 style neutral, like the non polite class in the case 
 of politeness transfer. Automatic and human evaluation shows that our approach outperforms other 
 state of the art models on content preservation metrics while retaining the transfer accuracies.", PRON VERB DET NOUN ADP NOUN NOUN ADP SPACE PRON PRON VERB DET NOUN VERB ADP NOUN SPACE VERB ADP NOUN NOUN ADJ ADP DET PROPN SPACE PROPN PUNCT PRON VERB ADV VERB PROPN PROPN PROPN PROPN PUNCT NUM PUNCT SPACE PROPN PROPN PROPN PROPN PUNCT NUM ADP NOUN NOUN ADP VERB DET ADJ NOUN NOUN VERB PRON SPACE AUX DET ADJ NUM VERB NOUN ADP NOUN SPACE VERB NOUN NOUN PUNCT PRON VERB PRON NOUN SPACE AUX DET ADJ PART AUX ADJ ADP NOUN SCONJ DET NOUN AUX SPACE NOUN ADJ PUNCT ADP DET ADJ ADJ NOUN ADP DET NOUN SPACE ADP NOUN NOUN PUNCT ADJ CCONJ ADJ NOUN VERB SCONJ PRON NOUN VERB ADJ SPACE NOUN ADP DET NOUN NOUN ADP NOUN NOUN NOUN SCONJ VERB DET NOUN NOUN PUNCT,0.6311475409836066,30.5,5.188524590163935
326,38,Aman Madaan,"[' In this work, we propose a method that uses images for generating high-quality comparable\ntraining data without the need for bilingual translators. More specifically, our technique\nfor image selection and crowdsourcing results in useful training data for scenarios where\nfinding annotators proficient in both the languages is challenging, as demonstrated by human\nevaluation and downstream task performance. To the best of our knowledge, we are the first to introduce the idea of crowdsourcing comparable data using images for low resource settings. The IAPR TC-12 dataset (Grubinger\net al., 2006) is one of the earliest image captioning dataset that comprises of travel images\nwith captions in English, Spanish, and German provided by tour guides. The datasets released by Elliott et al. (2016) (30,000 images with captions in German and English) and\nFunaki & Nakayama (2015) (1000 images with captions in English and Japanese) were both\nobtained with the help of professional translators. Hewitt et al. (2018) and Bergsma &\nVan Durme (2011) rely on a corpus of images associated with words (accessed via image\nsearch engines) in the languages of interest. Similarities in images are then used to induce\nbilingual lexicons. In contrast, our method is ideal for settings where absolutely no resources are available for a low resource language. Further, Singhal et al.', '(2019) use a similar\nproprietary dataset obtained via Google search to learn multilingual embeddings. Hitschler\net al. (2016) and Chen et al. (2019) improve the quality of statistical machine translation\nand bilingual lexicon induction by using large monolingual image-captioning corpora. While\ntheir work is orthogonal to ours, it underscores the fact that the dataset generated by our\nmethod can indeed boost downstream tasks. In the future, we plan to use our data creation technique on extremely low-resource languages\nand release parallel corpora that can potentially propel the use of state-of-the-art NLP\ntechniques on these languages. It would also be interesting to explore methods to quantify\nthe definition of universality and select such images for tasks like ours.']",conclusion_chunked," In this work, we propose a method that uses images for generating high-quality comparable
training data without the need for bilingual translators. More specifically, our technique
for image selection and crowdsourcing results in useful training data for scenarios where
finding annotators proficient in both the languages is challenging, as demonstrated by human
evaluation and downstream task performance. To the best of our knowledge, we are the first to introduce the idea of crowdsourcing comparable data using images for low resource settings. The IAPR TC-12 dataset (Grubinger
et al., 2006) is one of the earliest image captioning dataset that comprises of travel images
with captions in English, Spanish, and German provided by tour guides. The datasets released by Elliott et al. (2016) (30,000 images with captions in German and English) and
Funaki & Nakayama (2015) (1000 images with captions in English and Japanese) were both
obtained with the help of professional translators. Hewitt et al. (2018) and Bergsma &
Van Durme (2011) rely on a corpus of images associated with words (accessed via image
search engines) in the languages of interest. Similarities in images are then used to induce
bilingual lexicons. In contrast, our method is ideal for settings where absolutely no resources are available for a low resource language. Further, Singhal et al.",42.814691252144115,30.71625077440773,326,0.5808903574943542," In this work, we propose a method that uses images for generating high quality comparable 
 training data without the need for bilingual translators. More specifically, our technique 
 for image selection and crowdsourcing results in useful training data for scenarios where 
 finding annotators proficient in both the languages is challenging, as demonstrated by human 
 evaluation and downstream task performance. To the best of our knowledge, we are the first to introduce the idea of crowdsourcing comparable data using images for low resource settings. The Propname Propname 00 dataset Propname 
 Propname Propname Propname, 0000 is one of the earliest image captioning dataset that comprises of travel images 
 with captions in Propname, Propname, and German provided by tour guides. The datasets released by Propname Propname Propname. and 
 Propname Propname were both 
 obtained with the help of professional translators. Propname Propname Propname. and Propname Propname Propname rely on a corpus of images associated with words accessed via image 
 search engines in the languages of interest. Similarities in images are then used to induce 
 bilingual lexicons. In contrast, our method is ideal for settings where absolutely no resources are available for a low resource language. Further, Propname Propname Propname."," In this work, we propose a method that uses images for generating high quality comparable 
 training data without the need for bilingual translators. More specifically, our technique 
 for image selection and crowdsourcing results in useful training data for scenarios where 
 finding annotators proficient in both the languages is challenging, as demonstrated by human 
 evaluation and downstream task performance. To the best of our knowledge, we are the first to introduce the idea of crowdsourcing comparable data using images for low resource settings. The Propname Propname 00 dataset Propname 
 Propname Propname Propname, 0000 is one of the earliest image captioning dataset that comprises of travel images 
 with captions in Propname, Propname, and German provided by tour guides. The datasets released by Propname Propname Propname. and 
 Propname Propname were both 
 obtained with the help of professional translators. Propname Propname Propname. and Propname Propname Propname rely on a corpus of images associated with words accessed via image 
 search engines in the languages of interest. Similarities in images are then used to induce 
 bilingual lexicons. In contrast, our method is ideal for settings where absolutely no resources are available for a low resource language. Further, Propname Propname Propname.", ADP DET NOUN PUNCT PRON VERB DET NOUN PRON VERB NOUN ADP VERB ADJ NOUN ADJ SPACE NOUN NOUN ADP DET NOUN ADP ADJ NOUN PUNCT ADV ADV PUNCT PRON NOUN SPACE ADP NOUN NOUN CCONJ NOUN NOUN ADP ADJ NOUN NOUN ADP NOUN SCONJ SPACE VERB NOUN ADJ ADP CCONJ DET NOUN AUX VERB PUNCT SCONJ VERB ADP ADJ SPACE NOUN CCONJ ADJ NOUN NOUN PUNCT ADP DET ADJ ADP PRON NOUN PUNCT PRON AUX DET ADJ PART VERB DET NOUN ADP VERB ADJ NOUN VERB NOUN ADP ADJ NOUN NOUN PUNCT DET PROPN PROPN NUM NOUN PROPN SPACE PROPN PROPN PROPN PUNCT NUM AUX NUM ADP DET ADJ NOUN NOUN NOUN PRON NOUN ADP NOUN NOUN SPACE ADP NOUN ADP PROPN PUNCT PROPN PUNCT CCONJ ADJ VERB ADP NOUN NOUN PUNCT DET NOUN VERB ADP PROPN PROPN PROPN PUNCT CCONJ SPACE PROPN PROPN AUX PRON SPACE VERB ADP DET NOUN ADP ADJ NOUN PUNCT PROPN PROPN PROPN PUNCT CCONJ PROPN PROPN PROPN VERB ADP DET NOUN ADP NOUN VERB ADP NOUN VERB ADP NOUN SPACE NOUN NOUN ADP DET NOUN ADP NOUN PUNCT NOUN ADP NOUN AUX ADV VERB PART VERB SPACE ADJ NOUN PUNCT ADP NOUN PUNCT PRON NOUN AUX ADJ ADP NOUN SCONJ ADV DET NOUN AUX ADJ ADP DET ADJ NOUN NOUN PUNCT ADV PUNCT PROPN PROPN PROPN PUNCT,0.514018691588785,19.454545454545453,5.214953271028038
327,39,Aman Madaan,"[' In this work, we propose a method that uses images for generating high-quality comparable\ntraining data without the need for bilingual translators. More specifically, our technique\nfor image selection and crowdsourcing results in useful training data for scenarios where\nfinding annotators proficient in both the languages is challenging, as demonstrated by human\nevaluation and downstream task performance. To the best of our knowledge, we are the first to introduce the idea of crowdsourcing comparable data using images for low resource settings. The IAPR TC-12 dataset (Grubinger\net al., 2006) is one of the earliest image captioning dataset that comprises of travel images\nwith captions in English, Spanish, and German provided by tour guides. The datasets released by Elliott et al. (2016) (30,000 images with captions in German and English) and\nFunaki & Nakayama (2015) (1000 images with captions in English and Japanese) were both\nobtained with the help of professional translators. Hewitt et al. (2018) and Bergsma &\nVan Durme (2011) rely on a corpus of images associated with words (accessed via image\nsearch engines) in the languages of interest. Similarities in images are then used to induce\nbilingual lexicons. In contrast, our method is ideal for settings where absolutely no resources are available for a low resource language. Further, Singhal et al.', '(2019) use a similar\nproprietary dataset obtained via Google search to learn multilingual embeddings. Hitschler\net al. (2016) and Chen et al. (2019) improve the quality of statistical machine translation\nand bilingual lexicon induction by using large monolingual image-captioning corpora. While\ntheir work is orthogonal to ours, it underscores the fact that the dataset generated by our\nmethod can indeed boost downstream tasks. In the future, we plan to use our data creation technique on extremely low-resource languages\nand release parallel corpora that can potentially propel the use of state-of-the-art NLP\ntechniques on these languages. It would also be interesting to explore methods to quantify\nthe definition of universality and select such images for tasks like ours.']",conclusion_chunked,"(2019) use a similar
proprietary dataset obtained via Google search to learn multilingual embeddings. Hitschler
et al. (2016) and Chen et al. (2019) improve the quality of statistical machine translation
and bilingual lexicon induction by using large monolingual image-captioning corpora. While
their work is orthogonal to ours, it underscores the fact that the dataset generated by our
method can indeed boost downstream tasks. In the future, we plan to use our data creation technique on extremely low-resource languages
and release parallel corpora that can potentially propel the use of state-of-the-art NLP
techniques on these languages. It would also be interesting to explore methods to quantify
the definition of universality and select such images for tasks like ours.",34.40846153846155,30.71625077440773,327,0.4615977108478546," use a similar 
 proprietary dataset obtained via Propname search to learn multilingual embeddings. Propname 
 et Propname. and Propname Propname Propname. improve the quality of statistical machine translation 
 and bilingual lexicon induction by using large monolingual image Propname Propname. While 
 their work is orthogonal to ours, it underscores the fact that the dataset generated by our 
 method can indeed boost downstream tasks. In the future, we plan to use our data creation technique on extremely low resource languages 
 and release parallel Propname that can potentially propel the use of state of the art Propname 
 techniques on these languages. It would also be interesting to explore methods to quantify 
 the definition of universality and select such images for tasks like ours."," use a similar 
 proprietary dataset obtained via Propname search to learn multilingual embeddings. Propname 
 et Propname. and Propname Propname Propname. improve the quality of statistical machine translation 
 and bilingual lexicon induction by using large monolingual image Propname Propname. While 
 their work is orthogonal to ours, it underscores the fact that the dataset generated by our 
 method can indeed boost downstream tasks. In the future, we plan to use our data creation technique on extremely low resource languages 
 and release parallel Propname that can potentially propel the use of state of the art Propname 
 techniques on these languages. It would also be interesting to explore methods to quantify 
 the definition of universality and select such images for tasks like ours.", VERB DET ADJ SPACE ADJ NOUN VERB ADP PROPN NOUN PART VERB ADJ NOUN PUNCT PROPN SPACE NOUN PROPN PUNCT CCONJ PROPN PROPN PROPN PUNCT VERB DET NOUN ADP ADJ NOUN NOUN SPACE CCONJ ADJ ADJ NOUN ADP VERB ADJ ADJ NOUN PROPN PROPN PUNCT SCONJ SPACE PRON NOUN AUX ADJ ADP PRON PUNCT PRON VERB DET NOUN SCONJ DET NOUN VERB ADP PRON SPACE NOUN AUX ADV VERB ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PART VERB PRON NOUN NOUN NOUN ADP ADV ADJ NOUN NOUN SPACE CCONJ VERB NOUN PROPN PRON AUX ADV VERB DET NOUN ADP NOUN ADP DET NOUN PROPN SPACE NOUN ADP DET NOUN PUNCT PRON AUX ADV AUX ADJ PART VERB NOUN PART VERB SPACE DET NOUN ADP NOUN CCONJ VERB ADJ NOUN ADP NOUN ADP NOUN PUNCT,0.6640625,18.285714285714285,5.203125
328,40,Aman Madaan,"[' We present the first detailed study of the task of numerical relation extraction, in which one of the arguments of the\nrelation is a quantity. Our preliminary analysis reveals several peculiarities that make the task differently challenging\nfrom standard IE. We employ these insights into a rule-based\nsystem, NumberRule, that can extract any numerical relation given input keywords for that relation. We also develop\nNumberTron, an extension of MultiR, which employs novel\ntask-specific features and can be trained via distant supervision or other heuristic labelings. By aggregating evidence from multiple features, NumberTron produces much higher recall at comparable precision compared to NumberRule. Both systems vastly outperform baselines and non-numerical IE systems, with NumberTron yielding almost 25 point F-score improvement. A key limitation of our research is lack of temporal modeling – many numerical relations change over time. In the future, we wish to extract numerical relations along with their\ntemporal scopes. Temporal identification will likely improve\nthe effectiveness of distant supervision too.']",conclusion_chunked," We present the first detailed study of the task of numerical relation extraction, in which one of the arguments of the
relation is a quantity. Our preliminary analysis reveals several peculiarities that make the task differently challenging
from standard IE. We employ these insights into a rule-based
system, NumberRule, that can extract any numerical relation given input keywords for that relation. We also develop
NumberTron, an extension of MultiR, which employs novel
task-specific features and can be trained via distant supervision or other heuristic labelings. By aggregating evidence from multiple features, NumberTron produces much higher recall at comparable precision compared to NumberRule. Both systems vastly outperform baselines and non-numerical IE systems, with NumberTron yielding almost 25 point F-score improvement. A key limitation of our research is lack of temporal modeling – many numerical relations change over time. In the future, we wish to extract numerical relations along with their
temporal scopes. Temporal identification will likely improve
the effectiveness of distant supervision too.",17.276111111111106,30.71625077440773,328,0.5023854970932007," We present the first detailed study of the task of Propname relation extraction, in which one of the arguments of the 
 relation is a quantity. Our preliminary analysis reveals several peculiarities that make the task differently challenging 
 from standard Propname. We employ these insights into a rule based 
 system, Propname, that can extract any numerical relation given input keywords for that relation. We also develop 
 Propname, an extension of Propname, which employs novel 
 task specific features and can be trained via distant supervision or other heuristic labelings. By aggregating evidence from multiple features, Propname produces much higher recall at comparable precision compared to Propname. Both systems vastly outperform baselines and Propname Propname Propname systems, with Propname yielding almost 00 point F score improvement. A key limitation of our research is lack of temporal modeling many numerical relations change over time. In the future, we wish to extract numerical relations along with their 
 temporal scopes. Temporal identification will likely improve 
 the effectiveness of distant supervision too."," We present the first detailed study of the task of Propname relation extraction, in which one of the arguments of the 
 relation is a quantity. Our preliminary analysis reveals several peculiarities that make the task differently challenging 
 from standard Propname. We employ these insights into a rule based 
 system, Propname, that can extract any numerical relation given input keywords for that relation. We also develop 
 Propname, an extension of Propname, which employs novel 
 task specific features and can be trained via distant supervision or other heuristic labelings. By aggregating evidence from multiple features, Propname produces much higher recall at comparable precision compared to Propname. Both systems vastly outperform baselines and Propname Propname Propname systems, with Propname yielding almost 00 point F score improvement. A key limitation of our research is lack of temporal modeling many numerical relations change over time. In the future, we wish to extract numerical relations along with their 
 temporal scopes. Temporal identification will likely improve 
 the effectiveness of distant supervision too.", PRON VERB DET ADJ ADJ NOUN ADP DET NOUN ADP PROPN NOUN NOUN PUNCT ADP PRON NUM ADP DET NOUN ADP DET SPACE NOUN AUX DET NOUN PUNCT PRON ADJ NOUN VERB ADJ NOUN PRON VERB DET NOUN ADV VERB SPACE ADP ADJ PROPN PUNCT PRON VERB DET NOUN ADP DET NOUN VERB SPACE NOUN PUNCT PROPN PUNCT PRON AUX VERB DET ADJ NOUN VERB NOUN NOUN ADP DET NOUN PUNCT PRON ADV VERB SPACE PROPN PUNCT DET NOUN ADP PROPN PUNCT PRON VERB ADJ SPACE NOUN ADJ NOUN CCONJ AUX AUX VERB ADP ADJ NOUN CCONJ ADJ ADJ NOUN PUNCT ADP VERB NOUN ADP ADJ NOUN PUNCT PROPN VERB ADV ADJ NOUN ADP ADJ NOUN VERB ADP PROPN PUNCT DET NOUN ADV VERB NOUN CCONJ PROPN PROPN PROPN NOUN PUNCT ADP PROPN VERB ADV NUM NOUN NOUN NOUN NOUN PUNCT DET ADJ NOUN ADP PRON NOUN AUX NOUN ADP ADJ NOUN ADJ ADJ NOUN VERB ADP NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PART VERB ADJ NOUN ADP ADP PRON SPACE ADJ NOUN PUNCT ADJ NOUN AUX ADV VERB SPACE DET NOUN ADP ADJ NOUN ADV PUNCT,0.6483516483516484,20.22222222222222,5.291208791208791
329,41,Hugo Touvron,"[' In this study, we have introduced Llama 2, a new family of pretrained and fine-tuned models with scales of 7 billion to 70 billion parameters. These models have demonstrated their competitiveness with existing open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation sets we examined, although they still lag behind other models like GPT-4. We meticulously elaborated on the methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research, we have responsibly opened access to Llama 2 and Llama 2-Chat. As part of our ongoing commitment to transparency and safety, we plan to make further improvements to Llama 2-Chat in future work']",conclusion_chunked," In this study, we have introduced Llama 2, a new family of pretrained and fine-tuned models with scales of 7 billion to 70 billion parameters. These models have demonstrated their competitiveness with existing open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation sets we examined, although they still lag behind other models like GPT-4. We meticulously elaborated on the methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research, we have responsibly opened access to Llama 2 and Llama 2-Chat. As part of our ongoing commitment to transparency and safety, we plan to make further improvements to Llama 2-Chat in future work",25.57847058823532,30.655460127574653,329,0.734826922416687," In this study, we have introduced Propname 0, a new family of pretrained and fine tuned models with scales of 0 billion to 00 billion parameters. These models have demonstrated their competitiveness with existing open source chat models, as well as competency that is equivalent to some proprietary models on evaluation sets we examined, although they still lag behind other models like Propname 0. We meticulously elaborated on the methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research, we have responsibly opened access to Propname 0 and Propname 0 Chat. As part of our ongoing commitment to transparency and safety, we plan to make further improvements to Propname 0 Chat in future work"," In this study, we have introduced Propname 0, a new family of pretrained and fine tuned models with scales of 0 billion to 00 billion parameters. These models have demonstrated their competitiveness with existing open source chat models, as well as competency that is equivalent to some proprietary models on evaluation sets we examined, although they still lag behind other models like Propname 0. We meticulously elaborated on the methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research, we have responsibly opened access to Propname 0 and Propname 0 Chat. As part of our ongoing commitment to transparency and safety, we plan to make further improvements to Propname 0 Chat in future work", ADP DET NOUN PUNCT PRON AUX VERB PROPN NUM PUNCT DET ADJ NOUN ADP VERB CCONJ ADJ VERB NOUN ADP NOUN ADP NUM NUM PART NUM NUM NOUN PUNCT DET NOUN AUX VERB PRON NOUN ADP VERB ADJ NOUN NOUN NOUN PUNCT ADV ADV ADP NOUN PRON AUX ADJ ADP DET ADJ NOUN ADP NOUN NOUN PRON VERB PUNCT SCONJ PRON ADV VERB ADP ADJ NOUN ADP PROPN NUM PUNCT PRON ADV VERB ADP DET NOUN CCONJ NOUN VERB ADP VERB PRON NOUN PUNCT ADP DET ADJ NOUN ADP PRON NOUN ADP DET NOUN ADP NOUN CCONJ NOUN PUNCT PART VERB ADV ADV ADP NOUN CCONJ VERB DET NOUN ADP NOUN PUNCT PRON AUX ADV VERB NOUN ADP PROPN NUM CCONJ PROPN NUM NOUN PUNCT ADP NOUN ADP PRON ADJ NOUN ADP NOUN CCONJ NOUN PUNCT PRON VERB PART VERB ADJ NOUN ADP PROPN NUM NOUN ADP ADJ NOUN,0.610738255033557,29.8,4.818791946308725
330,42,Hugo Touvron,"[' In this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10× smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robustness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al. (2022) that finetuning these models on instructions\nlead to promising results, and we plan to further\ninvestigate this in future work. Finally, we plan to\nrelease larger models trained on larger pretraining\ncorpora in the future, since we have seen a constant\nimprovement in performance as we were scaling.']",conclusion_chunked," In this paper, we presented a series of language
models that are released openly, and competitive
with state-of-the-art foundation models. Most
notably, LLaMA-13B outperforms GPT-3 while
being more than 10× smaller, and LLaMA-65B is
competitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, we show that it is possible
to achieve state-of-the-art performance by training
exclusively on publicly available data, without
resorting to proprietary datasets. We hope that
releasing these models to the research community
will accelerate the development of large language
models, and help efforts to improve their robustness and mitigate known issues such as toxicity and
bias. Additionally, we observed like Chung et al. (2022) that finetuning these models on instructions
lead to promising results, and we plan to further
investigate this in future work. Finally, we plan to
release larger models trained on larger pretraining
corpora in the future, since we have seen a constant
improvement in performance as we were scaling.",31.88375000000002,30.655460127574653,330,0.44393977522850037," In this paper, we presented a series of language 
 models that are released openly, and competitive 
 with state of the art foundation models. Most 
 notably, Propname 00B outperforms Propname 0 while 
 being more than 00 smaller, and Propname 00B is 
 competitive with Chinchilla 00B and Propname Propname Unlike previous studies, we show that it is possible 
 to achieve state of the art performance by training 
 exclusively on publicly available data, without 
 resorting to proprietary datasets. We hope that 
 releasing these models to the research community 
 will accelerate the development of large language 
 models, and help efforts to improve their robustness and mitigate known issues such as toxicity and 
 bias. Additionally, we observed like Propname Propname Propname. that finetuning these models on instructions 
 lead to promising results, and we plan to further 
 investigate this in future work. Finally, we plan to 
 release larger models trained on larger pretraining 
 Propname in the future, since we have seen a constant 
 improvement in performance as we were scaling."," In this paper, we presented a series of language 
 models that are released openly, and competitive 
 with state of the art foundation models. Most 
 notably, Propname 00B outperforms Propname 0 while 
 being more than 00 smaller, and Propname 00B is 
 competitive with Chinchilla 00B and Propname Propname Unlike previous studies, we show that it is possible 
 to achieve state of the art performance by training 
 exclusively on publicly available data, without 
 resorting to proprietary datasets. We hope that 
 releasing these models to the research community 
 will accelerate the development of large language 
 models, and help efforts to improve their robustness and mitigate known issues such as toxicity and 
 bias. Additionally, we observed like Propname Propname Propname. that finetuning these models on instructions 
 lead to promising results, and we plan to further 
 investigate this in future work. Finally, we plan to 
 release larger models trained on larger pretraining 
 Propname in the future, since we have seen a constant 
 improvement in performance as we were scaling.", ADP DET NOUN PUNCT PRON VERB DET NOUN ADP NOUN SPACE NOUN PRON AUX VERB ADV PUNCT CCONJ ADJ SPACE ADP NOUN ADP DET NOUN NOUN NOUN PUNCT ADJ SPACE ADV PUNCT PROPN NOUN VERB PROPN NUM SCONJ SPACE AUX ADJ ADP NUM ADJ PUNCT CCONJ PROPN NUM AUX SPACE ADJ ADP NOUN NUM CCONJ PROPN PROPN ADP ADJ NOUN PUNCT PRON VERB SCONJ PRON AUX ADJ SPACE PART VERB NOUN ADP DET NOUN NOUN ADP VERB SPACE ADV ADP ADV ADJ NOUN PUNCT ADP SPACE VERB ADP ADJ NOUN PUNCT PRON VERB SCONJ SPACE VERB DET NOUN ADP DET NOUN NOUN SPACE AUX VERB DET NOUN ADP ADJ NOUN SPACE NOUN PUNCT CCONJ VERB NOUN PART VERB PRON NOUN CCONJ VERB VERB NOUN ADJ ADP NOUN CCONJ SPACE NOUN PUNCT ADV PUNCT PRON VERB ADP PROPN PROPN PROPN PUNCT SCONJ VERB DET NOUN ADP NOUN SPACE VERB ADP VERB NOUN PUNCT CCONJ PRON VERB PART ADV SPACE VERB PRON ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB PART SPACE VERB ADJ NOUN VERB ADP ADJ VERB SPACE PROPN ADP DET NOUN PUNCT SCONJ PRON AUX VERB DET ADJ SPACE NOUN ADP NOUN SCONJ PRON AUX VERB PUNCT,0.580110497237569,30.166666666666668,4.966850828729282
331,43,Hugo Touvron,"[' Co-training submodels (cosub) is an effective way to improve existing deep residual networks. It is straightforward\nto implement, just involving a few lines of code. It does not\nneed a pre-trained teacher, and it only maintains a single set\nof weights for the model. Extensive experimental results\non image classification, transfer learning and semantic segmentation show that cosub is overall extremely effective. It\nworks off-the-shelf and improves the state of the art for various network architectures, including convnets like Regnet.']",conclusion_chunked," Co-training submodels (cosub) is an effective way to improve existing deep residual networks. It is straightforward
to implement, just involving a few lines of code. It does not
need a pre-trained teacher, and it only maintains a single set
of weights for the model. Extensive experimental results
on image classification, transfer learning and semantic segmentation show that cosub is overall extremely effective. It
works off-the-shelf and improves the state of the art for various network architectures, including convnets like Regnet.",30.655460127574653,30.655460127574653,331,0.6712803244590759," Co training submodels is an effective way to improve existing deep residual networks. It is straightforward 
 to implement, just involving a few lines of code. It does not 
 need a pre trained teacher, and it only maintains a single set 
 of weights for the model. Extensive experimental results 
 on image classification, transfer learning and semantic segmentation show that cosub is overall extremely effective. It 
 works off the shelf and improves the state of the art for various network architectures, including convnets like Propname."," Co training submodels is an effective way to improve existing deep residual networks. It is straightforward 
 to implement, just involving a few lines of code. It does not 
 need a pre trained teacher, and it only maintains a single set 
 of weights for the model. Extensive experimental results 
 on image classification, transfer learning and semantic segmentation show that cosub is overall extremely effective. It 
 works off the shelf and improves the state of the art for various network architectures, including convnets like Propname.", NOUN NOUN NOUN AUX DET ADJ NOUN PART VERB VERB ADJ ADJ NOUN PUNCT PRON AUX ADJ SPACE PART VERB PUNCT ADV VERB DET ADJ NOUN ADP NOUN PUNCT PRON AUX PART SPACE VERB DET ADJ VERB NOUN PUNCT CCONJ PRON ADV VERB DET ADJ NOUN SPACE ADP NOUN ADP DET NOUN PUNCT ADJ ADJ NOUN SPACE ADP NOUN NOUN PUNCT NOUN NOUN CCONJ ADJ NOUN NOUN SCONJ NOUN AUX ADV ADV ADJ PUNCT PRON SPACE VERB ADP DET NOUN CCONJ VERB DET NOUN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT VERB NOUN ADP PROPN PUNCT,0.75,18.4,4.891304347826087
332,44,Hugo Touvron,"[' This paper makes a simple contribution: it proposes improved baselines for vision\ntransformers trained in a supervised fashion that can serve (1) either as a comparison basis for new architectures; (2) or for other training approaches such as\nthose based on self-supervised learning. We hope that this stronger baseline will\nserve the community effort in making progress on learning foundation models\nthat could serve many tasks. Our experiments have also gathered a few insights\non how to train ViT for larger models with reduced resources without hurting accuracy, allowing us to train a one-billion parameter model with 4 nodes of 8 GPUs.']",conclusion_chunked," This paper makes a simple contribution: it proposes improved baselines for vision
transformers trained in a supervised fashion that can serve (1) either as a comparison basis for new architectures; (2) or for other training approaches such as
those based on self-supervised learning. We hope that this stronger baseline will
serve the community effort in making progress on learning foundation models
that could serve many tasks. Our experiments have also gathered a few insights
on how to train ViT for larger models with reduced resources without hurting accuracy, allowing us to train a one-billion parameter model with 4 nodes of 8 GPUs.",25.519117647058835,30.655460127574653,332,0.8134835958480835," This paper makes a simple contribution: it proposes improved baselines for vision 
 transformers trained in a supervised fashion that can serve either as a comparison basis for new architectures; or for other training approaches such as 
 those based on self supervised learning. We hope that this stronger baseline will 
 serve the community effort in making progress on learning foundation models 
 that could serve many tasks. Our experiments have also gathered a few insights 
 on how to train ViT for larger models with reduced resources without hurting accuracy, allowing us to train a one billion parameter model with 0 nodes of 0 GPUs."," This paper makes a simple contribution: it proposes improved baselines for vision 
 transformers trained in a supervised fashion that can serve either as a comparison basis for new architectures; or for other training approaches such as 
 those based on self supervised learning. We hope that this stronger baseline will 
 serve the community effort in making progress on learning foundation models 
 that could serve many tasks. Our experiments have also gathered a few insights 
 on how to train ViT for larger models with reduced resources without hurting accuracy, allowing us to train a one billion parameter model with 0 nodes of 0 GPUs.", DET NOUN VERB DET ADJ NOUN PUNCT PRON VERB ADJ NOUN ADP NOUN SPACE NOUN VERB ADP DET ADJ NOUN PRON AUX VERB CCONJ ADP DET NOUN NOUN ADP ADJ NOUN PUNCT CCONJ ADP ADJ NOUN NOUN ADJ ADP SPACE PRON VERB ADP NOUN ADJ NOUN PUNCT PRON VERB SCONJ DET ADJ NOUN AUX SPACE VERB DET NOUN NOUN ADP VERB NOUN ADP VERB NOUN NOUN SPACE PRON AUX VERB ADJ NOUN PUNCT PRON NOUN AUX ADV VERB DET ADJ NOUN SPACE ADP SCONJ PART VERB NOUN ADP ADJ NOUN ADP VERB NOUN ADP VERB NOUN PUNCT VERB PRON PART VERB DET NUM NUM NOUN NOUN ADP NUM NOUN ADP NUM NOUN PUNCT,0.7777777777777778,36.0,4.925925925925926
333,45,Hugo Touvron,"[' In this paper, we looked at three different topics related to Vision Transformers. First, we investigated a simple but effective way to parallelize them, showing a viable alternative to increase capacity without significantly increasing the working dimensionality. Whether this simple parallel design principle can be applied to other architectures is an exploration left for future work. Second, we considered different fine-tuning strategies and showed that fine-tuning the self-attention layer is sufficient in the context of resolution fine-tuning. This can also be interesting when transferring to other downstream classification tasks, especially when finetuning large models or/and transferring to a dataset with few training images. Last, we introduced a simple patch pre-processing stem, which processes patches independently across multiple linear layers interleaved with non-linearities and patch aggregation. It is especially useful when combined with mask-based selfsupervised learning such as BeiT.']",conclusion_chunked," In this paper, we looked at three different topics related to Vision Transformers. First, we investigated a simple but effective way to parallelize them, showing a viable alternative to increase capacity without significantly increasing the working dimensionality. Whether this simple parallel design principle can be applied to other architectures is an exploration left for future work. Second, we considered different fine-tuning strategies and showed that fine-tuning the self-attention layer is sufficient in the context of resolution fine-tuning. This can also be interesting when transferring to other downstream classification tasks, especially when finetuning large models or/and transferring to a dataset with few training images. Last, we introduced a simple patch pre-processing stem, which processes patches independently across multiple linear layers interleaved with non-linearities and patch aggregation. It is especially useful when combined with mask-based selfsupervised learning such as BeiT.",9.567769784172697,30.655460127574653,333,0.5931060910224915," In this paper, we looked at three different topics related to Propname Propname. First, we investigated a simple but effective way to parallelize them, showing a viable alternative to increase capacity without significantly increasing the working dimensionality. Whether this simple parallel design principle can be applied to other architectures is an exploration left for future work. Second, we considered different fine tuning strategies and showed that fine tuning the self attention layer is sufficient in the context of resolution fine tuning. This can also be interesting when transferring to other downstream classification tasks, especially when finetuning large models orand transferring to a dataset with few training images. Last, we introduced a simple patch pre processing stem, which processes patches independently across multiple linear layers interleaved with non linearities and patch aggregation. It is especially useful when combined with mask based selfsupervised learning such as Propname"," In this paper, we looked at three different topics related to Propname Propname. First, we investigated a simple but effective way to parallelize them, showing a viable alternative to increase capacity without significantly increasing the working dimensionality. Whether this simple parallel design principle can be applied to other architectures is an exploration left for future work. Second, we considered different fine tuning strategies and showed that fine tuning the self attention layer is sufficient in the context of resolution fine tuning. This can also be interesting when transferring to other downstream classification tasks, especially when finetuning large models orand transferring to a dataset with few training images. Last, we introduced a simple patch pre processing stem, which processes patches independently across multiple linear layers interleaved with non linearities and patch aggregation. It is especially useful when combined with mask based selfsupervised learning such as Propname", ADP DET NOUN PUNCT PRON VERB ADP NUM ADJ NOUN VERB ADP PROPN PROPN PUNCT ADV PUNCT PRON VERB DET ADJ CCONJ ADJ NOUN PART VERB PRON PUNCT VERB DET ADJ NOUN PART VERB NOUN ADP ADV VERB DET VERB NOUN PUNCT SCONJ DET ADJ ADJ NOUN NOUN AUX AUX VERB ADP ADJ NOUN AUX DET NOUN VERB ADP ADJ NOUN PUNCT ADV PUNCT PRON VERB ADJ ADJ NOUN NOUN CCONJ VERB SCONJ NOUN VERB DET NOUN NOUN NOUN AUX ADJ ADP DET NOUN ADP NOUN ADJ NOUN PUNCT PRON AUX ADV AUX ADJ SCONJ VERB ADP ADJ ADJ NOUN NOUN PUNCT ADV SCONJ VERB ADJ NOUN NOUN VERB ADP DET NOUN ADP ADJ NOUN NOUN PUNCT ADJ PUNCT PRON VERB DET ADJ NOUN X NOUN NOUN PUNCT PRON VERB VERB ADV ADP ADJ ADJ NOUN VERB ADP ADJ NOUN CCONJ VERB NOUN PUNCT PRON AUX ADV ADJ SCONJ VERB ADP NOUN VERB VERB NOUN ADJ ADP PROPN,0.7025316455696202,22.571428571428573,5.493670886075949
334,46,Hugo Touvron,"[' In this chapter, we have introduced Grafit, a method to learn image representations at a\nfiner granularity than the one offered by the annotation at training time. Inspired by recent selfsupervised learning approach, we carefully design a joint learning scheme integrating instance\nand coarse-label based classification losses. For the latter one, we exploit a knn strategy but with\na dedicated process to manage the memory both at train-time and for inference at test-time. We propose two original use-cases to deeply evaluate coarse-trained fine-grained testing evaluation, for which Grafit exhibits outstanding performance. It improves the performance for fine-grained category retrieval within a coarsely annotated\ncollection. For on-the-fly kNN classification, Grafit significantly reduces the gap with a network\ntrained with fine labels. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly\nclassification on ImageNet. This improvement is still +9.5% w.r.t. our own stronger baseline,\neverything being equal otherwise. Grafit also improves transfer learning: our experiments show\nthat our representation discriminates better at a finer granularity. It also translates into better\ntransfer learning performance to fine-grained datasets, outperforming the current state of the art\nwith a more efficient network.']",conclusion_chunked," In this chapter, we have introduced Grafit, a method to learn image representations at a
finer granularity than the one offered by the annotation at training time. Inspired by recent selfsupervised learning approach, we carefully design a joint learning scheme integrating instance
and coarse-label based classification losses. For the latter one, we exploit a knn strategy but with
a dedicated process to manage the memory both at train-time and for inference at test-time. We propose two original use-cases to deeply evaluate coarse-trained fine-grained testing evaluation, for which Grafit exhibits outstanding performance. It improves the performance for fine-grained category retrieval within a coarsely annotated
collection. For on-the-fly kNN classification, Grafit significantly reduces the gap with a network
trained with fine labels. For instance, we improve by +16.3% the top-1 accuracy for on-the-fly
classification on ImageNet. This improvement is still +9.5% w.r.t. our own stronger baseline,
everything being equal otherwise. Grafit also improves transfer learning: our experiments show
that our representation discriminates better at a finer granularity. It also translates into better
transfer learning performance to fine-grained datasets, outperforming the current state of the art
with a more efficient network.",21.339704902427428,30.655460127574653,334,0.7467244267463684," In this chapter, we have introduced Propname, a method to learn image representations at a 
 finer granularity than the one offered by the annotation at training time. Inspired by recent selfsupervised learning approach, we carefully design a joint learning scheme integrating instance 
 and coarse label based classification losses. For the latter one, we exploit a knn strategy but with 
 a dedicated process to manage the memory both at train time and for inference at test time. We propose two original use cases to deeply evaluate coarse trained fine grained testing evaluation, for which Propname exhibits outstanding performance. It improves the performance for fine grained category retrieval within a coarsely annotated 
 collection. For on the fly kNN classification, Propname significantly reduces the gap with a network 
 trained with fine labels. For instance, we improve by 00.0 the top 0 accuracy for on the fly 
 classification on Propname. This improvement is still 0.0 w.r.t. our own stronger baseline, 
 everything being equal otherwise. Propname also improves transfer learning: our experiments show 
 that our representation discriminates better at a finer granularity. It also translates into better 
 transfer learning performance to fine grained datasets, outperforming the current state of the art 
 with a more efficient network."," In this chapter, we have introduced Propname, a method to learn image representations at a 
 finer granularity than the one offered by the annotation at training time. Inspired by recent selfsupervised learning approach, we carefully design a joint learning scheme integrating instance 
 and coarse label based classification losses. For the latter one, we exploit a knn strategy but with 
 a dedicated process to manage the memory both at train time and for inference at test time. We propose two original use cases to deeply evaluate coarse trained fine grained testing evaluation, for which Propname exhibits outstanding performance. It improves the performance for fine grained category retrieval within a coarsely annotated 
 collection. For on the fly kNN classification, Propname significantly reduces the gap with a network 
 trained with fine labels. For instance, we improve by 00.0 the top 0 accuracy for on the fly 
 classification on Propname. This improvement is still 0.0 w.r.t. our own stronger baseline, 
 everything being equal otherwise. Propname also improves transfer learning: our experiments show 
 that our representation discriminates better at a finer granularity. It also translates into better 
 transfer learning performance to fine grained datasets, outperforming the current state of the art 
 with a more efficient network.", ADP DET NOUN PUNCT PRON AUX VERB PROPN PUNCT DET NOUN PART VERB NOUN NOUN ADP DET SPACE ADJ NOUN ADP DET NOUN VERB ADP DET NOUN ADP NOUN NOUN PUNCT VERB ADP ADJ VERB NOUN NOUN PUNCT PRON ADV VERB DET ADJ VERB NOUN VERB NOUN SPACE CCONJ ADJ NOUN VERB NOUN NOUN PUNCT ADP DET ADJ NUM PUNCT PRON VERB DET ADJ NOUN CCONJ ADP SPACE DET ADJ NOUN PART VERB DET NOUN CCONJ ADP NOUN NOUN CCONJ ADP NOUN ADP NOUN NOUN PUNCT PRON VERB NUM ADJ NOUN NOUN PART ADV VERB NOUN VERB ADJ VERB NOUN NOUN PUNCT ADP PRON PROPN VERB ADJ NOUN PUNCT PRON VERB DET NOUN ADP ADJ ADJ NOUN NOUN ADP DET ADV VERB SPACE NOUN PUNCT ADP ADP DET NOUN VERB NOUN PUNCT PROPN ADV VERB DET NOUN ADP DET NOUN SPACE VERB ADP ADJ NOUN PUNCT ADP NOUN PUNCT PRON VERB ADP NUM DET ADJ NUM NOUN ADP ADP DET NOUN SPACE NOUN ADP PROPN PUNCT DET NOUN AUX ADV NUM NOUN PUNCT PRON ADJ ADJ NOUN PUNCT SPACE PRON AUX ADJ ADV PUNCT PROPN ADV VERB NOUN VERB PUNCT PRON NOUN VERB SPACE SCONJ PRON NOUN VERB ADV ADP DET ADJ NOUN PUNCT PRON ADV VERB ADP ADJ SPACE NOUN VERB NOUN PART VERB ADJ NOUN PUNCT VERB DET ADJ NOUN ADP DET NOUN SPACE ADP DET ADV ADJ NOUN PUNCT,0.5855855855855856,20.181818181818183,5.112612612612613
335,47,Hugo Touvron,"[' In this paper, we introduced a full patch-based ConvNet with no pyramidal structure. We used an attention-based pooling on top of the trunk, akin to the attention mechanism in transformers, which offers visualization properties. Our model is only parametrized by its width and depth, and its training does not require a heavy hyper-parameter search. We demonstrated its interest on several computer vision tasks: classification, segmentation, detection. Limitations: There is no perfect metric for measuring the overall performance of a given neural network architecture [14]. We have provided 4 different metrics but there are probably some aspects that are not considered. Deep and wide models have the same behaviour with respect to FLOPs but the wider models have the advantage to be associated with a lower latency [21, 75]. We have mostly experimented with depth rather than width because deep models consume less memory at inference time, which makes them an appealing choice when dealing with higher resolution images [4], as is the case in segmentation and detection. Broader impact: Large scale deep learning models are effective for many different computer vision applications, but the way they reach their decision is still not yet fully understood. When deploying such machine learning-based systems, there would be a benefit to be able to illustrate their choices in critical applications. We hope that our model, by its simplicity, and by its built-in internal visualization mechanism, may foster this direction of interpretability.']",conclusion_chunked," In this paper, we introduced a full patch-based ConvNet with no pyramidal structure. We used an attention-based pooling on top of the trunk, akin to the attention mechanism in transformers, which offers visualization properties. Our model is only parametrized by its width and depth, and its training does not require a heavy hyper-parameter search. We demonstrated its interest on several computer vision tasks: classification, segmentation, detection. Limitations: There is no perfect metric for measuring the overall performance of a given neural network architecture [14]. We have provided 4 different metrics but there are probably some aspects that are not considered. Deep and wide models have the same behaviour with respect to FLOPs but the wider models have the advantage to be associated with a lower latency [21, 75]. We have mostly experimented with depth rather than width because deep models consume less memory at inference time, which makes them an appealing choice when dealing with higher resolution images [4], as is the case in segmentation and detection. Broader impact: Large scale deep learning models are effective for many different computer vision applications, but the way they reach their decision is still not yet fully understood. When deploying such machine learning-based systems, there would be a benefit to be able to illustrate their choices in critical applications. We hope that our model, by its simplicity, and by its built-in internal visualization mechanism, may foster this direction of interpretability.",32.6201346801347,30.655460127574653,335,0.35668259859085083," In this paper, we introduced a full patch based Propname with no pyramidal structure. We used an attention based pooling on top of the trunk, akin to the attention mechanism in transformers, which offers visualization properties. Our model is only parametrized by its width and depth, and its training does not require a heavy hyper parameter search. We demonstrated its interest on several computer vision tasks: classification, segmentation, detection. Limitations: There is no perfect metric for measuring the overall performance of a given neural network architecture. We have provided 0 different metrics but there are probably some aspects that are not considered. Deep and wide models have the same behaviour with respect to FLOPs but the wider models have the advantage to be associated with a lower latency. We have mostly experimented with depth rather than width because deep models consume less memory at inference time, which makes them an appealing choice when dealing with higher resolution images, as is the case in segmentation and detection. Broader impact: Large scale deep learning models are effective for many different computer vision applications, but the way they reach their decision is still not yet fully understood. When deploying such machine learning based systems, there would be a benefit to be able to illustrate their choices in critical applications. We hope that our model, by its simplicity, and by its built in internal visualization mechanism, may foster this direction of interpretability."," In this paper, we introduced a full patch based Propname with no pyramidal structure. We used an attention based pooling on top of the trunk, akin to the attention mechanism in transformers, which offers visualization properties. Our model is only parametrized by its width and depth, and its training does not require a heavy hyper parameter search. We demonstrated its interest on several computer vision tasks: classification, segmentation, detection. Limitations: There is no perfect metric for measuring the overall performance of a given neural network architecture. We have provided 0 different metrics but there are probably some aspects that are not considered. Deep and wide models have the same behaviour with respect to FLOPs but the wider models have the advantage to be associated with a lower latency. We have mostly experimented with depth rather than width because deep models consume less memory at inference time, which makes them an appealing choice when dealing with higher resolution images, as is the case in segmentation and detection. Broader impact: Large scale deep learning models are effective for many different computer vision applications, but the way they reach their decision is still not yet fully understood. When deploying such machine learning based systems, there would be a benefit to be able to illustrate their choices in critical applications. We hope that our model, by its simplicity, and by its built in internal visualization mechanism, may foster this direction of interpretability.", ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN VERB PROPN ADP DET ADJ NOUN PUNCT PRON VERB DET NOUN VERB NOUN ADP NOUN ADP DET NOUN PUNCT ADJ ADP DET NOUN NOUN ADP NOUN PUNCT PRON VERB NOUN NOUN PUNCT PRON NOUN AUX ADV VERB ADP PRON NOUN CCONJ NOUN PUNCT CCONJ PRON NOUN AUX PART VERB DET ADJ NOUN NOUN NOUN PUNCT PRON VERB PRON NOUN ADP ADJ NOUN NOUN NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN PUNCT PRON VERB DET ADJ NOUN ADP VERB DET ADJ NOUN ADP DET VERB ADJ NOUN NOUN PUNCT PRON AUX VERB NUM ADJ NOUN CCONJ PRON VERB ADV DET NOUN PRON AUX PART VERB PUNCT ADJ CCONJ ADJ NOUN VERB DET ADJ NOUN ADP NOUN ADP NOUN CCONJ DET ADJ NOUN VERB DET NOUN PART AUX VERB ADP DET ADJ NOUN PUNCT PRON AUX ADV VERB ADP NOUN ADV ADP VERB SCONJ ADJ NOUN VERB ADJ NOUN ADP NOUN NOUN PUNCT PRON VERB PRON DET ADJ NOUN SCONJ VERB ADP ADJ NOUN NOUN PUNCT SCONJ AUX DET NOUN ADP NOUN CCONJ NOUN PUNCT ADJ NOUN PUNCT ADJ NOUN ADJ NOUN NOUN AUX ADJ ADP ADJ ADJ NOUN NOUN NOUN PUNCT CCONJ DET NOUN PRON VERB PRON NOUN AUX ADV PART ADV ADV VERB PUNCT SCONJ VERB ADJ NOUN VERB VERB NOUN PUNCT PRON AUX AUX DET NOUN PART AUX ADJ PART VERB PRON NOUN ADP ADJ NOUN PUNCT PRON VERB SCONJ PRON NOUN PUNCT ADP PRON NOUN PUNCT CCONJ ADP PRON VERB ADP ADJ NOUN NOUN PUNCT AUX VERB DET NOUN ADP NOUN PUNCT,0.6075471698113207,24.09090909090909,4.879245283018868
336,48,Hugo Touvron,"[' In this paper, we have introduced DeiT, which are image transformers that\ndo not require very large amount of data to be trained, thanks to improved training and in particular a novel distillation procedure. Convolutional neural networks have optimized, both in terms of architecture and optimization\nduring almost a decade, including through extensive architecture search that\nis prone to overfiting, as it is the case for instance for EfficientNets [51]. For\nDeiT we have started the existing data augmentation and regularization strategies pre-existing for convnets, not introducing any significant architectural beyond our novel distillation token. Therefore it is likely that research on dataaugmentation more adapted or learned for transformers will bring further gains. Therefore, considering our results, where image transformers are on par\nwith convnets already, we believe that they will rapidly become a method of\nchoice considering their lower memory footprint for a given accuracy. We provide an open-source implementation of our method. It is available\nat https://github.com/facebookresearch/deit.']",conclusion_chunked," In this paper, we have introduced DeiT, which are image transformers that
do not require very large amount of data to be trained, thanks to improved training and in particular a novel distillation procedure. Convolutional neural networks have optimized, both in terms of architecture and optimization
during almost a decade, including through extensive architecture search that
is prone to overfiting, as it is the case for instance for EfficientNets [51]. For
DeiT we have started the existing data augmentation and regularization strategies pre-existing for convnets, not introducing any significant architectural beyond our novel distillation token. Therefore it is likely that research on dataaugmentation more adapted or learned for transformers will bring further gains. Therefore, considering our results, where image transformers are on par
with convnets already, we believe that they will rapidly become a method of
choice considering their lower memory footprint for a given accuracy. We provide an open-source implementation of our method. It is available
at https://github.com/facebookresearch/deit.",17.968260869565228,30.655460127574653,336,0.5505285859107971," In this paper, we have introduced Propname, which are image transformers that 
 do not require very large amount of data to be trained, thanks to improved training and in particular a novel distillation procedure. Convolutional neural networks have optimized, both in terms of architecture and optimization 
 during almost a decade, including through extensive architecture search that 
 is prone to overfiting, as it is the case for instance for Propname. For 
 DeiT we have started the existing data augmentation and regularization strategies pre existing for convnets, not introducing any significant architectural beyond our novel distillation Propname. Therefore it is likely that research on dataaugmentation more adapted or learned for transformers will bring further gains. Therefore, considering our results, where image transformers are on par 
 with convnets already, we believe that they will rapidly become a method of 
 choice considering their lower memory footprint for a given accuracy. We provide an open source implementation of our method. It is available 
 at"," In this paper, we have introduced Propname, which are image transformers that 
 do not require very large amount of data to be trained, thanks to improved training and in particular a novel distillation procedure. Convolutional neural networks have optimized, both in terms of architecture and optimization 
 during almost a decade, including through extensive architecture search that 
 is prone to overfiting, as it is the case for instance for Propname. For 
 DeiT we have started the existing data augmentation and regularization strategies pre existing for convnets, not introducing any significant architectural beyond our novel distillation Propname. Therefore it is likely that research on dataaugmentation more adapted or learned for transformers will bring further gains. Therefore, considering our results, where image transformers are on par 
 with convnets already, we believe that they will rapidly become a method of 
 choice considering their lower memory footprint for a given accuracy. We provide an open source implementation of our method. It is available 
 at", ADP DET NOUN PUNCT PRON AUX VERB PROPN PUNCT PRON AUX NOUN NOUN PRON SPACE AUX PART VERB ADV ADJ NOUN ADP NOUN PART AUX VERB PUNCT NOUN ADP ADJ NOUN CCONJ ADP ADJ DET ADJ NOUN NOUN PUNCT ADJ ADJ NOUN AUX VERB PUNCT CCONJ ADP NOUN ADP NOUN CCONJ NOUN SPACE ADP ADV PRON NOUN PUNCT VERB ADP ADJ NOUN NOUN PRON SPACE AUX ADJ ADP NOUN PUNCT SCONJ PRON AUX DET NOUN ADP NOUN ADP PROPN PUNCT ADP SPACE NOUN PRON AUX VERB DET VERB NOUN NOUN CCONJ NOUN NOUN VERB VERB ADP NOUN PUNCT PART VERB DET ADJ NOUN ADP PRON ADJ NOUN PROPN PUNCT ADV PRON AUX ADJ SCONJ NOUN ADP NOUN ADV VERB CCONJ VERB ADP NOUN AUX VERB ADJ NOUN PUNCT ADV PUNCT VERB PRON NOUN PUNCT SCONJ NOUN NOUN AUX ADP NOUN SPACE ADP NOUN ADV PUNCT PRON VERB SCONJ PRON AUX ADV VERB DET NOUN ADP SPACE NOUN VERB PRON ADJ NOUN NOUN ADP DET VERB NOUN PUNCT PRON VERB DET ADJ NOUN NOUN ADP PRON NOUN PUNCT PRON AUX ADJ SPACE ADP,0.6514285714285715,25.0,5.188571428571429
337,49,Hugo Touvron,"[' In this paper we have shown that a simple residual architecture, whose residual blocks consist of a one-hidden layer feed-forward network and a linear patch interaction layer, achieves an unexpectedly high performance on ImageNet classification benchmarks, provided that we adopt a modern training strategy such as those recently introduced for transformer-based architectures. Thanks to their simple structure, with linear layers as the main mean of communication between patches, we can vizualize the filters learned by this simple MLP. While some of the layers are similar to convolutional filters, we also observe sparse long-range interactions as early as the second layer of the network. We hope that our model free of spatial priors will contribute to further understanding of what networks with less priors learn, and potentially guide the design choices of future networks without the pyramidal design prior adopted by most convolutional neural networks.']",conclusion_chunked," In this paper we have shown that a simple residual architecture, whose residual blocks consist of a one-hidden layer feed-forward network and a linear patch interaction layer, achieves an unexpectedly high performance on ImageNet classification benchmarks, provided that we adopt a modern training strategy such as those recently introduced for transformer-based architectures. Thanks to their simple structure, with linear layers as the main mean of communication between patches, we can vizualize the filters learned by this simple MLP. While some of the layers are similar to convolutional filters, we also observe sparse long-range interactions as early as the second layer of the network. We hope that our model free of spatial priors will contribute to further understanding of what networks with less priors learn, and potentially guide the design choices of future networks without the pyramidal design prior adopted by most convolutional neural networks.",16.95750000000004,30.655460127574653,337,0.8180510401725769," In this paper we have shown that a simple residual architecture, whose residual blocks consist of a one hidden layer feed forward network and a linear patch interaction layer, achieves an unexpectedly high performance on Propname classification benchmarks, provided that we adopt a modern training strategy such as those recently introduced for transformer based architectures. Thanks to their simple structure, with linear layers as the main mean of communication between patches, we can vizualize the filters learned by this simple MLP. While some of the layers are similar to convolutional filters, we also observe sparse long range interactions as early as the second layer of the network. We hope that our model free of spatial priors will contribute to further understanding of what networks with less priors learn, and potentially guide the design choices of future networks without the pyramidal design prior adopted by most convolutional neural networks."," In this paper we have shown that a simple residual architecture, whose residual blocks consist of a one hidden layer feed forward network and a linear patch interaction layer, achieves an unexpectedly high performance on Propname classification benchmarks, provided that we adopt a modern training strategy such as those recently introduced for transformer based architectures. Thanks to their simple structure, with linear layers as the main mean of communication between patches, we can vizualize the filters learned by this simple MLP. While some of the layers are similar to convolutional filters, we also observe sparse long range interactions as early as the second layer of the network. We hope that our model free of spatial priors will contribute to further understanding of what networks with less priors learn, and potentially guide the design choices of future networks without the pyramidal design prior adopted by most convolutional neural networks.", ADP DET NOUN PRON AUX VERB SCONJ DET ADJ ADJ NOUN PUNCT DET ADJ NOUN VERB ADP DET NUM VERB NOUN NOUN ADV NOUN CCONJ DET ADJ NOUN NOUN NOUN PUNCT VERB DET ADV ADJ NOUN ADP PROPN NOUN NOUN PUNCT VERB SCONJ PRON VERB DET ADJ NOUN NOUN ADJ ADP PRON ADV VERB ADP NOUN VERB NOUN PUNCT NOUN ADP PRON ADJ NOUN PUNCT ADP ADJ NOUN ADP DET ADJ NOUN ADP NOUN ADP NOUN PUNCT PRON AUX VERB DET NOUN VERB ADP DET ADJ NOUN PUNCT SCONJ PRON ADP DET NOUN AUX ADJ ADP ADJ NOUN PUNCT PRON ADV VERB ADJ ADJ NOUN NOUN ADV ADV ADP DET ADJ NOUN ADP DET NOUN PUNCT PRON VERB SCONJ PRON NOUN ADJ ADP ADJ NOUN AUX VERB ADP ADJ NOUN ADP PRON NOUN ADP ADJ NOUN VERB PUNCT CCONJ ADV VERB DET NOUN NOUN ADP ADJ NOUN ADP DET ADJ NOUN ADV VERB ADP ADJ ADJ ADJ NOUN PUNCT,0.6729559748427673,39.75,5.132075471698113
338,50,Hugo Touvron,"[' In this paper, we have shown how train deeper transformer-based image classification neural networks when training on Imagenet only. We have also introduced the simple yet effective CaiT architecture designed in the spirit of\nencoder/decoder architectures. Our work further demonstrates that transformer models offer a competitive alternative to the best convolutional neural\nnetworks when considering trade-offs between accuracy and complexity.']",conclusion_chunked," In this paper, we have shown how train deeper transformer-based image classification neural networks when training on Imagenet only. We have also introduced the simple yet effective CaiT architecture designed in the spirit of
encoder/decoder architectures. Our work further demonstrates that transformer models offer a competitive alternative to the best convolutional neural
networks when considering trade-offs between accuracy and complexity.",30.655460127574653,30.655460127574653,338,0.7562617659568787," In this paper, we have shown how train deeper transformer based image classification neural networks when training on Propname only. We have also introduced the simple yet effective Propname architecture designed in the spirit of 
 encoderdecoder architectures. Our work further demonstrates that transformer models offer a competitive alternative to the best convolutional neural 
 networks when considering trade offs between accuracy and complexity."," In this paper, we have shown how train deeper transformer based image classification neural networks when training on Propname only. We have also introduced the simple yet effective Propname architecture designed in the spirit of 
 encoderdecoder architectures. Our work further demonstrates that transformer models offer a competitive alternative to the best convolutional neural 
 networks when considering trade offs between accuracy and complexity.", ADP DET NOUN PUNCT PRON AUX VERB SCONJ NOUN ADJ NOUN VERB NOUN NOUN ADJ NOUN SCONJ NOUN ADP PROPN ADV PUNCT PRON AUX ADV VERB DET ADJ ADV ADJ PROPN NOUN VERB ADP DET NOUN ADP SPACE ADJ NOUN PUNCT PRON NOUN ADV VERB SCONJ NOUN NOUN VERB DET ADJ NOUN ADP DET ADJ ADJ ADJ SPACE NOUN SCONJ VERB NOUN NOUN ADP NOUN CCONJ NOUN PUNCT,0.8484848484848485,22.0,5.863636363636363
339,51,Hugo Touvron,"[' This paper has introduced a procedure to learn a\nneural network that offers a finer granularity than\nthe one provided by the annotation. It improves the\nperformance for fine-grained category retrieval within\na coarsely annotated collection. For on-the-fly kNN\nclassification, Grafit significantly reduces the gap with\na network trained with fine labels. It also translates\ninto better transfer learning to fine-grained datasets,\noutperforming the current state of the art with a more\nefficient network.']",conclusion_chunked," This paper has introduced a procedure to learn a
neural network that offers a finer granularity than
the one provided by the annotation. It improves the
performance for fine-grained category retrieval within
a coarsely annotated collection. For on-the-fly kNN
classification, Grafit significantly reduces the gap with
a network trained with fine labels. It also translates
into better transfer learning to fine-grained datasets,
outperforming the current state of the art with a more
efficient network.",30.655460127574653,30.655460127574653,339,0.781660258769989," This paper has introduced a procedure to learn a 
 neural network that offers a finer granularity than 
 the one provided by the annotation. It improves the 
 performance for fine grained category retrieval within 
 a coarsely annotated collection. For on the fly kNN 
 classification, Propname significantly reduces the gap with 
 a network trained with fine labels. It also translates 
 into better transfer learning to fine grained datasets, 
 outperforming the current state of the art with a more 
 efficient network."," This paper has introduced a procedure to learn a 
 neural network that offers a finer granularity than 
 the one provided by the annotation. It improves the 
 performance for fine grained category retrieval within 
 a coarsely annotated collection. For on the fly kNN 
 classification, Propname significantly reduces the gap with 
 a network trained with fine labels. It also translates 
 into better transfer learning to fine grained datasets, 
 outperforming the current state of the art with a more 
 efficient network.", DET NOUN AUX VERB DET NOUN PART VERB DET SPACE ADJ NOUN PRON VERB DET ADJ NOUN ADP SPACE DET NOUN VERB ADP DET NOUN PUNCT PRON VERB DET SPACE NOUN ADP ADJ VERB NOUN NOUN ADP SPACE DET ADV VERB NOUN PUNCT ADP ADP DET NOUN VERB SPACE NOUN PUNCT PROPN ADV VERB DET NOUN ADP SPACE DET NOUN VERB ADP ADJ NOUN PUNCT PRON ADV VERB SPACE ADP ADJ NOUN VERB PART VERB VERB NOUN PUNCT SPACE VERB DET ADJ NOUN ADP DET NOUN ADP DET ADV SPACE ADJ NOUN PUNCT,0.7142857142857143,21.0,5.083333333333333
340,52,Hugo Touvron,"[' Powers of layers consists in iterating a residual block to learns a complex transformation with no direct supervision. On various\ntasks, power of layers gives similar performance to CycleGAN with fewer parameters. The flexibility offered by the common\nembedding space can be used to modulate the strength of a transformation or to compose several transformations. While in most\nexamples the discriminator is only used for training, Powers of layers can also exploit it to adjust the transformation to the input\nimage at inference time.']",conclusion_chunked," Powers of layers consists in iterating a residual block to learns a complex transformation with no direct supervision. On various
tasks, power of layers gives similar performance to CycleGAN with fewer parameters. The flexibility offered by the common
embedding space can be used to modulate the strength of a transformation or to compose several transformations. While in most
examples the discriminator is only used for training, Powers of layers can also exploit it to adjust the transformation to the input
image at inference time.",30.655460127574653,30.655460127574653,340,0.41256043314933777," Powers of layers consists in iterating a residual block to learns a complex transformation with no direct supervision. On various 
 tasks, power of layers gives similar performance to CycleGAN with fewer parameters. The flexibility offered by the common 
 embedding space can be used to modulate the strength of a transformation or to compose several transformations. While in most 
 examples the discriminator is only used for training, Powers of layers can also exploit it to adjust the transformation to the input 
 image at inference time."," Powers of layers consists in iterating a residual block to learns a complex transformation with no direct supervision. On various 
 tasks, power of layers gives similar performance to CycleGAN with fewer parameters. The flexibility offered by the common 
 embedding space can be used to modulate the strength of a transformation or to compose several transformations. While in most 
 examples the discriminator is only used for training, Powers of layers can also exploit it to adjust the transformation to the input 
 image at inference time.", NOUN ADP NOUN VERB ADP VERB DET ADJ NOUN PART NOUN DET ADJ NOUN ADP DET ADJ NOUN PUNCT ADP ADJ SPACE NOUN PUNCT NOUN ADP NOUN VERB ADJ NOUN ADP NOUN ADP ADJ NOUN PUNCT DET NOUN VERB ADP DET ADJ SPACE VERB NOUN AUX AUX VERB PART VERB DET NOUN ADP DET NOUN CCONJ PART VERB ADJ NOUN PUNCT SCONJ ADP ADJ SPACE NOUN DET NOUN AUX ADV VERB ADP NOUN PUNCT NOUN ADP NOUN AUX ADV VERB PRON PART VERB DET NOUN ADP DET NOUN SPACE NOUN ADP NOUN NOUN PUNCT,0.7,22.5,5.022222222222222
341,53,Hugo Touvron,"[' We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network’s pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown\nthat researchers waste resources when both training and testing strong networks at resolution 224 × 224; We introduce\na method that can “fix” these networks post-facto and thus\nimprove their performance. An open-source implementation of our method is available at https://github.com/\nfacebookresearch/FixRes.']",conclusion_chunked," We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network’s pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown
that researchers waste resources when both training and testing strong networks at resolution 224 × 224; We introduce
a method that can “fix” these networks post-facto and thus
improve their performance. An open-source implementation of our method is available at https://github.com/
facebookresearch/FixRes.",24.723038990825728,30.655460127574653,341,0.5538077354431152," We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the networks pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown 
 that researchers waste resources when both training and testing strong networks at resolution 000 000; We introduce 
 a method that can fix these networks post facto and thus 
 improve their performance. An open source implementation of our method is available at facebookresearchFixRes."," We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the networks pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown 
 that researchers waste resources when both training and testing strong networks at resolution 000 000; We introduce 
 a method that can fix these networks post facto and thus 
 improve their performance. An open source implementation of our method is available at facebookresearchFixRes.", PRON AUX VERB ADV DET NOUN ADP VERB ADJ NOUN CCONJ NOUN NOUN NOUN ADP DET NOUN ADP ADJ NOUN CCONJ ADP DET NOUN VERB NOUN PUNCT PRON AUX VERB SCONJ PUNCT ADP VERB DET NOUN NOUN CCONJ ADP DET ADJ CCONJ ADJ NOUN NOUN NOUN PUNCT PRON AUX ADJ PART VERB DET NOUN ADP ADJ NOUN ADV PUNCT PRON AUX ADJ ADV PUNCT PRON AUX ADV VERB SPACE SCONJ NOUN VERB NOUN SCONJ PRON NOUN CCONJ VERB ADJ NOUN ADP NOUN NUM NUM PUNCT PRON VERB SPACE DET NOUN PRON AUX VERB DET NOUN VERB X CCONJ ADV SPACE VERB PRON NOUN PUNCT DET ADJ NOUN NOUN ADP PRON NOUN AUX ADJ ADP X PUNCT,0.6991150442477876,28.25,5.221238938053097
342,54,Hugo Touvron,"[' In this paper, we demonstrate that database effect cannot\nbe properly regressed out if the effect of another confound,\nwhose distribution varies across databases, is not properly\nmodeled. We propose a simple strategy that compensates for\nthe residual variation in position and shape that can appear\nbetween the distributions of data adjusted with the generalized linear model. The benefit of this strategy has been\nhighlighted in the context of a CAD system discriminating\nAD vs healthy subjects. However, the fact that confounds can\nstill be predicted from adjusted data suggests that there is still\nsome room for improvement in the adjustment procedure. The risk of processing data corrupted by several confounding variables is that the adjusted data may still permit the\nprediction of confounds with above chance accuracy. In the\ncontext of a CAD system, confounds that are correlated with\nthe diagnosis may be responsible for ambiguity. To assess the\nreliability of a CAD system, we suggest the following guidelines: (i) test if the confounds are correlated with the target,\n(ii) test if the adjusted data still allow a good prediction of the\nconfounds, (iii) test if the classifier can be misled with new\ntesting data that have not the same distributions of confounds\nthan those of the training set.']",conclusion_chunked," In this paper, we demonstrate that database effect cannot
be properly regressed out if the effect of another confound,
whose distribution varies across databases, is not properly
modeled. We propose a simple strategy that compensates for
the residual variation in position and shape that can appear
between the distributions of data adjusted with the generalized linear model. The benefit of this strategy has been
highlighted in the context of a CAD system discriminating
AD vs healthy subjects. However, the fact that confounds can
still be predicted from adjusted data suggests that there is still
some room for improvement in the adjustment procedure. The risk of processing data corrupted by several confounding variables is that the adjusted data may still permit the
prediction of confounds with above chance accuracy. In the
context of a CAD system, confounds that are correlated with
the diagnosis may be responsible for ambiguity. To assess the
reliability of a CAD system, we suggest the following guidelines: (i) test if the confounds are correlated with the target,
(ii) test if the adjusted data still allow a good prediction of the
confounds, (iii) test if the classifier can be misled with new
testing data that have not the same distributions of confounds
than those of the training set.",30.95357142857145,30.655460127574653,342,0.2559678554534912," In this paper, we demonstrate that database effect can not 
 be properly regressed out if the effect of another confound, 
 whose distribution varies across databases, is not properly 
 modeled. We propose a simple strategy that compensates for 
 the residual variation in position and shape that can appear 
 between the distributions of data adjusted with the generalized linear model. The benefit of this strategy has been 
 highlighted in the context of a Propname system discriminating 
 AD vs healthy subjects. However, the fact that confounds can 
 still be predicted from adjusted data suggests that there is still 
 some room for improvement in the adjustment procedure. The risk of processing data corrupted by several confounding variables is that the adjusted data may still permit the 
 prediction of confounds with above chance accuracy. In the 
 context of a Propname system, confounds that are correlated with 
 the diagnosis may be responsible for ambiguity. To assess the 
 reliability of a Propname system, we suggest the following guidelines: test if the confounds are correlated with the target, test if the adjusted data still allow a good prediction of the 
 confounds, test if the classifier can be misled with new 
 testing data that have not the same distributions of confounds 
 than those of the training set."," In this paper, we demonstrate that database effect can not 
 be properly regressed out if the effect of another confound, 
 whose distribution varies across databases, is not properly 
 modeled. We propose a simple strategy that compensates for 
 the residual variation in position and shape that can appear 
 between the distributions of data adjusted with the generalized linear model. The benefit of this strategy has been 
 highlighted in the context of a Propname system discriminating 
 AD vs healthy subjects. However, the fact that confounds can 
 still be predicted from adjusted data suggests that there is still 
 some room for improvement in the adjustment procedure. The risk of processing data corrupted by several confounding variables is that the adjusted data may still permit the 
 prediction of confounds with above chance accuracy. In the 
 context of a Propname system, confounds that are correlated with 
 the diagnosis may be responsible for ambiguity. To assess the 
 reliability of a Propname system, we suggest the following guidelines: test if the confounds are correlated with the target, test if the adjusted data still allow a good prediction of the 
 confounds, test if the classifier can be misled with new 
 testing data that have not the same distributions of confounds 
 than those of the training set.", ADP DET NOUN PUNCT PRON VERB SCONJ NOUN NOUN AUX PART SPACE AUX ADV VERB ADP SCONJ DET NOUN ADP DET NOUN PUNCT SPACE DET NOUN VERB ADP NOUN PUNCT AUX PART ADV SPACE VERB PUNCT PRON VERB DET ADJ NOUN PRON VERB ADP SPACE DET ADJ NOUN ADP NOUN CCONJ NOUN PRON AUX VERB SPACE ADP DET NOUN ADP NOUN VERB ADP DET ADJ ADJ NOUN PUNCT DET NOUN ADP DET NOUN AUX AUX SPACE VERB ADP DET NOUN ADP DET PROPN NOUN VERB SPACE NOUN ADV ADJ NOUN PUNCT ADV PUNCT DET NOUN SCONJ VERB AUX SPACE ADV AUX VERB ADP VERB NOUN VERB SCONJ PRON VERB ADV SPACE DET NOUN ADP NOUN ADP DET NOUN NOUN PUNCT DET NOUN ADP NOUN NOUN VERB ADP ADJ VERB NOUN AUX SCONJ DET VERB NOUN AUX ADV VERB DET SPACE NOUN ADP NOUN ADP ADP NOUN NOUN PUNCT ADP DET SPACE NOUN ADP DET PROPN NOUN PUNCT VERB PRON AUX VERB ADP SPACE DET NOUN AUX AUX ADJ ADP NOUN PUNCT PART VERB DET SPACE NOUN ADP DET PROPN NOUN PUNCT PRON VERB DET VERB NOUN PUNCT NOUN SCONJ DET NOUN AUX VERB ADP DET NOUN PUNCT VERB SCONJ DET VERB NOUN ADV VERB DET ADJ NOUN ADP DET SPACE NOUN PUNCT VERB SCONJ DET NOUN AUX AUX VERB ADP ADJ SPACE NOUN NOUN PRON VERB PART DET ADJ NOUN ADP NOUN SPACE ADP PRON ADP DET NOUN NOUN PUNCT,0.5133928571428571,32.0,4.901785714285714
343,55,Hugo Touvron,"[' We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network’s pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown that researchers waste resources when both training and testing strong networks at resolution 224 × 224; We introduce a method that can “fix” these networks post-facto and thus improve their performance. An open-source implementation of our method is available at.']",conclusion_chunked," We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the network’s pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light-weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown that researchers waste resources when both training and testing strong networks at resolution 224 × 224; We introduce a method that can “fix” these networks post-facto and thus improve their performance. An open-source implementation of our method is available at.",28.295990566037744,30.655460127574653,343,0.5302602648735046," We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the networks pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown that researchers waste resources when both training and testing strong networks at resolution 000 000; We introduce a method that can fix these networks post facto and thus improve their performance. An open source implementation of our method is available at."," We have studied extensively the effect of using different train and test scale augmentations on the statistics of natural images and of the networks pooling activations. We have shown that, by adjusting the crop resolution and via a simple and light weight parameter adaptation, it is possible to increase the accuracy of standard classifiers significantly, everything being equal otherwise. We have also shown that researchers waste resources when both training and testing strong networks at resolution 000 000; We introduce a method that can fix these networks post facto and thus improve their performance. An open source implementation of our method is available at.", PRON AUX VERB ADV DET NOUN ADP VERB ADJ NOUN CCONJ NOUN NOUN NOUN ADP DET NOUN ADP ADJ NOUN CCONJ ADP DET NOUN VERB NOUN PUNCT PRON AUX VERB SCONJ PUNCT ADP VERB DET NOUN NOUN CCONJ ADP DET ADJ CCONJ ADJ NOUN NOUN NOUN PUNCT PRON AUX ADJ PART VERB DET NOUN ADP ADJ NOUN ADV PUNCT PRON AUX ADJ ADV PUNCT PRON AUX ADV VERB SCONJ NOUN VERB NOUN SCONJ PRON NOUN CCONJ VERB ADJ NOUN ADP NOUN NUM NUM PUNCT PRON VERB DET NOUN PRON AUX VERB DET NOUN VERB X CCONJ ADV VERB PRON NOUN PUNCT DET ADJ NOUN NOUN ADP PRON NOUN AUX ADJ ADP PUNCT,0.6964285714285714,28.0,5.071428571428571
344,56,Zhiqing Sun,"[' In this paper, we introduce SALMON, a new AI alignment paradigm where a principle-following reward model is trained to effectively and flexibly align language models with human values and intentions. During the RL training stage, by merely adjusting the principles that the reward model follows, we can gain full control over the preferences of the reward model, and subsequently influence the behavior of the RL-trained policy model. This eliminates the traditional reliance on the exhaustive collection of online human preferences. Combined with the SELF-ALIGN technique (Sun et al., 2023b), we build a powerful AI-assistant agent, Dromedary-2, with only six exemplars for in-context learning and 31 human-defined principles. Our self-aligned AI agent significantly surpasses the performance of several state-of-the-art RLHF-trained AI systems in chatbot, reasoning, coding, multilingualism, and truthfulness benchmarks.']",conclusion_chunked," In this paper, we introduce SALMON, a new AI alignment paradigm where a principle-following reward model is trained to effectively and flexibly align language models with human values and intentions. During the RL training stage, by merely adjusting the principles that the reward model follows, we can gain full control over the preferences of the reward model, and subsequently influence the behavior of the RL-trained policy model. This eliminates the traditional reliance on the exhaustive collection of online human preferences. Combined with the SELF-ALIGN technique (Sun et al., 2023b), we build a powerful AI-assistant agent, Dromedary-2, with only six exemplars for in-context learning and 31 human-defined principles. Our self-aligned AI agent significantly surpasses the performance of several state-of-the-art RLHF-trained AI systems in chatbot, reasoning, coding, multilingualism, and truthfulness benchmarks.",13.197307692307731,22.733194790065724,344,0.5388079881668091," In this paper, we introduce Propname, a new Propname alignment paradigm where a principle following reward model is trained to effectively and flexibly align language models with human values and intentions. During the Propname training stage, by merely adjusting the principles that the reward model follows, we can gain full control over the preferences of the reward model, and subsequently influence the behavior of the Propname trained policy model. This eliminates the traditional reliance on the exhaustive collection of online human preferences. Combined with the Propname Propname technique, we build a powerful Propname assistant agent, Propname 0, with only six exemplars for in context learning and 00 human defined principles. Our self aligned Propname agent significantly surpasses the performance of several state of the art Propname trained Propname systems in chatbot, reasoning, coding, multilingualism, and truthfulness benchmarks."," In this paper, we introduce Propname, a new Propname alignment paradigm where a principle following reward model is trained to effectively and flexibly align language models with human values and intentions. During the Propname training stage, by merely adjusting the principles that the reward model follows, we can gain full control over the preferences of the reward model, and subsequently influence the behavior of the Propname trained policy model. This eliminates the traditional reliance on the exhaustive collection of online human preferences. Combined with the Propname Propname technique, we build a powerful Propname assistant agent, Propname 0, with only six exemplars for in context learning and 00 human defined principles. Our self aligned Propname agent significantly surpasses the performance of several state of the art Propname trained Propname systems in chatbot, reasoning, coding, multilingualism, and truthfulness benchmarks.", ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET ADJ PROPN NOUN NOUN SCONJ DET NOUN VERB NOUN NOUN AUX VERB PART ADV CCONJ ADV ADJ NOUN NOUN ADP ADJ NOUN CCONJ NOUN PUNCT ADP DET PROPN NOUN NOUN PUNCT ADP ADV VERB DET NOUN PRON DET NOUN NOUN VERB PUNCT PRON AUX VERB ADJ NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT CCONJ ADV VERB DET NOUN ADP DET PROPN VERB NOUN NOUN PUNCT PRON VERB DET ADJ NOUN ADP DET ADJ NOUN ADP ADJ ADJ NOUN PUNCT VERB ADP DET PROPN PROPN NOUN PUNCT PRON VERB DET ADJ PROPN ADJ NOUN PUNCT PROPN NUM PUNCT ADP ADV NUM NOUN ADP ADP NOUN NOUN CCONJ NUM ADJ VERB NOUN PUNCT PRON NOUN VERB PROPN NOUN ADV VERB DET NOUN ADP ADJ NOUN ADP DET NOUN PROPN VERB PROPN NOUN ADP NOUN PUNCT NOUN PUNCT VERB PUNCT NOUN PUNCT CCONJ ADJ NOUN PUNCT,0.5909090909090909,30.8,5.285714285714286
345,57,Zhiqing Sun,"[' We proposed several strategies to tackle the multimodal misalignment problems, particularly for vision language models (VLMs), which often produce text inconsistent with the associated images. First, we enrich GPT-4 generated vision instruction tuning data from LLaVA with existing human-authored image-text pairs. Next, we adopt the Reinforcement Learning from Human Feedback (RLHF) algorithm from the text domain to bridge vision-language gaps, wherein human evaluators discern and mark the more hallucinated output. We train the VLM to optimize against simulated human preferences. Moreover, we introduce the Factually Augmented RLHF, leveraging additional factual information such as image captions to enhance the reward model, countering reward hacking in RLHF, and boosting model performance. For tangible real-world impact assessment, we have devised MMHAL-BENCH, an evaluation benchmark targeting the penalization of hallucination. Remarkably, LLaVA-RLHF, being the first VLM trained with RLHF, shows a notable surge in performance across benchmarks. We opensource our code, and data and hope our findings could help the future development of more reliable and human-aligned LLMs and LMMs.']",conclusion_chunked," We proposed several strategies to tackle the multimodal misalignment problems, particularly for vision language models (VLMs), which often produce text inconsistent with the associated images. First, we enrich GPT-4 generated vision instruction tuning data from LLaVA with existing human-authored image-text pairs. Next, we adopt the Reinforcement Learning from Human Feedback (RLHF) algorithm from the text domain to bridge vision-language gaps, wherein human evaluators discern and mark the more hallucinated output. We train the VLM to optimize against simulated human preferences. Moreover, we introduce the Factually Augmented RLHF, leveraging additional factual information such as image captions to enhance the reward model, countering reward hacking in RLHF, and boosting model performance. For tangible real-world impact assessment, we have devised MMHAL-BENCH, an evaluation benchmark targeting the penalization of hallucination. Remarkably, LLaVA-RLHF, being the first VLM trained with RLHF, shows a notable surge in performance across benchmarks. We opensource our code, and data and hope our findings could help the future development of more reliable and human-aligned LLMs and LMMs.",23.03250374251502,22.733194790065724,345,0.5273228287696838," We proposed several strategies to tackle the multimodal misalignment problems, particularly for vision language models, which often produce text inconsistent with the associated images. First, we enrich Propname 0 generated vision instruction tuning data from Propname with existing human authored image text pairs. Next, we adopt the Propname Propname from Propname Propname Propname from the text domain to bridge vision language gaps, wherein human evaluators discern and mark the more hallucinated output. We train the Propname to optimize against simulated human preferences. Moreover, we introduce the Propname Propname Propname, leveraging additional factual information such as image captions to enhance the reward model, countering reward hacking in Propname, and boosting model performance. For tangible real world impact assessment, we have devised Propname Propname, an evaluation benchmark targeting the penalization of hallucination. Remarkably, Propname Propname, being the first Propname trained with Propname, shows a notable surge in performance across benchmarks. We opensource our code, and data and hope our findings could help the future development of more reliable and human aligned LLMs and Propname."," We proposed several strategies to tackle the multimodal misalignment problems, particularly for vision language models, which often produce text inconsistent with the associated images. First, we enrich Propname 0 generated vision instruction tuning data from Propname with existing human authored image text pairs. Next, we adopt the Propname Propname from Propname Propname Propname from the text domain to bridge vision language gaps, wherein human evaluators discern and mark the more hallucinated output. We train the Propname to optimize against simulated human preferences. Moreover, we introduce the Propname Propname Propname, leveraging additional factual information such as image captions to enhance the reward model, countering reward hacking in Propname, and boosting model performance. For tangible real world impact assessment, we have devised Propname Propname, an evaluation benchmark targeting the penalization of hallucination. Remarkably, Propname Propname, being the first Propname trained with Propname, shows a notable surge in performance across benchmarks. We opensource our code, and data and hope our findings could help the future development of more reliable and human aligned LLMs and Propname.", PRON VERB ADJ NOUN PART VERB DET ADJ NOUN NOUN PUNCT ADV ADP NOUN NOUN NOUN PUNCT PRON ADV VERB NOUN NOUN ADP DET VERB NOUN PUNCT ADV PUNCT PRON VERB PROPN NUM VERB NOUN NOUN VERB NOUN ADP PROPN ADP VERB ADJ VERB NOUN NOUN NOUN PUNCT ADV PUNCT PRON VERB DET PROPN PROPN ADP PROPN PROPN PROPN ADP DET NOUN NOUN ADP VERB NOUN NOUN NOUN PUNCT SCONJ ADJ NOUN VERB CCONJ VERB DET ADV ADJ NOUN PUNCT PRON VERB DET PROPN PART VERB ADP ADJ ADJ NOUN PUNCT ADV PUNCT PRON VERB DET PROPN PROPN PROPN PUNCT VERB ADJ ADJ NOUN ADJ ADP NOUN NOUN PART VERB DET NOUN NOUN PUNCT VERB NOUN VERB ADP PROPN PUNCT CCONJ VERB NOUN NOUN PUNCT ADP ADJ ADJ NOUN NOUN NOUN PUNCT PRON AUX VERB PROPN PROPN PUNCT DET NOUN NOUN VERB DET NOUN ADP NOUN PUNCT ADV PUNCT PROPN PROPN PUNCT AUX DET ADJ PROPN VERB ADP PROPN PUNCT VERB DET ADJ NOUN ADP NOUN ADP NOUN PUNCT PRON VERB PRON NOUN PUNCT CCONJ NOUN CCONJ VERB PRON NOUN AUX VERB DET ADJ NOUN ADP ADV ADJ CCONJ ADJ VERB NOUN CCONJ PROPN PUNCT,0.5743589743589743,24.375,5.4051282051282055
346,58,Zhiqing Sun,"[' Models like Alpaca and Vicuna have shown that powerful conversational capabilities can be distilled from existing human-preference-aligned large language models (LLMs), into smaller models. In this paper, we introduce Dromedary , a model for the research community based on principle-driven self-alignment, trained from scratch and requiring very little human annotation. By harnessing the intrinsic knowledge within an LLM, we can define principles that guide how we want an LLM-based AI model to behave, resulting in an AI assistant that not only produces quality interactions but also produces responses that respect the guardrails defined by the model creator. This method represents a distinct direction from RLHF, and it focuses on developing novel alignment techniques for language models from scratch, independent of pre-existing, well-established AI systems. In other words, our approach seeks to explore the potential of aligning AI models in situations where reliance on or access to existing systems may not be feasible or desired. For future work, we propose the following research directions: • Conduct ablation studies on the Dromedary’s 16 self-alignment principles to evaluate the impact of adding or removing specific principles. • Apply Constitutional AI-based self-critique techniques [4] to enhance the performance of Dromedary further. • Perform human evaluations to assess the real-world applicability and effectiveness of SELF-ALIGN. • Investigate better utilization of existing open-source annotation data, such as the 15k original instruction-following data in [10].']",conclusion_chunked," Models like Alpaca and Vicuna have shown that powerful conversational capabilities can be distilled from existing human-preference-aligned large language models (LLMs), into smaller models. In this paper, we introduce Dromedary , a model for the research community based on principle-driven self-alignment, trained from scratch and requiring very little human annotation. By harnessing the intrinsic knowledge within an LLM, we can define principles that guide how we want an LLM-based AI model to behave, resulting in an AI assistant that not only produces quality interactions but also produces responses that respect the guardrails defined by the model creator. This method represents a distinct direction from RLHF, and it focuses on developing novel alignment techniques for language models from scratch, independent of pre-existing, well-established AI systems. In other words, our approach seeks to explore the potential of aligning AI models in situations where reliance on or access to existing systems may not be feasible or desired. For future work, we propose the following research directions: • Conduct ablation studies on the Dromedary’s 16 self-alignment principles to evaluate the impact of adding or removing specific principles. • Apply Constitutional AI-based self-critique techniques [4] to enhance the performance of Dromedary further. • Perform human evaluations to assess the real-world applicability and effectiveness of SELF-ALIGN. • Investigate better utilization of existing open-source annotation data, such as the 15k original instruction-following data in [10].",13.173228421554626,22.733194790065724,346,0.5870804190635681," Models like Propname and Propname have shown that powerful conversational capabilities can be distilled from existing human preference aligned large language models, into smaller models. In this paper, we introduce Propname, a model for the research community based on principle driven self alignment, trained from scratch and requiring very little human annotation. By harnessing the intrinsic knowledge within an Propname, we can define principles that guide how we want an Propname based Propname model to behave, resulting in an Propname assistant that not only produces quality interactions but also produces responses that respect the guardrails defined by the model creator. This method represents a distinct direction from Propname, and it focuses on developing novel alignment techniques for language models from Propname, independent of pre existing, well established Propname systems. In other words, our approach seeks to explore the potential of aligning Propname models in situations where reliance on or access to existing systems may not be feasible or desired. For future work, we propose the following research directions: Conduct ablation studies on the Propname 00 self alignment principles to evaluate the impact of adding or removing specific principles. Apply Propname Propname based self critique techniques to enhance the performance of Propname further. Perform human evaluations to assess the real world applicability and effectiveness of Propname Propname. Investigate better utilization of existing open source annotation data, such as the 00k original instruction following data in."," Models like Propname and Propname have shown that powerful conversational capabilities can be distilled from existing human preference aligned large language models, into smaller models. In this paper, we introduce Propname, a model for the research community based on principle driven self alignment, trained from scratch and requiring very little human annotation. By harnessing the intrinsic knowledge within an Propname, we can define principles that guide how we want an Propname based Propname model to behave, resulting in an Propname assistant that not only produces quality interactions but also produces responses that respect the guardrails defined by the model creator. This method represents a distinct direction from Propname, and it focuses on developing novel alignment techniques for language models from Propname, independent of pre existing, well established Propname systems. In other words, our approach seeks to explore the potential of aligning Propname models in situations where reliance on or access to existing systems may not be feasible or desired. For future work, we propose the following research directions: Conduct ablation studies on the Propname 00 self alignment principles to evaluate the impact of adding or removing specific principles. Apply Propname Propname based self critique techniques to enhance the performance of Propname further. Perform human evaluations to assess the real world applicability and effectiveness of Propname Propname. Investigate better utilization of existing open source annotation data, such as the 00k original instruction following data in.", NOUN ADP PROPN CCONJ PROPN AUX VERB SCONJ ADJ ADJ NOUN AUX AUX VERB ADP VERB ADJ NOUN VERB ADJ NOUN NOUN PUNCT ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PROPN PUNCT DET NOUN ADP DET NOUN NOUN VERB ADP ADJ VERB NOUN NOUN PUNCT VERB ADP NOUN CCONJ VERB ADV ADJ ADJ NOUN PUNCT ADP VERB DET ADJ NOUN ADP DET PROPN PUNCT PRON AUX VERB NOUN PRON VERB SCONJ PRON VERB DET PROPN VERB PROPN NOUN PART VERB PUNCT VERB ADP DET PROPN NOUN SCONJ PART ADV VERB NOUN NOUN CCONJ ADV VERB NOUN PRON VERB DET NOUN VERB ADP DET NOUN NOUN PUNCT DET NOUN VERB DET ADJ NOUN ADP PROPN PUNCT CCONJ PRON VERB ADP VERB ADJ NOUN NOUN ADP NOUN NOUN ADP PROPN PUNCT ADJ ADP ADJ ADJ PUNCT ADV VERB PROPN NOUN PUNCT ADP ADJ NOUN PUNCT PRON NOUN VERB PART VERB DET NOUN ADP VERB PROPN NOUN ADP NOUN SCONJ NOUN ADP CCONJ NOUN ADP VERB NOUN AUX PART AUX ADJ CCONJ VERB PUNCT ADP ADJ NOUN PUNCT PRON VERB DET VERB NOUN NOUN PUNCT NOUN NOUN NOUN ADP DET PROPN NUM NOUN NOUN NOUN PART VERB DET NOUN ADP VERB CCONJ VERB ADJ NOUN PUNCT VERB PROPN PROPN VERB NOUN NOUN NOUN PART VERB DET NOUN ADP PROPN ADV PUNCT VERB ADJ NOUN PART VERB DET ADJ NOUN NOUN CCONJ NOUN ADP PROPN PROPN PUNCT VERB ADJ NOUN ADP VERB ADJ NOUN NOUN NOUN PUNCT ADJ ADP DET NOUN ADJ NOUN VERB NOUN ADP PUNCT,0.578125,28.444444444444443,5.38671875
347,59,Zhiqing Sun,"[' We proposed DIFUSCO, a novel graph-based diffusion model for solving NP-complete combinatorial optimization problems. We compared two variants of graph-based diffusion models: one with continuous Gaussian noise and one with discrete Bernoulli noise. We found that the discrete variant performs better than the continuous one. Moreover, we designed a cosine inference schedule that enhances the effectiveness of our model. DIFUSCO achieves state-of-the-art results on TSP and MIS problems, surpassing previous probabilistic NPC solvers in both accuracy and scalability. For future work, we would like to explore the potential of DIFUSCO in solving a broader range of NPC problems, including Mixed Integer Programming (Appendix C). We would also like to explore the use of equivariant graph neural networks (Xu et al., 2021; Hoogeboom et al., 2022) for further improvement of the diffusion models on geometrical NP-complete combinatorial optimization problems such as Euclidean TSP. Finally, we are interested in utilizing (higher-order) accelerated inference techniques for diffusion model based solvers, such as those inspired by the continuous time framework for discrete diffusion (Campbell et al., 2022; Sunet al., 2022).']",conclusion_chunked," We proposed DIFUSCO, a novel graph-based diffusion model for solving NP-complete combinatorial optimization problems. We compared two variants of graph-based diffusion models: one with continuous Gaussian noise and one with discrete Bernoulli noise. We found that the discrete variant performs better than the continuous one. Moreover, we designed a cosine inference schedule that enhances the effectiveness of our model. DIFUSCO achieves state-of-the-art results on TSP and MIS problems, surpassing previous probabilistic NPC solvers in both accuracy and scalability. For future work, we would like to explore the potential of DIFUSCO in solving a broader range of NPC problems, including Mixed Integer Programming (Appendix C). We would also like to explore the use of equivariant graph neural networks (Xu et al., 2021; Hoogeboom et al., 2022) for further improvement of the diffusion models on geometrical NP-complete combinatorial optimization problems such as Euclidean TSP. Finally, we are interested in utilizing (higher-order) accelerated inference techniques for diffusion model based solvers, such as those inspired by the continuous time framework for discrete diffusion (Campbell et al., 2022; Sunet al., 2022).",21.07318181818181,22.733194790065724,347,0.7075648307800293," We proposed DIFUSCO, a novel graph based diffusion model for solving Propname complete combinatorial optimization problems. We compared two variants of graph based diffusion models: one with continuous Propname noise and one with discrete Propname noise. We found that the discrete variant performs better than the continuous one. Moreover, we designed a cosine inference schedule that enhances the effectiveness of our model. DIFUSCO achieves state of the art results on Propname and MIS problems, surpassing previous probabilistic Propname solvers in both accuracy and scalability. For future work, we would like to explore the potential of DIFUSCO in solving a broader range of Propname problems, including Propname Propname Propname. We would also like to explore the use of equivariant graph neural networks for further improvement of the diffusion models on geometrical Propname complete combinatorial optimization problems such as Propname Propname. Finally, we are interested in utilizing accelerated inference techniques for diffusion model based solvers, such as those inspired by the continuous time framework for discrete diffusion."," We proposed DIFUSCO, a novel graph based diffusion model for solving Propname complete combinatorial optimization problems. We compared two variants of graph based diffusion models: one with continuous Propname noise and one with discrete Propname noise. We found that the discrete variant performs better than the continuous one. Moreover, we designed a cosine inference schedule that enhances the effectiveness of our model. DIFUSCO achieves state of the art results on Propname and MIS problems, surpassing previous probabilistic Propname solvers in both accuracy and scalability. For future work, we would like to explore the potential of DIFUSCO in solving a broader range of Propname problems, including Propname Propname Propname. We would also like to explore the use of equivariant graph neural networks for further improvement of the diffusion models on geometrical Propname complete combinatorial optimization problems such as Propname Propname. Finally, we are interested in utilizing accelerated inference techniques for diffusion model based solvers, such as those inspired by the continuous time framework for discrete diffusion.", PRON VERB NOUN PUNCT DET ADJ NOUN VERB NOUN NOUN ADP VERB PROPN ADJ ADJ NOUN NOUN PUNCT PRON VERB NUM NOUN ADP NOUN VERB NOUN NOUN PUNCT NUM ADP ADJ PROPN NOUN CCONJ NUM ADP ADJ PROPN NOUN PUNCT PRON VERB SCONJ DET ADJ NOUN VERB ADV ADP DET ADJ NUM PUNCT ADV PUNCT PRON VERB DET NOUN NOUN NOUN PRON VERB DET NOUN ADP PRON NOUN PUNCT NOUN VERB NOUN ADP DET NOUN NOUN ADP PROPN CCONJ NOUN NOUN PUNCT VERB ADJ ADJ PROPN NOUN ADP DET NOUN CCONJ NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX VERB PART VERB DET NOUN ADP NOUN ADP VERB DET ADJ NOUN ADP PROPN NOUN PUNCT VERB PROPN PROPN PROPN PUNCT PRON AUX ADV VERB PART VERB DET NOUN ADP ADJ NOUN ADJ NOUN ADP ADJ NOUN ADP DET NOUN NOUN ADP ADJ PROPN ADJ ADJ NOUN NOUN ADJ ADP PROPN PROPN PUNCT ADV PUNCT PRON AUX ADJ ADP VERB ADJ NOUN NOUN ADP NOUN NOUN VERB NOUN PUNCT ADJ ADP PRON VERB ADP DET ADJ NOUN NOUN ADP ADJ NOUN PUNCT,0.5082872928176796,22.625,5.414364640883978
348,60,Zhiqing Sun,"[' In this paper, we propose a novel Temporal Stencil Modeling (TSM) method for solving time-dependent PDEs in conservation form. TSM can be regarded as the temporal generalization of classic finite volume solvers such as WENO (Shu, 2003; Gottlieb et al., 2006) and vanilla neural stencil modeling methods (Kochkov et al., 2021), in that TSM leverages the temporal information from trajectories, instead of only using the latest states, to approximate the (convective) flux more accurately. Our empirical evaluation on 2-D incompressible Navier-Stokes turbulent flow data shows that both the temporal information and its temporal feature encoding scheme are crucial to achieving state-ofthe-art simulation accuracy. We also show that TSM has a strong generalization ability on various out-of-distribution turbulent flows. Finally, we show that the proposed method works well on 1-D Kuramoto–Sivashinsky (KS) equation and 3-D Navier-Stokes. For future work, we plan to evaluate our TSM method with non-periodic boundary conditions. We are also interested in leveraging the Neural Architecture Search (NAS) technique to automatically find better features and neural architectures for solving the Navier-Stokes equation in the TSM framework.']",conclusion_chunked," In this paper, we propose a novel Temporal Stencil Modeling (TSM) method for solving time-dependent PDEs in conservation form. TSM can be regarded as the temporal generalization of classic finite volume solvers such as WENO (Shu, 2003; Gottlieb et al., 2006) and vanilla neural stencil modeling methods (Kochkov et al., 2021), in that TSM leverages the temporal information from trajectories, instead of only using the latest states, to approximate the (convective) flux more accurately. Our empirical evaluation on 2-D incompressible Navier-Stokes turbulent flow data shows that both the temporal information and its temporal feature encoding scheme are crucial to achieving state-ofthe-art simulation accuracy. We also show that TSM has a strong generalization ability on various out-of-distribution turbulent flows. Finally, we show that the proposed method works well on 1-D Kuramoto–Sivashinsky (KS) equation and 3-D Navier-Stokes. For future work, we plan to evaluate our TSM method with non-periodic boundary conditions. We are also interested in leveraging the Neural Architecture Search (NAS) technique to automatically find better features and neural architectures for solving the Navier-Stokes equation in the TSM framework.",16.185245901639377,22.733194790065724,348,0.7095984816551208," In this paper, we propose a novel Propname Propname Propname method for solving time dependent PDEs in conservation form. Propname can be regarded as the temporal generalization of classic finite volume solvers such as Propname and vanilla neural stencil modeling methods, in that Propname leverages the temporal information from trajectories, instead of only using the latest states, to approximate the flux more accurately. Our empirical evaluation on 0 D incompressible Propname Propname turbulent flow data shows that both the temporal information and its temporal feature encoding scheme are crucial to achieving state ofthe art simulation accuracy. We also show that Propname has a strong generalization ability on various out of distribution turbulent flows. Finally, we show that the proposed method works well on 0 Propname Propname equation and 0 Propname Navier Propname. For future work, we plan to evaluate our Propname method with non periodic boundary conditions. We are also interested in leveraging the Propname Propname Propname technique to automatically find better features and neural architectures for solving the Propname Propname equation in the Propname framework."," In this paper, we propose a novel Propname Propname Propname method for solving time dependent PDEs in conservation form. Propname can be regarded as the temporal generalization of classic finite volume solvers such as Propname and vanilla neural stencil modeling methods, in that Propname leverages the temporal information from trajectories, instead of only using the latest states, to approximate the flux more accurately. Our empirical evaluation on 0 D incompressible Propname Propname turbulent flow data shows that both the temporal information and its temporal feature encoding scheme are crucial to achieving state ofthe art simulation accuracy. We also show that Propname has a strong generalization ability on various out of distribution turbulent flows. Finally, we show that the proposed method works well on 0 Propname Propname equation and 0 Propname Navier Propname. For future work, we plan to evaluate our Propname method with non periodic boundary conditions. We are also interested in leveraging the Propname Propname Propname technique to automatically find better features and neural architectures for solving the Propname Propname equation in the Propname framework.", ADP DET NOUN PUNCT PRON VERB DET ADJ PROPN PROPN PROPN NOUN ADP VERB NOUN ADJ NOUN ADP NOUN NOUN PUNCT PROPN AUX AUX VERB ADP DET ADJ NOUN ADP ADJ ADJ NOUN NOUN ADJ ADP PROPN CCONJ NOUN ADJ NOUN NOUN NOUN PUNCT SCONJ SCONJ PROPN VERB DET ADJ NOUN ADP NOUN PUNCT ADV ADP ADV VERB DET ADJ NOUN PUNCT PART VERB DET NOUN ADV ADV PUNCT PRON ADJ NOUN ADP NUM ADJ ADJ PROPN PROPN ADJ NOUN NOUN VERB SCONJ CCONJ DET ADJ NOUN CCONJ PRON ADJ NOUN VERB NOUN AUX ADJ ADP VERB NOUN NOUN NOUN NOUN NOUN PUNCT PRON ADV VERB SCONJ PROPN VERB DET ADJ NOUN NOUN ADP ADJ ADP ADP NOUN ADJ NOUN PUNCT ADV PUNCT PRON VERB SCONJ DET VERB NOUN VERB ADV ADP NUM PROPN PROPN NOUN CCONJ NUM PROPN NOUN PROPN PUNCT ADP ADJ NOUN PUNCT PRON VERB PART VERB PRON PROPN NOUN ADP ADJ ADJ ADJ NOUN PUNCT PRON AUX ADV ADJ ADP VERB DET PROPN PROPN PROPN NOUN PART ADV VERB ADJ NOUN CCONJ ADJ NOUN ADP VERB DET PROPN PROPN NOUN ADP DET PROPN NOUN PUNCT,0.5978835978835979,27.0,5.365079365079365
349,61,Zhiqing Sun,"[' n this paper, we propose a novel recitation-augmented generation framework to improve language models’ performance in the closed-book question-answering setting. We hypothesize that for knowledge-intensive NLP tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. In addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. We show promising results over three large language models and across three different closed-book QA datasets, demonstrating the effectiveness of our proposed recite-and-answer approach. One limitation of our method is that updating time-sensitive knowledge for a pure LLM-based method requires training or fine-tuning the LLMs on the new corpus, which can be costly. For future work, we plan to further validate the effectiveness of recitation-augmented generation for other knowledge intensive NLP tasks in the closed-book setting, such as fact checking.']",conclusion_chunked," n this paper, we propose a novel recitation-augmented generation framework to improve language models’ performance in the closed-book question-answering setting. We hypothesize that for knowledge-intensive NLP tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. In addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. We show promising results over three large language models and across three different closed-book QA datasets, demonstrating the effectiveness of our proposed recite-and-answer approach. One limitation of our method is that updating time-sensitive knowledge for a pure LLM-based method requires training or fine-tuning the LLMs on the new corpus, which can be costly. For future work, we plan to further validate the effectiveness of recitation-augmented generation for other knowledge intensive NLP tasks in the closed-book setting, such as fact checking.",17.96384408602154,22.733194790065724,349,0.4790259599685669," n this paper, we propose a novel recitation augmented generation framework to improve language models performance in the closed book question answering setting. We hypothesize that for knowledge intensive Propname tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. In addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. We show promising results over three large language models and across three different closed book QA datasets, demonstrating the effectiveness of our proposed recite and answer approach. One limitation of our method is that updating time sensitive knowledge for a pure Propname based method requires training or fine tuning the LLMs on the new Propname, which can be costly. For future work, we plan to further validate the effectiveness of recitation augmented generation for other knowledge intensive Propname tasks in the closed book setting, such as fact checking."," n this paper, we propose a novel recitation augmented generation framework to improve language models performance in the closed book question answering setting. We hypothesize that for knowledge intensive Propname tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. In addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. We show promising results over three large language models and across three different closed book QA datasets, demonstrating the effectiveness of our proposed recite and answer approach. One limitation of our method is that updating time sensitive knowledge for a pure Propname based method requires training or fine tuning the LLMs on the new Propname, which can be costly. For future work, we plan to further validate the effectiveness of recitation augmented generation for other knowledge intensive Propname tasks in the closed book setting, such as fact checking.", CCONJ DET NOUN PUNCT PRON VERB DET ADJ NOUN VERB NOUN NOUN PART VERB NOUN NOUN NOUN ADP DET ADJ NOUN NOUN VERB VERB PUNCT PRON VERB SCONJ ADP NOUN ADJ PROPN NOUN PUNCT VERB DET NOUN PART ADV VERB DET ADJ NOUN NOUN AUX AUX ADJ ADP VERB PRON NOUN PUNCT ADP NOUN PUNCT PRON VERB SCONJ VERB DET NOUN NOUN AUX AUX ADJ ADV ADV SCONJ ADV PRON VERB ADJ NOUN NOUN PRON AUX AUX VERB PART VERB DET ADJ NOUN PUNCT PRON VERB ADJ NOUN ADP NUM ADJ NOUN NOUN CCONJ ADP NUM ADJ ADJ NOUN NOUN NOUN PUNCT VERB DET NOUN ADP PRON VERB NOUN CCONJ NOUN NOUN PUNCT NUM NOUN ADP PRON NOUN AUX SCONJ VERB NOUN ADJ NOUN ADP DET ADJ PROPN VERB NOUN VERB NOUN CCONJ ADJ VERB DET NOUN ADP DET ADJ PROPN PUNCT PRON AUX AUX ADJ PUNCT ADP ADJ NOUN PUNCT PRON VERB PART ADV VERB DET NOUN ADP NOUN VERB NOUN ADP ADJ NOUN ADJ PROPN NOUN ADP DET ADJ NOUN NOUN PUNCT ADJ ADP NOUN VERB PUNCT,0.6145251396648045,29.833333333333332,5.189944134078212
350,62,Zhiqing Sun,"[' In this paper, we address the limitations of ANN-based sparse attention methods and propose the Learning-to-Hash Attention (LHA) as our new solution. Specifically, LHA leverages separate learnable hash functions for queries and keys, respectively, and utilizes kernelized techniques for efficient approximation of attention utilities. The experiments on language modeling, natural language understanding, text classification, and image classification demonstrated the effectiveness of LHA. For future work, we would like to validate the effectiveness of LHA model on much larger language modeling datasets, such as PG-19 (Rae et al., 2019).']",conclusion_chunked," In this paper, we address the limitations of ANN-based sparse attention methods and propose the Learning-to-Hash Attention (LHA) as our new solution. Specifically, LHA leverages separate learnable hash functions for queries and keys, respectively, and utilizes kernelized techniques for efficient approximation of attention utilities. The experiments on language modeling, natural language understanding, text classification, and image classification demonstrated the effectiveness of LHA. For future work, we would like to validate the effectiveness of LHA model on much larger language modeling datasets, such as PG-19 (Rae et al., 2019).",22.733194790065724,22.733194790065724,350,0.674802303314209," In this paper, we address the limitations of Propname based sparse attention methods and propose the Propname to Propname Propname as our new solution. Specifically, Propname leverages separate learnable hash functions for queries and keys, respectively, and utilizes kernelized techniques for efficient approximation of attention utilities. The experiments on language modeling, natural language understanding, text classification, and image classification demonstrated the effectiveness of Propname. For future work, we would like to validate the effectiveness of Propname model on much larger language modeling datasets, such as Propname 00."," In this paper, we address the limitations of Propname based sparse attention methods and propose the Propname to Propname Propname as our new solution. Specifically, Propname leverages separate learnable hash functions for queries and keys, respectively, and utilizes kernelized techniques for efficient approximation of attention utilities. The experiments on language modeling, natural language understanding, text classification, and image classification demonstrated the effectiveness of Propname. For future work, we would like to validate the effectiveness of Propname model on much larger language modeling datasets, such as Propname 00.", ADP DET NOUN PUNCT PRON VERB DET NOUN ADP PROPN VERB ADJ NOUN NOUN CCONJ VERB DET PROPN ADP PROPN PROPN ADP PRON ADJ NOUN PUNCT ADV PUNCT PROPN NOUN VERB ADJ NOUN NOUN ADP NOUN CCONJ NOUN PUNCT ADV PUNCT CCONJ VERB VERB NOUN ADP ADJ NOUN ADP NOUN NOUN PUNCT DET NOUN ADP NOUN NOUN PUNCT ADJ NOUN NOUN PUNCT NOUN NOUN PUNCT CCONJ NOUN NOUN VERB DET NOUN ADP PROPN PUNCT ADP ADJ NOUN PUNCT PRON AUX VERB PART VERB DET NOUN ADP PROPN NOUN ADP ADV ADJ NOUN NOUN NOUN PUNCT ADJ ADP PROPN NUM PUNCT,0.62,25.0,5.58
351,63,Zhiqing Sun,"[' Aiming to accelerate the training convergence of DETR as well as to improve prediction power in object detection, we present an investigation on the causes of its slow convergence through extensive experiments, and propose two novel solutions, namely TSP-FCOS and TSP-RCNN, which require much less training time and achieve the state-of-the-art detection performance. For future work, we would like to investigate the successful use of sparse attention mechanism [5, 6, 36] for directly modeling the relationship among multi-level features.']",conclusion_chunked," Aiming to accelerate the training convergence of DETR as well as to improve prediction power in object detection, we present an investigation on the causes of its slow convergence through extensive experiments, and propose two novel solutions, namely TSP-FCOS and TSP-RCNN, which require much less training time and achieve the state-of-the-art detection performance. For future work, we would like to investigate the successful use of sparse attention mechanism [5, 6, 36] for directly modeling the relationship among multi-level features.",22.733194790065724,22.733194790065724,351,0.5871177911758423," Aiming to accelerate the training convergence of Propname as well as to improve prediction power in object detection, we present an investigation on the causes of its slow convergence through extensive experiments, and propose two novel solutions, namely Propname Propname and Propname Propname, which require much less training time and achieve the state of the art detection performance. For future work, we would like to investigate the successful use of sparse attention mechanism for directly modeling the relationship among multi level features."," Aiming to accelerate the training convergence of Propname as well as to improve prediction power in object detection, we present an investigation on the causes of its slow convergence through extensive experiments, and propose two novel solutions, namely Propname Propname and Propname Propname, which require much less training time and achieve the state of the art detection performance. For future work, we would like to investigate the successful use of sparse attention mechanism for directly modeling the relationship among multi level features.", VERB PART VERB DET NOUN NOUN ADP PROPN ADV ADV ADP PART VERB NOUN NOUN ADP ADJ NOUN PUNCT PRON VERB DET NOUN ADP DET NOUN ADP PRON ADJ NOUN ADP ADJ NOUN PUNCT CCONJ VERB NUM ADJ NOUN PUNCT ADV PROPN PROPN CCONJ PROPN PROPN PUNCT PRON VERB ADV ADJ NOUN NOUN CCONJ VERB DET NOUN ADP DET NOUN NOUN NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX VERB PART VERB DET ADJ NOUN ADP ADJ NOUN NOUN ADP ADV VERB DET NOUN ADP ADJ NOUN NOUN PUNCT,0.7078651685393258,44.5,5.280898876404494
352,64,Zhiqing Sun,"[' We have presented MobileBERT which is a taskagnostic compact variant of BERT. Empirical results on popular NLP benchmarks show that MobileBERT is comparable with BERTBASE while being much smaller and faster. MobileBERT can enable various NLP applications7 to be easily deployed on mobile devices. In this paper, we show that 1) it is crucial to keep MobileBERT deep and thin, 2) bottleneck/invertedbottleneck structures enable effective layer-wise knowledge transfer, and 3) progressive knowledge transfer can efficiently train MobileBERT. We believe our findings are generic and can be applied to other model compression problems.']",conclusion_chunked," We have presented MobileBERT which is a taskagnostic compact variant of BERT. Empirical results on popular NLP benchmarks show that MobileBERT is comparable with BERTBASE while being much smaller and faster. MobileBERT can enable various NLP applications7 to be easily deployed on mobile devices. In this paper, we show that 1) it is crucial to keep MobileBERT deep and thin, 2) bottleneck/invertedbottleneck structures enable effective layer-wise knowledge transfer, and 3) progressive knowledge transfer can efficiently train MobileBERT. We believe our findings are generic and can be applied to other model compression problems.",22.733194790065724,22.733194790065724,352,0.6407294869422913," We have presented MobileBERT which is a taskagnostic compact variant of Propname. Empirical results on popular Propname benchmarks show that Propname is comparable with Propname while being much smaller and faster. Propname can enable various Propname applications0 to be easily deployed on mobile devices. In this paper, we show that 0 it is crucial to keep MobileBERT deep and thin, 0 bottleneckinvertedbottleneck structures enable effective layer wise knowledge transfer, and 0 progressive knowledge transfer can efficiently train Propname. We believe our findings are generic and can be applied to other model compression problems."," We have presented MobileBERT which is a taskagnostic compact variant of Propname. Empirical results on popular Propname benchmarks show that Propname is comparable with Propname while being much smaller and faster. Propname can enable various Propname applications0 to be easily deployed on mobile devices. In this paper, we show that 0 it is crucial to keep MobileBERT deep and thin, 0 bottleneckinvertedbottleneck structures enable effective layer wise knowledge transfer, and 0 progressive knowledge transfer can efficiently train Propname. We believe our findings are generic and can be applied to other model compression problems.", PRON AUX VERB NOUN PRON AUX DET ADJ ADJ NOUN ADP PROPN PUNCT ADJ NOUN ADP ADJ PROPN NOUN VERB SCONJ PROPN AUX ADJ ADP PROPN SCONJ AUX ADV ADJ CCONJ ADV PUNCT PROPN AUX VERB ADJ PROPN ADJ PART AUX ADV VERB ADP ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB SCONJ NUM PRON AUX ADJ PART VERB NUM ADJ CCONJ ADJ PUNCT NUM NOUN NOUN VERB ADJ NOUN ADJ NOUN NOUN PUNCT CCONJ NUM ADJ NOUN NOUN AUX ADV VERB PROPN PUNCT PRON VERB PRON NOUN AUX ADJ CCONJ AUX AUX VERB ADP ADJ NOUN NOUN NOUN PUNCT,0.6831683168316832,20.2,5.376237623762377
353,65,Zhiqing Sun,"[' In this paper, we performed an extensive re-examination study of recent neural network based KGC techniques. We find that many such models have issues with their score functions. Combined with inappropriate evaluation protocol, such methods reported inflated performance. Based on our observations, we propose RANDOM evaluation protocol that can clearly distinguish between these affected methods from others. We also strongly encourage the research community to follow the RANDOM evaluation protocol for all KGC evaluation purposes.']",conclusion_chunked," In this paper, we performed an extensive re-examination study of recent neural network based KGC techniques. We find that many such models have issues with their score functions. Combined with inappropriate evaluation protocol, such methods reported inflated performance. Based on our observations, we propose RANDOM evaluation protocol that can clearly distinguish between these affected methods from others. We also strongly encourage the research community to follow the RANDOM evaluation protocol for all KGC evaluation purposes.",22.733194790065724,22.733194790065724,353,0.47636231780052185," In this paper, we performed an extensive re examination study of recent neural network based Propname techniques. We find that many such models have issues with their score functions. Combined with inappropriate evaluation protocol, such methods reported inflated performance. Based on our observations, we propose Propname evaluation protocol that can clearly distinguish between these affected methods from others. We also strongly encourage the research community to follow the Propname evaluation protocol for all Propname evaluation purposes."," In this paper, we performed an extensive re examination study of recent neural network based Propname techniques. We find that many such models have issues with their score functions. Combined with inappropriate evaluation protocol, such methods reported inflated performance. Based on our observations, we propose Propname evaluation protocol that can clearly distinguish between these affected methods from others. We also strongly encourage the research community to follow the Propname evaluation protocol for all Propname evaluation purposes.", ADP DET NOUN PUNCT PRON VERB DET ADJ ADP NOUN NOUN ADP ADJ ADJ NOUN VERB PROPN NOUN PUNCT PRON VERB SCONJ ADJ ADJ NOUN VERB NOUN ADP PRON NOUN NOUN PUNCT VERB ADP ADJ NOUN NOUN PUNCT ADJ NOUN VERB ADJ NOUN PUNCT VERB ADP PRON NOUN PUNCT PRON VERB PROPN NOUN NOUN PRON AUX ADV VERB ADP DET VERB NOUN ADP NOUN PUNCT PRON ADV ADV VERB DET NOUN NOUN PART VERB DET PROPN NOUN NOUN ADP DET PROPN NOUN NOUN PUNCT,0.75,16.8,5.619047619047619
354,66,Zhiqing Sun,"[' This paper proposes a novel EM approach to non-autoregressive conditional sequence generation, which efectively addresses the multi-modality issue in NAR training by iterative optimizing both the teacher AR model and the student NAR model. We also developed a principled plug-and-play decoding method for efficiently removing word duplication in the model’s output. Experimental results on three tasks prove the effectiveness of our approach. For future work, we plan to examine the effectiveness of ourmethod in a broader range of applications, such as text summarization.']",conclusion_chunked," This paper proposes a novel EM approach to non-autoregressive conditional sequence generation, which efectively addresses the multi-modality issue in NAR training by iterative optimizing both the teacher AR model and the student NAR model. We also developed a principled plug-and-play decoding method for efficiently removing word duplication in the model’s output. Experimental results on three tasks prove the effectiveness of our approach. For future work, we plan to examine the effectiveness of ourmethod in a broader range of applications, such as text summarization.",22.733194790065724,22.733194790065724,354,0.71416836977005," This paper proposes a novel EM approach to non autoregressive conditional sequence generation, which efectively addresses the multi modality issue in Propname training by iterative optimizing both the teacher Propname model and the student Propname model. We also developed a principled plug and play decoding method for efficiently removing word duplication in the models output. Experimental results on three tasks prove the effectiveness of our approach. For future work, we plan to examine the effectiveness of ourmethod in a broader range of applications, such as text summarization."," This paper proposes a novel EM approach to non autoregressive conditional sequence generation, which efectively addresses the multi modality issue in Propname training by iterative optimizing both the teacher Propname model and the student Propname model. We also developed a principled plug and play decoding method for efficiently removing word duplication in the models output. Experimental results on three tasks prove the effectiveness of our approach. For future work, we plan to examine the effectiveness of ourmethod in a broader range of applications, such as text summarization.", DET NOUN VERB DET ADJ NOUN NOUN ADP ADJ ADJ ADJ NOUN NOUN PUNCT PRON ADV VERB DET ADJ NOUN NOUN ADP PROPN NOUN ADP ADJ VERB CCONJ DET NOUN PROPN NOUN CCONJ DET NOUN PROPN NOUN PUNCT PRON ADV VERB DET ADJ NOUN CCONJ VERB VERB NOUN ADP ADV VERB NOUN NOUN ADP DET NOUN NOUN PUNCT ADJ NOUN ADP NUM NOUN VERB DET NOUN ADP PRON NOUN PUNCT ADP ADJ NOUN PUNCT PRON VERB PART VERB DET NOUN ADP NOUN ADP DET ADJ NOUN ADP NOUN PUNCT ADJ ADP NOUN NOUN PUNCT,0.7553191489361702,23.5,5.340425531914893
355,67,Zhiqing Sun,"[' Non-autoregressive sequence models have achieved impressive inference speedup but suffer from decoding inconsistency problem, and thus performs poorly compared to autoregressive sequence models. In this paper, we propose a novel framework to bridge the performance gap between non-autoregressive and autoregressive sequence models. Specifically, we use linear-chain Conditional Random Fields (CRF) to model the co-occurrence relationship between adjacent words during the decoding. We design two effective approximation methods to tackle the issue of the large vocabulary size, and further propose a dynamic transition technique to model positional contexts in the CRF. The results significantly outperform previous non-autoregressive baselines on WMT14 En-De and De-En datasets and achieve comparable performance to the autoregressive counterparts. In the future, we plan to utilize other existing techniques for our NART-CRF models to further bridge the gap between non-autoregressive and autoregressive sequence models. Besides, although the rescoring process is also parallelized, it severely increases the inference latency, as can be seen in Table 2. An additional module that can accurately predict the target length might be useful. As our major contribution in this paper is to model richer structural dependency in the non-autoregressive decoder, we leave this for future work.']",conclusion_chunked," Non-autoregressive sequence models have achieved impressive inference speedup but suffer from decoding inconsistency problem, and thus performs poorly compared to autoregressive sequence models. In this paper, we propose a novel framework to bridge the performance gap between non-autoregressive and autoregressive sequence models. Specifically, we use linear-chain Conditional Random Fields (CRF) to model the co-occurrence relationship between adjacent words during the decoding. We design two effective approximation methods to tackle the issue of the large vocabulary size, and further propose a dynamic transition technique to model positional contexts in the CRF. The results significantly outperform previous non-autoregressive baselines on WMT14 En-De and De-En datasets and achieve comparable performance to the autoregressive counterparts. In the future, we plan to utilize other existing techniques for our NART-CRF models to further bridge the gap between non-autoregressive and autoregressive sequence models. Besides, although the rescoring process is also parallelized, it severely increases the inference latency, as can be seen in Table 2. An additional module that can accurately predict the target length might be useful. As our major contribution in this paper is to model richer structural dependency in the non-autoregressive decoder, we leave this for future work.",10.523121420389487,22.733194790065724,355,0.32436487078666687," Non autoregressive sequence models have achieved impressive inference speedup but suffer from decoding inconsistency problem, and thus performs poorly compared to autoregressive sequence models. In this paper, we propose a novel framework to bridge the performance gap between non autoregressive and autoregressive sequence models. Specifically, we use linear chain Propname Propname Propname to model the co occurrence relationship between adjacent words during the decoding. We design two effective approximation methods to tackle the issue of the large vocabulary size, and further propose a dynamic transition technique to model positional contexts in the Propname. The results significantly outperform previous non autoregressive baselines on Propname Propname Propname and Propname Propname datasets and achieve comparable performance to the autoregressive counterparts. In the future, we plan to utilize other existing techniques for our Propname Propname models to further bridge the gap between non autoregressive and autoregressive sequence models. Besides, although the rescoring process is also parallelized, it severely increases the inference latency, as can be seen in Propname0. An additional module that can accurately predict the target length might be useful. As our major contribution in this paper is to model richer structural dependency in the Propname autoregressive decoder, we leave this for future work."," Non autoregressive sequence models have achieved impressive inference speedup but suffer from decoding inconsistency problem, and thus performs poorly compared to autoregressive sequence models. In this paper, we propose a novel framework to bridge the performance gap between non autoregressive and autoregressive sequence models. Specifically, we use linear chain Propname Propname Propname to model the co occurrence relationship between adjacent words during the decoding. We design two effective approximation methods to tackle the issue of the large vocabulary size, and further propose a dynamic transition technique to model positional contexts in the Propname. The results significantly outperform previous non autoregressive baselines on Propname Propname Propname and Propname Propname datasets and achieve comparable performance to the autoregressive counterparts. In the future, we plan to utilize other existing techniques for our Propname Propname models to further bridge the gap between non autoregressive and autoregressive sequence models. Besides, although the rescoring process is also parallelized, it severely increases the inference latency, as can be seen in Propname0. An additional module that can accurately predict the target length might be useful. As our major contribution in this paper is to model richer structural dependency in the Propname autoregressive decoder, we leave this for future work.", ADJ ADJ NOUN NOUN AUX VERB ADJ NOUN NOUN CCONJ VERB ADP VERB NOUN NOUN PUNCT CCONJ ADV VERB ADV VERB ADP ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB DET ADJ NOUN PART VERB DET NOUN NOUN ADP ADJ ADJ CCONJ ADJ NOUN NOUN PUNCT ADV PUNCT PRON VERB ADJ NOUN PROPN PROPN PROPN PART VERB DET NOUN NOUN NOUN ADP ADJ NOUN ADP DET NOUN PUNCT PRON VERB NUM ADJ NOUN NOUN PART VERB DET NOUN ADP DET ADJ ADJ NOUN PUNCT CCONJ ADV VERB DET ADJ NOUN NOUN AUX VERB ADJ NOUN ADP DET PROPN PUNCT DET NOUN ADV VERB ADJ ADJ ADJ NOUN ADP PROPN PROPN PROPN CCONJ PROPN PROPN NOUN CCONJ VERB ADJ NOUN ADP DET ADJ NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PART VERB ADJ VERB NOUN ADP PRON PROPN PROPN NOUN PART ADV VERB DET NOUN ADP ADJ ADJ CCONJ ADJ NOUN NOUN PUNCT SCONJ PUNCT SCONJ DET NOUN NOUN AUX ADV VERB PUNCT PRON ADV VERB DET NOUN NOUN PUNCT SCONJ AUX AUX VERB ADP PROPN PUNCT PUNCT DET ADJ NOUN PRON AUX ADV VERB DET NOUN NOUN AUX AUX ADJ PUNCT SCONJ PRON ADJ NOUN ADP DET NOUN AUX PART VERB ADJ ADJ NOUN ADP DET PROPN ADJ NOUN PUNCT PRON VERB PRON ADP ADJ NOUN PUNCT,0.5570776255707762,24.333333333333332,5.616438356164384
356,68,Zhiqing Sun,"[' In this paper, we propose an end-to-end method called DivGraphPointer for extracting diverse keyphrases. It formulates documents as graphs and applies graph convolutional networks for encoding the graphs, which efficiently capture the document-level wordsalience by modeling both the short- and long-range dependency between words in documents and aggregate the information of multiple appearances of identical words. To avoid extracting similar keyphrases, a diversified pointer network is proposed to generate diverse keyphrases from the nodes of the graphs. Experiments on five benchmark data sets show that our proposed DivGraphPointer model achieves state-of-the-art performance, significantly outperforming existing state-of-the-art supervised and unsuper-vised methods. Our research can be extended in many directions. To begin with, currently our diversified pointer network decoders extract keyphrase in an auto-regressive fashion. We could further leverage reinforcement learning to address the exposure bias as well as consequent error propagation problem in the sequential generation process. Moreover, our graph convolutional network encoder aggregates the word relation information through manually designed edge weights based on proximity at present. We would like to further explore utilizing graph attention networks (GATs) [41 ] to dynamically capture correlation between words in word graph. Finally, utilizing linguistic information when constructing edges in word graphs to ease keyphrase extraction is also an interesting future direction.']",conclusion_chunked," In this paper, we propose an end-to-end method called DivGraphPointer for extracting diverse keyphrases. It formulates documents as graphs and applies graph convolutional networks for encoding the graphs, which efficiently capture the document-level wordsalience by modeling both the short- and long-range dependency between words in documents and aggregate the information of multiple appearances of identical words. To avoid extracting similar keyphrases, a diversified pointer network is proposed to generate diverse keyphrases from the nodes of the graphs. Experiments on five benchmark data sets show that our proposed DivGraphPointer model achieves state-of-the-art performance, significantly outperforming existing state-of-the-art supervised and unsuper-vised methods. Our research can be extended in many directions. To begin with, currently our diversified pointer network decoders extract keyphrase in an auto-regressive fashion. We could further leverage reinforcement learning to address the exposure bias as well as consequent error propagation problem in the sequential generation process. Moreover, our graph convolutional network encoder aggregates the word relation information through manually designed edge weights based on proximity at present. We would like to further explore utilizing graph attention networks (GATs) [41 ] to dynamically capture correlation between words in word graph. Finally, utilizing linguistic information when constructing edges in word graphs to ease keyphrase extraction is also an interesting future direction.",11.485714285714323,22.733194790065724,356,0.6561166048049927," In this paper, we propose an end to end method called DivGraphPointer for extracting diverse keyphrases. It formulates documents as graphs and applies graph convolutional networks for encoding the graphs, which efficiently capture the document level wordsalience by modeling both the short and long range dependency between words in documents and aggregate the information of multiple appearances of identical words. To avoid extracting similar keyphrases, a diversified pointer network is proposed to generate diverse keyphrases from the nodes of the graphs. Experiments on five benchmark data sets show that our proposed DivGraphPointer model achieves state of the art performance, significantly outperforming existing state of the art supervised and unsuper vised methods. Our research can be extended in many directions. To begin with, currently our diversified pointer network decoders extract keyphrase in an auto regressive fashion. We could further leverage reinforcement learning to address the exposure bias as well as consequent error propagation problem in the sequential generation process. Moreover, our graph convolutional network encoder aggregates the word relation information through manually designed edge weights based on proximity at present. We would like to further explore utilizing graph attention networks to dynamically capture correlation between words in word graph. Finally, utilizing linguistic information when constructing edges in word graphs to ease keyphrase extraction is also an interesting future direction."," In this paper, we propose an end to end method called DivGraphPointer for extracting diverse keyphrases. It formulates documents as graphs and applies graph convolutional networks for encoding the graphs, which efficiently capture the document level wordsalience by modeling both the short and long range dependency between words in documents and aggregate the information of multiple appearances of identical words. To avoid extracting similar keyphrases, a diversified pointer network is proposed to generate diverse keyphrases from the nodes of the graphs. Experiments on five benchmark data sets show that our proposed DivGraphPointer model achieves state of the art performance, significantly outperforming existing state of the art supervised and unsuper vised methods. Our research can be extended in many directions. To begin with, currently our diversified pointer network decoders extract keyphrase in an auto regressive fashion. We could further leverage reinforcement learning to address the exposure bias as well as consequent error propagation problem in the sequential generation process. Moreover, our graph convolutional network encoder aggregates the word relation information through manually designed edge weights based on proximity at present. We would like to further explore utilizing graph attention networks to dynamically capture correlation between words in word graph. Finally, utilizing linguistic information when constructing edges in word graphs to ease keyphrase extraction is also an interesting future direction.", ADP DET NOUN PUNCT PRON VERB DET NOUN PART NOUN NOUN VERB NOUN ADP VERB ADJ NOUN PUNCT PRON VERB NOUN ADP NOUN CCONJ VERB ADJ ADJ NOUN ADP VERB DET NOUN PUNCT PRON ADV VERB DET NOUN NOUN NOUN ADP VERB CCONJ DET ADJ CCONJ ADJ NOUN NOUN ADP NOUN ADP NOUN CCONJ VERB DET NOUN ADP ADJ NOUN ADP ADJ NOUN PUNCT PART VERB VERB ADJ NOUN PUNCT DET ADJ NOUN NOUN AUX VERB PART VERB ADJ NOUN ADP DET NOUN ADP DET NOUN PUNCT NOUN ADP NUM ADJ NOUN NOUN VERB SCONJ PRON VERB NOUN NOUN VERB NOUN ADP DET NOUN NOUN PUNCT ADV VERB VERB NOUN ADP DET NOUN VERB CCONJ ADJ VERB NOUN PUNCT PRON NOUN AUX AUX VERB ADP ADJ NOUN PUNCT PART VERB ADP PUNCT ADV PRON ADJ NOUN NOUN NOUN VERB NOUN ADP DET NOUN ADJ NOUN PUNCT PRON AUX ADV VERB NOUN NOUN PART VERB DET NOUN NOUN ADV ADV ADP NOUN NOUN NOUN NOUN ADP DET ADJ NOUN NOUN PUNCT ADV PUNCT PRON NOUN ADJ NOUN NOUN VERB DET NOUN NOUN NOUN ADP ADV VERB NOUN NOUN VERB ADP NOUN ADP NOUN PUNCT PRON AUX VERB PART ADV VERB VERB NOUN NOUN NOUN PART ADV VERB NOUN ADP NOUN ADP NOUN NOUN PUNCT ADV PUNCT VERB ADJ NOUN SCONJ VERB NOUN ADP NOUN NOUN PART VERB NOUN NOUN AUX ADV DET ADJ ADJ NOUN PUNCT,0.6340425531914894,23.5,5.659574468085107
357,69,Zhiqing Sun,"[' We have proposed a new knowledge graph embedding method called RotatE, which represents entities as complex vectors and relations as rotations in complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Our experimental results show that the RotatE model outperforms all existing state-of-the-art models on four large-scale benchmarks. Moreover, RotatE also achieves state-of-the-art results on a benchmark that is explicitly designed for composition pattern inference and modeling. A deep investigation into RotatE relation embeddings shows that the three relation patterns are implicitly represented in the relation embeddings. In the future, we plan to evaluate the RotatE model on more datasets and leverage a probabilistic framework to model the uncertainties of entities and relations.']",conclusion_chunked," We have proposed a new knowledge graph embedding method called RotatE, which represents entities as complex vectors and relations as rotations in complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Our experimental results show that the RotatE model outperforms all existing state-of-the-art models on four large-scale benchmarks. Moreover, RotatE also achieves state-of-the-art results on a benchmark that is explicitly designed for composition pattern inference and modeling. A deep investigation into RotatE relation embeddings shows that the three relation patterns are implicitly represented in the relation embeddings. In the future, we plan to evaluate the RotatE model on more datasets and leverage a probabilistic framework to model the uncertainties of entities and relations.",11.074766666666676,22.733194790065724,357,0.5185592174530029," We have proposed a new knowledge graph embedding method called Propname, which represents entities as complex vectors and relations as rotations in complex vector space. In addition, we propose a novel self adversarial negative sampling technique for efficiently and effectively training the Propname model. Our experimental results show that the Propname model outperforms all existing state of the art models on four large scale benchmarks. Moreover, Propname also achieves state of the art results on a benchmark that is explicitly designed for composition pattern inference and modeling. A deep investigation into Propname relation embeddings shows that the three relation patterns are implicitly represented in the relation embeddings. In the future, we plan to evaluate the Propname model on more datasets and leverage a probabilistic framework to model the uncertainties of entities and relations."," We have proposed a new knowledge graph embedding method called Propname, which represents entities as complex vectors and relations as rotations in complex vector space. In addition, we propose a novel self adversarial negative sampling technique for efficiently and effectively training the Propname model. Our experimental results show that the Propname model outperforms all existing state of the art models on four large scale benchmarks. Moreover, Propname also achieves state of the art results on a benchmark that is explicitly designed for composition pattern inference and modeling. A deep investigation into Propname relation embeddings shows that the three relation patterns are implicitly represented in the relation embeddings. In the future, we plan to evaluate the Propname model on more datasets and leverage a probabilistic framework to model the uncertainties of entities and relations.", PRON AUX VERB DET ADJ NOUN NOUN VERB NOUN VERB PROPN PUNCT PRON VERB NOUN ADP ADJ NOUN CCONJ NOUN ADP NOUN ADP ADJ NOUN NOUN PUNCT ADP NOUN PUNCT PRON VERB DET ADJ NOUN ADJ ADJ NOUN NOUN ADP ADV CCONJ ADV VERB DET PROPN NOUN PUNCT PRON ADJ NOUN VERB SCONJ DET PROPN NOUN VERB DET VERB NOUN ADP DET NOUN NOUN ADP NUM ADJ NOUN NOUN PUNCT ADV PUNCT PROPN ADV VERB NOUN ADP DET NOUN NOUN ADP DET NOUN PRON AUX ADV VERB ADP NOUN NOUN NOUN CCONJ NOUN PUNCT DET ADJ NOUN ADP PROPN NOUN NOUN VERB SCONJ DET NUM NOUN NOUN AUX ADV VERB ADP DET NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON VERB PART VERB DET PROPN NOUN ADP ADJ NOUN CCONJ VERB DET ADJ NOUN PART VERB DET NOUN ADP NOUN CCONJ NOUN PUNCT,0.6363636363636364,23.833333333333332,5.398601398601398
358,70,Zhiqing Sun,"[' In this paper, we proposed a neural generative model for fully unsupervised Chinese word seg-mentation (CWS). To the best of knowledge, this is the first neural model for CWS. Our segmental language model is an intuitive generalization of vanilla neural language models that directly modeling the segmental nature of Chinese. Experimental results show that our models achieve competitive performance to the previous state-of-the-art statistical models on four datasets from SIGHAN 2005. We also show the improvement of incorporating ad hoc guidelines into our segmental language models. Our future work may include the following two directions. In this work, we only consider the sequential segmental language modeling. In the future, we are interested in build a hierarchical neural language model like the Pitman-Yor process. Like vanilla language models, the segmental language models can also provide useful information for semi-supervised learning tasks. It would also be interesting to explore our models in the semi-supervised schemes.']",conclusion_chunked," In this paper, we proposed a neural generative model for fully unsupervised Chinese word seg-mentation (CWS). To the best of knowledge, this is the first neural model for CWS. Our segmental language model is an intuitive generalization of vanilla neural language models that directly modeling the segmental nature of Chinese. Experimental results show that our models achieve competitive performance to the previous state-of-the-art statistical models on four datasets from SIGHAN 2005. We also show the improvement of incorporating ad hoc guidelines into our segmental language models. Our future work may include the following two directions. In this work, we only consider the sequential segmental language modeling. In the future, we are interested in build a hierarchical neural language model like the Pitman-Yor process. Like vanilla language models, the segmental language models can also provide useful information for semi-supervised learning tasks. It would also be interesting to explore our models in the semi-supervised schemes.",27.081970588235293,22.733194790065724,358,0.7315146923065186," In this paper, we proposed a neural generative model for fully unsupervised Chinese word Propname mentation. To the best of knowledge, this is the first neural model for Propname. Our segmental language model is an intuitive generalization of vanilla neural language models that directly modeling the segmental nature of Propname. Experimental results show that our models achieve competitive performance to the previous state of the art statistical models on four datasets from Propname 0000. We also show the improvement of incorporating ad hoc guidelines into our segmental language models. Our future work may include the following two directions. In this work, we only consider the sequential segmental language modeling. In the future, we are interested in build a hierarchical neural language model like the Propname Propname process. Like vanilla language models, the segmental language models can also provide useful information for semi supervised learning tasks. It would also be interesting to explore our models in the semi supervised schemes."," In this paper, we proposed a neural generative model for fully unsupervised Chinese word Propname mentation. To the best of knowledge, this is the first neural model for Propname. Our segmental language model is an intuitive generalization of vanilla neural language models that directly modeling the segmental nature of Propname. Experimental results show that our models achieve competitive performance to the previous state of the art statistical models on four datasets from Propname 0000. We also show the improvement of incorporating ad hoc guidelines into our segmental language models. Our future work may include the following two directions. In this work, we only consider the sequential segmental language modeling. In the future, we are interested in build a hierarchical neural language model like the Propname Propname process. Like vanilla language models, the segmental language models can also provide useful information for semi supervised learning tasks. It would also be interesting to explore our models in the semi supervised schemes.", ADP DET NOUN PUNCT PRON VERB DET ADJ ADJ NOUN ADP ADV ADJ ADJ NOUN PROPN NOUN PUNCT ADP DET ADJ ADP NOUN PUNCT PRON AUX DET ADJ ADJ NOUN ADP PROPN PUNCT PRON ADJ NOUN NOUN AUX DET ADJ NOUN ADP NOUN ADJ NOUN NOUN PRON ADV VERB DET ADJ NOUN ADP PROPN PUNCT ADJ NOUN VERB SCONJ PRON NOUN VERB ADJ NOUN ADP DET ADJ NOUN ADP DET NOUN ADJ NOUN ADP NUM NOUN ADP PROPN NUM PUNCT PRON ADV VERB DET NOUN ADP VERB NOUN X NOUN ADP PRON ADJ NOUN NOUN PUNCT PRON ADJ NOUN AUX VERB DET VERB NUM NOUN PUNCT ADP DET NOUN PUNCT PRON ADV VERB DET ADJ ADJ NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON AUX ADJ ADP VERB DET ADJ ADJ NOUN NOUN ADP DET PROPN PROPN NOUN PUNCT ADP NOUN NOUN NOUN PUNCT DET ADJ NOUN NOUN AUX ADV VERB ADJ NOUN ADP ADJ ADJ NOUN NOUN PUNCT PRON AUX ADV AUX ADJ PART VERB PRON NOUN ADP DET ADJ VERB NOUN PUNCT,0.5402298850574713,17.4,5.160919540229885
359,71,Timo Schick,"[' We have shown that providing task descriptions to pretrained language models can be combined with standard supervised training. Our proposed method, PET, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks. We finetune models for all pattern-verbalizer pairs and use them to create large annotated datasets on which standard classifiers can be trained. When the initial amount of training data is limited, PET gives large improvements over standard supervised training and strong semi-supervised approaches.']",conclusion_chunked," We have shown that providing task descriptions to pretrained language models can be combined with standard supervised training. Our proposed method, PET, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks. We finetune models for all pattern-verbalizer pairs and use them to create large annotated datasets on which standard classifiers can be trained. When the initial amount of training data is limited, PET gives large improvements over standard supervised training and strong semi-supervised approaches.",32.83053957444222,32.83053957444222,359,0.6363247632980347," We have shown that providing task descriptions to pretrained language models can be combined with standard supervised training. Our proposed method, Propname, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks. We finetune models for all pattern Propname pairs and use them to create large annotated datasets on which standard classifiers can be trained. When the initial amount of training data is limited, Propname gives large improvements over standard supervised training and strong semi supervised approaches."," We have shown that providing task descriptions to pretrained language models can be combined with standard supervised training. Our proposed method, Propname, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks. We finetune models for all pattern Propname pairs and use them to create large annotated datasets on which standard classifiers can be trained. When the initial amount of training data is limited, Propname gives large improvements over standard supervised training and strong semi supervised approaches.", PRON AUX VERB SCONJ VERB NOUN NOUN PART VERB NOUN NOUN AUX AUX VERB ADP ADJ ADJ NOUN PUNCT PRON VERB NOUN PUNCT PROPN PUNCT VERB ADP VERB NOUN ADP VERB NOUN NOUN CCONJ NOUN PRON VERB VERB DET NOUN VERB ADP VERB NOUN NOUN ADP ADJ NOUN PUNCT PRON VERB NOUN ADP DET NOUN PROPN NOUN CCONJ VERB PRON PART VERB ADJ ADJ NOUN ADP PRON ADJ NOUN AUX AUX VERB PUNCT SCONJ DET ADJ NOUN ADP NOUN NOUN AUX ADJ PUNCT PROPN VERB ADJ NOUN ADP ADJ ADJ NOUN CCONJ ADJ ADJ ADJ NOUN PUNCT,0.6907216494845361,24.25,5.587628865979381
360,72,Timo Schick,"[' We have proposed a simple yet effective modification of PET, enabling us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of PET combined with ALBERT: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates, and the underlying LM itself. We have shown that using PET, it is possible to achieve few-shot text classification performance similar to GPT-3 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This not only lowers financial cost, but above all reduces environmental impact immensely and leads to a much smaller carbon footprint. We see this as an important contribution to achieving the goal of an environmentally more friendly NLP. To enable comparisons with our work, we make our code, models and datasets publicly available. For future work, it would be interesting to see whether PET also works for generative tasks when combined with generative LMs and whether further improvements are possible in multi-task settings.']",conclusion_chunked," We have proposed a simple yet effective modification of PET, enabling us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of PET combined with ALBERT: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates, and the underlying LM itself. We have shown that using PET, it is possible to achieve few-shot text classification performance similar to GPT-3 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This not only lowers financial cost, but above all reduces environmental impact immensely and leads to a much smaller carbon footprint. We see this as an important contribution to achieving the goal of an environmentally more friendly NLP. To enable comparisons with our work, we make our code, models and datasets publicly available. For future work, it would be interesting to see whether PET also works for generative tasks when combined with generative LMs and whether further improvements are possible in multi-task settings.",22.573125000000033,32.83053957444222,360,0.5635378956794739," We have proposed a simple yet effective modification of Propname, enabling us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of Propname combined with Propname: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates, and the underlying Propname itself. We have shown that using Propname, it is possible to achieve few shot text classification performance similar to Propname 0 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This not only lowers financial cost, but above all reduces environmental impact immensely and leads to a much smaller carbon footprint. We see this as an important contribution to achieving the goal of an environmentally more friendly Propname. To enable comparisons with our work, we make our code, models and datasets publicly available. For future work, it would be interesting to see whether Propname also works for generative tasks when combined with generative LMs and whether further improvements are possible in multi task settings."," We have proposed a simple yet effective modification of Propname, enabling us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of Propname combined with Propname: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates, and the underlying Propname itself. We have shown that using Propname, it is possible to achieve few shot text classification performance similar to Propname 0 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This not only lowers financial cost, but above all reduces environmental impact immensely and leads to a much smaller carbon footprint. We see this as an important contribution to achieving the goal of an environmentally more friendly Propname. To enable comparisons with our work, we make our code, models and datasets publicly available. For future work, it would be interesting to see whether Propname also works for generative tasks when combined with generative LMs and whether further improvements are possible in multi task settings.", PRON AUX VERB DET ADJ ADV ADJ NOUN ADP PROPN PUNCT VERB PRON PART VERB PRON ADP NOUN PRON VERB VERB ADJ NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX VERB ADJ NOUN ADJ ADP DET ADJ NOUN ADP PROPN VERB ADP PROPN PUNCT DET NOUN PART ADV VERB ADJ NOUN ADP VERB NOUN ADP ADJ NOUN PUNCT DET NOUN PART VERB ADP NOUN PRON AUX ADJ PART VERB PUNCT DET NOUN ADP VERB NOUN PART VERB NOUN NOUN PUNCT CCONJ DET VERB PROPN PRON PUNCT PRON AUX VERB SCONJ VERB PROPN PUNCT PRON AUX ADJ PART VERB ADJ NOUN NOUN NOUN NOUN ADJ ADP PROPN NUM ADP NUM ADP NOUN PRON VERB NUM NOUN ADP NOUN ADJ NOUN PUNCT PRON PART ADV VERB ADJ NOUN PUNCT CCONJ ADP PRON VERB ADJ NOUN ADV CCONJ VERB ADP DET ADV ADJ NOUN NOUN PUNCT PRON VERB PRON ADP DET ADJ NOUN ADP VERB DET NOUN ADP DET ADV ADV ADJ PROPN PUNCT PART VERB NOUN ADP PRON NOUN PUNCT PRON VERB PRON NOUN PUNCT NOUN CCONJ NOUN ADV ADJ PUNCT ADP ADJ NOUN PUNCT PRON AUX AUX ADJ PART VERB SCONJ PROPN ADV VERB ADP ADJ NOUN SCONJ VERB ADP ADJ NOUN CCONJ SCONJ ADJ NOUN AUX ADJ ADP ADJ NOUN NOUN PUNCT,0.6367924528301887,30.285714285714285,5.136792452830188
361,73,Timo Schick,"[' We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they reduce perplexity on future tokens. Toolformer considerably improves zero-shot performance of a 6.7B parameter GPT-J model, enabling it to even outperform a much larger GPT-3 model on a range of different downstream tasks.']",conclusion_chunked," We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they reduce perplexity on future tokens. Toolformer considerably improves zero-shot performance of a 6.7B parameter GPT-J model, enabling it to even outperform a much larger GPT-3 model on a range of different downstream tasks.",32.83053957444222,32.83053957444222,361,0.6080512404441833," We have introduced Propname, a language model that learns in a self supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they reduce perplexity on future tokens. Propname considerably improves zero shot performance of a 0.0B parameter Propname Propname model, enabling it to even outperform a much larger Propname 0 model on a range of different downstream tasks."," We have introduced Propname, a language model that learns in a self supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they reduce perplexity on future tokens. Propname considerably improves zero shot performance of a 0.0B parameter Propname Propname model, enabling it to even outperform a much larger Propname 0 model on a range of different downstream tasks.", PRON AUX VERB PROPN PUNCT DET NOUN NOUN PRON VERB ADP DET NOUN VERB NOUN SCONJ PART VERB ADJ NOUN ADJ ADP NOUN NOUN PUNCT NOUN PUNCT CCONJ NOUN NOUN ADP ADJ NOUN NOUN PUNCT PRON AUX VERB ADP VERB ADP DET ADJ NOUN ADP VERB NOUN NOUN PRON AUX VERB VERB ADP SCONJ PRON VERB NOUN ADP ADJ NOUN PUNCT PROPN ADV VERB NUM NOUN NOUN ADP DET NOUN NOUN PROPN PROPN NOUN PUNCT VERB PRON PART ADV VERB DET ADV ADJ PROPN NUM NOUN ADP DET NOUN ADP ADJ ADJ NOUN PUNCT,0.723404255319149,31.333333333333332,4.787234042553192
362,74,Timo Schick,"[' In this paper, we have shown that large language models are capable of performing self-diagnosis, i.e., of investigating their own outputs with regards to the presence of undesirable attributes using only their internal knowledge and textual descriptions. Based on this finding, we have proposed a decoding algorithm that reduces the probability of a model generating biased text by comparing the original probability of a token with its probability if undesired behavior is explicitly encouraged. As our evaluation is limited to two English datasets covering only a small portion of potentially undesired behaviors in an imperfect fashion, it is important to extend our analysis to other kinds of behaviors and biases, languages, benchmarks and models. It is clear that self-diagnosis and self-debiasing only reduce and do not eliminate corpus-based bias. For this reason, they are not a viable path towards bias-free models if used in isolation. However, we hope that future work can leverage our proposals, e.g., by combining them with complementary models or by extending them to build stronger debiasing solutions.']",conclusion_chunked," In this paper, we have shown that large language models are capable of performing self-diagnosis, i.e., of investigating their own outputs with regards to the presence of undesirable attributes using only their internal knowledge and textual descriptions. Based on this finding, we have proposed a decoding algorithm that reduces the probability of a model generating biased text by comparing the original probability of a token with its probability if undesired behavior is explicitly encouraged. As our evaluation is limited to two English datasets covering only a small portion of potentially undesired behaviors in an imperfect fashion, it is important to extend our analysis to other kinds of behaviors and biases, languages, benchmarks and models. It is clear that self-diagnosis and self-debiasing only reduce and do not eliminate corpus-based bias. For this reason, they are not a viable path towards bias-free models if used in isolation. However, we hope that future work can leverage our proposals, e.g., by combining them with complementary models or by extending them to build stronger debiasing solutions.",18.149513487475957,32.83053957444222,362,0.26975372433662415," In this paper, we have shown that large language models are capable of performing self diagnosis, ie, of investigating their own outputs with regards to the presence of undesirable attributes using only their internal knowledge and textual descriptions. Based on this finding, we have proposed a decoding Propname that reduces the probability of a model generating biased text by comparing the original probability of a token with its probability if undesired behavior is explicitly encouraged. As our evaluation is limited to two English datasets covering only a small portion of potentially undesired behaviors in an imperfect fashion, it is important to extend our analysis to other kinds of behaviors and biases, languages, benchmarks and models. It is clear that self diagnosis and self debiasing only reduce and do not eliminate Propname based bias. For this reason, they are not a viable path towards bias free models if used in isolation. However, we hope that future work can leverage our proposals, Propname, by combining them with complementary models or by extending them to build stronger debiasing solutions."," In this paper, we have shown that large language models are capable of performing self diagnosis, ie, of investigating their own outputs with regards to the presence of undesirable attributes using only their internal knowledge and textual descriptions. Based on this finding, we have proposed a decoding Propname that reduces the probability of a model generating biased text by comparing the original probability of a token with its probability if undesired behavior is explicitly encouraged. As our evaluation is limited to two English datasets covering only a small portion of potentially undesired behaviors in an imperfect fashion, it is important to extend our analysis to other kinds of behaviors and biases, languages, benchmarks and models. It is clear that self diagnosis and self debiasing only reduce and do not eliminate Propname based bias. For this reason, they are not a viable path towards bias free models if used in isolation. However, we hope that future work can leverage our proposals, Propname, by combining them with complementary models or by extending them to build stronger debiasing solutions.", ADP DET NOUN PUNCT PRON AUX VERB SCONJ ADJ NOUN NOUN AUX ADJ ADP VERB NOUN NOUN PUNCT ADV PUNCT ADP VERB PRON ADJ NOUN ADP NOUN ADP DET NOUN ADP ADJ NOUN VERB ADV PRON ADJ NOUN CCONJ ADJ NOUN PUNCT VERB ADP DET NOUN PUNCT PRON AUX VERB DET VERB PROPN PRON VERB DET NOUN ADP DET NOUN VERB ADJ NOUN ADP VERB DET ADJ NOUN ADP DET NOUN ADP PRON NOUN SCONJ ADJ NOUN AUX ADV ADJ PUNCT SCONJ PRON NOUN AUX VERB ADP NUM ADJ NOUN VERB ADV DET ADJ NOUN ADP ADV ADJ NOUN ADP DET ADJ NOUN PUNCT PRON AUX ADJ PART VERB PRON NOUN ADP ADJ NOUN ADP NOUN CCONJ NOUN PUNCT NOUN PUNCT NOUN CCONJ NOUN PUNCT PRON AUX ADJ SCONJ NOUN NOUN CCONJ NOUN VERB ADV VERB CCONJ AUX PART VERB PROPN VERB NOUN PUNCT ADP DET NOUN PUNCT PRON AUX PART DET ADJ NOUN ADP NOUN ADJ NOUN SCONJ VERB ADP NOUN PUNCT ADV PUNCT PRON VERB SCONJ ADJ NOUN AUX VERB PRON NOUN PUNCT PROPN PUNCT ADP VERB PRON ADP ADJ NOUN CCONJ ADP VERB PRON PART VERB ADJ ADJ NOUN PUNCT,0.616580310880829,32.166666666666664,4.9067357512953365
363,75,Timo Schick,"[' We have devised PETAL, a simple approach that enriches PET with the ability to automatically map labels to words. Qualitative and quantitative analysis shows that our approach is able to identify words that are suitable to represent labels with as little as 50 examples and almost matches the performance of hand-crafted mappings for some tasks. For future work, it would be interesting to see whether the patterns required by PET can similarly be obtained in an automated fashion.']",conclusion_chunked," We have devised PETAL, a simple approach that enriches PET with the ability to automatically map labels to words. Qualitative and quantitative analysis shows that our approach is able to identify words that are suitable to represent labels with as little as 50 examples and almost matches the performance of hand-crafted mappings for some tasks. For future work, it would be interesting to see whether the patterns required by PET can similarly be obtained in an automated fashion.",32.83053957444222,32.83053957444222,363,0.700637698173523," We have devised PETAL, a simple approach that enriches Propname with the ability to automatically map labels to words. Qualitative and quantitative analysis shows that our approach is able to identify words that are suitable to represent labels with as little as 00 examples and almost matches the performance of hand crafted mappings for some tasks. For future work, it would be interesting to see whether the patterns required by Propname can similarly be obtained in an automated fashion."," We have devised PETAL, a simple approach that enriches Propname with the ability to automatically map labels to words. Qualitative and quantitative analysis shows that our approach is able to identify words that are suitable to represent labels with as little as 00 examples and almost matches the performance of hand crafted mappings for some tasks. For future work, it would be interesting to see whether the patterns required by Propname can similarly be obtained in an automated fashion.", PRON AUX VERB NOUN PUNCT DET ADJ NOUN PRON VERB PROPN ADP DET NOUN PART ADV VERB NOUN ADP NOUN PUNCT ADJ CCONJ ADJ NOUN VERB SCONJ PRON NOUN AUX ADJ PART VERB NOUN PRON AUX ADJ PART VERB NOUN ADP ADV ADJ ADP NUM NOUN CCONJ ADV VERB DET NOUN ADP NOUN VERB NOUN ADP DET NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX AUX ADJ PART VERB SCONJ DET NOUN VERB ADP PROPN AUX ADV AUX VERB ADP DET VERB NOUN PUNCT,0.7738095238095238,28.0,4.916666666666667
364,76,Timo Schick,"[' We have introduced DINO, a method for using large PLMs to generate entire datasets of labeled sentence pairs from scratch, requiring no labeled data and no parameter updates. This is achieved by providing instructions in natural language, combined with the self-debiasing method of Schick et al. (2021). With appropriate measures for handling noisy data, models trained on datasets generated with DINO achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with DINO can further be reduced, e.g., by using different sets of instructions (Jiang et al., 2020; Schick and Schütze, 2021a) or by supplementing our pipeline with some additional filtering steps.']",conclusion_chunked," We have introduced DINO, a method for using large PLMs to generate entire datasets of labeled sentence pairs from scratch, requiring no labeled data and no parameter updates. This is achieved by providing instructions in natural language, combined with the self-debiasing method of Schick et al. (2021). With appropriate measures for handling noisy data, models trained on datasets generated with DINO achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with DINO can further be reduced, e.g., by using different sets of instructions (Jiang et al., 2020; Schick and Schütze, 2021a) or by supplementing our pipeline with some additional filtering steps.",33.40707692307694,32.83053957444222,364,0.545721173286438," We have introduced Propname, a method for using large PLMs to generate entire datasets of labeled sentence pairs from scratch, requiring no labeled data and no parameter updates. This is achieved by providing instructions in natural language, combined with the self debiasing method of Propname Propname Propname.. With appropriate measures for handling noisy data, models trained on datasets generated with Propname achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with Propname can further be reduced, Propname, by using different sets of instructions or by supplementing our pipeline with some additional filtering steps."," We have introduced Propname, a method for using large PLMs to generate entire datasets of labeled sentence pairs from scratch, requiring no labeled data and no parameter updates. This is achieved by providing instructions in natural language, combined with the self debiasing method of Propname Propname Propname.. With appropriate measures for handling noisy data, models trained on datasets generated with Propname achieve strong results on several semantic textual similarity datasets. For future work, it would be interesting to see whether the noise in datasets generated with Propname can further be reduced, Propname, by using different sets of instructions or by supplementing our pipeline with some additional filtering steps.", PRON AUX VERB PROPN PUNCT DET NOUN ADP VERB ADJ NOUN PART VERB ADJ NOUN ADP VERB NOUN NOUN ADP NOUN PUNCT VERB DET VERB NOUN CCONJ DET NOUN NOUN PUNCT PRON AUX VERB ADP VERB NOUN ADP ADJ NOUN PUNCT VERB ADP DET NOUN NOUN NOUN ADP PROPN PROPN PROPN PUNCT PUNCT ADP ADJ NOUN ADP VERB ADJ NOUN PUNCT NOUN VERB ADP NOUN VERB ADP PROPN VERB ADJ NOUN ADP ADJ ADJ ADJ NOUN NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX AUX ADJ PART VERB SCONJ DET NOUN ADP NOUN VERB ADP PROPN AUX ADV AUX VERB PUNCT PROPN PUNCT ADP VERB ADJ NOUN ADP NOUN CCONJ ADP VERB PRON NOUN ADP DET ADJ ADJ NOUN PUNCT,0.6890756302521008,39.666666666666664,5.277310924369748
365,77,Timo Schick,"[' We have introduced WNLaMPro, a new dataset that allows us to explicitly investigate the ability of language models to understand rare words. Using this dataset, we have shown that BERT struggles with words if they are too rare. To address this problem, we proposed to apply Attentive Mimicking (AM). For AM to work, we introduced one-token approximation (OTA), an effective method to obtain “single-token” embeddings for multi-token words. Using this method, we showed that AM is able to substantially improve BERT’s understanding of rare words. Future work might investigate whether more complex architectures than AM can bring further benefit to deep language models; it would also be interesting to see whether training AM on a larger corpus – such as the one used for training BERT by Devlin et al. (2019) – is beneficial. Furthermore, it would be interesting to see the impact of integrating AM on downstream tasks.']",conclusion_chunked," We have introduced WNLaMPro, a new dataset that allows us to explicitly investigate the ability of language models to understand rare words. Using this dataset, we have shown that BERT struggles with words if they are too rare. To address this problem, we proposed to apply Attentive Mimicking (AM). For AM to work, we introduced one-token approximation (OTA), an effective method to obtain “single-token” embeddings for multi-token words. Using this method, we showed that AM is able to substantially improve BERT’s understanding of rare words. Future work might investigate whether more complex architectures than AM can bring further benefit to deep language models; it would also be interesting to see whether training AM on a larger corpus – such as the one used for training BERT by Devlin et al. (2019) – is beneficial. Furthermore, it would be interesting to see the impact of integrating AM on downstream tasks.",45.8701838235294,32.83053957444222,365,0.6275902390480042," We have introduced WNLaMPro, a new dataset that allows us to explicitly investigate the ability of language models to understand rare words. Using this dataset, we have shown that Propname struggles with words if they are too rare. To address this problem, we proposed to apply Propname Propname. For Propname to work, we introduced one token approximation, an effective method to obtain single token embeddings for multi token words. Using this method, we showed that Propname is able to substantially improve BERTs understanding of rare words. Future work might investigate whether more complex architectures than Propname can bring further benefit to deep language models; it would also be interesting to see whether training Propname on a larger corpus such as the one used for training Propname by Propname Propname Propname. is beneficial. Furthermore, it would be interesting to see the impact of integrating Propname on downstream tasks."," We have introduced WNLaMPro, a new dataset that allows us to explicitly investigate the ability of language models to understand rare words. Using this dataset, we have shown that Propname struggles with words if they are too rare. To address this problem, we proposed to apply Propname Propname. For Propname to work, we introduced one token approximation, an effective method to obtain single token embeddings for multi token words. Using this method, we showed that Propname is able to substantially improve BERTs understanding of rare words. Future work might investigate whether more complex architectures than Propname can bring further benefit to deep language models; it would also be interesting to see whether training Propname on a larger corpus such as the one used for training Propname by Propname Propname Propname. is beneficial. Furthermore, it would be interesting to see the impact of integrating Propname on downstream tasks.", PRON AUX VERB ADJ PUNCT DET ADJ NOUN PRON VERB PRON PART ADV VERB DET NOUN ADP NOUN NOUN PART VERB ADJ NOUN PUNCT VERB DET NOUN PUNCT PRON AUX VERB SCONJ PROPN VERB ADP NOUN SCONJ PRON AUX ADV ADJ PUNCT PART VERB DET NOUN PUNCT PRON VERB PART VERB PROPN PROPN PUNCT SCONJ PROPN PART VERB PUNCT PRON VERB NUM NOUN NOUN PUNCT DET ADJ NOUN PART VERB ADJ ADJ NOUN ADP ADJ ADJ NOUN PUNCT VERB DET NOUN PUNCT PRON VERB SCONJ PROPN AUX ADJ PART ADV VERB NOUN NOUN ADP ADJ NOUN PUNCT ADJ NOUN AUX VERB SCONJ ADV ADJ NOUN SCONJ PROPN AUX VERB ADJ NOUN ADP ADJ NOUN NOUN PUNCT PRON AUX ADV AUX ADJ PART VERB SCONJ VERB PROPN ADP DET ADJ NOUN ADJ ADP DET NOUN VERB ADP VERB PROPN ADP PROPN PROPN PROPN PUNCT AUX ADJ PUNCT ADV PUNCT PRON AUX AUX ADJ PART VERB DET NOUN ADP VERB PROPN ADP ADJ NOUN PUNCT,0.5644171779141104,20.375,4.901840490797546
366,78,Timo Schick,"[' We have shown how Pattern-Exploiting Training (PET) can be transferred to text generation tasks by (i) introducing the concept of decoder prefixes and (ii) combining patterns through knowledge distillation where target sequences are generated from randomly chosen patterns. With these modifications, a pretrained PEGASUS model finetuned with PET clearly outperforms regular finetuning in few-shot settings. For future work, it would be interesting to see whether it is also possible to make pretrained language models understand more complex task descriptions than the simple prompts we have used, especially as some concurrent work (Weller et al., 2020; Efrat and Levy, 2020) suggests this might not be the case.']",conclusion_chunked," We have shown how Pattern-Exploiting Training (PET) can be transferred to text generation tasks by (i) introducing the concept of decoder prefixes and (ii) combining patterns through knowledge distillation where target sequences are generated from randomly chosen patterns. With these modifications, a pretrained PEGASUS model finetuned with PET clearly outperforms regular finetuning in few-shot settings. For future work, it would be interesting to see whether it is also possible to make pretrained language models understand more complex task descriptions than the simple prompts we have used, especially as some concurrent work (Weller et al., 2020; Efrat and Levy, 2020) suggests this might not be the case.",21.72449685534596,32.83053957444222,366,0.2857867479324341," We have shown how Propname Exploiting Propname can be transferred to text generation tasks by introducing the concept of decoder prefixes and combining patterns through knowledge distillation where target sequences are generated from randomly chosen patterns. With these modifications, a pretrained PEGASUS model finetuned with Propname clearly outperforms regular finetuning in few shot settings. For future work, it would be interesting to see whether it is also possible to make pretrained language models understand more complex task descriptions than the simple prompts we have used, especially as some concurrent work suggests this might not be the case."," We have shown how Propname Exploiting Propname can be transferred to text generation tasks by introducing the concept of decoder prefixes and combining patterns through knowledge distillation where target sequences are generated from randomly chosen patterns. With these modifications, a pretrained PEGASUS model finetuned with Propname clearly outperforms regular finetuning in few shot settings. For future work, it would be interesting to see whether it is also possible to make pretrained language models understand more complex task descriptions than the simple prompts we have used, especially as some concurrent work suggests this might not be the case.", PRON AUX VERB SCONJ PROPN VERB PROPN AUX AUX VERB ADP NOUN NOUN NOUN ADP VERB DET NOUN ADP NOUN NOUN CCONJ VERB NOUN ADP NOUN NOUN SCONJ NOUN NOUN AUX VERB ADP ADV VERB NOUN PUNCT ADP DET NOUN PUNCT DET VERB ADJ NOUN VERB ADP PROPN ADV VERB ADJ NOUN ADP ADJ NOUN NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX AUX ADJ PART VERB SCONJ PRON AUX ADV ADJ PART VERB VERB NOUN NOUN VERB ADV ADJ NOUN NOUN ADP DET ADJ NOUN PRON AUX VERB PUNCT ADV SCONJ DET ADJ NOUN VERB PRON AUX PART AUX DET NOUN PUNCT,0.8349514563106796,34.333333333333336,5.475728155339806
367,79,Timo Schick,"[' We investigated the ability of pretrained language models to make use of simple instructions with the aim of enabling more data-efficient text generation. We identified three major challenges: enabling language models to make good use of the instructions provided, ensuring that the instructions are useful and preventing overfitting. We tackle these in our proposed approach, GENPET, by (i) introducing the concept of decoder prefixes, (ii) combining instructions through knowledge distillation where target sequences are generated with probabilistically sampled instructions and (iii) making use of unsupervised scoring and joint training. A pretrained PEGASUS model finetuned with GENPET clearly outperforms regular finetuning in few-shot settings.']",conclusion_chunked," We investigated the ability of pretrained language models to make use of simple instructions with the aim of enabling more data-efficient text generation. We identified three major challenges: enabling language models to make good use of the instructions provided, ensuring that the instructions are useful and preventing overfitting. We tackle these in our proposed approach, GENPET, by (i) introducing the concept of decoder prefixes, (ii) combining instructions through knowledge distillation where target sequences are generated with probabilistically sampled instructions and (iii) making use of unsupervised scoring and joint training. A pretrained PEGASUS model finetuned with GENPET clearly outperforms regular finetuning in few-shot settings.",8.213313106796136,32.83053957444222,367,0.5642620921134949," We investigated the ability of pretrained language models to make use of simple instructions with the aim of enabling more data efficient text generation. We identified three major challenges: enabling language models to make good use of the instructions provided, ensuring that the instructions are useful and preventing overfitting. We tackle these in our proposed approach, Propname, by introducing the concept of decoder prefixes, combining instructions through knowledge distillation where target sequences are generated with probabilistically sampled instructions and making use of unsupervised scoring and joint training. A pretrained PEGASUS model finetuned with GENPET clearly outperforms regular finetuning in few shot settings."," We investigated the ability of pretrained language models to make use of simple instructions with the aim of enabling more data efficient text generation. We identified three major challenges: enabling language models to make good use of the instructions provided, ensuring that the instructions are useful and preventing overfitting. We tackle these in our proposed approach, Propname, by introducing the concept of decoder prefixes, combining instructions through knowledge distillation where target sequences are generated with probabilistically sampled instructions and making use of unsupervised scoring and joint training. A pretrained PEGASUS model finetuned with GENPET clearly outperforms regular finetuning in few shot settings.", PRON VERB DET NOUN ADP VERB NOUN NOUN PART VERB NOUN ADP ADJ NOUN ADP DET NOUN ADP VERB ADJ NOUN ADJ NOUN NOUN PUNCT PRON VERB NUM ADJ NOUN PUNCT VERB NOUN NOUN PART VERB ADJ NOUN ADP DET NOUN VERB PUNCT VERB SCONJ DET NOUN AUX ADJ CCONJ VERB NOUN PUNCT PRON VERB PRON ADP PRON VERB NOUN PUNCT PROPN PUNCT ADP VERB DET NOUN ADP NOUN NOUN PUNCT VERB NOUN ADP NOUN NOUN SCONJ NOUN NOUN AUX VERB ADP ADV VERB NOUN CCONJ VERB NOUN ADP ADJ NOUN CCONJ ADJ NOUN PUNCT DET VERB ADJ NOUN VERB ADP NOUN ADV VERB ADJ NOUN ADP ADJ NOUN NOUN PUNCT,0.6846846846846847,27.75,5.738738738738738
368,80,Timo Schick,"[' We have introduced PEER, a language model that can act as a writing assistant by following plans to perform a variety of different textual edits, ranging from syntactic and stylistic edits to changing the meaning of a text by removing, updating or adding information. Through extensive experiments, we have shown that training variants of PEER capable of infilling various parts of the editing process enables it to perform edits in different domains, makes it better at following instructions and improves its ability to cite and quote from relevant documents.']",conclusion_chunked," We have introduced PEER, a language model that can act as a writing assistant by following plans to perform a variety of different textual edits, ranging from syntactic and stylistic edits to changing the meaning of a text by removing, updating or adding information. Through extensive experiments, we have shown that training variants of PEER capable of infilling various parts of the editing process enables it to perform edits in different domains, makes it better at following instructions and improves its ability to cite and quote from relevant documents.",32.83053957444222,32.83053957444222,368,0.6467500329017639," We have introduced Propname, a language model that can act as a writing assistant by following plans to perform a variety of different textual edits, ranging from syntactic and stylistic edits to changing the meaning of a text by removing, updating or adding information. Through extensive experiments, we have shown that training variants of PEER capable of infilling various parts of the editing process enables it to perform edits in different domains, makes it better at following instructions and improves its ability to cite and quote from relevant documents."," We have introduced Propname, a language model that can act as a writing assistant by following plans to perform a variety of different textual edits, ranging from syntactic and stylistic edits to changing the meaning of a text by removing, updating or adding information. Through extensive experiments, we have shown that training variants of PEER capable of infilling various parts of the editing process enables it to perform edits in different domains, makes it better at following instructions and improves its ability to cite and quote from relevant documents.", PRON AUX VERB PROPN PUNCT DET NOUN NOUN PRON AUX VERB ADP DET NOUN NOUN ADP VERB NOUN PART VERB DET NOUN ADP ADJ ADJ NOUN PUNCT VERB ADP ADJ CCONJ ADJ NOUN ADP VERB DET NOUN ADP DET NOUN ADP VERB PUNCT VERB CCONJ VERB NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX VERB SCONJ NOUN NOUN ADP ADJ ADJ ADP VERB ADJ NOUN ADP DET NOUN NOUN VERB PRON PART VERB NOUN ADP ADJ NOUN PUNCT VERB PRON ADJ ADP VERB NOUN CCONJ VERB PRON NOUN PART VERB CCONJ INTJ ADP ADJ NOUN PUNCT,0.7083333333333334,48.0,4.96875
369,81,Timo Schick,"[' We have introduced attentive mimicking (AM) and showed that attending to informative and reliable contexts improves representations of rare and medium-frequency words for a diverse set of evaluations. In future work, one might investigate whether attention mechanisms on the word level (cf. Ling et al., 2015) can further improve the model’s performance. Furthermore, it would be interesting to investigate whether the proposed architecture is also beneficial for languages typologically different from English, e.g., morphologically rich languages.']",conclusion_chunked," We have introduced attentive mimicking (AM) and showed that attending to informative and reliable contexts improves representations of rare and medium-frequency words for a diverse set of evaluations. In future work, one might investigate whether attention mechanisms on the word level (cf. Ling et al., 2015) can further improve the model’s performance. Furthermore, it would be interesting to investigate whether the proposed architecture is also beneficial for languages typologically different from English, e.g., morphologically rich languages.",32.83053957444222,32.83053957444222,369,0.628900408744812," We have introduced attentive mimicking and showed that attending to informative and reliable contexts improves representations of rare and medium frequency words for a diverse set of evaluations. In future work, one might investigate whether attention mechanisms on the word level can further improve the models performance. Furthermore, it would be interesting to investigate whether the proposed architecture is also beneficial for languages typologically different from Propname, Propname, morphologically rich languages."," We have introduced attentive mimicking and showed that attending to informative and reliable contexts improves representations of rare and medium frequency words for a diverse set of evaluations. In future work, one might investigate whether attention mechanisms on the word level can further improve the models performance. Furthermore, it would be interesting to investigate whether the proposed architecture is also beneficial for languages typologically different from Propname, Propname, morphologically rich languages.", PRON AUX VERB ADJ NOUN CCONJ VERB SCONJ VERB PART VERB CCONJ ADJ NOUN VERB NOUN ADP ADJ CCONJ ADJ ADJ NOUN ADP DET ADJ NOUN ADP NOUN PUNCT ADP ADJ NOUN PUNCT PRON AUX VERB SCONJ NOUN NOUN ADP DET NOUN NOUN AUX ADV VERB DET NOUN NOUN PUNCT ADV PUNCT PRON AUX AUX ADJ PART VERB SCONJ DET VERB NOUN AUX ADV ADJ ADP NOUN ADV ADJ ADP PROPN PUNCT PROPN PUNCT ADV ADJ NOUN PUNCT,0.7948717948717948,26.0,5.82051282051282
370,82,Timo Schick,"[' We have introduced BERTRAM, a novel architecture for inducing high-quality representations for rare words in BERT’s and RoBERTa’s embedding spaces. This is achieved by employing a powerful pretrained language model and deeply integrating surface-form and context information. By replacing important words with rare synonyms, we created downstream task datasets that are more challenging and support the evaluation of NLP models on the task of understanding rare words, a capability that human speakers have. On all of these datasets, BERTRAM improves over standard BERT and RoBERTa, demonstrating the usefulness of our method. Our analysis showed that BERTRAM is beneficial not only for rare words (our main target in this paper), but also for frequent words. In future work, we want to investigate BERTRAM’s potential benefits for such frequent words. Furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level CNN similar to the one of Kim et al. (2016) – to balance out the potency of BERTRAM’s form and context parts.']",conclusion_chunked," We have introduced BERTRAM, a novel architecture for inducing high-quality representations for rare words in BERT’s and RoBERTa’s embedding spaces. This is achieved by employing a powerful pretrained language model and deeply integrating surface-form and context information. By replacing important words with rare synonyms, we created downstream task datasets that are more challenging and support the evaluation of NLP models on the task of understanding rare words, a capability that human speakers have. On all of these datasets, BERTRAM improves over standard BERT and RoBERTa, demonstrating the usefulness of our method. Our analysis showed that BERTRAM is beneficial not only for rare words (our main target in this paper), but also for frequent words. In future work, we want to investigate BERTRAM’s potential benefits for such frequent words. Furthermore, it would be interesting to explore more complex ways of incorporating surface-form information – e.g., by using a character-level CNN similar to the one of Kim et al. (2016) – to balance out the potency of BERTRAM’s form and context parts.",36.192531424581034,32.83053957444222,370,0.7440765500068665," We have introduced Propname, a novel architecture for inducing high quality representations for rare words in BERTs and RoBERTas embedding spaces. This is achieved by employing a powerful pretrained language model and deeply integrating surface form and context information. By replacing important words with rare synonyms, we created downstream task datasets that are more challenging and support the evaluation of Propname models on the task of understanding rare words, a capability that human speakers have. On all of these datasets, Propname improves over standard Propname and Propname, demonstrating the usefulness of our method. Our analysis showed that Propname is beneficial not only for rare words, but also for frequent words. In future work, we want to investigate BERTRAMs potential benefits for such frequent words. Furthermore, it would be interesting to explore more complex ways of incorporating surface form Propname Propname, by using a character level Propname similar to the one of Propname Propname Propname. to balance out the potency of BERTRAMs form and context parts."," We have introduced Propname, a novel architecture for inducing high quality representations for rare words in BERTs and RoBERTas embedding spaces. This is achieved by employing a powerful pretrained language model and deeply integrating surface form and context information. By replacing important words with rare synonyms, we created downstream task datasets that are more challenging and support the evaluation of Propname models on the task of understanding rare words, a capability that human speakers have. On all of these datasets, Propname improves over standard Propname and Propname, demonstrating the usefulness of our method. Our analysis showed that Propname is beneficial not only for rare words, but also for frequent words. In future work, we want to investigate BERTRAMs potential benefits for such frequent words. Furthermore, it would be interesting to explore more complex ways of incorporating surface form Propname Propname, by using a character level Propname similar to the one of Propname Propname Propname. to balance out the potency of BERTRAMs form and context parts.", PRON AUX VERB PROPN PUNCT DET ADJ NOUN ADP VERB ADJ NOUN NOUN ADP ADJ NOUN ADP NOUN CCONJ NOUN VERB NOUN PUNCT PRON AUX VERB ADP VERB DET ADJ VERB NOUN NOUN CCONJ ADV VERB NOUN NOUN CCONJ NOUN NOUN PUNCT ADP VERB ADJ NOUN ADP ADJ NOUN PUNCT PRON VERB ADJ NOUN NOUN PRON AUX ADV ADJ CCONJ VERB DET NOUN ADP PROPN NOUN ADP DET NOUN ADP VERB ADJ NOUN PUNCT DET NOUN PRON ADJ NOUN VERB PUNCT ADP PRON ADP DET NOUN PUNCT PROPN VERB ADP ADJ PROPN CCONJ PROPN PUNCT VERB DET NOUN ADP PRON NOUN PUNCT PRON NOUN VERB SCONJ PROPN AUX ADJ PART ADV ADP ADJ NOUN PUNCT CCONJ ADV ADP ADJ NOUN PUNCT ADP ADJ NOUN PUNCT PRON VERB PART VERB NOUN ADJ NOUN ADP ADJ ADJ NOUN PUNCT ADV PUNCT PRON AUX AUX ADJ PART VERB ADV ADJ NOUN ADP VERB NOUN NOUN PROPN PROPN PUNCT ADP VERB DET NOUN NOUN PROPN ADJ ADP DET NUM ADP PROPN PROPN PROPN PUNCT PART VERB ADP DET NOUN ADP NOUN NOUN CCONJ NOUN NOUN PUNCT,0.5934065934065934,22.75,5.1098901098901095
371,83,Timo Schick,"[' In light of recent work casting doubt on the performance of prompt-based approaches in true few-shot settings (Perez et al., 2021), we have conducted an extensive study of PET. In a controlled environment, we found that manually designed instructions consistently outperform null prompts, with Q&Astyle prompts performing best (Q1, Q2). Across different tasks, models and training set sizes, PET consistently outperforms even the best individual prompt (Q1, Q2). We have also shown that PET is robust to uninformative prompts and to different choices of hyperparameters (Q3, Q5), that as little as four prompts are sufficient to reach good performance (Q4), and that synthetic examples can be used to replace large amounts of unlabeled data (Q6). On the basis of these insights, we applied PET to a benchmark of real-world tasks, where it achieves near-human performance for 7 out of 11 tasks without any tuning on a development set, demonstrating the power of instruction-based approaches in true few-shot settings.']",conclusion_chunked," In light of recent work casting doubt on the performance of prompt-based approaches in true few-shot settings (Perez et al., 2021), we have conducted an extensive study of PET. In a controlled environment, we found that manually designed instructions consistently outperform null prompts, with Q&Astyle prompts performing best (Q1, Q2). Across different tasks, models and training set sizes, PET consistently outperforms even the best individual prompt (Q1, Q2). We have also shown that PET is robust to uninformative prompts and to different choices of hyperparameters (Q3, Q5), that as little as four prompts are sufficient to reach good performance (Q4), and that synthetic examples can be used to replace large amounts of unlabeled data (Q6). On the basis of these insights, we applied PET to a benchmark of real-world tasks, where it achieves near-human performance for 7 out of 11 tasks without any tuning on a development set, demonstrating the power of instruction-based approaches in true few-shot settings.",35.15422641509434,32.83053957444222,371,0.44799739122390747," In light of recent work casting doubt on the performance of prompt based approaches in true few shot settings, we have conducted an extensive study of Propname. In a controlled environment, we found that manually designed instructions consistently outperform null prompts, with Propname prompts performing best. Across different tasks, models and training set sizes, Propname consistently outperforms even the best individual prompt. We have also shown that Propname is robust to uninformative prompts and to different choices of hyperparameters, that as little as four prompts are sufficient to reach good performance, and that synthetic examples can be used to replace large amounts of unlabeled data. On the basis of these insights, we applied Propname to a benchmark of real world tasks, where it achieves near human performance for 0 out of 00 tasks without any tuning on a development set, demonstrating the power of instruction based approaches in true few shot settings."," In light of recent work casting doubt on the performance of prompt based approaches in true few shot settings, we have conducted an extensive study of Propname. In a controlled environment, we found that manually designed instructions consistently outperform null prompts, with Propname prompts performing best. Across different tasks, models and training set sizes, Propname consistently outperforms even the best individual prompt. We have also shown that Propname is robust to uninformative prompts and to different choices of hyperparameters, that as little as four prompts are sufficient to reach good performance, and that synthetic examples can be used to replace large amounts of unlabeled data. On the basis of these insights, we applied Propname to a benchmark of real world tasks, where it achieves near human performance for 0 out of 00 tasks without any tuning on a development set, demonstrating the power of instruction based approaches in true few shot settings.", ADP NOUN ADP ADJ NOUN VERB NOUN ADP DET NOUN ADP NOUN VERB NOUN ADP ADJ ADJ NOUN NOUN PUNCT PRON AUX VERB DET ADJ NOUN ADP PROPN PUNCT ADP DET VERB NOUN PUNCT PRON VERB PRON ADV VERB NOUN ADV VERB ADJ NOUN PUNCT ADP PROPN NOUN VERB ADV PUNCT ADP ADJ NOUN PUNCT NOUN CCONJ NOUN VERB NOUN PUNCT PROPN ADV VERB ADV DET ADJ ADJ NOUN PUNCT PRON AUX ADV VERB SCONJ PROPN AUX ADJ ADP ADJ NOUN CCONJ ADP ADJ NOUN ADP NOUN PUNCT SCONJ ADV ADJ ADP NUM NOUN AUX ADJ PART VERB ADJ NOUN PUNCT CCONJ SCONJ ADJ NOUN AUX AUX VERB PART VERB ADJ NOUN ADP ADJ NOUN PUNCT ADP DET NOUN ADP DET NOUN PUNCT PRON VERB PROPN ADP DET NOUN ADP ADJ NOUN NOUN PUNCT SCONJ PRON VERB ADP ADJ NOUN ADP NUM ADP ADP NUM NOUN ADP DET NOUN ADP DET NOUN NOUN PUNCT VERB DET NOUN ADP NOUN VERB NOUN ADP ADJ ADJ NOUN NOUN PUNCT,0.6167664670658682,33.4,4.952095808383233
372,84,Timo Schick,"[' We have presented a model that is capable of inferring\nhigh-quality representations for novel words by processing\nboth the word’s internal structure and words in its context. This is done by intelligently combining an embedding based\non n-grams with an embedding obtained from averaging\nover all context words. Our algorithm can be trained from\nand combined with any preexisting word embedding model. On both the Definitional Nonce dataset and the Contextual Rare Words dataset, our model outperforms all previous ap-\nproaches to learning embeddings of rare words by a large\nmargin, even beating the embedding algorithm it was trained\nfrom on the latter dataset. Careful analysis of our combined\nmodel showed that in many cases, it is able to effectively\nbalance out the influences of both embeddings it is com-\nposed of, allowing it to greatly improve upon representations\nthat are either purely surface-form-based or purely context-\nbased. By providing a development set that complements the\nCRW dataset, we hope to further spur research in the area of\n“few-shot learning” for word embeddings. While we showed that a context-dependent combination\nof surface-form and context embeddings substantially im-\nproves the model’s performance on the Definitional Nonce\ntask, results on the Contextual Rare Words dataset indicate\nthat there is still room for further enhancement.', 'This could\npotentially be achieved by incorporating the number and in-\nformativeness of the available contexts into the composition\nfunction; i.e., the gate would not only be conditioned on\nthe embeddings, but on richer information about the context\nsentences. It would also be interesting to investigate whether\nour model profits from using more complex ways than aver-\naging to obtain surface-form and context embeddings, re-\nspectively. For example, one might introduce weights for\nn-grams and words depending on their contexts (i.e. the\nn-grams or words surrounding them). For scenarios in which\nnot just one, but multiple contexts are available to infer a\nword’s embedding, a promising extension of our model is\nto weight the influence of each context based on its “defi-\nnitional quality”; a similar modification was also proposed\nby Herbelot and Baroni (2017) for their Nonce2Vec model. Yet another interesting approach would be to integrate rela-\ntive position information into our model. This could be done\nsimilar to Shaw, Uszkoreit, and Vaswani (2018) by addition-\nally learning position embeddings and weighting the influ-\nence of context words based on those embeddings.']",conclusion_chunked," We have presented a model that is capable of inferring
high-quality representations for novel words by processing
both the word’s internal structure and words in its context. This is done by intelligently combining an embedding based
on n-grams with an embedding obtained from averaging
over all context words. Our algorithm can be trained from
and combined with any preexisting word embedding model. On both the Definitional Nonce dataset and the Contextual Rare Words dataset, our model outperforms all previous ap-
proaches to learning embeddings of rare words by a large
margin, even beating the embedding algorithm it was trained
from on the latter dataset. Careful analysis of our combined
model showed that in many cases, it is able to effectively
balance out the influences of both embeddings it is com-
posed of, allowing it to greatly improve upon representations
that are either purely surface-form-based or purely context-
based. By providing a development set that complements the
CRW dataset, we hope to further spur research in the area of
“few-shot learning” for word embeddings. While we showed that a context-dependent combination
of surface-form and context embeddings substantially im-
proves the model’s performance on the Definitional Nonce
task, results on the Contextual Rare Words dataset indicate
that there is still room for further enhancement.",24.26444954128442,32.83053957444222,372,0.4590963125228882," We have presented a model that is capable of inferring 
 high quality representations for novel words by processing 
 both the words internal structure and words in its context. This is done by intelligently combining an embedding based 
 on n grams with an embedding obtained from averaging 
 over all context words. Our algorithm can be trained from 
 and combined with any preexisting word embedding model. On both the Propname Propname dataset and the Contextual Propname Words dataset, our model outperforms all previous Propname proaches to learning embeddings of rare words by a large 
 margin, even beating the embedding Propname it was trained 
 from on the latter dataset. Careful analysis of our combined 
 model showed that in many cases, it is able to effectively 
 balance out the influences of both embeddings it is com posed of, allowing it to greatly improve upon representations 
 that are either purely surface form based or purely context based. By providing a development set that complements the 
 CRW dataset, we hope to further spur research in the area of 
 few shot learning for word embeddings. While we showed that a context dependent combination 
 of surface form and context embeddings substantially i m proves the models performance on the Propname Propname 
 task, results on the Contextual Propname Words dataset indicate 
 that there is still room for further enhancement."," We have presented a model that is capable of inferring 
 high quality representations for novel words by processing 
 both the words internal structure and words in its context. This is done by intelligently combining an embedding based 
 on n grams with an embedding obtained from averaging 
 over all context words. Our algorithm can be trained from 
 and combined with any preexisting word embedding model. On both the Propname Propname dataset and the Contextual Propname Words dataset, our model outperforms all previous Propname proaches to learning embeddings of rare words by a large 
 margin, even beating the embedding Propname it was trained 
 from on the latter dataset. Careful analysis of our combined 
 model showed that in many cases, it is able to effectively 
 balance out the influences of both embeddings it is com posed of, allowing it to greatly improve upon representations 
 that are either purely surface form based or purely context based. By providing a development set that complements the 
 CRW dataset, we hope to further spur research in the area of 
 few shot learning for word embeddings. While we showed that a context dependent combination 
 of surface form and context embeddings substantially i m proves the models performance on the Propname Propname 
 task, results on the Contextual Propname Words dataset indicate 
 that there is still room for further enhancement.", PRON AUX VERB DET NOUN PRON AUX ADJ ADP VERB SPACE ADJ NOUN NOUN ADP ADJ NOUN ADP VERB SPACE CCONJ DET NOUN ADJ NOUN CCONJ NOUN ADP PRON NOUN PUNCT PRON AUX VERB ADP ADV VERB DET VERB VERB SPACE ADP NOUN NOUN ADP DET VERB VERB ADP VERB SPACE ADP DET NOUN NOUN PUNCT PRON NOUN AUX AUX VERB ADP SPACE CCONJ VERB ADP DET VERB NOUN VERB NOUN PUNCT ADP PRON DET PROPN PROPN NOUN CCONJ DET ADJ PROPN NOUN NOUN PUNCT PRON NOUN VERB DET ADJ PROPN NOUN ADP VERB NOUN ADP ADJ NOUN ADP DET ADJ SPACE NOUN PUNCT ADV VERB DET VERB PROPN PRON AUX VERB SPACE ADP ADP DET ADJ NOUN PUNCT ADJ NOUN ADP PRON VERB SPACE NOUN VERB SCONJ ADP ADJ NOUN PUNCT PRON AUX ADJ PART ADV SPACE VERB ADP DET NOUN ADP DET NOUN PRON AUX NOUN VERB ADP PUNCT VERB PRON PART ADV VERB SCONJ NOUN SPACE PRON AUX CCONJ ADV NOUN NOUN VERB CCONJ ADV NOUN VERB PUNCT ADP VERB DET NOUN VERB PRON VERB DET SPACE NOUN NOUN PUNCT PRON VERB PART ADV VERB NOUN ADP DET NOUN ADP SPACE ADJ NOUN VERB ADP NOUN NOUN PUNCT SCONJ PRON VERB SCONJ DET NOUN ADJ NOUN SPACE ADP NOUN NOUN CCONJ NOUN NOUN ADV PRON AUX VERB DET NOUN NOUN ADP DET PROPN PROPN SPACE NOUN PUNCT NOUN ADP DET ADJ PROPN NOUN NOUN VERB SPACE SCONJ PRON VERB ADV NOUN ADP ADJ NOUN PUNCT,0.5364806866952789,33.285714285714285,4.965665236051502
373,85,Timo Schick,"[' We have presented a model that is capable of inferring\nhigh-quality representations for novel words by processing\nboth the word’s internal structure and words in its context. This is done by intelligently combining an embedding based\non n-grams with an embedding obtained from averaging\nover all context words. Our algorithm can be trained from\nand combined with any preexisting word embedding model. On both the Definitional Nonce dataset and the Contextual Rare Words dataset, our model outperforms all previous ap-\nproaches to learning embeddings of rare words by a large\nmargin, even beating the embedding algorithm it was trained\nfrom on the latter dataset. Careful analysis of our combined\nmodel showed that in many cases, it is able to effectively\nbalance out the influences of both embeddings it is com-\nposed of, allowing it to greatly improve upon representations\nthat are either purely surface-form-based or purely context-\nbased. By providing a development set that complements the\nCRW dataset, we hope to further spur research in the area of\n“few-shot learning” for word embeddings. While we showed that a context-dependent combination\nof surface-form and context embeddings substantially im-\nproves the model’s performance on the Definitional Nonce\ntask, results on the Contextual Rare Words dataset indicate\nthat there is still room for further enhancement.', 'This could\npotentially be achieved by incorporating the number and in-\nformativeness of the available contexts into the composition\nfunction; i.e., the gate would not only be conditioned on\nthe embeddings, but on richer information about the context\nsentences. It would also be interesting to investigate whether\nour model profits from using more complex ways than aver-\naging to obtain surface-form and context embeddings, re-\nspectively. For example, one might introduce weights for\nn-grams and words depending on their contexts (i.e. the\nn-grams or words surrounding them). For scenarios in which\nnot just one, but multiple contexts are available to infer a\nword’s embedding, a promising extension of our model is\nto weight the influence of each context based on its “defi-\nnitional quality”; a similar modification was also proposed\nby Herbelot and Baroni (2017) for their Nonce2Vec model. Yet another interesting approach would be to integrate rela-\ntive position information into our model. This could be done\nsimilar to Shaw, Uszkoreit, and Vaswani (2018) by addition-\nally learning position embeddings and weighting the influ-\nence of context words based on those embeddings.']",conclusion_chunked,"This could
potentially be achieved by incorporating the number and in-
formativeness of the available contexts into the composition
function; i.e., the gate would not only be conditioned on
the embeddings, but on richer information about the context
sentences. It would also be interesting to investigate whether
our model profits from using more complex ways than aver-
aging to obtain surface-form and context embeddings, re-
spectively. For example, one might introduce weights for
n-grams and words depending on their contexts (i.e. the
n-grams or words surrounding them). For scenarios in which
not just one, but multiple contexts are available to infer a
word’s embedding, a promising extension of our model is
to weight the influence of each context based on its “defi-
nitional quality”; a similar modification was also proposed
by Herbelot and Baroni (2017) for their Nonce2Vec model. Yet another interesting approach would be to integrate rela-
tive position information into our model. This could be done
similar to Shaw, Uszkoreit, and Vaswani (2018) by addition-
ally learning position embeddings and weighting the influ-
ence of context words based on those embeddings.",31.902894736842143,32.83053957444222,373,0.3621578514575958," This could 
 potentially be achieved by incorporating the number and in formativeness of the available contexts into the composition 
 function; ie, the gate would not only be conditioned on 
 the embeddings, but on richer information about the context 
 sentences. It would also be interesting to investigate whether 
 our model profits from using more complex ways than aver aging to obtain surface form and context embeddings, re spectively. For example, one might introduce weights for 
 Propname grams and words depending on their contexts ie the 
 Propname grams or words surrounding them. For scenarios in which 
 not just one, but multiple contexts are available to infer a 
 words embedding, a promising extension of our model is 
 to weight the influence of each context based on its defi nitional quality; a similar modification was also proposed 
 by Propname and Propname for their Propname model. Yet another interesting approach would be to integrate Propname tive position information into our model. This could be done 
 similar to Propname, Propname, and Propname by addition ally learning position embeddings and weighting the influ ence of context words based on those embeddings."," This could 
 potentially be achieved by incorporating the number and in formativeness of the available contexts into the composition 
 function; ie, the gate would not only be conditioned on 
 the embeddings, but on richer information about the context 
 sentences. It would also be interesting to investigate whether 
 our model profits from using more complex ways than aver aging to obtain surface form and context embeddings, re spectively. For example, one might introduce weights for 
 Propname grams and words depending on their contexts ie the 
 Propname grams or words surrounding them. For scenarios in which 
 not just one, but multiple contexts are available to infer a 
 words embedding, a promising extension of our model is 
 to weight the influence of each context based on its defi nitional quality; a similar modification was also proposed 
 by Propname and Propname for their Propname model. Yet another interesting approach would be to integrate Propname tive position information into our model. This could be done 
 similar to Propname, Propname, and Propname by addition ally learning position embeddings and weighting the influ ence of context words based on those embeddings.", PRON AUX SPACE ADV AUX VERB ADP VERB DET NOUN CCONJ ADP NOUN ADP DET ADJ NOUN ADP DET NOUN SPACE NOUN PUNCT ADV PUNCT DET NOUN AUX PART ADV AUX VERB ADP SPACE DET NOUN PUNCT CCONJ ADP ADJ NOUN ADP DET NOUN SPACE NOUN PUNCT PRON AUX ADV AUX ADJ PART VERB SCONJ SPACE PRON NOUN NOUN ADP VERB ADJ ADJ NOUN ADP NOUN NOUN PART VERB NOUN NOUN CCONJ NOUN NOUN PUNCT ADP ADV PUNCT ADP NOUN PUNCT PRON AUX VERB NOUN ADP SPACE PROPN NOUN CCONJ NOUN VERB ADP PRON NOUN ADP DET SPACE PROPN NOUN CCONJ NOUN VERB PRON PUNCT ADP NOUN ADP PRON SPACE PART ADV NUM PUNCT CCONJ ADJ NOUN AUX ADJ PART VERB DET SPACE NOUN VERB PUNCT DET ADJ NOUN ADP PRON NOUN AUX SPACE PART VERB DET NOUN ADP DET NOUN VERB ADP PRON ADJ ADJ NOUN PUNCT DET ADJ NOUN AUX ADV VERB SPACE ADP PROPN CCONJ PROPN ADP PRON PROPN NOUN PUNCT ADV DET ADJ NOUN AUX AUX PART VERB PROPN NOUN NOUN NOUN ADP PRON NOUN PUNCT PRON AUX AUX VERB SPACE ADJ ADP PROPN PUNCT PROPN PUNCT CCONJ PROPN ADP NOUN NOUN VERB NOUN NOUN CCONJ VERB DET ADJ NOUN ADP NOUN NOUN VERB ADP DET NOUN PUNCT,0.545,33.333333333333336,4.965
374,86,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked," We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences
can easily be inferred through application of the yield function. We chose the principle
component of our approach to be the transition system SAMR, whose set of transitions
TAMR defines how the transformation from AMR graphs to suitable trees can be per-
formed. Some transitions contained within this set, such as Merge, Swap and Delete,
have an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and
defined the score of a transition sequence to be a linear combination of the probabilities
of all its transitions and the probability assigned to the resulting sentence by a language
model. We approximated these probabilities using maximum entropy models that were
trained with a set of gold transitions extracted from a large corpus of AMR graphs and
corresponding realizations.",36.06432584269663,32.83053957444222,374,0.628487765789032," We have devised a novel approach for the challenging task of Propname to text generation. Our core idea was to turn input Propname graphs into ordered trees from which sentences 
 can easily be inferred through application of the yield function. We chose the principle 
 component of our approach to be the transition system SAMR, whose set of transitions 
 TAMR defines how the transformation from Propname graphs to suitable trees can be per formed. Some transitions contained within this set, such as Propname, Propname and Propname, 
 have an equivalent in the likewise transition based text to Propname parser by Propname Propname Propname., which served as a model for our approach. In order to turn Propname into a generator, we assigned probabilities to transitions and 
 defined the score of a transition sequence to be a linear combination of the probabilities 
 of all its transitions and the probability assigned to the resulting sentence by a language 
 model. We approximated these probabilities using maximum entropy models that were 
 trained with a set of gold transitions extracted from a large corpus of Propname graphs and 
 corresponding realizations."," We have devised a novel approach for the challenging task of Propname to text generation. Our core idea was to turn input Propname graphs into ordered trees from which sentences 
 can easily be inferred through application of the yield function. We chose the principle 
 component of our approach to be the transition system SAMR, whose set of transitions 
 TAMR defines how the transformation from Propname graphs to suitable trees can be per formed. Some transitions contained within this set, such as Propname, Propname and Propname, 
 have an equivalent in the likewise transition based text to Propname parser by Propname Propname Propname., which served as a model for our approach. In order to turn Propname into a generator, we assigned probabilities to transitions and 
 defined the score of a transition sequence to be a linear combination of the probabilities 
 of all its transitions and the probability assigned to the resulting sentence by a language 
 model. We approximated these probabilities using maximum entropy models that were 
 trained with a set of gold transitions extracted from a large corpus of Propname graphs and 
 corresponding realizations.", PRON AUX VERB DET ADJ NOUN ADP DET ADJ NOUN ADP PROPN PART VERB NOUN PUNCT PRON NOUN NOUN AUX PART VERB NOUN PROPN NOUN ADP VERB NOUN ADP PRON NOUN SPACE AUX ADV AUX VERB ADP NOUN ADP DET NOUN NOUN PUNCT PRON VERB DET ADJ SPACE NOUN ADP PRON NOUN PART AUX DET NOUN NOUN VERB PUNCT DET NOUN ADP NOUN SPACE NOUN VERB SCONJ DET NOUN ADP PROPN NOUN ADP ADJ NOUN AUX AUX SCONJ VERB PUNCT DET NOUN VERB ADP DET NOUN PUNCT ADJ ADP PROPN PUNCT PROPN CCONJ PROPN PUNCT SPACE VERB DET NOUN ADP DET ADJ NOUN VERB NOUN ADP PROPN NOUN ADP PROPN PROPN PROPN PUNCT PUNCT PRON VERB ADP DET NOUN ADP PRON NOUN PUNCT ADP NOUN PART VERB PROPN ADP DET NOUN PUNCT PRON VERB NOUN ADP NOUN CCONJ SPACE VERB DET NOUN ADP DET NOUN NOUN PART AUX DET ADJ NOUN ADP DET NOUN SPACE ADP DET PRON NOUN CCONJ DET NOUN VERB ADP DET VERB NOUN ADP DET NOUN SPACE NOUN PUNCT PRON VERB DET NOUN VERB ADJ NOUN NOUN PRON AUX SPACE VERB ADP DET NOUN ADP NOUN NOUN VERB ADP DET ADJ NOUN ADP PROPN NOUN CCONJ SPACE VERB NOUN PUNCT,0.5412371134020618,32.333333333333336,5.015463917525773
375,87,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked,"As an exhaustive search for the highest-scoring transition
sequence given some input would be far too time-consuming, we developed an algorithm
that approximates this sequence in two phases: In a first phase, only transitions from
a subset Trestr of TAMR are greedily applied without taking the language model into
consideration; in a second phase, the output of this first phase is processed bottom-
up, considering multiple partial transition sequences at each step and factoring in the
language model. Through parametrized pruning, we restricted the number of sequences
to be considered, allowing us to find a good balance between required time and quality
of the generated sentences. We introduced the concepts of syntactic annotations and
default realizations to help our system decide which transition to apply next. To further
improve our results, we defined some postprocessing steps – such as the insertion of
punctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we
obtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,
the second best result reported so far and the best without using parsed sentences
from an external source such as Gigaword (LDC2011T07) as additional training data.",34.921247037914725,32.83053957444222,375,0.4942128658294678," As an exhaustive search for the highest scoring transition 
 sequence given some input would be far too time consuming, we developed an Propname 
 that approximates this sequence in two phases: In a first phase, only transitions from 
 a subset Propname of Propname are greedily applied without taking the language model into 
 consideration; in a second phase, the output of this first phase is processed bottom up, considering multiple partial transition sequences at each step and factoring in the 
 language model. Through parametrized pruning, we restricted the number of sequences 
 to be considered, allowing us to find a good balance between required time and quality 
 of the generated sentences. We introduced the concepts of syntactic annotations and 
 default realizations to help our system decide which transition to apply next. To further 
 improve our results, we defined some postprocessing steps such as the insertion of 
 punctuation marks to revise the tree structure obtained from our transition system. In experiments carried out using a Propname based implementation of our generator, we 
 obtained a lower cased 0... 0 gram Propname score of 00.0 on the LDC0000T00 test set, 
 the second best result reported so far and the best without using parsed sentences 
 from an external source such as Propname as additional training data."," As an exhaustive search for the highest scoring transition 
 sequence given some input would be far too time consuming, we developed an Propname 
 that approximates this sequence in two phases: In a first phase, only transitions from 
 a subset Propname of Propname are greedily applied without taking the language model into 
 consideration; in a second phase, the output of this first phase is processed bottom up, considering multiple partial transition sequences at each step and factoring in the 
 language model. Through parametrized pruning, we restricted the number of sequences 
 to be considered, allowing us to find a good balance between required time and quality 
 of the generated sentences. We introduced the concepts of syntactic annotations and 
 default realizations to help our system decide which transition to apply next. To further 
 improve our results, we defined some postprocessing steps such as the insertion of 
 punctuation marks to revise the tree structure obtained from our transition system. In experiments carried out using a Propname based implementation of our generator, we 
 obtained a lower cased 0... 0 gram Propname score of 00.0 on the LDC0000T00 test set, 
 the second best result reported so far and the best without using parsed sentences 
 from an external source such as Propname as additional training data.", ADP DET ADJ NOUN ADP DET ADJ NOUN NOUN SPACE NOUN VERB DET NOUN AUX AUX ADV ADV NOUN VERB PUNCT PRON VERB DET PROPN SPACE PRON VERB DET NOUN ADP NUM NOUN PUNCT ADP DET ADJ NOUN PUNCT ADV NOUN ADP SPACE DET NOUN PROPN ADP PROPN AUX ADV VERB ADP VERB DET NOUN NOUN ADP SPACE NOUN PUNCT ADP DET ADJ NOUN PUNCT DET NOUN ADP DET ADJ NOUN AUX VERB ADV ADP PUNCT VERB ADJ ADJ NOUN NOUN ADP DET NOUN CCONJ NOUN ADP DET SPACE NOUN NOUN PUNCT ADP ADJ NOUN PUNCT PRON VERB DET NOUN ADP NOUN SPACE PART AUX VERB PUNCT VERB PRON PART VERB DET ADJ NOUN ADP VERB NOUN CCONJ NOUN SPACE ADP DET VERB NOUN PUNCT PRON VERB DET NOUN ADP ADJ NOUN CCONJ SPACE NOUN NOUN PART VERB PRON NOUN VERB DET NOUN PART VERB ADV PUNCT PART ADV SPACE VERB PRON NOUN PUNCT PRON VERB DET VERB NOUN ADJ ADP DET NOUN ADP SPACE NOUN NOUN PART VERB DET NOUN NOUN VERB ADP PRON NOUN NOUN PUNCT ADP NOUN VERB ADP VERB DET PROPN VERB NOUN ADP PRON NOUN PUNCT PRON SPACE VERB DET ADJ VERB NUM PUNCT PUNCT PUNCT NUM NOUN PROPN NOUN ADP NUM ADP DET NOUN NOUN NOUN PUNCT SPACE DET ADJ ADJ NOUN VERB ADV ADV CCONJ DET ADJ ADP VERB ADJ NOUN SPACE ADP DET ADJ NOUN ADJ ADP PROPN ADP ADJ NOUN NOUN PUNCT,0.6194690265486725,45.2,4.960176991150442
376,88,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked,"This result strongly suggests that our transition-based transformation of AMR graphs
into ordered tree structures is indeed quite a promising approach for the AMR-to-text
generation task. Throughout this work, we have highlighted a number of ways in which the results
obtained by our system may further be improved upon. As outlined in Section 6, one
promising way that could easily be implemented, but would require access to Gigaword,
would be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences
with AMR graphs using a parser to augment the number of available training data; as
pointed out in Section 6, it is reasonable to assume that implementing this idea would
have a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of
Merge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be
merged. In this context, one may also investigate whether the generator could further
be tweaked by revising other classes of transitions.",36.88430110837439,32.83053957444222,376,0.46955159306526184," This result strongly suggests that our transition based transformation of Propname graphs 
 into ordered tree structures is indeed quite a promising approach for the Propname to text 
 generation task. Throughout this work, we have highlighted a number of ways in which the results 
 obtained by our system may further be improved upon. As outlined in Section 0, one 
 promising way that could easily be implemented, but would require access to Propname, 
 would be to replace the used 0 gram language model with some higher order model. One could also follow the idea of Propname Propname Propname. and annotate Propname sentences 
 with Propname graphs using a parser to augment the number of available training data; as 
 pointed out in Section 0, it is reasonable to assume that implementing this idea would 
 have a major impact on the quality of our generator. Another possible modification shown to be promising in Section 0 is the redefinition of 
 Propname transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be 
 merged. In this context, one may also investigate whether the generator could further 
 be tweaked by revising other classes of transitions."," This result strongly suggests that our transition based transformation of Propname graphs 
 into ordered tree structures is indeed quite a promising approach for the Propname to text 
 generation task. Throughout this work, we have highlighted a number of ways in which the results 
 obtained by our system may further be improved upon. As outlined in Section 0, one 
 promising way that could easily be implemented, but would require access to Propname, 
 would be to replace the used 0 gram language model with some higher order model. One could also follow the idea of Propname Propname Propname. and annotate Propname sentences 
 with Propname graphs using a parser to augment the number of available training data; as 
 pointed out in Section 0, it is reasonable to assume that implementing this idea would 
 have a major impact on the quality of our generator. Another possible modification shown to be promising in Section 0 is the redefinition of 
 Propname transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be 
 merged. In this context, one may also investigate whether the generator could further 
 be tweaked by revising other classes of transitions.", DET NOUN ADV VERB SCONJ PRON NOUN VERB NOUN ADP PROPN NOUN SPACE ADP VERB NOUN NOUN AUX ADV DET DET ADJ NOUN ADP DET PROPN PART VERB SPACE NOUN NOUN PUNCT ADP DET NOUN PUNCT PRON AUX VERB DET NOUN ADP NOUN ADP PRON DET NOUN SPACE VERB ADP PRON NOUN AUX ADV AUX VERB SCONJ PUNCT SCONJ VERB ADP NOUN NUM PUNCT NUM SPACE ADJ NOUN PRON AUX ADV AUX VERB PUNCT CCONJ AUX VERB NOUN ADP PROPN PUNCT SPACE AUX AUX PART VERB DET VERB NUM NOUN NOUN NOUN ADP DET ADJ NOUN NOUN PUNCT PRON AUX ADV VERB DET NOUN ADP PROPN PROPN PROPN PUNCT CCONJ VERB PROPN NOUN SPACE ADP PROPN NOUN VERB DET NOUN PART VERB DET NOUN ADP ADJ NOUN NOUN PUNCT SCONJ SPACE VERB ADP ADP NOUN NUM PUNCT PRON AUX ADJ PART VERB SCONJ VERB DET NOUN AUX SPACE VERB DET ADJ NOUN ADP DET NOUN ADP PRON NOUN PUNCT DET ADJ NOUN VERB PART AUX VERB ADP NOUN NUM AUX DET NOUN ADP SPACE PROPN NOUN PART VERB ADP DET NOUN ADP NOUN NOUN PUNCT PRON AUX ADV ADJ PART VERB DET NOUN ADP DET NOUN PRON VERB ADP NOUN NOUN ADP ADJ NOUN PART AUX SPACE VERB PUNCT ADP DET NOUN PUNCT PRON AUX ADV VERB SCONJ DET NOUN AUX ADV SPACE AUX VERB ADP VERB ADJ NOUN ADP NOUN PUNCT,0.5610859728506787,27.625,4.764705882352941
377,89,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked,"Of course, such a revision does not
have to be limited to the formal definitions of the transitions themselves, but may also
be extended to the extraction of gold transitions from a training corpus as done by the
oracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training
of our maximum entropy models, one could of course also try to improve our generator’s
output by adding new features extracted from the given contexts. In addition, it should
be investigated whether the conditional probability P (t | c) of a transition t given a
configuration c and the various conditional probabilities of syntactic annotations can
be predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network
architectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic
neural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps
introduced in Section 4.4.",23.60075409836068,32.83053957444222,377,0.46446728706359863," Of course, such a revision does not 
 have to be limited to the formal definitions of the transitions themselves, but may also 
 be extended to the extraction of gold transitions from a training corpus as done by the 
 oracle Propname introduced in Propname 0.0.0. While we have put plenty of effort into the selection of suitable features for the training 
 of our maximum entropy models, one could of course also try to improve our generators 
 output by adding new features extracted from the given contexts. In addition, it should 
 be investigated whether the conditional probability P of a transition t given a 
 configuration c and the various conditional probabilities of syntactic annotations can 
 be predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in Propname generation and parsing made with neural network 
 architectures, especially probabilistic 
 neural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps 
 introduced in Section 0.0."," Of course, such a revision does not 
 have to be limited to the formal definitions of the transitions themselves, but may also 
 be extended to the extraction of gold transitions from a training corpus as done by the 
 oracle Propname introduced in Propname 0.0.0. While we have put plenty of effort into the selection of suitable features for the training 
 of our maximum entropy models, one could of course also try to improve our generators 
 output by adding new features extracted from the given contexts. In addition, it should 
 be investigated whether the conditional probability P of a transition t given a 
 configuration c and the various conditional probabilities of syntactic annotations can 
 be predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in Propname generation and parsing made with neural network 
 architectures, especially probabilistic 
 neural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps 
 introduced in Section 0.0.", ADV ADV PUNCT DET DET NOUN AUX PART SPACE VERB PART AUX VERB ADP DET ADJ NOUN ADP DET NOUN PRON PUNCT CCONJ AUX ADV SPACE AUX VERB ADP DET NOUN ADP NOUN NOUN ADP DET NOUN NOUN SCONJ VERB ADP DET SPACE NOUN PROPN VERB ADP PROPN NOUN PUNCT SCONJ PRON AUX VERB NOUN ADP NOUN ADP DET NOUN ADP ADJ NOUN ADP DET NOUN SPACE ADP PRON ADJ NOUN NOUN PUNCT PRON AUX ADP NOUN ADV VERB PART VERB PRON NOUN SPACE NOUN ADP VERB ADJ NOUN VERB ADP DET VERB NOUN PUNCT ADP NOUN PUNCT PRON AUX SPACE AUX VERB SCONJ DET ADJ NOUN NOUN ADP DET NOUN NOUN VERB DET SPACE NOUN NOUN CCONJ DET ADJ ADJ NOUN ADP ADJ NOUN AUX SPACE AUX VERB ADV ADV ADP DET NOUN ADV ADJ ADP ADJ NOUN NOUN PUNCT ADP NOUN ADP ADJ NOUN ADP PROPN NOUN CCONJ VERB VERB ADP ADJ NOUN SPACE NOUN PUNCT ADV ADJ SPACE ADJ NOUN VERB ADP VERB PUNCT DET ADJ NOUN PART VERB NOUN AUX AUX PART VERB CCONJ VERB DET VERB NOUN SPACE VERB ADP NOUN NUM PUNCT,0.632768361581921,35.4,4.943502824858757
378,90,Timo Schick,"[' We have devised a novel approach for the challenging task of AMR-to-text generation. Our core idea was to turn input AMR graphs into ordered trees from which sentences\ncan easily be inferred through application of the yield function. We chose the principle\ncomponent of our approach to be the transition system SAMR, whose set of transitions\nTAMR defines how the transformation from AMR graphs to suitable trees can be per-\nformed. Some transitions contained within this set, such as Merge, Swap and Delete,\nhave an equivalent in the likewise transition-based text-to-AMR parser by Wang et al. (2015), which served as a model for our approach. In order to turn SAMR into a generator, we assigned probabilities to transitions and\ndefined the score of a transition sequence to be a linear combination of the probabilities\nof all its transitions and the probability assigned to the resulting sentence by a language\nmodel. We approximated these probabilities using maximum entropy models that were\ntrained with a set of gold transitions extracted from a large corpus of AMR graphs and\ncorresponding realizations.', 'As an exhaustive search for the highest-scoring transition\nsequence given some input would be far too time-consuming, we developed an algorithm\nthat approximates this sequence in two phases: In a first phase, only transitions from\na subset Trestr of TAMR are greedily applied without taking the language model into\nconsideration; in a second phase, the output of this first phase is processed bottom-\nup, considering multiple partial transition sequences at each step and factoring in the\nlanguage model. Through parametrized pruning, we restricted the number of sequences\nto be considered, allowing us to find a good balance between required time and quality\nof the generated sentences. We introduced the concepts of syntactic annotations and\ndefault realizations to help our system decide which transition to apply next. To further\nimprove our results, we defined some postprocessing steps – such as the insertion of\npunctuation marks – to revise the tree structure obtained from our transition system. In experiments carried out using a Java-based implementation of our generator, we\nobtained a lower-cased 1 . . . 4-gram Bleu score of 27.4 on the LDC2014T12 test set,\nthe second best result reported so far and the best without using parsed sentences\nfrom an external source such as Gigaword (LDC2011T07) as additional training data.', 'This result strongly suggests that our transition-based transformation of AMR graphs\ninto ordered tree structures is indeed quite a promising approach for the AMR-to-text\ngeneration task. Throughout this work, we have highlighted a number of ways in which the results\nobtained by our system may further be improved upon. As outlined in Section 6, one\npromising way that could easily be implemented, but would require access to Gigaword,\nwould be to replace the used 3-gram language model with some higher-order model. One could also follow the idea of Konstas et al. (2017) and annotate Gigaword sentences\nwith AMR graphs using a parser to augment the number of available training data; as\npointed out in Section 6, it is reasonable to assume that implementing this idea would\nhave a major impact on the quality of our generator. Another possible modification shown to be promising in Section 6 is the redefinition of\nMerge transitions to allow for a merging of neighboring vertices. It is also conceivable to modify this transition in a way that allows for vertex groups of arbitrary size to be\nmerged. In this context, one may also investigate whether the generator could further\nbe tweaked by revising other classes of transitions.', 'Of course, such a revision does not\nhave to be limited to the formal definitions of the transitions themselves, but may also\nbe extended to the extraction of gold transitions from a training corpus as done by the\noracle algorithm introduced in Section 4.3.3. While we have put plenty of effort into the selection of suitable features for the training\nof our maximum entropy models, one could of course also try to improve our generator’s\noutput by adding new features extracted from the given contexts. In addition, it should\nbe investigated whether the conditional probability P (t | c) of a transition t given a\nconfiguration c and the various conditional probabilities of syntactic annotations can\nbe predicted more reliably by a model more powerful than maximum entropy models. In view of recent advances in AMR generation and parsing made with neural network\narchitectures (see van Noord and Bos, 2017; Konstas et al., 2017), especially probabilistic\nneural networks come to mind. A further way to improve results may be to extend or revise the postprocessing steps\nintroduced in Section 4.4.', 'For instance, the assignment of punctuation marks could be\nrefined – or even be integrated into the actual transition system – as the current output\nof punctuation marks by our generator shows some room for improvement, especially\nwith respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the\ncurrent implementation in order to make it more resource-friendly and time-efficient;\nas outlined in Section 6, the latter could be achieved through parallelization. A time-\noptimized implementation may also lead to better results in terms of Bleu score, as it\nwould allow us to both drop some of the transition constraints introduced in Section 5.1\nand increase the maximum values allowed for performance-relevant hyperparameters\nused by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed\nin Section 1, in fact transferable to other languages. As indicated in Section 4.1, this\nwould require us to revise the concept of syntactic annotations to properly reflect the\nlinguistic peculiarities of the considered language. Unfortunately, however, such an\ninvestigation is not feasible at present, as no sufficiently large AMR corpus is available\nfor any other language than English.']",conclusion_chunked,"For instance, the assignment of punctuation marks could be
refined – or even be integrated into the actual transition system – as the current output
of punctuation marks by our generator shows some room for improvement, especially
with respect to the placement of commas. Yet another possibility for enhancing the quality of our generator lies in editing the
current implementation in order to make it more resource-friendly and time-efficient;
as outlined in Section 6, the latter could be achieved through parallelization. A time-
optimized implementation may also lead to better results in terms of Bleu score, as it
would allow us to both drop some of the transition constraints introduced in Section 5.1
and increase the maximum values allowed for performance-relevant hyperparameters
used by the best transition sequence algorithm. Finally, it would also be interesting to investigate in how far our results are, as claimed
in Section 1, in fact transferable to other languages. As indicated in Section 4.1, this
would require us to revise the concept of syntactic annotations to properly reflect the
linguistic peculiarities of the considered language. Unfortunately, however, such an
investigation is not feasible at present, as no sufficiently large AMR corpus is available
for any other language than English.",13.077941176470631,32.83053957444222,378,0.3994494676589966," For instance, the assignment of punctuation marks could be 
 refined or even be integrated into the actual transition system as the current output 
 of punctuation marks by our generator shows some room for improvement, especially 
 with respect to the placement of Propname. Yet another possibility for enhancing the quality of our generator lies in editing the 
 current implementation in order to make it more resource friendly and time efficient; 
 as outlined in Section 0, the latter could be achieved through parallelization. A time optimized implementation may also lead to better results in terms of Propname score, as it 
 would allow us to both drop some of the transition constraints introduced in Section 0.0 
 and increase the maximum values allowed for performance relevant hyperparameters 
 used by the best transition sequence Propname. Finally, it would also be interesting to investigate in how far our results are, as claimed 
 in Section 0, in fact transferable to other languages. As indicated in Section 0.0, this 
 would require us to revise the concept of syntactic annotations to properly reflect the 
 linguistic peculiarities of the considered language. Unfortunately, however, such an 
 investigation is not feasible at present, as no sufficiently large Propname corpus is available 
 for any other language than Propname."," For instance, the assignment of punctuation marks could be 
 refined or even be integrated into the actual transition system as the current output 
 of punctuation marks by our generator shows some room for improvement, especially 
 with respect to the placement of Propname. Yet another possibility for enhancing the quality of our generator lies in editing the 
 current implementation in order to make it more resource friendly and time efficient; 
 as outlined in Section 0, the latter could be achieved through parallelization. A time optimized implementation may also lead to better results in terms of Propname score, as it 
 would allow us to both drop some of the transition constraints introduced in Section 0.0 
 and increase the maximum values allowed for performance relevant hyperparameters 
 used by the best transition sequence Propname. Finally, it would also be interesting to investigate in how far our results are, as claimed 
 in Section 0, in fact transferable to other languages. As indicated in Section 0.0, this 
 would require us to revise the concept of syntactic annotations to properly reflect the 
 linguistic peculiarities of the considered language. Unfortunately, however, such an 
 investigation is not feasible at present, as no sufficiently large Propname corpus is available 
 for any other language than Propname.", ADP NOUN PUNCT DET NOUN ADP NOUN NOUN AUX AUX SPACE VERB CCONJ ADV AUX VERB ADP DET ADJ NOUN NOUN SCONJ DET ADJ NOUN SPACE ADP NOUN NOUN ADP PRON NOUN VERB DET NOUN ADP NOUN PUNCT ADV SPACE ADP NOUN ADP DET NOUN ADP PROPN PUNCT ADV DET NOUN ADP VERB DET NOUN ADP PRON NOUN VERB ADP VERB DET SPACE ADJ NOUN ADP NOUN PART VERB PRON ADJ NOUN ADJ CCONJ NOUN ADJ PUNCT SPACE SCONJ VERB ADP NOUN NUM PUNCT DET ADJ AUX AUX VERB ADP NOUN PUNCT DET NOUN VERB NOUN AUX ADV VERB ADP ADJ NOUN ADP NOUN ADP PROPN NOUN PUNCT SCONJ PRON SPACE AUX VERB PRON PART PRON VERB PRON ADP DET NOUN NOUN VERB ADP NOUN NUM SPACE CCONJ VERB DET ADJ NOUN VERB ADP NOUN ADJ NOUN SPACE VERB ADP DET ADJ NOUN NOUN PROPN PUNCT ADV PUNCT PRON AUX ADV AUX ADJ PART VERB ADP SCONJ ADV PRON NOUN AUX PUNCT SCONJ VERB SPACE ADP NOUN NUM PUNCT ADP NOUN ADJ ADP ADJ NOUN PUNCT SCONJ VERB ADP NOUN NUM PUNCT PRON SPACE AUX VERB PRON PART VERB DET NOUN ADP ADJ NOUN PART ADV VERB DET SPACE ADJ NOUN ADP DET VERB NOUN PUNCT ADV PUNCT ADV PUNCT DET DET SPACE NOUN AUX PART ADJ ADP ADJ PUNCT SCONJ DET ADV ADJ PROPN NOUN AUX ADJ SPACE ADP DET ADJ NOUN ADP PROPN PUNCT,0.5874439461883408,37.166666666666664,5.026905829596412
