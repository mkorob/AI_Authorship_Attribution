import nltk
nltk.download('punkt')

tokenizer_reg = nltk.RegexpTokenizer(r"\w+")

text = "I do not like green eggs and ham. I do not like them Sam-I-am."
a_list = nltk.tokenize.sent_tokenize(text)
a_list

tokenizer_reg.tokenize("I do not like green eggs and ham.")

tok_ratio = 3/4
max_seq =512

def split_text_to_chunks(text):
  list_sequences = []
  list_sentences = nltk.tokenize.sent_tokenize(text)
  #parse through each sentence
  sequence_out = ""
  token_count = 0
  for sentence in list_sentences:
      words = tokenizer_reg.tokenize(sentence)
      if len(words)+token_count < int(max_seq*tok_ratio):
        sequence_out = sequence_out+sentence
        token_count = token_count + len(words)
      else:
        list_sequences.append(sequence_out)
        sequence_out = sentence
        token_count = len(words)

  list_sequences.append(sequence_out)
  return list_sequences

joined_data['abstract_chunked'] = joined_data['Abstract'].apply(split_text_to_chunks)

joined_data['intro_chunked'] = joined_data['Introduction'].apply(split_text_to_chunks)

joined_data['conclusion_chunked'] = joined_data['Conclusion'].apply(split_text_to_chunks)

joined_data['all_chunks'] = joined_data['abstract_chunked']+joined_data['intro_chunked']+joined_data['conclusion_chunked']

joined_data['all_chunks_length'] = joined_data['all_chunks'].apply(len)

joined_data[['Author', 'all_chunks_length']].groupby("Author").sum()

joined_data['all_chunks'][10]

df_chunksplit = expand_dataframe(joined_data)

df_chunksplit['Author'].value_counts()

df_chunksplit.head()

"""#5. Try out on a small sample"""

# initialise the stop words
with open('stopwords.txt', 'r') as file:
    # Read each line, strip whitespace and store it in a list
    stopwords = [line.strip() for line in file]
    stopwords = set(stopwords)

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, FunctionTransformer
from sklearn.base import BaseEstimator, TransformerMixin
import string
from sklearn.model_selection import train_test_split

punct_vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize, use_idf=False, norm='l1', vocabulary=string.punctuation)
stopword_bigram_vectorizer = CountVectorizer(ngram_range=(1, 3), tokenizer=nltk.word_tokenize, vocabulary=stopwords)

# Preparing the data
X = df_chunksplit['Chunk'].values
y = df_chunksplit['Author'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



# Feature Extractors
class LexicalDiversity(BaseEstimator, TransformerMixin):
    def fit(self, x, y=None):
        return self

    def transform(self, papers):
        return [[len(set(nltk.word_tokenize(paper))) / len(nltk.word_tokenize(paper))] for paper in papers]

class AverageWordsPerSentence(BaseEstimator, TransformerMixin):
    def fit(self, x, y=None):
        return self

    def transform(self, papers):
        return [[np.mean([len(sentence.split()) for sentence in nltk.sent_tokenize(paper)])] for paper in papers]

# Create feature extraction pipelines
lexical_pipeline = Pipeline([
    ('lexical_div', LexicalDiversity()),
    ('scaler', StandardScaler())
])

avg_words_pipeline = Pipeline([
    ('avg_words', AverageWordsPerSentence()),
    ('scaler', StandardScaler())
])

punct_vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize, use_idf=False, norm='l1', vocabulary=string.punctuation)

# For bigrams of stop words
stopword_bigram_vectorizer = CountVectorizer(ngram_range=(1, 3), tokenizer=nltk.word_tokenize, vocabulary=stopwords)

transformerss = [
    ('lexical_features', lexical_pipeline),
    ('avg_words_per_sentence', avg_words_pipeline),
    ('punctuation_features', punct_vectorizer),
    ('stopword_bigram_features', stopword_bigram_vectorizer)
]

# Combine all feature extractors
feature_pipeline = FeatureUnion(transformerss)

# Create a complete pipeline with feature extraction and SVM classifier
pipeline = Pipeline([
    ('features', feature_pipeline),
    ('classifier', SVC(kernel='linear'))
])

# Preparing the data
X = df_chunksplit['Chunk'].values
y = df_chunksplit['Author'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the pipeline
pipeline.fit(X_train, y_train)

# Extract feature names from each transformer
feature_names = []
for name, transformer in transformerss:
    feature_names.extend(transformer.transform(X_train))

# Predict
y_pred = pipeline.predict(X_test)

feature_names

y_pred == y_test

!pip install eli5
import eli5
from eli5 import show_prediction, show_weights

show_weights(pipeline['classifier'])

pipeline['features']['lexical_features']