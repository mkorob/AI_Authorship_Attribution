{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#0. Preliminaries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\fl_project_analysis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\maria\\anaconda3\\envs\\fl_project_analysis\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", max_length = 512, truncation = True)\n",
    "\n",
    "distilled_student_sentiment_classifier = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "    tokenizer = tokenizer,\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import Data\n",
    "df = pd.read_csv(\"../data/chunked_author_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val, df_test = train_test_split(df, train_size = 0.8, random_state = 42)\n",
    "df_train, df_val = train_test_split(df_train_val, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flesch \n",
    "def flesch_readability_scale(text):\n",
    "    try:\n",
    "      r = Readability(text)\n",
    "      f = r.flesch()\n",
    "      score_out = f.score\n",
    "    #not possible if less than 100 words\n",
    "    except:\n",
    "      score_out = np.nan\n",
    "      \n",
    "    return score_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis (Positive Score)\n",
    "tokenizer_kwargs = {'truncation':True,'max_length':512}\n",
    "def sentiment_analysis_score(text):\n",
    "    results_senti = distilled_student_sentiment_classifier(text, **tokenizer_kwargs)\n",
    "    positive_score = [x['score'] for x in results_senti[0] if x['label'] == 'positive']\n",
    "    score_out = positive_score[0] if len(positive_score) == 1 else np.nan\n",
    "    return score_out                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation n-grams\n",
    "punct_vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize, use_idf=False, norm='l1', vocabulary=string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectorizer = TfidfVectorizer(ngram_range=(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_vectorizer = CountVectorizer(ngram_range=(1, 3), tokenizer=nltk.word_tokenize, vocabulary=stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bullet_points(text):\n",
    "    bulletpoint_delimiters = re.compile(r'(\\(i\\)|\\(ii\\)|•)')\n",
    "    text = re.sub(bulletpoint_delimiters, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove proper nouns and POS-tag n grams\n",
    "def POS_preprocessing(text):\n",
    "    POS_string = \"\"\n",
    "    cleaned_string = \"\"\n",
    "    list_sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    for sentence in list_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            #first add the text back\n",
    "            string_out = \"Propname\" if token.pos_ == \"PROPN\" else token.text\n",
    "            sep_out = \"\" if token.pos_ == \"PUNCT\" else \" \"\n",
    "            cleaned_string = cleaned_string + sep_out + string_out\n",
    "            #second \n",
    "            #POS_out = \"\" if token.pos_ == \"PUNCT\" else token.pos_\n",
    "            POS_string = POS_string + \" \" + token.pos_\n",
    "    return pd.Series({\n",
    "        'cleaned_string': cleaned_string,\n",
    "        'POS_string': POS_string\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Features to Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, text_col, train = False):\n",
    "    df = df.reset_index(drop = True)\n",
    "    #these should be run first before cleaning punctuation and private words and stuff\n",
    "    #df['flesch_score'] = df[text_col].apply(flesch_readability_scale)\n",
    "    #commented out for now as takes long to run\n",
    "    #df['sent_score'] = df[text_col].apply(sentiment_analysis_score)\n",
    "    #train has to be run first - a catch statement for that\n",
    "    df[text_col] = df[text_col].apply(remove_bullet_points)\n",
    "    #removing double space should be after removing bullet points! leaves a double space sometimes\n",
    "    df[['text', 'POS_string']] = df[text_col].apply(POS_preprocessing)\n",
    "\n",
    "    try:\n",
    "        punct_features = punct_vectorizer.fit_transform(df[text_col]) if train else punct_vectorizer.transform(df[text_col])\n",
    "        punct_features_df = pd.DataFrame(punct_features.toarray(), columns=punct_vectorizer.get_feature_names_out()).reset_index(drop = True)\n",
    "        df = pd.concat([df, punct_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Punctuation N-grams: {e}\")\n",
    "\n",
    "    try:\n",
    "        POS_features = pos_vectorizer.fit_transform(df['POS_string']) if train else pos_vectorizer.transform(df['POS_string'])\n",
    "        POS_features_df = pd.DataFrame(POS_features.toarray(), columns=pos_vectorizer.get_feature_names_out()).reset_index(drop = True)\n",
    "        df = pd.concat([df, POS_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Punctuation N-grams: {e}\")\n",
    "    try:\n",
    "        stopwords_features = stopword_vectorizer.fit_transform(df[text_col]) if train else stopword_vectorizer.transform(df[text_col])\n",
    "        stopwords_features_df = pd.DataFrame(stopwords_features.toarray(), columns=stopword_vectorizer.get_feature_names_out()).reset_index(drop = True)\n",
    "        df = pd.concat([df, stopwords_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Stopword N-grams: {e}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\fl_project_analysis\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\maria\\anaconda3\\envs\\fl_project_analysis\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_train_processed = preprocess_data(df_train, 'Chunk', train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_processed = preprocess_data(df_val, 'Chunk', train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Author</th>\n",
       "      <th>Pub</th>\n",
       "      <th>Chunk</th>\n",
       "      <th>text</th>\n",
       "      <th>POS_string</th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>...</th>\n",
       "      <th>shouldn</th>\n",
       "      <th>shouldn't</th>\n",
       "      <th>wasn</th>\n",
       "      <th>wasn't</th>\n",
       "      <th>weren</th>\n",
       "      <th>weren't</th>\n",
       "      <th>won</th>\n",
       "      <th>won't</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>wouldn't</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' Providing pretrained language models with s...</td>\n",
       "      <td>Providing pretrained language models with sim...</td>\n",
       "      <td>Providing pretrained language models with s...</td>\n",
       "      <td>SPACE NOUN VERB NOUN NOUN ADP ADJ NOUN NOUN C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>[' Reasoning about events and tracking their i...</td>\n",
       "      <td>Humans are adept at anticipating and reasonin...</td>\n",
       "      <td>Humans are adept at anticipating and reason...</td>\n",
       "      <td>SPACE NOUN AUX ADJ ADP VERB CCONJ VERB ADP NO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' When trained on large, unfiltered crawls fr...</td>\n",
       "      <td>In this paper, we have shown that large langu...</td>\n",
       "      <td>In this paper, we have shown that large lan...</td>\n",
       "      <td>SPACE ADP DET NOUN  PRON AUX VERB SCONJ ADJ N...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' Recently, neural networks purely based on a...</td>\n",
       "      <td>In this paper, we have introduced DeiT, which...</td>\n",
       "      <td>In this paper, we have introduced Propname,...</td>\n",
       "      <td>SPACE ADP DET NOUN  PRON AUX VERB PROPN  PRON...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' To obtain high-quality sentence embeddings ...</td>\n",
       "      <td>While pretrained language models (PLMs) achie...</td>\n",
       "      <td>While pretrained language models( Propname)...</td>\n",
       "      <td>SPACE SCONJ VERB NOUN NOUN  PROPN  VERB ADJ N...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>210</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' Pretraining deep neural network architectur...</td>\n",
       "      <td>We have introduced WNLaMPro, a new dataset th...</td>\n",
       "      <td>We have introduced WNLaMPro, a new dataset ...</td>\n",
       "      <td>SPACE PRON AUX VERB ADJ  DET ADJ NOUN PRON VE...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>148</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Numerical simulation of non-linear partial ...</td>\n",
       "      <td>Specifically, in this paper, we focus on traje...</td>\n",
       "      <td>Specifically, in this paper, we focus on traj...</td>\n",
       "      <td>ADV  ADP DET NOUN  PRON VERB ADP VERB ADJ  NO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>150</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' We propose a new paradigm to help Large Lan...</td>\n",
       "      <td>We propose a new paradigm to help Large Langu...</td>\n",
       "      <td>We propose a new paradigm to help Large Pro...</td>\n",
       "      <td>SPACE PRON VERB DET ADJ NOUN PART VERB ADJ PR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>114</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' We propose a simple architecture to address...</td>\n",
       "      <td>Powers of layers consists in iterating a resi...</td>\n",
       "      <td>Propname of layers consists in iterating a ...</td>\n",
       "      <td>SPACE PROPN ADP NOUN VERB ADP VERB DET ADJ NO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>129</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Supervised Fine-Tuning (SFT) on response de...</td>\n",
       "      <td>The vanilla (stand-alone) reward models in RLH...</td>\n",
       "      <td>The vanilla( stand- alone) reward models in P...</td>\n",
       "      <td>DET NOUN  VERB  ADV  NOUN NOUN ADP PROPN CCON...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>242</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' This work addresses the task of generating ...</td>\n",
       "      <td>Finally, the inclusion of a language model int...</td>\n",
       "      <td>Finally, the inclusion of a language model in...</td>\n",
       "      <td>ADV  DET NOUN ADP DET NOUN NOUN ADP PRON NOUN...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>69</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' In this work, we develop and release Llama ...</td>\n",
       "      <td>We hope that this openness will enable the com...</td>\n",
       "      <td>We hope that this openness will enable the co...</td>\n",
       "      <td>PRON VERB SCONJ DET NOUN AUX VERB DET NOUN PA...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>88</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' Nowadays, machine learning and more particu...</td>\n",
       "      <td>Nowadays, machine learning and more particula...</td>\n",
       "      <td>Nowadays, machine learning and more particu...</td>\n",
       "      <td>SPACE ADV  NOUN NOUN CCONJ ADV ADV ADJ NOUN V...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>162</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Natural Language Processing (NLP) has recen...</td>\n",
       "      <td>The NLP community has witnessed a revolution ...</td>\n",
       "      <td>The Propname community has witnessed a revo...</td>\n",
       "      <td>SPACE DET PROPN NOUN AUX VERB DET NOUN ADP SP...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>168</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Autoregressive (AR) models have been the do...</td>\n",
       "      <td>Autoregressive (AR) models have been the domi...</td>\n",
       "      <td>Propname( Propname) models have been the do...</td>\n",
       "      <td>SPACE PROPN  PROPN  NOUN AUX AUX DET VERB NOU...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>145</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Neural network-based Combinatorial Optimiza...</td>\n",
       "      <td>We proposed DIFUSCO, a novel graph-based diff...</td>\n",
       "      <td>We proposed DIFUSCO, a novel graph- based d...</td>\n",
       "      <td>SPACE PRON VERB NOUN  DET ADJ NOUN  VERB NOUN...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>134</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Large Multimodal Models (LMM) are built acr...</td>\n",
       "      <td>We proposed several strategies to tackle the ...</td>\n",
       "      <td>We proposed several strategies to tackle th...</td>\n",
       "      <td>SPACE PRON VERB ADJ NOUN PART VERB DET ADJ NO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>153</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' We propose a new paradigm to help Large Lan...</td>\n",
       "      <td>n this paper, we propose a novel recitation-a...</td>\n",
       "      <td>n this paper, we propose a novel recitation...</td>\n",
       "      <td>SPACE CCONJ DET NOUN  PRON VERB DET ADJ NOUN ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>172</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Autoregressive sequence models achieve stat...</td>\n",
       "      <td>Autoregressive sequence models achieve state-...</td>\n",
       "      <td>Autoregressive sequence models achieve stat...</td>\n",
       "      <td>SPACE ADJ NOUN NOUN VERB NOUN  ADP  DET  NOUN...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>245</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' This work addresses the task of generating ...</td>\n",
       "      <td>In view of recent advances in AMR generation a...</td>\n",
       "      <td>In view of recent advances in Propname genera...</td>\n",
       "      <td>ADP NOUN ADP ADJ NOUN ADP PROPN NOUN CCONJ VE...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>161</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Natural Language Processing (NLP) has recen...</td>\n",
       "      <td>Natural Language Processing (NLP) has recentl...</td>\n",
       "      <td>Propname Propname Propname( Propname) has r...</td>\n",
       "      <td>SPACE PROPN PROPN PROPN  PROPN  AUX ADV VERB ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>128</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Supervised Fine-Tuning (SFT) on response de...</td>\n",
       "      <td>That is, these RLAIF methods inherit the\\nheav...</td>\n",
       "      <td>That is, these Propname methods inherit the \\...</td>\n",
       "      <td>ADV ADV  DET PROPN NOUN VERB DET SPACE ADJ NO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>59</td>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>[' We study a novel task of numerical relation...</td>\n",
       "      <td>We study a novel task of numerical relation e...</td>\n",
       "      <td>We study a novel task of Propname relation ...</td>\n",
       "      <td>SPACE PRON VERB DET ADJ NOUN ADP PROPN NOUN N...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>93</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' We show how to augment any convolutional ne...</td>\n",
       "      <td>Then we show a âResNet-50â augmented by ad...</td>\n",
       "      <td>Then we show a âResNet-50â augmented by a...</td>\n",
       "      <td>ADV PRON VERB DET NOUN VERB ADP VERB PRON VER...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>[' Like humans, large language models (LLMs) d...</td>\n",
       "      <td>Like humans, large language models (LLMs) do ...</td>\n",
       "      <td>Like humans, large language models( Propnam...</td>\n",
       "      <td>SPACE ADP NOUN  ADJ NOUN NOUN  PROPN  AUX PAR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' We present ResMLP, an architecture built en...</td>\n",
       "      <td>Recently, the transformer architecture [60], ...</td>\n",
       "      <td>Recently, the transformer architecture [ 60...</td>\n",
       "      <td>SPACE ADV  DET NOUN NOUN X NUM   VERB ADP PRO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>[' Like humans, large language models (LLMs) d...</td>\n",
       "      <td>We present SELF-REFINE: a novel approach that...</td>\n",
       "      <td>We present SELF- Propname: a novel approach...</td>\n",
       "      <td>SPACE PRON VERB NOUN  PROPN  DET ADJ NOUN PRO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>[' Large language models (LLMs) are now availa...</td>\n",
       "      <td>Human problem-solving inherently follows a mu...</td>\n",
       "      <td>Human problem- solving inherently follows a...</td>\n",
       "      <td>SPACE ADJ NOUN  NOUN ADV VERB DET ADJ ADJ ADJ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>143</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Neural network-based Combinatorial Optimiza...</td>\n",
       "      <td>To apply the iterative denoising process of di...</td>\n",
       "      <td>To apply the iterative denoising process of d...</td>\n",
       "      <td>PART VERB DET ADJ NOUN NOUN ADP NOUN NOUN ADP...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>125</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' Data-augmentation is key to the training of...</td>\n",
       "      <td>We have studied extensively the effect of usi...</td>\n",
       "      <td>We have studied extensively the effect of u...</td>\n",
       "      <td>SPACE PRON AUX VERB ADV DET NOUN ADP VERB ADJ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>240</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' This work addresses the task of generating ...</td>\n",
       "      <td>Semantic representations of natural language ...</td>\n",
       "      <td>Semantic representations of natural languag...</td>\n",
       "      <td>SPACE ADJ NOUN ADP ADJ NOUN AUX ADP ADJ NOUN ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>133</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Large Multimodal Models (LMM) are built acr...</td>\n",
       "      <td>To prevent reward hacking, previous work (Bai ...</td>\n",
       "      <td>To prevent reward hacking, previous work( Pro...</td>\n",
       "      <td>PART VERB NOUN NOUN  ADJ NOUN  PROPN PROPN PR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>74</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' We introduce LLaMA, a collection of foundat...</td>\n",
       "      <td>In this paper, we presented a series of langu...</td>\n",
       "      <td>In this paper, we presented a series of lan...</td>\n",
       "      <td>SPACE ADP DET NOUN  PRON VERB DET NOUN ADP NO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>29</td>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>[' Large LMs such as GPT-3 are powerful, but\\n...</td>\n",
       "      <td>We maintain a memory M of\\nsuch feedback as a ...</td>\n",
       "      <td>We maintain a memory Propname of \\n such feed...</td>\n",
       "      <td>PRON VERB DET NOUN PROPN ADP SPACE ADJ NOUN A...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>13</td>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>[' We address the general task of structured c...</td>\n",
       "      <td>Despite these struggles, the recent success of...</td>\n",
       "      <td>Despite these struggles, the recent success o...</td>\n",
       "      <td>SCONJ DET NOUN  DET ADJ NOUN ADP SPACE ADJ  N...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>186</td>\n",
       "      <td>Zhiqing Sun</td>\n",
       "      <td>[' Previous traditional approaches to unsuperv...</td>\n",
       "      <td>In this paper, we proposed a neural generativ...</td>\n",
       "      <td>In this paper, we proposed a neural generat...</td>\n",
       "      <td>SPACE ADP DET NOUN  PRON VERB DET ADJ ADJ NOU...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>78</td>\n",
       "      <td>Hugo Touvron</td>\n",
       "      <td>[' We introduce submodel co-training, a regula...</td>\n",
       "      <td>Co-training submodels (cosub) is an effective...</td>\n",
       "      <td>Co - training submodels( cosub) is an effec...</td>\n",
       "      <td>SPACE NOUN ADJ NOUN NOUN  X  AUX DET ADJ NOUN...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>193</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' Language models (LMs) exhibit remarkable ab...</td>\n",
       "      <td>Language models (LMs) exhibit remarkable abil...</td>\n",
       "      <td>Language models( LMs) exhibit remarkable ab...</td>\n",
       "      <td>SPACE NOUN NOUN  NOUN  VERB ADJ NOUN PART VER...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>228</td>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>[' Pretraining deep language models has led to...</td>\n",
       "      <td>By giving BERTRAM access to both surface form ...</td>\n",
       "      <td>By giving BERTRAM access to both surface form...</td>\n",
       "      <td>ADP VERB ADJ NOUN ADP DET NOUN NOUN CCONJ NOU...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>36</td>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>[' Defeasible reasoning is a mode of reasoning...</td>\n",
       "      <td>Defeasible inference (Rudinger et al., 2020) ...</td>\n",
       "      <td>Defeasible inference( Propname Propname Pro...</td>\n",
       "      <td>SPACE ADJ NOUN  PROPN PROPN PROPN PROPN  NUM ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 8854 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0        Author  \\\n",
       "0          211   Timo Schick   \n",
       "1           43   Aman Madaan   \n",
       "2          200   Timo Schick   \n",
       "3           98  Hugo Touvron   \n",
       "4          205   Timo Schick   \n",
       "5          210   Timo Schick   \n",
       "6          148   Zhiqing Sun   \n",
       "7          150   Zhiqing Sun   \n",
       "8          114  Hugo Touvron   \n",
       "9          129   Zhiqing Sun   \n",
       "10         242   Timo Schick   \n",
       "11          69  Hugo Touvron   \n",
       "12          88  Hugo Touvron   \n",
       "13         162   Zhiqing Sun   \n",
       "14         168   Zhiqing Sun   \n",
       "15         145   Zhiqing Sun   \n",
       "16         134   Zhiqing Sun   \n",
       "17         153   Zhiqing Sun   \n",
       "18         172   Zhiqing Sun   \n",
       "19         245   Timo Schick   \n",
       "20         161   Zhiqing Sun   \n",
       "21         128   Zhiqing Sun   \n",
       "22          59   Aman Madaan   \n",
       "23          93  Hugo Touvron   \n",
       "24           4   Aman Madaan   \n",
       "25         100  Hugo Touvron   \n",
       "26           7   Aman Madaan   \n",
       "27           1   Aman Madaan   \n",
       "28         143   Zhiqing Sun   \n",
       "29         125  Hugo Touvron   \n",
       "30         240   Timo Schick   \n",
       "31         133   Zhiqing Sun   \n",
       "32          74  Hugo Touvron   \n",
       "33          29   Aman Madaan   \n",
       "34          13   Aman Madaan   \n",
       "35         186   Zhiqing Sun   \n",
       "36          78  Hugo Touvron   \n",
       "37         193   Timo Schick   \n",
       "38         228   Timo Schick   \n",
       "39          36   Aman Madaan   \n",
       "\n",
       "                                                  Pub  \\\n",
       "0   [' Providing pretrained language models with s...   \n",
       "1   [' Reasoning about events and tracking their i...   \n",
       "2   [' When trained on large, unfiltered crawls fr...   \n",
       "3   [' Recently, neural networks purely based on a...   \n",
       "4   [' To obtain high-quality sentence embeddings ...   \n",
       "5   [' Pretraining deep neural network architectur...   \n",
       "6   [' Numerical simulation of non-linear partial ...   \n",
       "7   [' We propose a new paradigm to help Large Lan...   \n",
       "8   [' We propose a simple architecture to address...   \n",
       "9   [' Supervised Fine-Tuning (SFT) on response de...   \n",
       "10  [' This work addresses the task of generating ...   \n",
       "11  [' In this work, we develop and release Llama ...   \n",
       "12  [' Nowadays, machine learning and more particu...   \n",
       "13  [' Natural Language Processing (NLP) has recen...   \n",
       "14  [' Autoregressive (AR) models have been the do...   \n",
       "15  [' Neural network-based Combinatorial Optimiza...   \n",
       "16  [' Large Multimodal Models (LMM) are built acr...   \n",
       "17  [' We propose a new paradigm to help Large Lan...   \n",
       "18  [' Autoregressive sequence models achieve stat...   \n",
       "19  [' This work addresses the task of generating ...   \n",
       "20  [' Natural Language Processing (NLP) has recen...   \n",
       "21  [' Supervised Fine-Tuning (SFT) on response de...   \n",
       "22  [' We study a novel task of numerical relation...   \n",
       "23  [' We show how to augment any convolutional ne...   \n",
       "24  [' Like humans, large language models (LLMs) d...   \n",
       "25  [' We present ResMLP, an architecture built en...   \n",
       "26  [' Like humans, large language models (LLMs) d...   \n",
       "27  [' Large language models (LLMs) are now availa...   \n",
       "28  [' Neural network-based Combinatorial Optimiza...   \n",
       "29  [' Data-augmentation is key to the training of...   \n",
       "30  [' This work addresses the task of generating ...   \n",
       "31  [' Large Multimodal Models (LMM) are built acr...   \n",
       "32  [' We introduce LLaMA, a collection of foundat...   \n",
       "33  [' Large LMs such as GPT-3 are powerful, but\\n...   \n",
       "34  [' We address the general task of structured c...   \n",
       "35  [' Previous traditional approaches to unsuperv...   \n",
       "36  [' We introduce submodel co-training, a regula...   \n",
       "37  [' Language models (LMs) exhibit remarkable ab...   \n",
       "38  [' Pretraining deep language models has led to...   \n",
       "39  [' Defeasible reasoning is a mode of reasoning...   \n",
       "\n",
       "                                                Chunk  \\\n",
       "0    Providing pretrained language models with sim...   \n",
       "1    Humans are adept at anticipating and reasonin...   \n",
       "2    In this paper, we have shown that large langu...   \n",
       "3    In this paper, we have introduced DeiT, which...   \n",
       "4    While pretrained language models (PLMs) achie...   \n",
       "5    We have introduced WNLaMPro, a new dataset th...   \n",
       "6   Specifically, in this paper, we focus on traje...   \n",
       "7    We propose a new paradigm to help Large Langu...   \n",
       "8    Powers of layers consists in iterating a resi...   \n",
       "9   The vanilla (stand-alone) reward models in RLH...   \n",
       "10  Finally, the inclusion of a language model int...   \n",
       "11  We hope that this openness will enable the com...   \n",
       "12   Nowadays, machine learning and more particula...   \n",
       "13   The NLP community has witnessed a revolution ...   \n",
       "14   Autoregressive (AR) models have been the domi...   \n",
       "15   We proposed DIFUSCO, a novel graph-based diff...   \n",
       "16   We proposed several strategies to tackle the ...   \n",
       "17   n this paper, we propose a novel recitation-a...   \n",
       "18   Autoregressive sequence models achieve state-...   \n",
       "19  In view of recent advances in AMR generation a...   \n",
       "20   Natural Language Processing (NLP) has recentl...   \n",
       "21  That is, these RLAIF methods inherit the\\nheav...   \n",
       "22   We study a novel task of numerical relation e...   \n",
       "23  Then we show a âResNet-50â augmented by ad...   \n",
       "24   Like humans, large language models (LLMs) do ...   \n",
       "25   Recently, the transformer architecture [60], ...   \n",
       "26   We present SELF-REFINE: a novel approach that...   \n",
       "27   Human problem-solving inherently follows a mu...   \n",
       "28  To apply the iterative denoising process of di...   \n",
       "29   We have studied extensively the effect of usi...   \n",
       "30   Semantic representations of natural language ...   \n",
       "31  To prevent reward hacking, previous work (Bai ...   \n",
       "32   In this paper, we presented a series of langu...   \n",
       "33  We maintain a memory M of\\nsuch feedback as a ...   \n",
       "34  Despite these struggles, the recent success of...   \n",
       "35   In this paper, we proposed a neural generativ...   \n",
       "36   Co-training submodels (cosub) is an effective...   \n",
       "37   Language models (LMs) exhibit remarkable abil...   \n",
       "38  By giving BERTRAM access to both surface form ...   \n",
       "39   Defeasible inference (Rudinger et al., 2020) ...   \n",
       "\n",
       "                                                 text  \\\n",
       "0      Providing pretrained language models with s...   \n",
       "1      Humans are adept at anticipating and reason...   \n",
       "2      In this paper, we have shown that large lan...   \n",
       "3      In this paper, we have introduced Propname,...   \n",
       "4      While pretrained language models( Propname)...   \n",
       "5      We have introduced WNLaMPro, a new dataset ...   \n",
       "6    Specifically, in this paper, we focus on traj...   \n",
       "7      We propose a new paradigm to help Large Pro...   \n",
       "8      Propname of layers consists in iterating a ...   \n",
       "9    The vanilla( stand- alone) reward models in P...   \n",
       "10   Finally, the inclusion of a language model in...   \n",
       "11   We hope that this openness will enable the co...   \n",
       "12     Nowadays, machine learning and more particu...   \n",
       "13     The Propname community has witnessed a revo...   \n",
       "14     Propname( Propname) models have been the do...   \n",
       "15     We proposed DIFUSCO, a novel graph- based d...   \n",
       "16     We proposed several strategies to tackle th...   \n",
       "17     n this paper, we propose a novel recitation...   \n",
       "18     Autoregressive sequence models achieve stat...   \n",
       "19   In view of recent advances in Propname genera...   \n",
       "20     Propname Propname Propname( Propname) has r...   \n",
       "21   That is, these Propname methods inherit the \\...   \n",
       "22     We study a novel task of Propname relation ...   \n",
       "23   Then we show a âResNet-50â augmented by a...   \n",
       "24     Like humans, large language models( Propnam...   \n",
       "25     Recently, the transformer architecture [ 60...   \n",
       "26     We present SELF- Propname: a novel approach...   \n",
       "27     Human problem- solving inherently follows a...   \n",
       "28   To apply the iterative denoising process of d...   \n",
       "29     We have studied extensively the effect of u...   \n",
       "30     Semantic representations of natural languag...   \n",
       "31   To prevent reward hacking, previous work( Pro...   \n",
       "32     In this paper, we presented a series of lan...   \n",
       "33   We maintain a memory Propname of \\n such feed...   \n",
       "34   Despite these struggles, the recent success o...   \n",
       "35     In this paper, we proposed a neural generat...   \n",
       "36     Co - training submodels( cosub) is an effec...   \n",
       "37     Language models( LMs) exhibit remarkable ab...   \n",
       "38   By giving BERTRAM access to both surface form...   \n",
       "39     Defeasible inference( Propname Propname Pro...   \n",
       "\n",
       "                                           POS_string    !    \"    #    $  \\\n",
       "0    SPACE NOUN VERB NOUN NOUN ADP ADJ NOUN NOUN C...  0.0  0.0  0.0  0.0   \n",
       "1    SPACE NOUN AUX ADJ ADP VERB CCONJ VERB ADP NO...  0.0  0.0  0.0  0.0   \n",
       "2    SPACE ADP DET NOUN  PRON AUX VERB SCONJ ADJ N...  0.0  0.0  0.0  0.0   \n",
       "3    SPACE ADP DET NOUN  PRON AUX VERB PROPN  PRON...  0.0  0.0  0.0  0.0   \n",
       "4    SPACE SCONJ VERB NOUN NOUN  PROPN  VERB ADJ N...  0.0  0.0  0.0  0.0   \n",
       "5    SPACE PRON AUX VERB ADJ  DET ADJ NOUN PRON VE...  0.0  0.0  0.0  0.0   \n",
       "6    ADV  ADP DET NOUN  PRON VERB ADP VERB ADJ  NO...  0.0  0.0  0.0  0.0   \n",
       "7    SPACE PRON VERB DET ADJ NOUN PART VERB ADJ PR...  0.0  0.0  0.0  0.0   \n",
       "8    SPACE PROPN ADP NOUN VERB ADP VERB DET ADJ NO...  0.0  0.0  0.0  0.0   \n",
       "9    DET NOUN  VERB  ADV  NOUN NOUN ADP PROPN CCON...  0.0  0.0  0.0  0.0   \n",
       "10   ADV  DET NOUN ADP DET NOUN NOUN ADP PRON NOUN...  0.0  0.0  0.0  0.0   \n",
       "11   PRON VERB SCONJ DET NOUN AUX VERB DET NOUN PA...  0.0  0.0  0.0  0.0   \n",
       "12   SPACE ADV  NOUN NOUN CCONJ ADV ADV ADJ NOUN V...  0.0  0.0  0.0  0.0   \n",
       "13   SPACE DET PROPN NOUN AUX VERB DET NOUN ADP SP...  0.0  0.0  0.0  0.0   \n",
       "14   SPACE PROPN  PROPN  NOUN AUX AUX DET VERB NOU...  0.0  0.0  0.0  0.0   \n",
       "15   SPACE PRON VERB NOUN  DET ADJ NOUN  VERB NOUN...  0.0  0.0  0.0  0.0   \n",
       "16   SPACE PRON VERB ADJ NOUN PART VERB DET ADJ NO...  0.0  0.0  0.0  0.0   \n",
       "17   SPACE CCONJ DET NOUN  PRON VERB DET ADJ NOUN ...  0.0  0.0  0.0  0.0   \n",
       "18   SPACE ADJ NOUN NOUN VERB NOUN  ADP  DET  NOUN...  0.0  0.0  0.0  0.0   \n",
       "19   ADP NOUN ADP ADJ NOUN ADP PROPN NOUN CCONJ VE...  0.0  0.0  0.0  0.0   \n",
       "20   SPACE PROPN PROPN PROPN  PROPN  AUX ADV VERB ...  0.0  0.0  0.0  0.0   \n",
       "21   ADV ADV  DET PROPN NOUN VERB DET SPACE ADJ NO...  0.0  0.0  0.0  0.0   \n",
       "22   SPACE PRON VERB DET ADJ NOUN ADP PROPN NOUN N...  0.0  0.0  0.0  0.0   \n",
       "23   ADV PRON VERB DET NOUN VERB ADP VERB PRON VER...  0.0  0.0  0.0  0.0   \n",
       "24   SPACE ADP NOUN  ADJ NOUN NOUN  PROPN  AUX PAR...  0.0  0.0  0.0  0.0   \n",
       "25   SPACE ADV  DET NOUN NOUN X NUM   VERB ADP PRO...  0.0  0.0  0.0  0.0   \n",
       "26   SPACE PRON VERB NOUN  PROPN  DET ADJ NOUN PRO...  0.0  0.0  0.0  0.0   \n",
       "27   SPACE ADJ NOUN  NOUN ADV VERB DET ADJ ADJ ADJ...  0.0  0.0  0.0  0.0   \n",
       "28   PART VERB DET ADJ NOUN NOUN ADP NOUN NOUN ADP...  0.0  0.0  0.0  0.0   \n",
       "29   SPACE PRON AUX VERB ADV DET NOUN ADP VERB ADJ...  0.0  0.0  0.0  0.0   \n",
       "30   SPACE ADJ NOUN ADP ADJ NOUN AUX ADP ADJ NOUN ...  0.0  0.0  0.0  0.0   \n",
       "31   PART VERB NOUN NOUN  ADJ NOUN  PROPN PROPN PR...  0.0  0.0  0.0  0.0   \n",
       "32   SPACE ADP DET NOUN  PRON VERB DET NOUN ADP NO...  0.0  0.0  0.0  0.0   \n",
       "33   PRON VERB DET NOUN PROPN ADP SPACE ADJ NOUN A...  0.0  0.0  0.0  0.0   \n",
       "34   SCONJ DET NOUN  DET ADJ NOUN ADP SPACE ADJ  N...  0.0  0.0  0.0  0.0   \n",
       "35   SPACE ADP DET NOUN  PRON VERB DET ADJ ADJ NOU...  0.0  0.0  0.0  0.0   \n",
       "36   SPACE NOUN ADJ NOUN NOUN  X  AUX DET ADJ NOUN...  0.0  0.0  0.0  0.0   \n",
       "37   SPACE NOUN NOUN  NOUN  VERB ADJ NOUN PART VER...  0.0  0.0  0.0  0.0   \n",
       "38   ADP VERB ADJ NOUN ADP DET NOUN NOUN CCONJ NOU...  0.0  0.0  0.0  0.0   \n",
       "39   SPACE ADJ NOUN  PROPN PROPN PROPN PROPN  NUM ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    ...  shouldn  shouldn't  wasn  wasn't  weren  weren't  won  won't  wouldn  \\\n",
       "0   ...        0          0     0       0      0        0    0      0       0   \n",
       "1   ...        0          0     0       0      0        0    0      0       0   \n",
       "2   ...        0          0     0       0      0        0    0      0       0   \n",
       "3   ...        0          0     0       0      0        0    0      0       0   \n",
       "4   ...        0          0     0       0      0        0    0      0       0   \n",
       "5   ...        0          0     0       0      0        0    0      0       0   \n",
       "6   ...        0          0     0       0      0        0    0      0       0   \n",
       "7   ...        0          0     0       0      0        0    0      0       0   \n",
       "8   ...        0          0     0       0      0        0    0      0       0   \n",
       "9   ...        0          0     0       0      0        0    0      0       0   \n",
       "10  ...        0          0     0       0      0        0    0      0       0   \n",
       "11  ...        0          0     0       0      0        0    0      0       0   \n",
       "12  ...        0          0     0       0      0        0    0      0       0   \n",
       "13  ...        0          0     0       0      0        0    0      0       0   \n",
       "14  ...        0          0     0       0      0        0    0      0       0   \n",
       "15  ...        0          0     0       0      0        0    0      0       0   \n",
       "16  ...        0          0     0       0      0        0    0      0       0   \n",
       "17  ...        0          0     0       0      0        0    0      0       0   \n",
       "18  ...        0          0     0       0      0        0    0      0       0   \n",
       "19  ...        0          0     0       0      0        0    0      0       0   \n",
       "20  ...        0          0     0       0      0        0    0      0       0   \n",
       "21  ...        0          0     0       0      0        0    0      0       0   \n",
       "22  ...        0          0     0       0      0        0    0      0       0   \n",
       "23  ...        0          0     0       0      0        0    0      0       0   \n",
       "24  ...        0          0     0       0      0        0    0      0       0   \n",
       "25  ...        0          0     0       0      0        0    0      0       0   \n",
       "26  ...        0          0     0       0      0        0    0      0       0   \n",
       "27  ...        0          0     0       0      0        0    0      0       0   \n",
       "28  ...        0          0     0       0      0        0    0      0       0   \n",
       "29  ...        0          0     0       0      0        0    0      0       0   \n",
       "30  ...        0          0     0       0      0        0    0      0       0   \n",
       "31  ...        0          0     0       0      0        0    0      0       0   \n",
       "32  ...        0          0     0       0      0        0    0      0       0   \n",
       "33  ...        0          0     0       0      0        0    0      0       0   \n",
       "34  ...        0          0     0       0      0        0    0      0       0   \n",
       "35  ...        0          0     0       0      0        0    0      0       0   \n",
       "36  ...        0          0     0       0      0        0    0      0       0   \n",
       "37  ...        0          0     0       0      0        0    0      0       0   \n",
       "38  ...        0          0     0       0      0        0    0      0       0   \n",
       "39  ...        0          0     0       0      0        0    0      0       0   \n",
       "\n",
       "    wouldn't  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "5          0  \n",
       "6          0  \n",
       "7          0  \n",
       "8          0  \n",
       "9          0  \n",
       "10         0  \n",
       "11         0  \n",
       "12         0  \n",
       "13         0  \n",
       "14         0  \n",
       "15         0  \n",
       "16         0  \n",
       "17         0  \n",
       "18         0  \n",
       "19         0  \n",
       "20         0  \n",
       "21         0  \n",
       "22         0  \n",
       "23         0  \n",
       "24         0  \n",
       "25         0  \n",
       "26         0  \n",
       "27         0  \n",
       "28         0  \n",
       "29         0  \n",
       "30         0  \n",
       "31         0  \n",
       "32         0  \n",
       "33         0  \n",
       "34         0  \n",
       "35         0  \n",
       "36         0  \n",
       "37         0  \n",
       "38         0  \n",
       "39         0  \n",
       "\n",
       "[40 rows x 8854 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_processed.drop(columns = [\"Author\", \"Chunk\", \"text\", \"POS_string\", \"Pub\"])\n",
    "y_train = df_train_processed['Author']\n",
    "X_val = df_val_processed.drop(columns = [\"Author\", \"Chunk\", \"text\", \"POS_string\", \"Pub\"])\n",
    "y_val = df_val_processed['Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(X_train, y_train, X_val, y_val, model):\n",
    "    # Make predictions on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    # Evaluate the accuracy of the model\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. SVC\n",
    "SVC_model = SVC()\n",
    "predictions_SVC = run_classifier(X_train, y_train, X_val, y_val, SVC_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(y_val == predictions_SVC)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\envs\\fl_project_analysis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "predictions_LR = run_classifier(X_train, y_train, X_val, y_val, clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
