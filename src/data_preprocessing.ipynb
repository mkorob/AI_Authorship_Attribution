{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#0. Preliminaries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import Data\n",
    "#chunked test from data_generation, needs to be redone \n",
    "df = pd.read_csv(\"../data/author_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Introduction</th>\n",
       "      <th>Conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>AutoMix: Automatically Mixing Language Models</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://arxiv.org/pdf/2310.12963.pdf</td>\n",
       "      <td>Large language models (LLMs) are now available...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>Self-refine: Iterative refinement with self-fe...</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://arxiv.org/pdf/2303.17651</td>\n",
       "      <td>Like humans, large language models (LLMs) do n...</td>\n",
       "      <td>Although large language models (LLMs) can gene...</td>\n",
       "      <td>We present SELF-REFINE: a novel approach that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>Learning performance-improving code edits</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://arxiv.org/pdf/2302.07867</td>\n",
       "      <td>The waning of Mooreâs Law has shifted the fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>Language models of code are few-shot commonsen...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://arxiv.org/pdf/2210.07128</td>\n",
       "      <td>We address the general task of structured comm...</td>\n",
       "      <td>The growing capabilities of large pre-trained ...</td>\n",
       "      <td>We present the first work to employ large lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aman Madaan</td>\n",
       "      <td>Text and patterns: For effective chain of thou...</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://arxiv.org/pdf/2209.07686</td>\n",
       "      <td>In the past decade, we witnessed dramatic gain...</td>\n",
       "      <td>The ability to learn a previously unseen task ...</td>\n",
       "      <td>This work evaluates the capacity of COT to ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>BERTRAM: Improved word embeddings have big imp...</td>\n",
       "      <td>2019</td>\n",
       "      <td>https://arxiv.org/pdf/1910.07181.pdf</td>\n",
       "      <td>Pretraining deep language models has led to la...</td>\n",
       "      <td>As word embedding algorithms (e.g. Mikolov et ...</td>\n",
       "      <td>We have introduced BERTRAM, a novel architectu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>True few-shot learning with promptsâa real-w...</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Prompt-based approaches are strong at fewshot ...</td>\n",
       "      <td>With pretrained language models (LMs) getting\\...</td>\n",
       "      <td>In light of recent work casting doubt on the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>Learning semantic representations for novel wo...</td>\n",
       "      <td>2019</td>\n",
       "      <td>https://arxiv.org/pdf/1811.03866.pdf</td>\n",
       "      <td>Word embeddings are a key component of high-pe...</td>\n",
       "      <td>Distributed word representations (or embedding...</td>\n",
       "      <td>We have presented a model that is capable of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>Transition-based generation from abstract mean...</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://arxiv.org/pdf/1707.07591.pdf</td>\n",
       "      <td>This work addresses the task of generating Eng...</td>\n",
       "      <td>Semantic representations of natural language a...</td>\n",
       "      <td>We have devised a novel approach for the chall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Timo Schick</td>\n",
       "      <td>Few-shot learning with language models: Learni...</td>\n",
       "      <td>2022</td>\n",
       "      <td>this is a PHD ziziz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Author                                              Title  Year  \\\n",
       "0   Aman Madaan      AutoMix: Automatically Mixing Language Models  2023   \n",
       "1   Aman Madaan  Self-refine: Iterative refinement with self-fe...  2023   \n",
       "2   Aman Madaan          Learning performance-improving code edits  2023   \n",
       "3   Aman Madaan  Language models of code are few-shot commonsen...  2022   \n",
       "4   Aman Madaan  Text and patterns: For effective chain of thou...  2022   \n",
       "..          ...                                                ...   ...   \n",
       "59  Timo Schick  BERTRAM: Improved word embeddings have big imp...  2019   \n",
       "60  Timo Schick  True few-shot learning with promptsâa real-w...  2022   \n",
       "61  Timo Schick  Learning semantic representations for novel wo...  2019   \n",
       "62  Timo Schick  Transition-based generation from abstract mean...  2017   \n",
       "63  Timo Schick  Few-shot learning with language models: Learni...  2022   \n",
       "\n",
       "                                    Link  \\\n",
       "0   https://arxiv.org/pdf/2310.12963.pdf   \n",
       "1       https://arxiv.org/pdf/2303.17651   \n",
       "2       https://arxiv.org/pdf/2302.07867   \n",
       "3       https://arxiv.org/pdf/2210.07128   \n",
       "4       https://arxiv.org/pdf/2209.07686   \n",
       "..                                   ...   \n",
       "59  https://arxiv.org/pdf/1910.07181.pdf   \n",
       "60                                   NaN   \n",
       "61  https://arxiv.org/pdf/1811.03866.pdf   \n",
       "62  https://arxiv.org/pdf/1707.07591.pdf   \n",
       "63                   this is a PHD ziziz   \n",
       "\n",
       "                                             Abstract  \\\n",
       "0   Large language models (LLMs) are now available...   \n",
       "1   Like humans, large language models (LLMs) do n...   \n",
       "2   The waning of Mooreâs Law has shifted the fo...   \n",
       "3   We address the general task of structured comm...   \n",
       "4   In the past decade, we witnessed dramatic gain...   \n",
       "..                                                ...   \n",
       "59  Pretraining deep language models has led to la...   \n",
       "60  Prompt-based approaches are strong at fewshot ...   \n",
       "61  Word embeddings are a key component of high-pe...   \n",
       "62  This work addresses the task of generating Eng...   \n",
       "63                                                NaN   \n",
       "\n",
       "                                         Introduction  \\\n",
       "0                                                 NaN   \n",
       "1   Although large language models (LLMs) can gene...   \n",
       "2                                                 NaN   \n",
       "3   The growing capabilities of large pre-trained ...   \n",
       "4   The ability to learn a previously unseen task ...   \n",
       "..                                                ...   \n",
       "59  As word embedding algorithms (e.g. Mikolov et ...   \n",
       "60  With pretrained language models (LMs) getting\\...   \n",
       "61  Distributed word representations (or embedding...   \n",
       "62  Semantic representations of natural language a...   \n",
       "63                                                NaN   \n",
       "\n",
       "                                           Conclusion  \n",
       "0                                                 NaN  \n",
       "1   We present SELF-REFINE: a novel approach that ...  \n",
       "2                                                 NaN  \n",
       "3   We present the first work to employ large lang...  \n",
       "4   This work evaluates the capacity of COT to ele...  \n",
       "..                                                ...  \n",
       "59  We have introduced BERTRAM, a novel architectu...  \n",
       "60  In light of recent work casting doubt on the p...  \n",
       "61  We have presented a model that is capable of i...  \n",
       "62  We have devised a novel approach for the chall...  \n",
       "63                                                NaN  \n",
       "\n",
       "[64 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flesch \n",
    "def flesch_readability_scale(text):\n",
    "    r = Readability(text)\n",
    "    f = r.flesch()\n",
    "    return f.score\n",
    "    \n",
    "df['flesch_score'] = df['text'].apply(flesch_readability_scale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
