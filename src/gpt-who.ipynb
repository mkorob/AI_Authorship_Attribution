{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/lluneta/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: readability-lxml in /home/lluneta/anaconda3/lib/python3.9/site-packages (0.8.1)\n",
      "Requirement already satisfied: readability in /home/lluneta/anaconda3/lib/python3.9/site-packages (0.3.1)\n",
      "Requirement already satisfied: spacy in /home/lluneta/anaconda3/lib/python3.9/site-packages (3.7.2)\n",
      "Requirement already satisfied: keras==2.8 in /home/lluneta/anaconda3/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: click in /home/lluneta/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in /home/lluneta/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in /home/lluneta/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: lxml in /home/lluneta/anaconda3/lib/python3.9/site-packages (from readability-lxml) (4.8.0)\n",
      "Requirement already satisfied: chardet in /home/lluneta/anaconda3/lib/python3.9/site-packages (from readability-lxml) (4.0.0)\n",
      "Requirement already satisfied: cssselect in /home/lluneta/anaconda3/lib/python3.9/site-packages (from readability-lxml) (1.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lluneta/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk readability-lxml readability spacy keras==2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lluneta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0. Preliminaries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#from readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lluneta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:41:31.344916: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-08 19:41:31.489316: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-08 19:41:31.489339: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-08 19:41:31.525589: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-08 19:41:32.120319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-08 19:41:32.120436: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-08 19:41:32.120445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/lluneta/anaconda3/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", max_length = 512, truncation = True)\n",
    "\n",
    "distilled_student_sentiment_classifier = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "    tokenizer = tokenizer,\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import Data\n",
    "df = pd.read_csv(\"../data/chunked_author_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val, df_test = train_test_split(df, train_size = 0.8, random_state = 42)\n",
    "df_train, df_val = train_test_split(df_train_val, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(string: str):\n",
    "    \"\"\"\n",
    "    Function that applies regular expressions to a string based on the specified model.\n",
    "    :param string: The input string.\n",
    "    :param model: The model to determine which regular expressions to apply.\n",
    "    :return: The modified string.\n",
    "    \"\"\"\n",
    "    string = re.sub(r'e\\.g\\.', 'eg', string)  # replace e.g. with eg\n",
    "    string = re.sub(r'i\\.e\\.', 'ie', string)  # replace i.e. with ie\n",
    "    string = re.sub(r'-', ' ', string)  # replace - with space\n",
    "    string = re.sub(r'[0-9]', '0', string)  # each digit will be represented as a 0\n",
    "    string = re.sub(r'\\(.*?\\)', '', string) # remove parentheses and the text within\n",
    "    string = re.sub(r'\\[.*?\\]', '', string) # remove brackets and the text within\n",
    "    string = re.sub(r\"https?:\\/\\/[a-zA-Z0-9.\\/]+\", \"@\", string) # remove links\n",
    "    string = re.sub('[^A-Za-z0-9\\s\\.,\\?!:;]+', '', string)# Remove special characters, so math formulas simplified.\n",
    "    string = re.sub(r'\\s\\s+', ' ', string) # remove if there is more than 1 space, inlcuding new line \\n and tab \\t\n",
    "    return string.strip() # Remove extra spaces at the begningining and end.\n",
    "\n",
    "assert regex(\"a\\t\\n b\") == \"a b\"\n",
    "assert regex(\"q123\") == \"q000\"\n",
    "assert regex(\"a (something something) b (sth th)\") == \"a b\"\n",
    "assert regex(\"a [something something] b [sth12 th]\") == \"a b\"\n",
    "assert regex(\"2 + 5 % 2\") == \"0 0 0\"\n",
    "assert regex(\",!;:.?\") == \",!;:.?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flesch \n",
    "def flesch_readability_scale(text):\n",
    "    try:\n",
    "        r = Readability(text)\n",
    "        f = r.flesch()\n",
    "        score_out = f.score\n",
    "    #not possible if less than 100 words\n",
    "    except:\n",
    "        score_out = np.nan\n",
    "    return score_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis (Positive Score)\n",
    "tokenizer_kwargs = {'truncation':True,'max_length':512}\n",
    "def sentiment_analysis_score(text):\n",
    "    results_senti = distilled_student_sentiment_classifier(text, **tokenizer_kwargs)\n",
    "    positive_score = [x['score'] for x in results_senti[0] if x['label'] == 'positive']\n",
    "    score_out = positive_score[0] if len(positive_score) == 1 else np.nan\n",
    "    return score_out                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "assert lexical_diversity('a a b') == 2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def avg_word_per_sentence(text):\n",
    "    return np.mean([len(word_tokenize(sentence)) for sentence in sent_tokenize(text)])\n",
    "\n",
    "assert avg_word_per_sentence('I like muffins. Please buy me two of them.') == 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(text):\n",
    "    return np.mean([len(word) for sentence in sent_tokenize(text) for word in word_tokenize(sentence)])\n",
    "\n",
    "assert avg_word_length('I like giant muffins. Please buy me two of them.') == 3.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation n-grams\n",
    "punct_vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize, use_idf=False, norm='l1', vocabulary=string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectorizer = TfidfVectorizer(ngram_range=(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_vectorizer = CountVectorizer(ngram_range=(1, 3), tokenizer=nltk.word_tokenize, vocabulary=stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bullet_points(text):\n",
    "    bulletpoint_delimiters = re.compile(r'(\\(i\\)|\\(ii\\)|â€¢)')\n",
    "    text = re.sub(bulletpoint_delimiters, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove proper nouns and POS-tag n grams\n",
    "def POS_preprocessing(text):\n",
    "    POS_string = \"\"\n",
    "    cleaned_string = \"\"\n",
    "    list_sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    for sentence in list_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            #first add the text back\n",
    "            string_out = \"Propname\" if token.pos_ == \"PROPN\" else token.text\n",
    "            sep_out = \"\" if token.pos_ == \"PUNCT\" else \" \"\n",
    "            cleaned_string = cleaned_string + sep_out + string_out\n",
    "            #second \n",
    "            #POS_out = \"\" if token.pos_ == \"PUNCT\" else token.pos_\n",
    "            POS_string = POS_string + \" \" + token.pos_\n",
    "    return pd.Series({\n",
    "        'cleaned_string': cleaned_string,\n",
    "        'POS_string': POS_string\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Features to Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, text_col, train = False):\n",
    "    df = df.reset_index(drop = True)\n",
    "    #these should be run first before cleaning punctuation and private words and stuff\n",
    "    #df['flesch_score'] = df[text_col].apply(flesch_readability_scale)\n",
    "    #commented out for now as takes long to run\n",
    "    #df['sent_score'] = df[text_col].apply(sentiment_analysis_score)\n",
    "    #train has to be run first - a catch statement for that\n",
    "    df[text_col] = df[text_col].apply(remove_bullet_points)\n",
    "    #removing double space should be after removing bullet points! leaves a double space sometimes\n",
    "    df[['text', 'POS_string']] = df[text_col].apply(POS_preprocessing)\n",
    "    df['lexical_diversity'] = df[text_col].apply(lexical_diversity)\n",
    "    df['avg_word_per_sentence'] = df[text_col].apply(avg_word_per_sentence)\n",
    "    df['avg_word_length'] = df[text_col].apply(avg_word_length)\n",
    "    try:\n",
    "        punct_features = punct_vectorizer.fit_transform(df[text_col]) if train else punct_vectorizer.transform(df[text_col])\n",
    "        columns = [f'punct_{c}' for c in punct_vectorizer.get_feature_names_out()]\n",
    "        punct_features_df = pd.DataFrame(punct_features.toarray(), columns=columns).reset_index(drop = True)\n",
    "        df = pd.concat([df, punct_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Punctuation N-grams: {e}\")\n",
    "    try:\n",
    "        POS_features = pos_vectorizer.fit_transform(df['POS_string']) if train else pos_vectorizer.transform(df['POS_string'])\n",
    "        columns = [f'pos_{c}' for c in pos_vectorizer.get_feature_names_out()]\n",
    "        POS_features_df = pd.DataFrame(POS_features.toarray(), columns=columns).reset_index(drop = True)\n",
    "        df = pd.concat([df, POS_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating POS N-grams: {e}\")\n",
    "    try:\n",
    "        stopwords_features = stopword_vectorizer.fit_transform(df[text_col]) if train else stopword_vectorizer.transform(df[text_col])\n",
    "        columns = [f'stop_{c}' for c in stopword_vectorizer.get_feature_names_out()]\n",
    "        stopwords_features_df = pd.DataFrame(stopwords_features.toarray(), columns=columns).reset_index(drop = True)\n",
    "        df = pd.concat([df, stopwords_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Stopword N-grams: {e}\")\n",
    "    try:\n",
    "        words_features = word_vectorizer.fit_transform(df[text_col]) if train else word_vectorizer.transform(df[text_col])\n",
    "        columns = [f'word_{c}' for c in word_vectorizer.get_feature_names_out()]\n",
    "        words_features_df = pd.DataFrame(words_features.toarray(), columns=columns).reset_index(drop = True)\n",
    "        df = pd.concat([df, words_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Word N-grams: {e}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['re_text'] = df_train['Chunk'].apply(regex)\n",
    "df_val['re_text'] = df_val['Chunk'].apply(regex)\n",
    "df_test['re_text'] = df_test['Chunk'].apply(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed = preprocess_data(df_train, 're_text', train = True)\n",
    "df_val_processed = preprocess_data(df_val, 're_text', train = False)\n",
    "df_test_processed = preprocess_data(df_test, 're_text', train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_processed.drop(columns = [\"Author\", \"Chunk\", \"text\", \"re_text\", \"POS_string\", \"Pub\"])\n",
    "y_train = df_train_processed['Author']\n",
    "X_val = df_val_processed.drop(columns = [\"Author\", \"Chunk\", \"text\",  \"re_text\", \"POS_string\", \"Pub\"])\n",
    "y_val = df_val_processed['Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(X_train, y_train, X_val, y_val, model):\n",
    "    # Make predictions on the test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    # Evaluate the accuracy of the model\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. SVC\n",
    "SVC_model = SVC()\n",
    "predictions_SVC = run_classifier(X_train, y_train, X_val, y_val, SVC_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(y_val == predictions_SVC)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lluneta/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "predictions_LR = run_classifier(X_train, y_train, X_val, y_val, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
