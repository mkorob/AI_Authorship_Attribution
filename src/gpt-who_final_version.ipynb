{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk py-readability-metrics spacy==3.6.1 keras==2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0. Preliminaries\n",
    "import pandas as pd\n",
    "import nltk, warnings\n",
    "nltk.download('punkt')\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textatistic import Textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", max_length = 512, truncation = True)\n",
    "\n",
    "distilled_student_sentiment_classifier = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "    tokenizer = tokenizer,\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import Data\n",
    "df = pd.read_csv(\"../data/chunked_author_data_UTF8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(string: str):\n",
    "    \"\"\"\n",
    "    Function that applies regular expressions to a string based on the specified model.\n",
    "    :param string: The input string.\n",
    "    :param model: The model to determine which regular expressions to apply.\n",
    "    :return: The modified string.\n",
    "    \"\"\"\n",
    "    string = re.sub(r'e\\.g\\.', 'eg', string)  # replace e.g. with eg\n",
    "    string = re.sub(r'i\\.e\\.', 'ie', string)  # replace i.e. with ie\n",
    "    string= re.sub(r'https?://[a-zA-Z0-9\\n./-]+', \"weblink\", string)\n",
    "    #string = re.sub(r'https?://[a-zA-Z0-9./-]+', \"weblink\", string) # remove links\n",
    "    string = re.sub(r'-', ' ', string)  # replace - with space\n",
    "    string = re.sub(r'[0-9]', '0', string)  # each digit will be represented as a 0\n",
    "    string = re.sub(r'\\(.*?\\)', '', string) # remove parentheses and the text within\n",
    "    string = re.sub(r'\\[.*?\\]', '', string) # remove brackets and the text within\n",
    "    # expression to remove \\n or \\t\n",
    "    string = re.sub(r'[\\n\\t]', \" \", string)\n",
    "    string = re.sub('[^A-Za-z0-9\\s\\.,\\?!:;]+', '', string)# Remove special characters, so math formulas simplified.\n",
    "    string = re.sub(r'\\s\\s+', ' ', string) # remove if there is more than 1 space, inlcuding new line \\n and tab \\t\n",
    "    return string.strip() # Remove extra spaces at the begningining and end.\n",
    "\n",
    "assert regex(\"a\\t\\n b\") == \"a b\"\n",
    "assert regex(\"q123\") == \"q000\"\n",
    "assert regex(\"a (something something) b (sth th)\") == \"a b\"\n",
    "assert regex(\"a [something something] b [sth12 th]\") == \"a b\"\n",
    "assert regex(\"2 + 5 % 2\") == \"0 0 0\"\n",
    "assert regex(\",!;:.?\") == \",!;:.?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis (Positive Score)\n",
    "tokenizer_kwargs = {'truncation':True,'max_length':512}\n",
    "def sentiment_analysis_score(text):\n",
    "    results_senti = distilled_student_sentiment_classifier(text, **tokenizer_kwargs)\n",
    "    positive_score = [x['score'] for x in results_senti[0] if x['label'] == 'positive']\n",
    "    score_out = positive_score[0] if len(positive_score) == 1 else np.nan\n",
    "    return score_out                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "assert lexical_diversity('a a b') == 2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def avg_word_per_sentence(text):\n",
    "    return np.mean([len(word_tokenize(sentence)) for sentence in sent_tokenize(text)])\n",
    "\n",
    "assert avg_word_per_sentence('I like muffins. Please buy me two of them.') == 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(text):\n",
    "    return np.mean([len(word) for sentence in sent_tokenize(text) for word in word_tokenize(sentence)])\n",
    "\n",
    "assert avg_word_length('I like giant muffins. Please buy me two of them.') == 3.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove proper nouns and POS-tag n grams\n",
    "def POS_preprocessing(text):\n",
    "    POS_string = \"\"\n",
    "    cleaned_string = \"\"\n",
    "    list_sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    for sentence in list_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            #first add the text back\n",
    "            string_out = \"Propname\" if token.pos_ == \"PROPN\" else token.text\n",
    "            sep_out = \"\" if token.pos_ == \"PUNCT\" else \" \"\n",
    "            cleaned_string = cleaned_string + sep_out + string_out\n",
    "            #second \n",
    "            #POS_out = \"\" if token.pos_ == \"PUNCT\" else token.pos_\n",
    "            POS_string = POS_string + \" \" + token.pos_\n",
    "    return pd.Series({\n",
    "        'cleaned_string': cleaned_string,\n",
    "        'POS_string': POS_string\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bullet_points(text):\n",
    "    bulletpoint_delimiters = re.compile(r'(\\(i\\)|\\(ii\\)|â€¢)')\n",
    "    text = re.sub(bulletpoint_delimiters, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['flesch_score'] = df['Chunk'].apply(flesch_readability_scale)\n",
    "def updated_Flesch_score(text):\n",
    "    s = Textatistic(text)\n",
    "    return s.flesch_score, s.word_count, s.sent_count, s.sybl_count\n",
    "\n",
    "df[['flesch_score_v2','word_count', 'sent_count', 'sybl_count' ]] = df['Chunk'].apply(updated_Flesch_score).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_score'] = df['Chunk'].apply(sentiment_analysis_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['re_text'] = df['Chunk'].apply(regex)\n",
    "#train has to be run first - a catch statement for that\n",
    "df['re_text'] = df['re_text'].apply(remove_bullet_points)\n",
    "#removing double space should be after removing bullet points! leaves a double space sometimes\n",
    "df[['re_text', 'POS_string']] = df['re_text'].apply(POS_preprocessing)\n",
    "df['lexical_diversity'] = df['re_text'].apply(lexical_diversity)\n",
    "df['avg_word_per_sentence'] = df['re_text'].apply(avg_word_per_sentence)\n",
    "df['avg_word_length'] = df['re_text'].apply(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['re_text'][38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/df_with_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"../data/df_with_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(columns=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stats = df[['Author', 'flesch_score_v2', 'word_count', 'sent_count', 'sybl_count', 'sent_score', 'lexical_diversity', 'avg_word_per_sentence', 'avg_word_length']].groupby('Author').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = df[['Author', 'sybl_count', 'word_count', 'sent_count']].groupby(\"Author\").mean().reset_index()\n",
    "statistics['word_sent_ratio'] = statistics.word_count/statistics.sent_count\n",
    "statistics['syl_word_ratio'] = statistics.sybl_count/statistics.word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_stats.to_clipboard(float_format='%.2f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistics.to_clipboard(float_format='%.2f', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with GPT-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_data = pd.read_csv(\"../data/GPT_data_UTF8_additional_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_data = GPT_data.reset_index(drop = True)\n",
    "df_w_features_GPT = df[df['Author'] == \"GPT-3.5\"].reset_index(drop = True)\n",
    "df_w_features_GPT['Author_org'] = \"\"\n",
    "\n",
    "for df_row in df_w_features_GPT.index:\n",
    "    text = df_w_features_GPT.loc[df_row, 'Chunk'][1:500]\n",
    "    type_text= df_w_features_GPT.loc[df_row, 'Type']\n",
    "    col_type = \"GPT_abstract\" if type_text == \"abstract_chunked\" else (\"GPT_introduction\" if type_text == \"intro_chunked\" else \"GPT_conclusion\")\n",
    "    for texts in GPT_data.index:\n",
    "        if text in GPT_data.loc[texts, col_type]:\n",
    "            df_w_features_GPT.loc[df_row, 'Author_org'] = GPT_data.loc[texts, 'Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_features_GPT.loc[df_w_features_GPT['Author_org'] == \"\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked manually\n",
    "df_w_features_GPT.loc[df_w_features_GPT['Author_org'] == \"\", \"Author_org\"] = \"Hugo Touvron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_GPT_v_authors = df_w_features_GPT[['Author_org', 'flesch_score_v2', 'sent_score', 'lexical_diversity', 'avg_word_per_sentence', 'avg_word_length']].groupby('Author_org').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_GPT_v_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_GPT_v_authors.to_csv(\"test_gpt.csv\", float_format='%.2f', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_equal_groups(group: pd.core.groupby.generic.DataFrameGroupBy, n: int):\n",
    "\n",
    "    return group.sample(min(n, len(group)), random_state=42)\n",
    "\n",
    "share_train = 0.7\n",
    "share_test = 0.3\n",
    "samples_train = int(len(df)*(share_train*(1-share_test)))\n",
    "samples_per_group = int(samples_train/5)\n",
    "\n",
    "df['ID'] = range(0, len(df))\n",
    "df_abstract_intro = df[~df['Type'].str.contains(\"conclusion_chunked\")]\n",
    "df_train= df_abstract_intro.groupby(\"Author\", group_keys=False)\\\n",
    "        .apply(select_equal_groups, samples_per_group)\\\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "df_test_val = df[~df['ID'].isin(df_train['ID'])]\n",
    "df_val = df_test_val[~df_test_val['Type'].str.contains(\"conclusion_chunked\")]\n",
    "df_val, df_test_0  = train_test_split(df_val, train_size = 0.5, stratify = df_val['Author'], random_state = 42)\n",
    "df_test_1 = df_test_val[df_test_val['Type'].str.contains(\"conclusion_chunked\")]\n",
    "#df_test, df_val  = train_test_split(df_test_val, train_size = 0.5, stratify = df_test_val['Author'], random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['Author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_0['Author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1['Author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test_0, df_val])\n",
    "print(df_test['Author'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation n-grams\n",
    "punct_vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize, use_idf=False, norm='l1', vocabulary=string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectorizer = TfidfVectorizer(ngram_range=(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_vectorizer = TfidfVectorizer(ngram_range=(1, 3), tokenizer=nltk.word_tokenize, vocabulary=stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "def clean_punct(test_str):\n",
    "    for ele in test_str:\n",
    "        if ele in punc:\n",
    "            test_str = test_str.replace(ele, \"\")\n",
    "    return test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(example_text):\n",
    "    word_tokens = word_tokenize(example_text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stopwords.words(\"english\")]\n",
    "    return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, text_col, train = False):\n",
    "    df = df.reset_index(drop = True)\n",
    "    #these should be run first before cleaning punctuation and private words and stuff\n",
    "    #df['flesch_score'] = df[text_col].apply(flesch_readability_scale)\n",
    "    #commented out for now as takes long to run\n",
    "    try:\n",
    "        punct_features = punct_vectorizer.fit_transform(df[text_col]) if train else punct_vectorizer.transform(df[text_col])\n",
    "        columns = [f'punct_{c}' for c in punct_vectorizer.get_feature_names_out()]\n",
    "        punct_features_df = pd.DataFrame(punct_features.toarray(), columns=columns).reset_index(drop = True)\n",
    "        df = pd.concat([df, punct_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Punctuation N-grams: {e}\")\n",
    "    try:\n",
    "        POS_features = pos_vectorizer.fit_transform(df['POS_string']) if train else pos_vectorizer.transform(df['POS_string'])\n",
    "        columns = [f'pos_{c}' for c in pos_vectorizer.get_feature_names_out()]\n",
    "        POS_features_df = pd.DataFrame(POS_features.toarray(), columns=columns).reset_index(drop = True)\n",
    "        df = pd.concat([df, POS_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating POS N-grams: {e}\")\n",
    "\n",
    "    #lowercase everything\n",
    "    df[text_col] = df[text_col].apply(str.lower)\n",
    "    df[text_col] = df[text_col].apply(clean_punct)\n",
    "\n",
    "    try:\n",
    "        stopwords_features = stopword_vectorizer.fit_transform(df[text_col]) if train else stopword_vectorizer.transform(df[text_col])\n",
    "        columns = [f'stop_{c}' for c in stopword_vectorizer.get_feature_names_out()]\n",
    "        stopwords_features_df = pd.DataFrame(stopwords_features.toarray(), columns=columns).reset_index(drop = True)\n",
    "        df = pd.concat([df, stopwords_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Stopword N-grams: {e}\")\n",
    "\n",
    "    #remove stopwords here\n",
    "    df[text_col] = df[text_col].apply(remove_stopwords)\n",
    "    try:\n",
    "        words_features = word_vectorizer.fit_transform(df[text_col]) if train else word_vectorizer.transform(df[text_col])\n",
    "        columns = [f'word_{c}' for c in word_vectorizer.get_feature_names_out()]\n",
    "        words_features_df = pd.DataFrame(words_features.toarray(), columns=columns).reset_index(drop = True)\n",
    "        df = pd.concat([df, words_features_df], axis = 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error In Generating Word N-grams: {e}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed = preprocess_data(df_train, 're_text', train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_processed = preprocess_data(df_val, 're_text', train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_0_processed = preprocess_data(df_test_0, 're_text', train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1_processed = preprocess_data(df_test_1, 're_text', train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val_processed.to_pickle(\"./df_val_processed.pkl\")\n",
    "# df_test_0_processed.to_pickle(\"./df_test_0_processed.pkl\")\n",
    "# df_test_1_processed.to_pickle(\"./df_test_1_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = df_train_processed.drop(columns = [\"Author\", \"Chunk\", \"re_text\", \"POS_string\", \"Type\", \"Pub\", \"ID\", \"Unnamed: 0\", \"word_count\", \"sent_count\", \"sybl_count\"])\n",
    "colnames_reg = X_train_df.columns\n",
    "y_train = df_train_processed['Author']\n",
    "\n",
    "X_val_df = df_val_processed.drop(columns = [\"Author\", \"Chunk\", \"re_text\", \"POS_string\", \"Type\", \"Pub\", \"ID\", \"Unnamed: 0\", \"word_count\", \"sent_count\", \"sybl_count\"])\n",
    "y_val = df_val_processed['Author']\n",
    "\n",
    "X_test_0_df = df_test_0_processed.drop(columns = [\"Author\", \"Chunk\", \"re_text\", \"POS_string\", \"Type\", \"Pub\", \"ID\", \"Unnamed: 0\", \"word_count\", \"sent_count\", \"sybl_count\"])\n",
    "y_test_0 = df_test_0_processed['Author']\n",
    "\n",
    "X_test_1_df = df_test_1_processed.drop(columns = [\"Author\", \"Chunk\", \"re_text\", \"POS_string\", \"Type\", \"Pub\", \"ID\", \"Unnamed: 0\", \"word_count\", \"sent_count\", \"sybl_count\"])\n",
    "y_test_1 = df_test_1_processed['Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "X_val_scaled = scaler.transform(X_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_0_scaled = scaler.transform(X_test_0_df)\n",
    "X_test_1_scaled = scaler.transform(X_test_1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exploring the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "def plot_corr(df, size=10):\n",
    "    '''Plot a graphical correlation matrix for a dataframe.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        size: vertical and horizontal size of the plot'''\n",
    "\n",
    "    # Compute the correlation matrix for the received dataframe\n",
    "    # Don't use df.corr() as it is is extremely slow\n",
    "    corr = np.corrcoef(df.T)\n",
    "    \n",
    "    # Plot the correlation matrix\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    cax = ax.matshow(corr, cmap='RdYlGn')\n",
    "    ax.set_xticks([0, len(corr)])\n",
    "    ax.set_yticks([0, len(corr)])\n",
    "    \n",
    "    # Add the colorbar legend\n",
    "    cbar = fig.colorbar(cax, ticks=[-1, 0, 1], aspect=40, shrink=.8)\n",
    "\n",
    "plot_corr(X_train_df, size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "feature_names = colnames_reg.values.tolist()\n",
    "\n",
    "ax = plt.axes()\n",
    "\n",
    "target_features = ['flesch_score_v2', 'sent_score', 'lexical_diversity', 'avg_word_per_sentence', 'avg_word_length']\n",
    "assert [f for f in X_train_df.columns if not f.startswith('punct_') and not f.startswith('word_') and not f.startswith('pos_') and not f.startswith('stop_')] == target_features\n",
    "\n",
    "im = ax.imshow(np.corrcoef(X_train_df[target_features].T), cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "ax.set_xticks(list(range(len(target_features))))\n",
    "ax.set_xticklabels(target_features, rotation=90)\n",
    "ax.set_yticks(list(range(len(target_features))))\n",
    "ax.set_yticklabels(list(target_features))\n",
    "\n",
    "plt.colorbar(im).ax.set_ylabel(\"$r$\", rotation=0)\n",
    "ax.set_title(\"Feature correlation matrix\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_names = colnames_reg.values.tolist()\n",
    "\n",
    "pca = PCA(n_components=min(len(feature_names), len(X_train_df.index)), random_state = 42)\n",
    "pca.fit_transform(X_train_df)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0, 1])\n",
    "var_explained = pca.explained_variance_ratio_.cumsum()\n",
    "var_explained = np.insert(var_explained, 0, 0)\n",
    "\n",
    "components = list(range(len(var_explained)))\n",
    "ax.plot(components, var_explained)\n",
    "ax.set(xlabel='Number of features', ylabel='Variance explained.',\n",
    "       title='PCA analysis')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"pca_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the first 25 components.\n",
    "pca = PCA(n_components=25, random_state = 42)\n",
    "X_train_pca = pca.fit_transform(X_train_df)\n",
    "X_val_pca = pca.transform(X_val_df)\n",
    "X_test_0_pca = pca.transform(X_test_0_df)\n",
    "X_test_1_pca = pca.transform(X_test_1_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Also suppress warnings in other processes during grid search.\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def grid_search(model, params, X_scaled, X_pca, y, cv=KFold(5, shuffle=True, random_state=42)):\n",
    "    X_scaled = np.nan_to_num(X_scaled)\n",
    "    X_pca = np.nan_to_num(X_pca)\n",
    "    y = np.nan_to_num(y)\n",
    "    gridSearch_scaled = GridSearchCV(model, params, cv=cv, verbose=1, n_jobs=128)\n",
    "    gridSearch_scaled.fit(X_scaled, y)\n",
    "    gridSearch_pca = GridSearchCV(model, params, cv=cv, verbose=1, n_jobs=128)\n",
    "    gridSearch_pca.fit(X_pca, y)\n",
    "    return gridSearch_scaled, gridSearch_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_scaled, logi_pca = grid_search(\n",
    "    Pipeline([('logi', LogisticRegression())]),\n",
    "    {'logi__penalty': ['l2', 'l1'],\n",
    "     'logi__solver': ['saga'],\n",
    "     'logi__random_state': [42]},\n",
    "    X_train_scaled,\n",
    "    X_train_pca,\n",
    "    y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_scaled_best_params = pd.DataFrame.from_dict(logi_scaled.cv_results_).sort_values(by=['rank_test_score'])\n",
    "logi_scaled_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_pca_best_params = pd.DataFrame.from_dict(logi_pca.cv_results_).sort_values(by=['rank_test_score'])\n",
    "logi_pca_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scaled, rf_pca = grid_search(\n",
    "    Pipeline([('rf', RandomForestClassifier())]),\n",
    "    {'rf__n_estimators': [10, 100, 500, 1000],\n",
    "     'rf__criterion': ['gini', 'entropy'],\n",
    "     'rf__random_state': [42]},\n",
    "    X_train_scaled,\n",
    "    X_train_pca,\n",
    "    y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scaled_best_params = pd.DataFrame.from_dict(rf_scaled.cv_results_).sort_values(by=['rank_test_score'])\n",
    "rf_scaled_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pca_best_params = pd.DataFrame.from_dict(rf_pca.cv_results_).sort_values(by=['rank_test_score'])\n",
    "rf_pca_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_scaled, svc_pca = grid_search(\n",
    "    Pipeline([('svc', SVC())]),\n",
    "    {'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "     'svc__C': [0.5, 1., 2.],\n",
    "     'svc__random_state': [42]},\n",
    "    X_train_scaled,\n",
    "    X_train_pca,\n",
    "    y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_scaled_best_params = pd.DataFrame.from_dict(svc_scaled.cv_results_).sort_values(by=['rank_test_score'])\n",
    "svc_scaled_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pca_best_params = pd.DataFrame.from_dict(svc_pca.cv_results_).sort_values(by=['rank_test_score'])\n",
    "svc_pca_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the best models from each gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge val and test_0\n",
    "X_test_scaled = np.concatenate((X_val_scaled, X_test_0_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val_scaled.shape)\n",
    "print(X_test_0_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = np.concatenate((X_val_pca, X_test_0_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate((y_val, y_test_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(X_val, X_test, model):\n",
    "    # Make predictions on the test set\n",
    "    #model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_1 = model.predict(X_test)\n",
    "    # Evaluate the accuracy of the model\n",
    "    return y_pred, y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import permutation_importance\n",
    "# perm_importance = permutation_importance(SVC_model, X_test_scaled, y_test)\n",
    "# features_array = np.array(colnames_reg)\n",
    "# sorted_idx = perm_importance.importances_mean.argsort()\n",
    "# sorted_idx_10 = sorted_idx[0:10]\n",
    "# plt.barh(features_array[sorted_idx_10], perm_importance.importances_mean[sorted_idx_10])\n",
    "# plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. SVC\n",
    "params = svc_scaled_best_params.iloc[0].params\n",
    "assert params == {'svc__C': 0.5, 'svc__kernel': 'linear', 'svc__random_state': 42}\n",
    "SVC_model = SVC(kernel = params['svc__kernel'], C = params['svc__C'], random_state = 42)\n",
    "SVC_model.fit(X_train_scaled, y_train)\n",
    "predictions_SVC, predictions_SVC_conclusion = run_classifier(X_test_scaled, X_test_1_scaled, SVC_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a. SVC PCA\n",
    "params = svc_pca_best_params.iloc[0].params\n",
    "assert params == {'svc__C': 1.0, 'svc__kernel': 'linear', 'svc__random_state': 42}\n",
    "SVC_model_pca = SVC(kernel = params['svc__kernel'], C = params['svc__C'], random_state = 42)\n",
    "SVC_model_pca.fit(X_train_pca, y_train)\n",
    "predictions_SVC_pca, predictions_SVC_pca_conclusion = run_classifier(X_test_pca, X_test_1_pca, SVC_model_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. RF\n",
    "params = rf_scaled_best_params.iloc[0].params\n",
    "assert params == {'rf__criterion': 'gini', 'rf__n_estimators': 500, 'rf__random_state': 42}\n",
    "RF_model = RandomForestClassifier(criterion = params['rf__criterion'], n_estimators = params['rf__n_estimators'], random_state = 42)\n",
    "RF_model.fit(X_train_scaled, y_train)\n",
    "predictions_RF, predictions_RF_conclusion = run_classifier(X_test_scaled, X_test_1_scaled, RF_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a. RF PCA\n",
    "params = rf_pca_best_params.iloc[0].params\n",
    "assert params == {'rf__criterion': 'gini', 'rf__n_estimators': 1000, 'rf__random_state': 42}\n",
    "RF_model_pca = RandomForestClassifier(criterion = params['rf__criterion'], n_estimators = params['rf__n_estimators'], random_state = 42)\n",
    "RF_model_pca.fit(X_train_pca, y_train)\n",
    "predictions_RF_pca, predictions_RF_pca_conclusion = run_classifier(X_test_pca, X_test_1_pca, RF_model_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_importances = pd.Series(RF_model.feature_importances_, index=colnames_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_importances.sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. LR\n",
    "#same parameters for all data and final \n",
    "params = logi_scaled_best_params.iloc[0].params\n",
    "assert params == {'logi__penalty': 'l2', 'logi__random_state': 42, 'logi__solver': 'saga'}\n",
    "LR_model = LogisticRegression(penalty = params['logi__penalty'], solver = params['logi__solver'], random_state = 42)\n",
    "LR_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "params = logi_pca_best_params.iloc[0].params\n",
    "assert params == {'logi__penalty': 'l2', 'logi__random_state': 42, 'logi__solver': 'saga'}\n",
    "LR_model_pca = LogisticRegression(penalty = params['logi__penalty'], solver = params['logi__solver'], random_state = 42)\n",
    "\n",
    "LR_model_pca.fit(X_train_pca, y_train)\n",
    "predictions_LR, predictions_LR_conclusion = run_classifier(X_test_scaled, X_test_1_scaled, LR_model)\n",
    "predictions_LR_pca, predictions_LR_pca_conclusion = run_classifier(X_test_pca, X_test_1_pca, LR_model_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final = pd.concat([df_val, df_test_0])\n",
    "df_test_final['prediction_SVC'] = predictions_SVC\n",
    "df_test_final['prediction_SVC_pca'] = predictions_SVC_pca\n",
    "df_test_final['prediction_RF'] = predictions_RF\n",
    "df_test_final['prediction_RF_pca'] = predictions_RF_pca\n",
    "df_test_final['prediction_LR'] = predictions_LR\n",
    "df_test_final['prediction_LR_pca'] = predictions_LR_pca\n",
    "df_test_final.to_csv(\"../data/predictions_test_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1.loc[:, 'prediction_SVC'] = predictions_SVC_conclusion\n",
    "df_test_1.loc[:, 'prediction_SVC_pca'] = predictions_SVC_pca_conclusion\n",
    "df_test_1.loc[:, 'prediction_RF'] = predictions_RF_conclusion\n",
    "df_test_1.loc[:, 'prediction_RF_pca'] = predictions_RF_pca_conclusion\n",
    "df_test_1.loc[:, 'prediction_LR'] = predictions_LR_conclusion\n",
    "df_test_1.loc[:, 'prediction_LR_pca'] = predictions_LR_pca_conclusion\n",
    "df_test_1.to_csv(\"../data/predictions_test_conclusion_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "default_metrics = {\n",
    "    'accuracy': accuracy_score,\n",
    "    'recall': lambda y_t, y_p: recall_score(y_t, y_p, zero_division=\"warn\", average='micro'),\n",
    "    'precision': lambda y_t, y_p: precision_score(y_t, y_p, zero_division=\"warn\", average='micro'),\n",
    "    'f1': lambda y_t, y_p: f1_score(y_t, y_p, zero_division= \"warn\", average ='macro')\n",
    "}\n",
    "\n",
    "mapping_authors = {'Aman Madaan': 1, 'Hugo Touvron': 2, 'Timo Schick': 3, 'Zhiqing Sun': 4,'GPT-3.5': 5}\n",
    "def plot_accuracy(df, column, metrics):\n",
    "    y_true = df['Author'].map(mapping_authors)\n",
    "    y_pred = df[column].map(mapping_authors)\n",
    "\n",
    "    # Remove labels and display_labels not present in y_true\n",
    "    labels = [1, 2, 3, 4, 5]\n",
    "    display_labels = ['Author 1', 'Author 2', 'Author 3', 'Author 4', 'GPT-3.5']\n",
    "\n",
    "    # Plot count confusion matrix\n",
    "    cm_disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, labels=labels, display_labels=display_labels)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {metric_name: metric_func(y_true, y_pred) for metric_name, metric_func in metrics.items()}\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_final, 'prediction_LR', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_final, 'prediction_LR_pca', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_final, 'prediction_SVC', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_final, 'prediction_SVC_pca', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_final, 'prediction_RF', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_final, 'prediction_RF_pca', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_1, 'prediction_SVC_pca', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_1, 'prediction_SVC', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_1, 'prediction_LR', default_metrics)\n",
    "plot_accuracy(df_test_1, 'prediction_LR_pca', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(df_test_1, 'prediction_RF', default_metrics)\n",
    "plot_accuracy(df_test_1, 'prediction_RF_pca', default_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames_reg_list = colnames_reg.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(LR_model, feature_names = colnames_reg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_LR_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final = df_test_final.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final[df_test_final['Author'] == \"GPT-3.5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(RF_model, feature_names = colnames_reg_list, top = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames_reg_list = colnames_reg.to_list()\n",
    "eli5.show_prediction(RF_model, X_test_scaled[100], top =20, feature_names = colnames_reg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "model = AgglomerativeClustering(distance_threshold = 0, n_clusters=None)\n",
    "\n",
    "model = model.fit(X_train_df)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode=\"level\", p=2)\n",
    "plt.xlabel(\"Number of papers in node (or index of paper if the point has no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
